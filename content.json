{"meta":{"title":"leek 自留地","subtitle":"","description":"","author":"leek","url":"https://imlike.cc","root":"/"},"pages":[{"title":"","date":"2024-03-14T14:55:29.473Z","updated":"2024-03-14T14:55:29.473Z","comments":true,"path":"README.en.html","permalink":"https://imlike.cc/README.en.html","excerpt":"","text":"back_blog_markdownDescription备份md博客文件 Software ArchitectureSoftware architecture description Installation xxxx xxxx xxxx Instructions xxxx xxxx xxxx Contribution Fork the repository Create Feat_xxx branch Commit your code Create Pull Request"},{"title":"","date":"2024-03-14T14:55:29.473Z","updated":"2024-03-14T14:55:29.473Z","comments":true,"path":"README.html","permalink":"https://imlike.cc/README.html","excerpt":"","text":"back_blog_markdown介绍备份md博客文件 软件架构软件架构说明 安装教程 xxxx xxxx xxxx 使用说明 xxxx xxxx xxxx 参与贡献 Fork 本仓库 新建 Feat_xxx 分支 提交代码 新建 Pull Request"},{"title":"about","date":"2020-04-18T00:42:34.000Z","updated":"2024-03-14T14:55:29.487Z","comments":true,"path":"about/index.html","permalink":"https://imlike.cc/about/index.html","excerpt":"","text":"记录平时零散的知识,分享工作的"},{"title":"标签云","date":"2020-04-18T00:44:49.000Z","updated":"2024-03-14T14:55:29.584Z","comments":false,"path":"tags/index.html","permalink":"https://imlike.cc/tags/index.html","excerpt":"","text":""},{"title":"","date":"2024-03-14T14:55:29.582Z","updated":"2024-03-14T14:55:29.582Z","comments":true,"path":"page.html","permalink":"https://imlike.cc/page.html","excerpt":"","text":"Page Sandbox body { padding: 0 0.5em; } .box { margin-top: 1em; } #txtURL { width: 100%; height: 2em; text-indent: 0.5em; padding: 0.25em 0; } #btnGo { width: 100%; font-size: 1.5em; } #list a { margin: 1em; } #ShowUpBox { all: unset; width: auto; height: auto; position: absolute; z-index: 10240; color: black; display: inline-block; line-height: 0; vertical-align: baseline; box-sizing: content-box; } #showUpBody { min-width: 20px; max-width: 750px; min-height: 20px; max-height: 500px; display: block; border:solid 2px rgb(144,144,144); border-radius:1px; background:rgba(252, 252, 252, 1); } #popupWrapper { all: unset; margin: 3px 2px 3.8px 2px; display:block; line-height: 0; font-size:0; } #transPanel { line-height: normal; width: auto; font-size: 16px; overflow: auto; display: none; } #popupWrapper > a { all: unset; margin: 0px 2px; } #popupWrapper img { all: unset; margin: 0px; height: 24px; width: 24px; border-radius: 0px; padding: 0px; display: inline-block; transition-duration: 0.1s; -moz-transition-duration: 0.1s; -webkit-transition-duration: 0.1s; } #popupWrapper img:hover { margin: -1px -1px; height: 26px; width: 26px; } #popupTip { display: inline-block; clear: both; height: 9px; width: 9px; } .tipUp { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAkAAAAJCAYAAADgkQYQAAAAZElEQVQYlXXMIQ4CMBBE0UWhwHDw+ddCAhILEgWy4WNo0tAyyZidly1AwNba1CSvJOf6h/oduC/RAN7qfkJ9BC7VM6LhQ1O3E+pN8lAPNeYHHGsV4PkFN3WzREmuwEndLUFVfQC3Xa8Jl+92RAAAAABJRU5ErkJggg==) 0px 0px no-repeat transparent; margin-top: -2px; margin-bottom: 0px; } .tipDown { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAkAAAAJCAYAAADgkQYQAAAAXElEQVQYlYXMsQ5AQBREUSoVjQ+f+S0lSi2livJlr4ZEZDcmud3JVJIW2yPQVqXZPmwjaQXqEuJJ0vCLbrgDfRZFxBsH0GTRCybbcxF9HhPQZdEHbkUUEUg6JU0Xm2KvCU6v27kAAAAASUVORK5CYII=) 0px 0px no-repeat transparent; margin-top: 0px; margin-bottom: -2px; } #ShowUpBox a { text-decoration: none; display: inline-block; } Go 切换线路: 演示服务-香港节点演示服务-新加坡节点当前站点 const PAGE_CONF_SET = 110 const PAGE_CONF_GET = 111 const SW_CONF_RETURN = 112 const SW_CONF_CHANGE = 113 const PAGE_READY_CHECK = 200 const SW_READY = 201 const sw = navigator.serviceWorker sw.addEventListener('message', onSwMsg) sendMsgToSw(PAGE_READY_CHECK) btnGo.onclick = function() { const text = txtURL.value.trim() if (text) { const url = './-----' + text open(url, '_blank', 'noopener,noreferrer') } } txtURL.onkeypress = function(e) { if (e.keyCode === 13) { btnGo.onclick() } } txtURL.setSelectionRange(0, txtURL.value.length) function onSwMsg(e) { const [cmd, msg] = e.data switch (cmd) { case SW_CONF_RETURN: conf = msg showConf() break case SW_CONF_CHANGE: conf = msg updateSelected() break case SW_READY: console.log('sw ready') showIcons() sendMsgToSw(PAGE_CONF_GET) break } } function onSwFail(err) { txtURL.value = err } selNode.onchange = function() { const item = this.options[this.selectedIndex] const node = item.value conf.node_default = node sendMsgToSw(PAGE_CONF_SET, conf) } function sendMsgToSw(cmd, val) { const ctl = sw.controller if (!ctl) { console.log('ctl is null') return } ctl.postMessage([cmd, val]) } const SITE_LIST = [ ['google', ''], ['youtube', ''], ['twitter', 'twitter.com/google'], ['flickr', ''], ['quora', 'www.quora.com/topic/JavaScript-programming-language'], ['twitch', 'www.twitch.tv/'], ['reddit', ''], ['wiki', 'zh.wikipedia.org/'], ['gist', 'gist.github.com/'], ['facebook', 'facebook.com/Google/'], ['blogger', ''], ] function showIcons() { list.innerHTML = SITE_LIST.map(v => { let [id, url] = v url = url || `www.${id}.com/` return `\\ \\ ` }).join('') } function addNodeItem(id, text) { const optEl = document.createElement('option') optEl.id = '--' + id optEl.text = text optEl.value = id selNode.appendChild(optEl) } function updateSelected() { const id = conf.node_default const item = document.getElementById('--' + id) if (item) { item.selected = true } else { console.warn('unknown node:', id) } } function showConf() { for (const [id, node] of Object.entries(conf.node_map)) { if (!node.hidden) { addNodeItem(id, node.label) } } updateSelected() } PANTOOL-PLUGIN {display: inline;cursor: pointer;}PANTOOL-PLUGIN-TIPS {white-space: nowrap;}.PANTOOL-PLUGIN-VALIDATE-NOTPASS {text-decoration: line-through;color: #ccc;}PANTOOL-PLUGIN-TIPS::before {background-position: center;background-size: 100% 100%;background-repeat: no-repeat;box-sizing: border-box;width: 1em;height: 1em;margin: 0 1px .15em 1px;vertical-align: middle;display: inline-block;}.PANTOOL-PLUGIN-VALIDATE-NOTPASS>PANTOOL-PLUGIN-TIPS::before {content: '';background-image: url(data:image/png;base64,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)}.PANTOOL-PLUGIN-VALIDATE-PASS>PANTOOL-PLUGIN-TIPS::before {content: '';background-image: url(data:image/png;base64,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)}.PANTOOL-PLUGIN-VALIDATE-UNCERTAINTY>PANTOOL-PLUGIN-TIPS::before {content: '';background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAAZGElEQVR4Xu1deZgcVbX/neqeSTLT1ZCQmWqCgiAIGMQFUZE9LA9E8CmrCsqSdAcQlcWNxxIXEJUHAg+SrgREQEECPhc2UcK+PVAEggiyiPiFqk5IoKtnJkt3/d53axaSycx03erqnu5A/ZPvy5y9Tt+699yzCDbAhyduNNlb076DJIztSWwHke0BThGgg0DHW/9KWqlPsChAL4HewX8BWQ7yWRh4Fr7/rJlY/YzMfXPFhmYu2RAU8mZaM2DIHgD3AGQHCLrqoxcLABYDvBc07jdt5+768Gkc1ZZ0AG92Zi/43BciuwPYo3HmGoETcS+E90PkLnOec8+4yhKBeUs4AOfAKP3b2gsGjgDkUAimRtC1/ijEUghuBnFjappzr8yBX3+mtXFoagfondX9yYrIcRT5rACb1KZqg7FJFyI3J+Bf15EvPNxg7qHZNaUDFHPWZwB8WyCfCK1JEwOSfChB/LBzvntLs4nZNA7Aw5HomdL9RcL4FoD3N5uh4pGHi0V4QefrhRtkISrx0KyNSlM4QE82c1AFuEgE76tNndbAJvgPqWC2ucBdNN4Sj6sDrDyha+s1SWMeIPuMtyHGgz/JW9vFP3Vifuk/xoO/4jkuDsDjp5qlZOJ7AE6GSNt4Kd8UfMk1hFxqlsvflauWeY2WqaEOEBznlnTPAowfNO1RrtFvYIgfCwTONPPuVaKCkw16GuYAxaw5Fej4rYh8skG6tSYb4l6i57C07S1rhAINcYDizK7dxTAWQsRqhFJj8SCxHIAnYImCN4LvILExISkApgimjLeMIF9LCA9tRPyg7g5QzGa+KYIfNdSoxL8geFzA5yDyvFHxn580sbxYLlteDCMHs5M38oy2HQwY28DnthRsC+BjgGwWBj8uGKF/Rsou/Hdc9EaiUzcHUOf60mTrOogcVU8FAtoq6gbcTQOL2n0ummgXXqwHz5XZ7veuNmSG+JgBwQxAuuvBZ22aJH5u2s5x9doX1MUBeDjaS5Ot30Nk/3oZiOSbhuAmwr8mlV96f70MNJr8BKSU69qdNL4M4HARMeumK3iLudw9VBZiddw8YneAYPmU9lsFsmvcwvbT4x8hssCc59xYH/r6VHksJpYmZg4Bma1bTIO4N1UuHxz3UTFWB/COy3ShjXdB5AP6Zhxz61YG8Csale+n5y57Ll7a8VIrZa0PEPg2gCMhkoiXOp5IlXv3lSuLaiMbyxObA/RlJ2++Bu33iMiWsUg2SIS0k1h93iR7xb9ipVtnYn2zM+8p+/gmBCfGyUqFkZOVNTM6Fiz/dxx0Y3EAnpCe4iUnPSaQreIQqn+lx0KjXP5651XLlsRGcxwI9Rw/dZrflrwMwOfiYk/yRbPS97E4VoKaHYCnYlKp13oYkA/GoSCBfwpx/IaQbrW2PUrZ7v19MfICvCcOOwF4IkVjN7GX9NZCryYHCI56UzJ/ArBXLUIM4RJzU6ud0+RqrIyFXpMRCTaLEzIXA5gdj2hclFru7l/L1XJNDuBlMzdBcGityhBYkQCO7sw7t9VKqxXw+xNe5GcCTK5ZXmKhaTtHRKUT2QG8bGYOBOdGZfzWr553S9L/QuqKpU7NtFqIQOmkrgzLiYUQ7Faz2ORZpu2eF4VOJAfwZmX2hPBuiETCX+vl/zRlu6c1OogTxVD1wOkPJll5QGbVRp9+wufuHfMLD+nS0X6B3myrm5S/1ZakSZ9ELm27C3QF3hDhvaz1VQguBsSIrB/ppip979c9GWg5gPJYL2c9KJBdoguqqm/8z6bswp2RaWyAiCotzgduhKAjsnrk3Snb3UdnRdVyAC9rnQeRM2sQ0BVgv5TtPh2ZxgaM2DO768O+b9xe07U58V3TduaENVNoB+jJTv2Ij8TjUb/7JN8QkV3NvPO3sMK9HeG8WZnpFD4c+XKJJAzsaM5zF4exXygHCFK5XrOeDOruojzESkPKu3fmlz0eBf3thlOc3b2r+MYiCNqj6E7g8XTe2TkMbigH6N+kyCVhCK4HQ1ZEeGAqX/hjJPy3KVJPzjrYB34TeWNI/yTTLsytZr6qDlCa2W35CXlB+lOmtB+Bf3oqX7hIG/EdBBRzmW8JcEEUU5D00Na7Zfpy7/Wx8Ks6gJe1ro+a1UPw9+m8e0gUBeLG6ZtpbVlJcFefxo4inAZiGkWmgXyXiCQBvkmiKCLq34IBPklDnmhj5YkJ+aUv6Oys45S9mMvcLsABUWiSvC5tu8dEdoAg4GMgUskziVfMPk6Xa92eKMLXisOTulKlihwNGPsM9A2oIX2LDonrE4Z/bee8pU/UKpsOvkqwKUn7M1HzERPwPzlWcumYK4CXs56OtPEj1wiw03gc93qz1sd9wSxSPl/TmXrUt8TFFM5Ozys8qPMia4HtyU39qM/EQ1GKaEg8lradj43Gf1QHKOW69yOMiMEa/0QzX5hXi9K6uMWTzU1Q7viZQA7WxdWGV0ctyLxUH7/RqBXOy2a+AoHKK9B/iBmjXa+P6gBezroLkBm63Ag+mM67tV9waDAu5boOIBPX1K81zMjCqNyFdlT2b1Rtn5fL3BuxI8ofzLwz4j5iRAdQwQgYqheO5kNWmKhMb2TeXilnXUCIKikfl4fA60bFn5FaUHiq3gJ4MzfZHkbymSjBOCF3HOmTPLID5DILARymrRD9y0278BVtvIgIXjbzPQjOjogeG5rqMmaAe6TyhSdjIzoKIS9r5SGS1efD6828+4XheOs5QJDMSLysy0Dl6ZuVvq10b6N0+QzCl3LdZxDGTzTxXwX4F0CeBH0HwqUCeROA5VPeJcCHINgTkIwmXZXEuDi13P1QLdk5YXiqzGu286UocZlEYvXmHVcsf3VtPus5gJftPhtiqNJtzYdnm3n3B5pIkcC9XNceQEJ9D8M9xN9p+DPD7tx7c927VCAng/iCznLbiFIupbCXy3wXwDnhlF8LaoTEkfUcoJi1XoqQ2t2XWrVymlz9RlBsWc+nP4kio/YnodrIkLzYXOF+I8ovUx2/KkxcLyJbh9GJZI9Zrmwad/HGcN4qJwMUlSY/IYxcgzAkX0jb7jajrgA9szI7+wb+T4foAOwVZt45OQKeNoqX7c5BVFeRME/tq1KQAOPjkbA/CvH9L6XmF64NI10tMKWctYCQE3RpGCzv1Gkv+8sg3jorgJezLgXkFC2iJJOGbDVpnvNPLbwIwAMpVEvCfKNVUWXado6NwGY9lOCenokho41Fk8Ttadv5VBx8x6JRzG6ynUjbs7p81IqYtt3TRnaAbKage5ZuZLw/6DOQSNxXTWm1Kzd7MS3OII2Xs34FSIjsW5ZTebe9EXcHke4JSNe03aFN7tAKENa46xmfOMK0HXVsrPtTzFoXicipVRmR/2Part5KVoWoN6v7UBjGTVV5AzDWlDdrREWTl7OOBOSGMDKtDZOocJeOBe4j6v+GHMDLZs6FIHQqkUIONj3T3I1lDlTxZt2fsBtUsrJ72l76QJwCDWy8VB+Cqo9RKX+0c8GyP1cFrBFAFZp47VZBN3uI5H+lbff8dR0gQphRwCtTeXdmjXqEQmcWbSXJVK+PJ9ek4HaKjTWhCIcECvYfWasS5lho+Dy4UV1Bi7mMKjDR3OvwLjPv7jvkAEFDhylWD6DuxTUeH3uZ853w53EN0sNBe2dOeVcl0b5OEGNkclxs5t2Yy9P7ORWzVklEOqupIT73S813Vclc3Z9SztqHED1exMrUCieljsbBJ6A0y9qXhmilbKkkz7Tt1l7aFNJEQVKqJEMsq295d0jSocBCr0Dqu1rxP9iIuwEl+ECqflE7MijYW7W373eAnHU+Id8JZYlBIPI3pu1+VgunBuDgxg+J20OQuMnMO4eHgNMC0Yk+kj1djWrzNrAy3SIiB2kpBHzPzDvnBg5QzFoq0PFxLQLk10zbvVQLp4WBw17CqNr9tO2GihzGZY5Stvt0inGhFj3iAdN2dg8cwMtaZd12JqNdL2oJ0SLAQYRU+GiYDSDJq9K2qx2hq8UUOoGqtfj0mXmnQ/pO2HSLcpJaUbxGf/9rMU6tuKqvsdeWfCpsYwf6/Ex6vvu7Wvnq4EfdB6h4hapO1d5FErgjnXcO1BGyFWGD7ic91u8gEhyZQjx/S+WdHRoRBRwui5ez/qTfoayyp+hdrgywJS81bfdrIQzSsiA8ZUraW932R4GMmlA5XDmD+HSn7dw6Hkp7uczlAE7S4U2fJ0gxm/mxCL6hg6javJt55wpNnJYBH+jwdTsE22kIPc/MO7F2BNPgrU5yXyPkpzo4Av5QlXv/GhCt45yA+6by7l06zFoFVh33iMSvdfofkHzUhLt73NFHHZtpHJPXIssbpZizHtKt909UVr87rj51OkrWG9bLZU4CeIlWRJR4IIVVnxZ7hUotG7dHVT6VE/KSpgD3qRXgr7ot3lJ5xxiPjY6mcqHBeYzVWeqQq3UTYUn/x+aKwplRso1CCxcSkKdgQml1Rre72p+lmLX+ETblKZCF6DVtp2o8PKTc4w4WdDiVCXcKgpbwoR7V1cygf1SzdTnxclZFp5qYwHPiZa0lENk0lOb9HlAw8+64D34IL+/okD0zp+7kG8k7dMbXBDMAUTms0172WhwyxEmjmM28rjXwgvi3WgFU546NwgoyHqHOsLLpwKnLpQoSi8LrTh/EBakV7jnNsOSPpKuXzbwCweZh7aBWMvFyGd0BRU+YeecjYZk0I5wKnVZo3CPoHx9f9VEzgYEjm719rZe1FkNkelV9BgFUEa+XzazSakVCPmXabix9gUMLGiOganhBw3g6dO4j+UzCX3NAK5x6tKu5idUqEKT13SD4UjrvvjfGd9JQUuGTO4P9zqJUh/tpuRh9DRUyIrNiNvNPEWwRFl3VNaoVQOu70cqbwKDOHsnHwhgo+D6uxrbmzxy1/LfEU8xllukFsPCK2gM8E7bKJrBCCx8DvVzm5rB9+w3wkM68+/uWePMDQmp/zsHFKhL4qM6Fh+Jl5p2qvYWazXD9N3uZN0Ltd4h7TduJpwV+gwwxUDLn67Aj+LCKBGo3gmhU3ruOMtVgdVK6xvNWr5oeo/09Sl6HGsClAkG/gMh6deNjCjKQUBhV2PHA83KWmuiVr8abxPOm7WzXaqHuKC19CFyj9gDapcYUZtPz3PnVjNlMf/dy1vcBOauqTMRc03a07tWr0mwAQP9FFlROgMbDs6WU6z6aMLSqWQlcmM47ujkEGoLFD6pRVnaeabvVHSV+EWui6OWsnwKimaTDo6R3pvWJSkIe1uFO8ndp2/2MDs54w3pZax5EctXkIPmdtO1G6s5ZjXY9/17MZm4TgVaanioVF9VeTcqdmqPK6Zh5V+MCqZ6qh6MdpE7DqNpCTkA7Zbu/DEe1eaCKWWuFiGysI1FquTOhvy4gZ70ZOi4+wKENlfc1qj2ajlJvR1gvl1HdUlQ8J/wzUCY+WBewCCJ7h8cGWnEjqKNfK8FG2QAONrIYcAD90nBg5LZjrWS4DUVWL0Jbv8G9zqAD7A3BIj2DtN4+QE+/1oGO8v0fbCIdOEB/PplV0kqGBJAQfLxjnhOlqVTrWLfJJQ2mi9DQa4YxvDx8YCP4gEB21dK3wZ1BtWR7mwCHLVpd1xxcZObdfdT/DV3qRCkRV1em5qZOd6NaxLxN3mloNTkHSW9JkNKnmaTrn2vmC0Ez0LUcIHT9/ToC0vc/l55f+N/QUr8DGJsFvNmZI0D8SpvgWnc5Qw4QNByaYLm68QCCt6TzbtUAi7aQ7yBUtUCU6B+J5Wnb2WSQ+LqNIrPWfIjoNX0i2Sb+tu8Ehaq+r1gBguAPqZJA9XIzhu3b1kEuZrt2E0ncryspiWvTtvMlXbzxgu/JTt2UkjjCB/YRiuo8drckKze30gRzL2ddB8gXdW04/OS2frPonPWiQLbSI8yywcrmzVgsMVyP0qzuY3yRy4f31iNYEvIM0y5UzRnQs0380KqaqSztL+tUAQXHffLltO2u825HaBcfJSoYkL/MzLtfjV/d+CiGaYZh+JVPdc5fGqYZVXyCaVLyspkrINAvRQ/TLj7qwAilQ7LCrSYtcLWHTWjqHxk8XAY0C6nl7rRmrf5ZOWvq+9YYyeeiGCHJVVtMsleoNvNDz8gjY7LWDRA5Up9JfXr06cuxPobOHKS2cmWbiVcufSEOvnHT8LKW9sVdvwwhR8YoUB1jDVdwPJokhTFy/yxeCdW8SXz/gNT8wh/C0G0kjE7D6hHkmj7S5PZRjxCRWpH3c301tdzZWhaiel/fBlovqAROJENNLzdQ3rnZJp2rOE1pgqU2ftrzjMaK1YwxN1BzLs/aL7MO7dpr9ZWBfsh9YXbOQaZMkzlw2JS2kexkSOUjo428HTOIUMxlHhPgo1GMbwAHdead26Lg1gsn1IzBJuyAVsx2/6eIETXcfp+Zd/YczaZjO0DICR0jEQ+6aFT87VMLCqF67Nfrpa+zMM2BUXoto2LnI85EVNO2zRXusc10AuidvclmZT/5rO5MgEG9q33OqoYRIxWOvGX1+1J5Z69mK7Lwct3nAIY65WxJ0gHkPqCyIO4hE7U69UAH0Ed0S/cG+arCj3Te+fJYclR1gNJJXRmWEy9Gn8TNS8y8+/VajfF2xI921z9w6CM9tPVumb7ce70mB1DIxWzmmyL4UfSX0Php4tFlbQ7MUtY6lSIXRZVGyNNStntxNfyqK4AiMJB4oL5DEdug0xcf/9GoKRrVlG72v/fM6jrQN4xbwpxYRtSF+HtqmjNd5qBqtXAoB1BMokwVWfdkyB4D2GWkCdbN/kIaKd9A6/cHAUyKyjfh+7t2zC88FAY/tAMoYrWcRQe+TA582dec7+gVMYTRZAOA8WZbO4BQ5frdkdXRzNPUcoCB7OFHdTuLDlsJ3kiSB4X10MiGaDFEVaNZNnBn1ONeoC75VGqau5NOjqaWAygeqhHBmoT/dE2CAqsM4tDxaq3ebL4x0Oj5N7rDoNf5YYGlNq6ePvy2r5qu2g4QnApmWYeIIb+tRnzsv9MX8LhUvnBNbXRaG7uUyxxHcEHkDd+A+lFnFUZygGA/EGXQ9Ejviv7lqQmF0+UyrGrtV6knfXC5025dApGsHub60AQuSued06PQiewAPByJ0mTrNojsH4XxOjjkMxA5YqTrypppNyEBldBJQM0kCN2gelQ1yDtT09wDwxz5RqIR2QGCPccpmOCtytwvgp1rtjOxEvBPM+3C3JppNTEBL2t9FSI/ruV7P6iealxtTnBn1LJ61uQAgRNkJ29UQvuDWj1qx3hBBB8USHZDWw2CXz15lfZ8xtF/+k+mEv5ucsXSUi3+XrMDBPuB4zJdaIca6/ruWoQZwiVV33s7Vek9S64sLo+F5jgR4QnpKaXEpPMhmFXrRm/olw++ZJb7do7DNrE4gBIsOB4mfTVla5u4bK3G0wOwE+XKhZ1XLVsSF91G0Ok5fuq0SjJxBoCsfu3e6BKqNnZtWLWf7nFvNIqxOUDwOVDenuxQk6w/HKuRyTWE/BKJ8g/Tc5dFyoiNVZ4xiBVPnLot/OSZQn4eIm2x8iX/mkqsnCFz31wRF91YHSBwgpO6UqWKoSaR7ReXkMPo/EHgX9e5qnCTXK02juP/qCNdz4Tuwwg5Vn94Y0j51W6/D5+Ta121Ksb2xO4AgRPMQbK0xLoWIkfFJukwQgSLQtxI378mvWCpdjlbrXIFvXlnZ/akz2MgOEy3qFaLP7EwNc05KupRbyxedXGAQYbFXOZbQp6nO5hayziBx9FVLW5ILGpPYNHEua7u+LRQLFee0LX1mqQxA8QMCPau6dImFEeWhXJmynZ+Ego8AlBdHUDJExScwrgJIg0bNEXiFRH8GeSzYsgLBv3nJrWXn5HLlhfD2EiNje1bnZzui7EtfW5N4AMi2AmQzcLgxwJDLkkID+vIF7SaeOryrrsDKIH6j4m8vm7fRw2tVX28EklA1RNpBYVJIUxCUqoTvtbULQ2+WqDknUTvF9O2p9nAU4tLANwQBxjYFxilJZmzITwnrvOwvrrNjqEmk3FOyi78oFGJtA1zgEHTBxkvvnE1RHZs9tfRUPmIvxhGZeZoBRz1kqXhDjC0GryWmU3yPN3+tvUyxHjRDWYT0f+vlF2Y16hf/dq6josDDAqg9gZsx4UCtEx3kdgchSRFfo5kzxnVUrdj4zkCoXF1gLU/CxUm1HfvU/VUtlloq2LNhPjnNHq5H0n/pnCAIUdQY92YUHGD2nMMmuVtryUHgTsSKJ/dTJXHTeUAg7YKhlgYOF+3g3kTvvMBkXhXooKzOha4jzSbjE3pAEOOkOvepUI5loKj6hpqrcNbGQhV35AAruqw3UfrwCIWkk3tAIMaqtp+b+Pug8QwjgZwUBzZNLFYb30iq0DcQvq/MN8o3NpsPQaafg8Q5qWo20avYuwjkAMIHCDAe8Lg1QtGtV4TyB0k7zAl8Sexl/TWi1c96LbECjCW4qprVtlIHOhDlDOoRgiRS6pCGriPxD0C3tHGyh0T5y97PiReU4K1vAMMt2p/4Ur53QZlSxqyBRhM096cgo0F6CDQ8da/klb4wfca6CXQO/Qv8QaAf0Hwivh8xRe+3FZJvjrpytdeaco3GVGo/we2riCAXTLCBAAAAABJRU5ErkJggg==)}.PANTOOL-PLUGIN-VALIDATE-LOCK>PANTOOL-PLUGIN-TIPS::before{content: '';background-image: url(data:image/png;base64,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)} @namespace url(http://www.w3.org/1999/xhtml); /*! * font-adobe-family-2.0.3.scss * https://n2o.io/go?page=custom.css * * Copyright 2019 Francis Z. * https://n2o.io/license * * Date: 2019-09-15T19:00+06:00 */ /* i really want this to be global */ html{-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;text-rendering:optimizeLegibility}html:lang(de) body{quotes:\"„\" \"“\"}html:lang(es) body,html:lang(no) body{quotes:\"«\" \"»\"}html:lang(fr) body{quotes:\"« \" \" »\"}html:lang(zh-CN) body,html:lang(zh-SG) body{font-family:\"Source Han Sans SC\",\"Noto Sans CJK SC\",\"HanHei SC\",-apple-system,BlinkMacSystemFont,system-ui,-apple-system,BlinkMacSystemFont,system-ui,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\",\"Noto Color Emoji\"}html:lang(zh-TW) body{font-family:\"Source Han Sans TC\",\"Noto Sans CJK TC\",\"HanHei TC\",-apple-system,BlinkMacSystemFont,system-ui,-apple-system,BlinkMacSystemFont,system-ui,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\",\"Noto Color Emoji\";quotes:\"「\" \"」\"}html:lang(zh-HK) body,html:lang(zh-MO) body{font-family:\"Source Han Sans HC\",\"Source Han Sans TC\",\"Noto Sans CJK TC\",\"HanHei TC\",-apple-system,BlinkMacSystemFont,system-ui,-apple-system,BlinkMacSystemFont,system-ui,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\",\"Noto Color Emoji\";quotes:\"「\" \"」\"}html:lang(ja) body{font-family:\"Source Han Sans\",\"Noto Sans CJK JP\",\"Hiragino Kaku Gothic Pro\",-apple-system,BlinkMacSystemFont,system-ui,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\",\"Noto Color Emoji\";quotes:\"「\" \"」\"}html:lang(ko) body{font-family:\"Source Han Sans K\",\"Noto Sans CJK KR\",\"Apple Gothic\",-apple-system,BlinkMacSystemFont,system-ui,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\",\"Noto Color Emoji\"}body{font-family:\"Source Sans Pro\",\"Source Han Sans SC\",\"Noto Sans CJK SC\",\"HanHei SC\",-apple-system,BlinkMacSystemFont,system-ui,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\",\"Noto Color Emoji\";quotes:\"“\" \"”\"}button,input,keygen,select,textarea{font-family:inherit}button:disabled,input:disabled,keygen:disabled,select:disabled,textarea:disabled{cursor:not-allowed}@font-face{font-family:\"Georgia\";src:local(\"Source Han Serif SC\")}@font-face{font-family:\"Times\";src:local(\"Source Han Serif SC\")}@font-face{font-family:\"Time New Roman\";src:local(\"Source Han Serif SC\")}@font-face{font-family:\"Arial\";src:local(\"Source Han Sans SC\")}@font-face{font-family:\"Helvetica\";src:local(\"Source Han Sans SC\")}@font-face{font-family:\"Helvetica Neue\";src:local(\"Source Han Sans SC\")}@font-face{font-family:\"Segoe UI\";src:local(\"Source Han Sans SC\")}@font-face{font-family:\"Tahoma\";src:local(\"Source Han Sans SC\")}@font-face{font-family:\"Trebuchet\";src:local(\"Source Han Sans SC\")}@font-face{font-family:\"Trebuchet MS\";src:local(\"Source Han Sans SC\")}@font-face{font-family:\"Verdana\";src:local(\"Source Han Sans SC\")}@font-face{font-family:\"Consolas\";src:local(\"Source Code Pro\")}@font-face{font-family:\"Courier\";src:local(\"Source Code Pro\")}@font-face{font-family:\"Courier New\";src:local(\"Source Code Pro\")}@font-face{font-family:\"SimSun\";src:local(\"Source Han Sans SC\")}@font-face{font-family:\"SimSun-ExtB\";src:local(\"Source Han Sans SC\")}@font-face{font-family:\"宋体\";src:local(\"Source Han Sans SC\")}@font-face{font-family:\"NSimSun\";src:local(\"Source Han Sans SC\")}@font-face{font-family:\"新宋体\";src:local(\"Source Han Sans SC\")}@font-face{font-family:\"SimHei\";src:local(\"Source Han Sans SC\")}@font-face{font-family:\"黑体\";src:local(\"Source Han Sans SC\")}@font-face{font-family:\"Microsoft YaHei\";src:local(\"Source Han Sans SC\")}@font-face{font-family:\"微软雅黑\";src:local(\"Source Han Sans SC\")}@font-face{font-family:\"STHeiti SC\";src:local(\"Source Han Sans SC\")}@font-face{font-family:\"PingFang SC\";src:local(\"Source Han Sans SC\")}@font-face{font-family:\"苹方-简\";src:local(\"Source Han Sans SC\")}@font-face{font-family:\"MingLiU\";src:local(\"Source Han Sans TC\")}@font-face{font-family:\"MingLiU-ExtB\";src:local(\"Source Han Sans TC\")}@font-face{font-family:\"PMingLiU\";src:local(\"Source Han Sans TC\")}@font-face{font-family:\"PMingLiU-ExtB\";src:local(\"Source Han Sans TC\")}@font-face{font-family:\"Microsoft JhengHei\";src:local(\"Source Han Sans TC\")}@font-face{font-family:\"微軟正黑體\";src:local(\"Source Han Sans TC\")}@font-face{font-family:\"STHeiti TC\";src:local(\"Source Han Sans TC\")}@font-face{font-family:\"PingFang TC\";src:local(\"Source Han Sans TC\")}@font-face{font-family:\"蘋方-繁\";src:local(\"Source Han Sans TC\")}@font-face{font-family:\"MingLiU_HKSCS\";src:local(\"Source Han Sans HC\")}@font-face{font-family:\"MingLiU_HKSCS-ExtB\";src:local(\"Source Han Sans HC\")}@font-face{font-family:\"PingFang HK\";src:local(\"Source Han Sans HC\")}@font-face{font-family:\"蘋方-港\";src:local(\"Source Han Sans HC\")}@font-face{font-family:\"MS Gothic\";src:local(\"Source Han Sans\")}@font-face{font-family:\"MS PGothic\";src:local(\"Source Han Sans\")}@font-face{font-family:\"MS UI Gothic\";src:local(\"Source Han Sans\")}@font-face{font-family:\"Yu Gothic\";src:local(\"Source Han Sans\")}@font-face{font-family:\"Yu Gothic UI\";src:local(\"Source Han Sans\")}@font-face{font-family:\"Malgun Gothic\";src:local(\"Source Han Sans K\")}.adsbygoogle{display:none !important}"}],"posts":[{"title":"计算机符号和原码和反码以及位运算","slug":"计算机符号和原码和反码以及位运算","date":"2025-04-10T03:02:51.000Z","updated":"2025-04-10T03:03:00.700Z","comments":true,"path":"9bee7e85.html","link":"","permalink":"https://imlike.cc/9bee7e85.html","excerpt":"","text":"为什么1bit对应 8 byteASCII 编码字符串都是这样 二进制 十进制 十六进制 缩写 Unicode 表示法 脱出字符 表示法 名称／意义 0000 0000 0 00 NUL ␀ ^@ 空字符（Null） 0000 0001 1 01 SOH ␁ ^A 标题开始 0000 0010 2 02 STX ␂ ^B 本文开始 0000 0011 3 03 ETX ␃ ^C 本文结束 0000 0100 4 04 EOT ␄ ^D 传输结束 0000 0101 5 05 ENQ ␅ ^E 请求 0000 0110 6 06 ACK ␆ ^F 确认回应 0000 0111 7 07 BEL ␇ ^G 响铃 0000 1000 8 08 BS ␈ ^H 退格 0000 1001 9 09 HT ␉ ^I 水平定位符号 0000 1010 10 0A LF ␊ ^J 换行键 0000 1011 11 0B VT ␋ ^K 垂直定位符号 0000 1100 12 0C FF ␌ ^L 换页键 0000 1101 13 0D CR ␍ ^M CR (字符) 0000 1110 14 0E SO ␎ ^N 取消变换（Shift out） 0000 1111 15 0F SI ␏ ^O 启用变换（Shift in） 0001 0000 16 10 DLE ␐ ^P 跳出数据通讯 0001 0001 17 11 DC1 ␑ ^Q 设备控制一（XON 激活软件速度控制） 0001 0010 18 12 DC2 ␒ ^R 设备控制二 0001 0011 19 13 DC3 ␓ ^S 设备控制三（XOFF 停用软件速度控制） 0001 0100 20 14 DC4 ␔ ^T 设备控制四 0001 0101 21 15 NAK ␕ ^U 确认失败回应 0001 0110 22 16 SYN ␖ ^V 同步用暂停 0001 0111 23 17 ETB ␗ ^W 区块传输结束 0001 1000 24 18 CAN ␘ ^X 取消 0001 1001 25 19 EM ␙ ^Y 连线介质中断 0001 1010 26 1A SUB ␚ ^Z 替换 0001 1011 27 1B ESC ␛ ^[ 退出键 0001 1100 28 1C FS ␜ ^\\ 文件分割符 0001 1101 29 1D GS ␝ ^] 组群分隔符 0001 1110 30 1E RS ␞ ^^ 记录分隔符 0001 1111 31 1F US ␟ ^_ 单元分隔符 0111 1111 127 7F DEL ␡ ^? Delete字符 二进制 十进制 十六进制 图形 0010 0000 32 20 (space) 0010 0001 33 21 ! 0010 0010 34 22 “ 0010 0011 35 23 # 0010 0100 36 24 $ 0010 0101 37 25 % 0010 0110 38 26 &amp; 0010 0111 39 27 ‘ 0010 1000 40 28 ( 0010 1001 41 29 ) 0010 1010 42 2A * 0010 1011 43 2B + 0010 1100 44 2C , 0010 1101 45 2D - 0010 1110 46 2E . 0010 1111 47 2F / 0011 0000 48 30 0 0011 0001 49 31 1 0011 0010 50 32 2 0011 0011 51 33 3 0011 0100 52 34 4 0011 0101 53 35 5 0011 0110 54 36 6 0011 0111 55 37 7 0011 1000 56 38 8 0011 1001 57 39 9 0011 1010 58 3A : 0011 1011 59 3B ; 0011 1100 60 3C &lt; 0011 1101 61 3D = 0011 1110 62 3E &gt; 0011 1111 63 3F ? 0100 0000 64 40 @ 0100 0001 65 41 A 0100 0010 66 42 B 0100 0011 67 43 C 0100 0100 68 44 D 0100 0101 69 45 E 0100 0110 70 46 F 0100 0111 71 47 G 0100 1000 72 48 H 0100 1001 73 49 I 0100 1010 74 4A J 0100 1011 75 4B K 0100 1100 76 4C L 0100 1101 77 4D M 0100 1110 78 4E N 0100 1111 79 4F O 0101 0000 80 50 P 0101 0001 81 51 Q 0101 0010 82 52 R 0101 0011 83 53 S 0101 0100 84 54 T 0101 0101 85 55 U 0101 0110 86 56 V 0101 0111 87 57 W 0101 1000 88 58 X 0101 1001 89 59 Y 0101 1010 90 5A Z 0101 1011 91 5B [ 0101 1100 92 5C [](https://zh.wikipedia.org/wiki/反斜线) 0101 1101 93 5D []](https://zh.wikipedia.org/wiki/括號) 0101 1110 94 5E ^ 0101 1111 95 5F _ 0110 0000 96 60 [`](https://zh.wikipedia.org/wiki/重音符) 0110 0001 97 61 a 0110 0010 98 62 b 0110 0011 99 63 c 0110 0100 100 64 d 0110 0101 101 65 e 0110 0110 102 66 f 0110 0111 103 67 g 0110 1000 104 68 h 0110 1001 105 69 i 0110 1010 106 6A j 0110 1011 107 6B k 0110 1100 108 6C l 0110 1101 109 6D m 0110 1110 110 6E n 0110 1111 111 6F o 0111 0000 112 70 p 0111 0001 113 71 q 0111 0010 114 72 r 0111 0011 115 73 s 0111 0100 116 74 t 0111 0101 117 75 u 0111 0110 118 76 v 0111 0111 119 77 w 0111 1000 120 78 x 0111 1001 121 79 y 0111 1010 122 7A z 0111 1011 123 7B { 0111 1100 124 7C | 0111 1101 125 7D } 0111 1110 126 7E ~ 计算机数字表示在计算机中，有符号数和无符号数的区别主要体现在以下三个方面： 符号位的定义与数值范围有符号数的最高位（二进制最左侧位）被定义为符号位：• 0 表示正数，其余位表示数值大小（如 01111111 表示 +127）• 1 表示负数，其余位存储补码形式的数值（如 10101111 表示 -81）而无符号数所有位都用于表示数值大小，因此 正数范围比同位数有符号数大一倍（例如 8 位无符号数范围是 0-255，而有符号数范围是 -128~127） 补码转换规则负数在计算机中以补码形式存储。补码的计算步骤为：• 原码 → 符号位不变，其余位取反 → 末位 +1例如 -81 的补码转换过程：原码：10100001 → 取反后：11011110 → 加 1 → 11011111（即 0xAF）补码的作用是统一加减法运算逻辑，简化硬件设计 类型转换时的截断与符号扩展当长位数据（如 Int）转换为短位类型（如 Byte）时：• 截断规则：直接保留低位字节（如 0xFFFFFFAF → 0xAF）• 符号位识别：目标类型若为有符号，则最高位决定正负（如 0xAF 的最高位为 1，解析为负数）在 Scala 中，Byte 默认是有符号类型，因此 0xAF 会被解析为 -81。若需无符号处理，需通过 &amp; 0xFF 强制转换为正数 具体代码分析 1println((-81.toInt).toByte) // 输出 -81 • -81.toInt 的补码是 0xFFFFFFAF（32位）• .toByte 截断后保留 0xAF（二进制 10101111）• 有符号 Byte 将最高位 1 识别为负号，剩余位 0101111 的补码还原为 81，最终结果为 -81 无符号处理对比若强制转换为无符号数（例如使用 (pd(0) &amp; 0xFF)）： 12val byteValue: Byte = 0xAF.toByte // -81（有符号）val unsignedValue = byteValue &amp; 0xFF // 175（无符号） 此时 0xAF 会被视为 175，因为 &amp; 0xFF 将符号位清零，并扩展为 32 位整数 总结有符号/无符号的区分本质是程序对二进制位的解释规则不同，而计算机存储的二进制数据本身没有符号属性。这一特性在涉及跨数据类型转换（如 Int → Byte）或位运算时需要特别注意。","categories":[],"tags":[]},{"title":"idea-mac激活","slug":"idea-mac激活","date":"2025-04-09T11:07:44.000Z","updated":"2025-04-09T11:08:23.472Z","comments":true,"path":"63e78f83.html","link":"","permalink":"https://imlike.cc/63e78f83.html","excerpt":"","text":"https://yby6.com/archives/ja-netfilte Could not set environment: 150: Operation not permitted while System Integrity Protection is engaged","categories":[],"tags":[]},{"title":"anthropic(Claude) promote 推荐","slug":"promote-recommend","date":"2025-03-13T08:19:45.000Z","updated":"2025-04-09T07:47:42.466Z","comments":true,"path":"12713d51.html","link":"","permalink":"https://imlike.cc/12713d51.html","excerpt":"","text":"https://docs.anthropic.com/zh-CN/prompt-library 这个网站汇集了很多的promote 非常实用","categories":[],"tags":[]},{"title":"学习的步骤讨论","slug":"学习的步骤讨论","date":"2025-03-07T02:51:01.000Z","updated":"2025-04-09T07:47:42.467Z","comments":true,"path":"9de51b87.html","link":"","permalink":"https://imlike.cc/9de51b87.html","excerpt":"","text":"跟着ChatGPT4o学全栈，我看到未来“学习”的模样 在讨论之前，我们先了解下学习过程 学习是一个复杂而多阶段的过程，每个阶段都有其独特的特点和目标。我们可以将学习过程分为以下几个阶段： 1.接触（Exposure）：学习的起点。首先，我们需要注意新的信息、知识或技能，并对其产生兴趣。 2.收集（Gathering）：系统地广泛地获取更多相关的信息和资源，以便更全面地理解新知识或技能。 3.理解（Understanding）：指将所收集的信息进行消化、分析和综合，以便形成对新知识或技能的全面认识。 4.记忆（Memorization）：涉及将理解的知识或技能存储在长期记忆中，以便将来可以方便地检索和应用。知识的理解程度对记忆的影响非常大。 5.应用（Application）：指将所学的知识或技能在实际情境中加以运用，以解决问题或完成任务。 6.反馈（Feedback）：根据应用的结果和他人的评价，反思和改进学习方法和策略。 失败的学习随处可见 众所周知，学习过程中最耗费精力的是收集资料。寻找资料源、筛选高质量资料、系统化组织资料，这些任务不仅耗时费力，而且结果往往不尽如人意。 在理解知识的过程中，我们需要大量的阅读、训练和思考。由于缺乏系统指导，学习者常常感到困惑和迷茫。 长周期的学习需要及时反馈。比如，学习一门新的编程语言，从基本语法开始逐步掌握，可能需要一到两个月才能开始实际操作项目。这么长的学习周期可能让我们失去目标感，增加中途放弃的风险，最终可能感到挫败。 在学习过程中，缺乏足够的动力、信息过载、容易陷入单调重复、缺乏实践应用场景、反馈时间过长或无法得到反馈等都是障碍。这些问题都可能导致人们放弃学习。因此，我们常常看到失败的学习案例，而成功有效的学习案例则寥寥无几。 大模型让学习更容易成功 通过上述演示，我们可以看到大型模型在各个学习环节中都能提供巨大的帮助，甚至在某些方面产生颠覆性的影响，这些影响能够提高学习成功的几率。 在资源收集方面，传统的模式（主要是搜索）需要在许多来源中收集。 在此基础上还需要进一步做筛选和判断，这将消耗大量的精力。 然而，大模型在资源收集上有两个优势： 高度整合的知识：大模型拥有大量的高质量知识，尤其在编程方面； 按需定制：大模型可以根据个人水平实现定制化的教学课程； 当然，GPT在这里也有一些限制： 知识更新不及时：通用的大模型不会使用未经验证的知识进行训练，虽然如此，但目前大模型的知识水平已经足够应对大部分的学习需求了。 知识缺乏系统化：大型模型就像一只具有优秀记忆力的鹦鹉，它本身并未形成系统化的知识，需要借助prompt（像本文中学习前端的例子）或agent来实现结构化和系统化。 我们都知道，理解是形成长期记忆的关键，而通过知识的类比和迁移来理解新知识是一种有效的方法。 然而，建立类比和迁移这座连接新旧知识的桥梁并不容易。这不仅需要学习者理解新的概念，还需要将这些概念与已有的知识结构相结合，以便在新的情境中使用。如果对新概念理解不足，或者旧知识基础薄弱，就无法建立这种联系。 大型模型在我们理解知识的过程中带来了新的启示，它具备非常强的知识类比能力。大模型以Java工程师的角度解释了如何理解React组件生命周期。它通过类比Java类的生命周期来讲解React组件，这个方法非常有效。我相信只有既熟悉前端又熟悉后端的人才能如此解释，而GPT的回答如此自然。 同样，当我们尝试用类比（如将state与Java的成员变量进行比较）来理解新知识时，它会给我们提供更丰富的建议，更深入的解释，以及更多样的案例。 反馈对于学习的重要性，毋庸再言。在这一阶段，大模型同样展现出强大的能力。 实时反馈：大型模型能够提供即时反馈，让学习者在学习过程中随时了解自己的表现。例如，当我们完成一段代码或者一个功能时，大型模型可以立即进行审查并提供改进建议。这并不止是效率的提升，更重要的是，它可能会引发更深层的变革。 学习是反人性的，人类也并不擅长学习，重要的原因是学习过程充满未知，可预期性差。尽管人类天生好奇，但遗憾的是这种好奇心并不能维持很长时间。聪明的教育者会运用各种技巧来提升学习的可预期性（更明确的奖赏），以尽可能地延长好奇心的持续时间。 提升反馈效率不仅表面上会提升效率，同时也会提升学习的可预期性，可能大模型会使学习这件事情不那么反人性。 多样化的案例：大模型能够提供多种多样的案例，甚至我们可以用大模型创造案例； 面向未来 既然模型“全知全能”，我们还要不要学习知识？ 答案是肯定的：我们仍然需要学习知识。 多年前就有人提出了一个类似的问题：“在现代社会，为什么我们需要记忆呢？我们可以在几秒钟内通过互联网查找到任何需要的事实性信息。”这并不是大数据模型时代特有的问题。我认为，我们仍然需要学习，并应该利用大数据模型来加速学习进程。 首先，知识是思考的基础，没有知识就无法谈论分析能力或批判性思维等高级能力。许多人认为思考过程类似于计算器的功能，计算器具有一系列的函数（如加法、乘法等），这些函数可以应用于任何数据。它的数据和函数是完全分开的。因此，一旦学习了新的函数，就可以处理所有的数字。 然而，人类的思考方式并非如此，至少对大多数人来说，功能（模型）和数据是无法完全分离的。例如，我们能够批判性地思考欧洲地缘政治如何导致第二次世界大战，并不意味着我们也能批判性地思 考中东当前的局势。再比如，我们知道在使用金字塔模型进行分类时，应确保每个分类之间既无重叠也无遗漏。然而，即使我们在某个领域成功实现了这一点，但在仅仅理解模型的情况下，并不能保证在另一个领域也能做到无重无漏。 其次，通过知识的学习，可以提高我们的记忆力。我们把人脑分成工作记忆和长期记忆，工作记忆的空间是有限的，它的上限决定了我们的推理能力。这一点其实也很容易理解，当我们涉足到非常专业的领域时，会遇到大量的专有名词。例如，在营销运营领域，我们会遇到像“招选搭投”、“盘货池”这样的专有名词。这些名词都包含大量的背景知识。如果我们不清楚这些知识，我们根本是无法讨论问题（判断、推理、分析、决策等）的。 让我们想象一下，在不熟悉背景知识的情况下，我们如何探讨“直接将盘货池暴露给搭建页面是否合理？”时，首先，我要解释一下什么是盘货池，它是根据特定的商品指标规则圈选的商品集合。然后，我会介绍什么是搭建，在这一通输出之后，估计大家的脑子已经炸了。可是，我们还没有开始讨论真正有意义的内容。 发散一下：这种情况是不是跟当今的大模型所遇到的困境非常像？ 实际上，我们可以通过学习知识来解决这个问题。我们可以将知识压缩成一小块一小块，然后存入长期记忆中。当我们需要时，可以随时调取。例如，当我提到”盘货池”这个概念，我们的脑海中立刻会想到”按指标圈选的商品集合”。而且，我们还可以联想到更多，例如它的存储形式，它在数据流向中的位置等等。我们能联想到的内容完全取决于我们的大脑中存储了多少信息。这样，我们就大大节省了工作记忆空间，从而变相地突破了工作记忆空间的限制。另外，学习知识还能更容易地触发长期记忆，也就是说，学得越多，记忆力越好。由于篇幅原因，我就不再详细说明这一点了。 在大模型时代，要怎么学习？ 前面，花了大量的篇幅在论证，我们要不要学习，接下来，我们阐述一下，应该怎么学习。 与机器赛跑，仅仅靠技能型学习是不够的模型最擅长的是什么？显然，大型模型在重复型技能学习方面表现出色。无论是对某种模式的判断，还是执行某些套路化的动作，模型都能高效而准确地完成。在这些领域，人类难以与大模型竞争。 令人沮丧的是，这些技能我们并不能抛弃，并且他们非常重要。就像篮球运动员必须具备扎实的基本功，才能完成复杂的动作一样，基本技能是我们掌握高级能力的基础。没有这些基本技能，任何高层次的应用和创新都无从谈起。 然而，在这个时代，仅仅依赖这些基本技能是不够的。面对“与机器赛跑”的挑战，我们需要培养更高等级的能力，如分析、判断、决策和创新。这些能力能够让我们在与大模型的竞争中脱颖而出。 未来也不全是坏事儿，值得高兴的是，在大模型的加持下，技能型学习过程将显著加快，从而提高这一阶段的学习效率。通过高度整合的、定制化的知识，提供多样化的案例，并及时给予学习者反馈，学习者可以更快地掌握基本技能。这不仅节省了大量的时间和精力，同时也使我们能够将更多的资源投入到更高层次的分析、判断和创新中，促进整体学习效果的提升。 要从技能型学习到专家型学习的转变专家型学习这个概念不太容易定义，首先解释一下什么是专家能力，以帮助大家理解什么是专家型学习。 不要误解，此专家非彼专家。 首先，专家具备概念框架和分析能力。 当专家面对一个问题时，他们首先会运用概念框架来锚定问题的类型。例如，在处理一个需要在多个服务器之间共享数据的系统时，专家首先会锚定这是一个分布式问题，然后再进一步锚定是分布式当中数据一致性的问题，其后才会着手解决数据如何在不同节点之间实现一致。 其次，专家能够清晰地识别模式、关系和差异。 这一点或许有些抽象。专家不仅能应用概念模型，还能在复杂情境中看清楚各方关系和角色。比如，在设计复杂的软件系统时，专家能识别核心服务和辅助服务，理解它们如何协同工作，并优化接口和数据流。他们能预见扩展系统时的挑战，并提前设计解决方案。 再次，专家具备扎实的基础知识，并能够顺畅地提取和运用这些知识。 扎实的基础是专家型学习的基石。假设一个人连基本的概念都无法理解，就无法指望他能深入分析和解决问题。 那么，什么是专家型学习，或者换句话说，如何通过学习达到专家能力？ 扎实的基础知识是专家型学习的第一步，这一步是可以通过系统的学习和反复练习来实现的。然而，令人沮丧的是，后续的高级能力——如分析能力、系统化思维、批判性思维、决策和创新等——并不能通过简单的学习获得。 这些高级能力需要大量实践来培养，我相信阿里的“借事修人”的理念，但是这首先需要我们“躬身入局”。具体来说，主动承担有挑战性的任务，尝试跨领域学习和应用，迫使自己运用高级能力；持续反思和总结经验，改进策略，不断输出，寻求反馈；这样才有可能将高级能力内化为自己的技能。 软实力从来没有变得像今天那么重要在现今社会，许多技术性的任务可能被机器取代，但判断和决策仍然需要人来完成。因此，在可预见的未来，软实力（沟通能力、同理心、逆商、团队精神、领导力等）的重要性将变得更为突出，比以往任何时候都更为关键。","categories":[],"tags":[]},{"title":"mac软件汇集(转载)","slug":"mac软件汇集","date":"2025-03-07T02:46:10.000Z","updated":"2025-04-09T07:47:42.467Z","comments":true,"path":"27d3a647.html","link":"","permalink":"https://imlike.cc/27d3a647.html","excerpt":"","text":"转载地址 https://weekly.howie6879.com/soft/mac.html mac-soft 记录我在使用 macOS 过程中使用&amp;看到的软件项目，本文会持续更新，同时也开源在 GitHub，欢迎 Star ❤️ 基础工具 Alfred ：本地搜索及应用快速启动，替代品： Raycast：目前免费状态，这是其开源插件仓库 Cerebro：开源替代品 BackgroundMusic：音频管理工具，可以管控各个 APP 的音量（开源） CatchMouse：多显示器切换软件，根据快捷键快速切换到目标显示器（免费） Karabiner-Elements：键盘键位修改神器，特别针对有多款键盘的人（开源） KeyboardHolder：一款补足 macOS 自带有一个自动切换到文稿到输入法功能遗憾的软件（免费） mac-mouse-fix：轻量级鼠标控制功能定制软件（开源） Mounty：一款 NTFS 格式读写工具 Mos：一个平滑你的鼠标滚动效果或单独设置滚动方向的小工具，让你的滚轮爽如触控板（开源） peazip：解压缩工具（开源） Rectangle：多窗口管理工具（开源） RDM：轻松将 Mac Retina 显示器设置为更高的分辨率（开源） 系统管理 AltTab：AltTab 将 Windows 的 Alt-Tab 窗口切换器带到 MacOS（开源） Bartender ：方便的状态栏管理工具，开源替代品有： Dozer hidden balenaEtcher：镜像制作工具（开源） cakebrew： 如果使用 Homebrew 的话，可以使用这款来进行管理（开源） eul：系统状态监控工具（开源） flux：一款比较好的免费屏幕护眼软件 KeepingYouAwake：根据自己的需求设置 Mac 多少分钟/多少小时后进入睡眠 Maccy：剪贴板管理（开源） MonitorControl：控制外部显示器的亮度和音量（开源） OnlySwitch：系统功能快速开关工具（开源） Parallels Desktop：能让您在 Mac 上同时使用 Windows 等系统 SourceCodeSyntaxHighlight：增强文件预览（开源） Tunnelblick：OpenVPN 客户端（开源） 腾讯柠檬：腾讯出品的系统清理工具（开源） 编程开发 Anaconda：数据科学常用 Python 环境管理程序 DevToysMac：程序员常用工具集（开源） Docker：用容器化的形式使用一些常用软件，推荐使用 portainer 管理（开源） MacVim：文本编辑器（开源） Offset Explorer ：个人免费的跨平台 1Kafka 界面管理工具， kafka-ui：Web 自部署，简洁好用、速度快、颜值高（开源） VSCode：微软出品的免费跨平台代码编辑器（开源） 终端： electerm：一款基于 Electron 开发的 SSH/SFTP 跨平台客户端、（开源） Iterm2：免费好用的终端工具，一般搭配 Brew 和 ZSH 使用 tabby：一个可高度配置的终端模拟器和 SSH 或串口跨平台客户端（开源） WindTerm：基于 C 语言开发的的专业级替代品（开源） 数据库： 通用： beekeeper-studio：跨平台的 SQL 编辑器和数据库管理器（开源） DBeaver：为开发人员、SQL 程序员、数据库管理员和分析人员提供的免费多平台数据库工具，有 JDBC 驱动即可支持使用（开源） dbgate：支持 Mysql、Postgresql、SQL Server、Mongodb、Redis、SQLite 等数据库的管理软件，同时也满足在 Windows、Linux、Mac Web 下运行，还提供插件体系（开源） Elasticsearch： ElasticView：简单好用的 ES 可视化客户端，支持连接6，7，8版本（开源） appbaseio/mirage：用于简化 ES 查询语句编写的项目，你可以用基于块的交互形式来组成 Elasticsearch 查询语句（开源） elasticvue：基于 Vue 的 Elasticsearch 管理客户端，支持浏览器插件和 Docker 自建（开源） MongoDB： mongo-express：可自建的 MongoDB 数据库管理软件，针对线上数据库需要共享的需求，可以统一部署在一台服务器给所有员工用，然后设置只读模式，非常实用（开源） Robo 3T：MongoDB 数据库管理软件（开源） MySQL： Sequel-Ace：原生的 MySQL/MariaDB 数据库管理软件（开源） Redis： AnotherRedisDesktopManager：跨平台的 Redis 桌面管理客户端，性能出众（开源） RedisFish：一款便捷、跨平台、专注于数据的 Redis GUI 客户端（开源） PostgreSQL： pgadmin4：跨平台的 PostgreSQL 数据库管理软件（开源） SQLite： sqlitebrowser：DB4S 是一个高质量、可视化、开源的工具，用于创建、设计和编辑与 SQLite 兼容的数据库（开源） API 请求调试： hoppscotch：基于 Web 提供服务（开源） insomnia：跨平台 （开源） Postman：比较老牌的 API 接口功能测试工具 showdoc：一个非常适合 IT 团队的在线 API 文档、技术文档工具（开源） Thunder Client：如果使用VSCode，推荐使用这款插件 💡 浏览娱乐 aliyundrive：阿里云盘 BilibiliVideoDownload：跨平台下载 bilibili 视频桌面端软件（开源） Chrome：浏览器 iina：现代化的视频播放器（开源） Irvue：基于 Unsplash 的免费壁纸切换工具 itsycal：一款简单优雅的菜单栏日历（开源） licecap：轻量小巧的录屏工具（开源） 沟通：微信 | 钉钉 | TG 恒星播放器：一款纯粹专注的播放器 网易云音乐：推荐一款开源替代品 YesPlayMusic 阅读写作 Bob：翻译工具（开源，但是正式版买断收费） fluent-reader：RSS 阅读器（开源） Joplin：跨平台笔记管理工具，有不错的社区支持（开源） Pngyu](https://joplinapp.org/)：免费的PNG图片压缩工具，同样好用的还有： TinyPNG4Mac（开源） ImageOptim（开源） Typora：比较经典的所见即所得 Markdown 编辑器（提供免费版） uPic：原生的图床管理工具（开源） Xnip：挺不错的截图软","categories":[],"tags":[]},{"title":"如何找出Java进程占用CPU高的元凶(转发)","slug":"如何找出Java进程占用CPU高的元凶","date":"2025-01-18T07:04:12.000Z","updated":"2025-01-18T07:15:56.842Z","comments":true,"path":"129d01fc.html","link":"","permalink":"https://imlike.cc/129d01fc.html","excerpt":"","text":"转发: 如何找出Java进程占用CPU高的元凶 开发的时候登录OSS控制台预发机器，发现Java进程CPU占用率其高，按道理预发机器只是内部开发及产品会使用，根本不可能占用这么大的资源，第一反应就是，进程在某个地方死循环了。 二、调查 猜测归猜测，还是要用事实说话。 第一步：top -Hp 15057 查看下进程中的线程资源占用情况 由上图可见，CPU时间片主要是被15393 这个线程给吃掉了， 所以目标锁定在 15393。 执行 printf “%x\\n” 15393，计算出线程ID对应的16进制。 第二步：执行 sudo -u www jstack 15057，获取当前进程中各线程的调用栈 对比之前计算出来的二进制id：3c21，找到占用CPU资源最高的线程堆栈，如上图所示。 看样子是在LoguUtil的255行出了问题，赶紧定位到这一行看看： 这里写了一个死循环，并在255行调用了阻塞队列的poll（），很明显，方法调用错了，poll（）在队列会空的时候会直接返回null，并不会阻塞等待，所以造成进程一直在这儿死循环。 那么为什么测试的时候没有发现呢？因为只要有日志，代码很难走到255行，之所以能发现这个问题，是因为我登录机器这段时间是在深夜，刚好没有日志。 三、验证 将代码修改提交部署后，在登录机器看看CPU占用情况： 此时Java进程的CPU占用已经恢复正常了。 四、总结 平时开发需求，除了验证基本的功能完整性之外，一定要注意对性能的影响，及时发现并解决问题。","categories":[],"tags":[]},{"title":"mysql连接问题","slug":"mysql连接问题","date":"2025-01-18T06:52:45.000Z","updated":"2025-01-18T07:15:56.841Z","comments":true,"path":"22281c22.html","link":"","permalink":"https://imlike.cc/22281c22.html","excerpt":"","text":"在使用 bitnami 的镜像来搭建 mysql 的集群环境,但是突然驱动出现了问题,进入容器对于的那台机器查询对于的命令 /opt/bitnami/mysql/bin/mysqld –defaults-file=/opt/bitnami/mysql/conf/my.cnf –basedir=/opt/bitnami/mysql –datadir=/ bitnami/mysql/data –socket=/opt/bitnami/mysql/tmp/mysql.sock –bind-address=127.0.0.1 –skip-slave-start –upgrade=force 研究了一通是因为 –upgrade=force ,目前由于我们是bitnami集群时[bitnami/mysql] installed in the k8s cluster keeps prompting that the upgrade fails after restarting · Issue #44854 · bitnami/containers 同时在容器中常见的命令地址如下 12&#x2F;opt&#x2F;bitnami&#x2F;scripts&#x2F;mysql&#x2F;entrypoint.sh &#x2F;opt&#x2F;bitnami&#x2F;scripts&#x2F;mysql&#x2F;run.sh","categories":[],"tags":[]},{"title":"java线程池注意事项","slug":"java线程池注意事项","date":"2025-01-18T06:46:39.000Z","updated":"2025-01-18T07:15:56.841Z","comments":true,"path":"8a4449c5.html","link":"","permalink":"https://imlike.cc/8a4449c5.html","excerpt":"","text":"java 线程池使用的人很多,通常情况下为了不注册当前的线程的执行,同时提升执行的效率 但是需要有时候使用不当回出现意想不到的问题,不如下面这种问题 Java 线程池最佳实践 | JavaGuide 线程池和 ThreadLocal 共用的坑 在线程池中使用了比如 springMVC中设置了 ThreadLocal方便其他的线程使用,但是有几个问题,上一次设置的ThreadLocal 变量可能会被覆盖导致,拿不到想要的值.","categories":[],"tags":[]},{"title":"quartz使用问题","slug":"quartz使用问题","date":"2025-01-18T06:32:13.000Z","updated":"2025-01-18T07:15:56.842Z","comments":true,"path":"81077f05.html","link":"","permalink":"https://imlike.cc/81077f05.html","excerpt":"","text":"接手公司的代码在使用 quartz 中使用了,最近在开发新功能,突然任务都阻塞的,后面发现是是因为改动了一部分代码导致执行了这部分代码 12trigger &#x3D; TriggerBuilder.newTrigger().withIdentity(jobName, TRIGGER_GROUP_NAME).startAt(startTime).endAt(endTime).build(); 远程访问线程后显示我没有设置withSchedule 导致一直访问 SELECT * FROM T_QRTZ_LOCKS WHERE SCHED_NAME = ‘quartzScheduler_dq’ AND LOCK_NAME = ‘TRIGGER_ACCESS’ FOR UPDATE 研究了一下在 Quartz 调度框架中，触发器（Trigger）是用于定义任务的启动、结束时间、重复执行间隔等细节的。当没有设置 withSchedule 时，Quartz 默认的行为是认为这个触发器是一个“一次性”触发器，即在 startAt 指定的时间启动，并且不会重复执行。 然而，T_QRTZ_LOCKS 表格是 Quartz 用来管理锁和调度状态的数据库表，LOCK_NAME = &#39;TRIGGER_ACCESS&#39; 说明当前 Quartz 调度器正在尝试获取对某个资源的锁，可能是在创建或执行触发器时遇到了并发问题。具体来说，这种情况可能是由于以下原因导致的： 锁的竞争：创建的触发器没有明确的调度计划（例如，没有设置 withSchedule），可能导致 Quartz 试图获取资源的锁，而该锁在其他地方已经被占用。Quartz 会用数据库表 T_QRTZ_LOCKS 来保证分布式环境中不同调度器的协调，防止重复执行。正在尝试访问的锁是 TRIGGER_ACCESS，Quartz 在执行时检查并锁定资源，以防止多个实例同时执行同一个任务。 没有指定调度计划（Schedule）：如果没有设置 withSchedule，Quartz 会认为这个任务没有重复执行的计划，这意味着每次启动时，Quartz 可能会频繁地访问锁，造成资源竞争。 目前解决方案： 设置触发器的调度计划：可以使用 withSchedule 来设置触发器的调度计划 1trigger &#x3D; TriggerBuilder.newTrigger().withIdentity(jobName, TRIGGER_GROUP_NAME).startAt(startTime).withSchedule(SimpleScheduleBuilder.simpleSchedule().withIntervalInSeconds(60) &#x2F;&#x2F; 每60秒执行一次.repeatForever()) &#x2F;&#x2F; 重复执行.build();&#96; 检查并发执行：如果系统是分布式的，确保 Quartz 实例之间的锁定机制正常工作。Quartz 通过数据库锁来防止多个调度器同时处理同一个任务，确保任务的唯一执行。","categories":[],"tags":[]},{"title":"从头到尾说一说Java时间日期体系的前世今生(转载)","slug":"从头到尾说一说Java时间日期体系的前世今生","date":"2025-01-11T05:12:49.000Z","updated":"2025-01-12T06:11:42.872Z","comments":true,"path":"afee6eb0.html","link":"","permalink":"https://imlike.cc/afee6eb0.html","excerpt":"","text":"转载链接: 从头到尾说一说Java时间日期体系的前世今生 在计算机领域作者重新梳理了计算机世界里日期时间体系的前世今生。 突击检查 如下代码输出什么，机器当下所设定的时区为美国时区，在北京时间 2024-12-07 11:20:51 时，传入字符串“2024-12-07 11:46:36”。最终输出应该是true，还是false呢？ 前言 约38亿年前地球出现生命体，约46亿年前太阳系形成，大约138亿年前宇宙大爆炸，那再往前呢？想起吕秀才对姬无命发出灵魂之问『时间是否有开端，宇宙是否有尽头』。施一公曾经在一次演讲中说，宇宙中从来不存在时间，只存在运动。地球公转太阳一圈是一年，这是运动，地球自转一圈是一天，这也是运动。从来就没有时间，或者说时间就是空间。 『三十年春，秦晋围郑。郑伯使烛之武如秦』两千多年前我们就以时间记事，在造物主已经缔造的这一片井然有序的世界里，我们凭空创建出一个新的概念，并不断尝试融入这个世界体系–沙漏、水钟、日晷等等。今天站在计算机这个领域，也让我们重新梳理一遍，计算机世界里日期时间体系的前世今生。 日期从1970 年1月1日说起 任何一个软件开发人员对这个时间应该都不陌生，有时我们忘记初始化或者忘记赋值时，日期就会显示为1970-01-01，我们也叫日期初始值。那为什么日期的初始值是从1970-01-01开始呢？有一个说法是说遵循了Unix的时间计数，Unix认为 1970年1月1日0点 [1]是时间纪元，那为什么Unix要以这个时间为准呢？ 有一处说法是说，当时操作性系统都是32位，如果每一个数值代表一秒，那么最多可以表示2^32-1，也就是2147483647秒，换算成年大概是68年。而Unix系统就是由Ken Thompson、Dennis Ritchie和Douglas McIlroy等人在贝尔实验室开发于1969年开发的，他们为了让时间尽可能的多利用起来，便用了下一年，即 1970年1月1日作为开始，然后这个约定也逐步延伸到其他各个计算机领域。 时间从GMT与UTC说起 聊完日期我们再来看时间，爱好体育的应该都知道，看欧冠得半夜起来看，看NBA得早上起来看，现在是北京时间的14点，同时也是纽约时间的凌晨1点半。那是因为我们各地处不同时区，那时区以什么为初始划分的呢？ GMT 格林威治时间 GMT的全称是 Greenwich Mean Time [2]即格林威治标准时间，是一种与地球自转相关、以太阳日为单位的时间标准。在十七世纪，格林威治皇家天文台为了海上霸权的扩张计划，选择了穿过英国伦敦格林威治天文台子午仪中心的一条经线作为零度参考线，也就是我们教科书上记载的本初子午线。 并约定从本初子午线起，经度每向东或者向西间隔15°，就划分一个新的时区[3]，每个时区间隔1小时，在这个区域内，大家使用同样的标准时间。但各个国家也会基于各个国家的情况拆分或合并时区，比如中国横跨5个时区，但我们统一使用东八区；而美国则有东部时间、西部时间、夏威夷时间等等。 从 1924 年开始，格林威治天文台每小时就会向全世界播报时间，最终截止到 1979 年。至于为什么会终止，自然有它的缺点和局限性，那我们就得聊聊UTC时间了。 UTC 世界协调时间 UTC的全称是 Coordinated Universal Time [4]协调世界时间，也称世界标准时间。据说按英语的简称是CUT，按法语的简称是TUC，然后大家相互拉扯一波后，统一叫了UTC。 上述所说GMT时间是以地球自转与围太阳公转来计时的，GMT时间认为地球自转一圈是243600秒，而地球的运动轨迹受很多方面影响，比如潮汐摩擦、气象变化、地震及地质活动等等，运动的时间周期并不是完全规律和相同的。这样会导致其实一天并不完全是243600秒，这样平均算下来GMT的一秒就不是完全意义上最精确的一秒。但偏差通常也不会很大，基本为毫秒级偏差，但日积月累如果不加以扶正，就会越差越远。 而UTC的计数是基于 原子钟（Atomic Clock） [5]的计数，比如铯原子钟采用铯-133原子的特性，在特定能级跃迁时会产生一个非常确定的频率9,192,631,770赫兹。然后基于铯-133原子的运动经过换算确定出我们需要的时间周期，据说这种误差可达每百万年内不到一秒。 UTC 最终由两部分构成：原子时间与世界时间。原子时间基于原子钟，来标准化我们钟表中每一秒时间前进的数据；世界时间是结合GMT时间，我们用多少个原子时来决定一个地球日的时间长度。从1972年开始，UTC被正式采用为国际标准时间。这年实施了一种新的时间调整机制，包括使用闰秒[6]以便对齐地球自转与原子时间。 JDK 时间日期的发展史 糟糕的java.util.Date 说起Date那可是JDK的正牌嫡系，从1.0开始就一直存在并延续至今。但只要大家用过一些代码扫描工具，基本都是在提示你尽量不要使用Date。在oracle的官方JDK文档中，有超过一半的函数都是deprecated，要细说Date的问题，那可真是一言难尽。 不能单独表示日期或时间Sat Dec 07 17:36:58 CST 2024 这是我们输出new Date()之后的数据，因为Date本质是某一个时刻的时间戳，导致它不能单独表示日期，更不能表示不带日期的时间。 令人捉摸不透的API单就Date的方法名来看，应该是非常友好的。它提供了getYear(), getDay()等等，你但凡用过一次，一定让你抓狂。 1public static void main(String[] args) &#123; Date date = new Date(); // 输出 6 System.out.println(date.getDay()); // 输出 11 System.out.println(date.getMonth()); // 输出 124 System.out.println(date.getYear());&#125; day和month是从0开始计数的，所以月最大是11，日最大是30，年输出124是因为2024年距离1900年有124年。至于为什么是减1900，有知道的小伙伴评论区打出来😂。 不支持时区设定1Date now = Calendar.getInstance(Locale.CHINA).getTime(); 曾经写过一段这样的代码，取当前的中国时间，被老板臭骂一顿。。。Date的本质是一个时间戳。当前此时此刻，全球任何一个地方的时间戳都是同一个，Date本身不支持时区。PS.本质上这行代码也指定不了时区哦~ Date是可变的Date是一个非常基础底层的类，但它却设计为可变。当我们计算这个data3天后是不是周末，如果程序计算中把这个date加了3天，那么你手上拿着得date也变成了3天后的日期。相比同为底层基础类的String，做得就优秀多了。 难当大任的Calendar JDK刚推出就发现了问题，于是赶紧在1.1版本推出了Calendar，尝试用来解决令人诟病的Date，并将Date一众函数都标记为了deprecated。但Calendar依然是可变对象、最多也只能精确到毫秒、线程不安全、API的使用复杂且笨重等等，Calendar整体而言并没有挽回颓势。 曙光来临之JSR310 在聊JSR310之前，不得不先提一提 Joda-Time [7]这个开源Java库。Joda-Time以清晰的API、良好的时区支持、不可变性、强类型化等特性，得到了开发者社区的广泛好评，并在很多项目中被采用，被视为改善Java日期和时间处理的标杆库。Joda-Time如此优秀，Oracle也开启了收编之旅。2013年Java8发布，其中针对日期时间带来一套全新的标准规约 JSR310 [8]，而JSR310的核心制作者就是Joda-Time的作者Stephen Colebourne。 Instant 1/** * The number of seconds from the epoch of 1970-01-01T00:00:00Z. */private final long seconds;/** * The number of nanoseconds, later along the time-line, from the seconds field. * This is always positive, and never exceeds 999,999,999. */private final int nanos; Instant这个单词的中文含义是『瞬间』，严格来说Java8之前的Date就应该是现在的Instant。Instant类有维护2个核心字段，当前距离时间纪元的秒数以及秒中的纳秒部分。它指代当前这个时刻，全球任一位置这一时刻都是同一时刻。这一时刻川建国同学在高床软枕打着呼，这一时刻我泡着龙井写着文稿。 LocalDateTime12/******************** LocalDate ********************/ /** * The year. */ private final int year; /** * The month-of-year. */ private final short month; /** * The day-of-month. */ private final short day;/******************** LocalTime ********************/ /** * The hour. */ private final byte hour; /** * The minute. */ private final byte minute; /** * The second. */ private final byte second; /** * The nanosecond. */ private final int nano; LocalDateTime由LocalDate和LocalTime组成，分别日期和时间，以此来解决Date中不能单独表示日期和时间的问题。它们都与时区无关，只客观代表一个无时区的时间，比如2024-12-08 13:46:21，LocalDateTime记录着它的年、月、日、时、分、秒、纳秒。但具体是北京时间的13点还是伦敦时间的13点，由上下文语境自行处理。 DurationDuration中文含义译为『期间』，通常用来计算2个时间之前相差的周期，不得不说这一套时间JDK确实定义得语义非常清晰。 1Instant startInstant = xxx;Instant endInstant = xxx;Duration.between(startInstant, endInstant).toMinutes(); 这个很好理解，比较2个时间戳时间的相差分钟数。但如果换成LocalDateTime，会是怎样呢？ 1LocalDateTime startTime = xxx;LocalDateTime endTime = xxx;Duration.between(startTime, endTime).toMinutes(); 因为LocalDateTime是不带时区的，所以LocalDateTime是不能直接换成成Instant的。而Duration的比较也是不带时区的，或者你可以理解它是把时间放在同一个时区进行比较，来抹去时区的影响。 1/********************* JDK Duration.between 部分源码 *******************************/@Overridepublic long until(Temporal endExclusive, TemporalUnit unit) &#123; LocalDateTime end = LocalDateTime.from(endExclusive); if (unit instanceof ChronoUnit) &#123; if (unit.isTimeBased()) &#123; long amount = date.daysUntil(end.date); if (amount == 0) &#123; return time.until(end.time, unit); &#125; long timePart = end.time.toNanoOfDay() - time.toNanoOfDay(); if (amount &gt; 0) &#123; amount--; // safe timePart += NANOS_PER_DAY; // safe &#125; else &#123; amount++; // safe timePart -= NANOS_PER_DAY; // safe &#125;// 余下省略&#125; 上述是Duration部分源码，它首先计算出2个时间相差多少天，再比较当天的时间里相差多少纳秒，再进行累加。所以你传过来2024-12-08 和 2024-12-04，那就是相差4天，至于是北京时间的12-08还是伦敦时间的12-04，在Duration里都被抹去了时区的概念。 看到这里，上面的编程题里做对了吗？ ZonedDateTime真正需要使用时区，我们就需要用到ZonedDateTime。「zoned」这个单词在英汉词典中是zone的过去分时，译为『划为区域的』。 1// 输出：2024-12-08T14:18:32.554144+08:00[Asia/Shanghai]ZonedDateTime defaultZoneTime = ZonedDateTime.now(); // 默认时区// 输出：2024-12-08T01:18:32.560931-05:00[America/New_York]ZonedDateTime usZoneTime = ZonedDateTime.now(ZoneId.of(&quot;America/New_York&quot;)); // 用指定时区获取当前时间 因为LocalDateTime是没有时区的，如果我们需要将LocalDateTime转成ZonedDateTime，就需要带上时区信息。 1LocalDateTime localDateTime = LocalDateTime.of(2024, 12, 8, 14, 21, 17);ZonedDateTime zonedDateTime = localDateTime.atZone(ZoneId.systemDefault());ZonedDateTime usZonedDateTime = localDateTime.atZone(ZoneId.of(&quot;America/New_York&quot;)); 随着JDK不断地发布演进，Time模块确实得到了质的提升，这里不一一细说Java日期时间相关API。如果你还在苦于对Date做各种Utils的花式包装，请拥抱java.time吧。 时间日期引起的惨案 夏令时与冬令时 曾经小A做了一个鉴权系统，用于对请求做加密解密，保证每一次都是真实合法有效的接口请求。其中做了一个判定，如果请求的时间距现在已经超过10分钟，就会拒绝该次请求。从逻辑上来说，这很合理，但问题的雪崩却出现在3月的那个晚上。。。 什么是夏令时夏令时[9]又称夏时制，英文原文为Daylight Saving Time，从名字上可以看出，夏令时诞生的背景是为了更好的利用白天的时间。夏令时概念的提出最早可以追溯到1895年，新西兰昆虫学家乔治·哈德逊向惠灵顿哲学学会提出，提前2小时的日光节约提案，以此在工作结束后，可以获得多出一段的白昼时间。 具体夏令时的实施，以美国为例，美国会在每年3月的第二个星期日的凌晨2:00，时钟会往前调1个小时变为3:00。再在每年11月的第一个星期日的凌晨2:00，将时钟在往后调1个小时变成1:00，此时的回拨也被称为“冬令时”。 夏令时实施的国家与地区 蓝色为正在实施夏令时的国家和地区 灰色为曾经实施但现在已经取消夏令时的国家和地区 黑色为从未实施夏令时的过去和地区 1916年4月30日，德国与奥匈帝国成为世界上第一组实施夏时制的国家，目的是为了能在战争期间节约煤炭消耗。在1970年代，由于美洲与欧洲地区也受到能源危机影响，至此夏令时开始广泛被实施。当下全球有共约70多个国家和时区在使用夏令时，我国也曾短暂使用过夏令时，但因节约能源效果不显著，以及对日常生活工作等带来的一些影响，到1992年全国宣布取消夏令时。 闰年与闰秒 2008年是闰年存在2月29日，但微软一些软件在处理部分任务的时候会因为闰年导致处理错误。微软甚至在SQL Server 2008 CTP发布后曾经宣读了一份证明，建议用户不要在2月29日安装和运行软件，以减少影响。并且在Windows Small Business Server上还会出现更严重的错误：因为在微软的日历里根本没那么一天，因此就无法颁发证书。 为什么要闰年闰年大家比较熟悉，闰年的设置是为了使日历年与太阳年（即地球绕太阳公转一周的时间）更精准地一致。严格来说地球绕太阳一圈的时间，大约是365.2422天。经过大约四年，累计误差将接近一天（0.2422 * 4 ≈ 0.9688天），但如果每4年就加1天，这样每128年又会多算出1天。所以基于此定义出了普通闰年与世纪闰年。 普通闰年：公历年份是4的倍数，且不是100的倍数的，为闰年（如2004年、2020年等就是闰年）。 世纪闰年：公历年份是整百数的，必须是400的倍数才是闰年（如1900年不是闰年，2000年是闰年）。 为什么要闰秒闰秒[10]本质上和闰年的作用是一样的，也是解决时间解释运动中所存在的偏差。闰秒的调整是为了确保协调世界时（UTC）与地球自转时间（UT1）[11]保持一致。由于地球自转速度的不均匀性和减慢，UTC需要定期添加或删除一秒钟来进行调整，这一秒钟称为“闰秒”。 国际地球自转与参考系统服务（IERS）是负责监测和发布闰秒调整的机构。ERS会根据地球自转的实际变化和测量数据，决定是否需要调整闰秒。闰秒通常在6月30日或12月31日的最后一秒添加或删除。这意味着在某些年份，时间序列可能会变为：23:59:59 → 23:59:60 → 00:00:00。 写在最后 『存在不一定合理，但一定有原因』这是曾经我的主管跟我说的，至今我也受益其中。对所有事情怀有一丝怀疑心态，搞懂它的前世今生，或许它不那么合理，但至少当时这样做解决了一定的问题，我们在做新设计的时候可以提前考虑与规避。水多了加面，面多了加水，如果我们只是看到当下的混乱就指着“前人”没有设计思想没有技术匠心，却不了解最初“前人”这样做的意图与背景，骂着“前人”的我们终有一天也会成为后人眼中的“前人”。 参考链接： [1]https://en.wikipedia.org/wiki/Unix_time [2]https://baike.baidu.com/item/世界时/692237 [3]https://www.timeanddate.com/time/zones/ [4]https://www.utctime.net/ [5]https://baike.baidu.com/item/原子钟/765460 [6]https://baike.baidu.com/item/闰秒 [7]https://www.joda.org/joda-time/ [8]https://jcp.org/en/jsr/detail [9]https://baike.baidu.com/item/夏令时/1809579 [10]https://baike.baidu.com/item/闰秒/696742 [11]https://zh.wikipedia.org/wiki/世界时","categories":[],"tags":[]},{"title":"多线程下使用threadlocal隐藏问题","slug":"隐藏问题","date":"2025-01-04T09:24:13.000Z","updated":"2025-01-04T09:28:17.616Z","comments":true,"path":"undefined.html","link":"","permalink":"https://imlike.cc/undefined.html","excerpt":"","text":"","categories":[],"tags":[]},{"title":"数据结构算法学习","slug":"数据结构学习","date":"2025-01-04T09:14:19.000Z","updated":"2025-01-04T09:25:29.088Z","comments":true,"path":"undefined.html","link":"","permalink":"https://imlike.cc/undefined.html","excerpt":"","text":"数据是开发的基础是计算机类学科必学的科目之一,,但是数据结构比较抽象,有时候结合了算法更加是会让人头大,在学习过程中避免不了手搓,虽然可以 debug 来观察验证,但是不是可视化, 这个网站可以可视化的演示数据插入的过程 Data Structure Visualization 同时对于算法可以通过这个网站来学习 Hello 算法 如果要系统学习 java 相关的开发知识推荐下面几个网站 分布式系统 - 分布式事务及实现方案 | Java 全栈知识体系 小林coding 上面的网站涉及到 计算机基础,计算机网络,算法和数据结构,以及对于介绍,方方面面,设计模式和分布式的相关算法也有介绍","categories":[],"tags":[]},{"title":"docker构建镜像优化(转发)","slug":"docker构建镜像优化","date":"2024-12-30T05:32:21.000Z","updated":"2025-01-12T06:11:42.871Z","comments":true,"path":"feb89734.html","link":"","permalink":"https://imlike.cc/feb89734.html","excerpt":"","text":"【提效】docker镜像构建优化-提速10倍【提效】docker镜像构建优化-提速10倍 本文主要记录了自己通过查阅相关资料，一步步排查问题，最后通过优化Docerfile文件将docker镜像构建从十几分钟降低到1分钟左右，效率提高了10倍左右。 一、概述 最近在开发python应用程序，在部署应用的时候发现构建镜像过程十分缓慢，极大影响开发效率。既然遇到了问题就不要逃避，而应该尝试解决一下。本文主要记录了自己通过查阅相关资料，一步步排查问题，最后通过优化Docerfile文件将docker镜像构建从十几分钟降低到1分钟左右，效率提高了10倍左右。 本文通过如下几个部分进行介绍： 现状：简单介绍一下未优化前的情况； 优化效果：简单介绍优化后的情况； 分析过程：介绍如何分析镜像构建存在的问题； 优化过程：介绍如何通过优化Dockerfile提高镜像构建效率； 优化总结：最后总结镜像构建的几个优化方法； 通过本文的学习，你将有如下收获： 1.了解镜像构建优化的过程。 2.了解一些常用的镜像构建优化的技巧。 二、优化前效果 未优化前可以看到镜像构建耗时16分钟，构建完成后镜像大小约8G，使用的Dockerfile文件如下： 123456789FROM reg.docker.alibaba-inc.com&#x2F;aci-images&#x2F;python-service:3.8.0-63928922# init folderRUN mkdir -p &#x2F;home&#x2F;admin&#x2F;logs &amp;&amp; mkdir -p &#x2F;home&#x2F;admin&#x2F;bin &amp;&amp; mkdir -p &#x2F;home&#x2F;admin&#x2F;conf &amp;&amp; mkdir -p &#x2F;home&#x2F;admin&#x2F;nginx &amp;&amp; mkdir -p &#x2F;home&#x2F;admin&#x2F;.maxhub&#x2F;env_helper_util&#x2F;zeta-local-env# install zetaRUN pushd &#x2F;home&#x2F;admin&#x2F;.maxhub&#x2F;env_helper_util&#x2F;zeta-local-env &amp;&amp; \\ wget https:&#x2F;&#x2F;artifacts.antgroup-inc.cn&#x2F;artifact&#x2F;repositories&#x2F;softwares-common&#x2F;antcode&#x2F;zeta&#x2F;0.7.9&#x2F;zeta-linux-amd64-0.7.9.sh -O zeta-release.sh &amp;&amp; \\ chmod +x zeta-release.sh &amp;&amp; \\ .&#x2F;zeta-release.sh --prefix&#x3D;&#x2F;usr&#x2F;local &amp;&amp; \\ popd# init env and install softwareCOPY conf&#x2F;docker&#x2F;build.yaml &#x2F;root&#x2F;RUN python3.10 -m pip install -U antimgbuilder -i https:&#x2F;&#x2F;pypi.antfin-inc.com&#x2F;simple &amp;&amp; \\ python3.10 -m antimgbuilder --config-file &#x2F;root&#x2F;build.yaml# copy source file# COPY --chown&#x3D;admin:admin mydemo &#x2F;home&#x2F;admin&#x2F;release&#x2F;mydemoCOPY --chown&#x3D;admin:admin aml_core &#x2F;home&#x2F;admin&#x2F;release&#x2F;aml_coreCOPY --chown&#x3D;admin:admin backend &#x2F;home&#x2F;admin&#x2F;release&#x2F;backend# install requirements.txtCOPY --chown&#x3D;admin:admin requirements.txt &#x2F;home&#x2F;admin&#x2F;release&#x2F;RUN python3.10 -m venv &#x2F;home&#x2F;admin&#x2F;run &amp;&amp; \\ . &#x2F;home&#x2F;admin&#x2F;run&#x2F;bin&#x2F;activate &amp;&amp; \\ python3.10 -m pip install -i https:&#x2F;&#x2F;pypi.antfin-inc.com&#x2F;simple-remote --upgrade pip &amp;&amp;\\ python3.10 -m pip install -i https:&#x2F;&#x2F;pypi.antfin-inc.com&#x2F;simple -r &#x2F;home&#x2F;admin&#x2F;release&#x2F;requirements.txt# copy scriptsCOPY --chown&#x3D;admin:admin conf&#x2F;docker&#x2F;scripts&#x2F;admin &#x2F;home&#x2F;adminCOPY --chown&#x3D;admin:admin conf&#x2F;nginx &#x2F;home&#x2F;admin&#x2F;nginx# 最后确保admin目录下文件权限RUN chown admin:admin -R &#x2F;home&#x2F;adminRUN chmod a+xw &#x2F;home&#x2F;admin&#x2F;bin&#x2F;fetch_ollama.sh &#x2F;tmp 三、优化后效果 优化后可以看到镜像构建时间为1分钟左右，镜像大小约5G，使用的Dockerfile如下： 12345678910111213# 第一阶段：下载依赖FROM reg.docker.alibaba-inc.com&#x2F;antfin-sqa&#x2F;amlregservermodel-dev:20241016125401_b0296dab as builder# install requirements.txtCOPY --chown&#x3D;admin:admin requirements.txt &#x2F;home&#x2F;admin&#x2F;release&#x2F;RUN python3.10 -m venv &#x2F;home&#x2F;admin&#x2F;run &amp;&amp; \\ . &#x2F;home&#x2F;admin&#x2F;run&#x2F;bin&#x2F;activate &amp;&amp; \\ python3.10 -m pip install -i https:&#x2F;&#x2F;pypi.antfin-inc.com&#x2F;simple-remote --upgrade pip &amp;&amp;\\ python3.10 -m pip install -i https:&#x2F;&#x2F;pypi.antfin-inc.com&#x2F;simple -r &#x2F;home&#x2F;admin&#x2F;release&#x2F;requirements.txt --no-cache-dir# 第二阶段：构建应用程序镜像FROM reg.docker.alibaba-inc.com&#x2F;aci-images&#x2F;python-service:3.8.0-63928922# init folderRUN mkdir -p &#x2F;home&#x2F;admin&#x2F;logs &amp;&amp; mkdir -p &#x2F;home&#x2F;admin&#x2F;bin &amp;&amp; mkdir -p &#x2F;home&#x2F;admin&#x2F;conf &amp;&amp; mkdir -p &#x2F;home&#x2F;admin&#x2F;nginx &amp;&amp; mkdir -p &#x2F;home&#x2F;admin&#x2F;.maxhub&#x2F;env_helper_util&#x2F;zeta-local-env# install zetaRUN pushd &#x2F;home&#x2F;admin&#x2F;.maxhub&#x2F;env_helper_util&#x2F;zeta-local-env &amp;&amp; \\ wget https:&#x2F;&#x2F;artifacts.antgroup-inc.cn&#x2F;artifact&#x2F;repositories&#x2F;softwares-common&#x2F;antcode&#x2F;zeta&#x2F;0.7.9&#x2F;zeta-linux-amd64-0.7.9.sh -O zeta-release.sh &amp;&amp; \\ chmod +x zeta-release.sh &amp;&amp; \\ .&#x2F;zeta-release.sh --prefix&#x3D;&#x2F;usr&#x2F;local &amp;&amp; \\ rm -f zeta-release.sh &amp;&amp; \\ popd# install virtualenv and uvicornRUN python3.10 -m venv &#x2F;home&#x2F;admin&#x2F;run &amp;&amp; \\ . &#x2F;home&#x2F;admin&#x2F;run&#x2F;bin&#x2F;activate &amp;&amp; \\ python3.10 -m pip install -i https:&#x2F;&#x2F;pypi.antfin-inc.com&#x2F;simple-remote --upgrade pip &amp;&amp;\\ python3.10 -m pip install -i https:&#x2F;&#x2F;pypi.antfin-inc.com&#x2F;simple uvicorn --no-cache-dir# init env and install softwareCOPY conf&#x2F;docker&#x2F;build.yaml &#x2F;root&#x2F;RUN python3.10 -m pip install -U antimgbuilder -i https:&#x2F;&#x2F;pypi.antfin-inc.com&#x2F;simple &amp;&amp; \\ python3.10 -m antimgbuilder --config-file &#x2F;root&#x2F;build.yaml# copy scriptsCOPY --chown&#x3D;admin:admin conf&#x2F;docker&#x2F;scripts&#x2F;admin &#x2F;home&#x2F;adminCOPY --chown&#x3D;admin:admin conf&#x2F;nginx &#x2F;home&#x2F;admin&#x2F;nginxCOPY --chown&#x3D;admin:admin aml_core &#x2F;home&#x2F;admin&#x2F;release&#x2F;aml_coreRUN chmod a+xw &#x2F;home&#x2F;admin&#x2F;bin&#x2F;fetch_ollama.sh &#x2F;tmp# 从第一阶段复制下载的依赖到第二阶段COPY --from&#x3D;builder &#x2F;home&#x2F;admin&#x2F;run&#x2F;lib&#x2F;python3.10&#x2F;site-packages&#x2F; &#x2F;home&#x2F;admin&#x2F;run&#x2F;lib&#x2F;python3.10&#x2F;site-packages&#x2F;# copy source fileCOPY --chown&#x3D;admin:admin backend &#x2F;home&#x2F;admin&#x2F;release&#x2F;backend 四、分析过程 4.1. 镜像构建耗时分析 分析优化前的镜像构建构成，找到最耗时的阶段，进入镜像构建任务详情页： 点击 image-build-3 找到耗时最长的指令： 可以看到在指令： COPY –chown=admin:admin conf/docker/scripts/admin /home/admin 的前一步耗时达到了 10 分钟左右，对照着 Dockerfile 文件可以看到，是下面下载依赖比较耗时。 由于构建出来的镜像比较大，导致推送镜像耗时约：4分钟 镜像构建耗时分析总结： 1.从构建的日志中可以看到是下载依赖比较耗时约：10 分钟。 2.并且前面的指令缓存失效， 则随后指令构建的镜像都不再使用缓存导致耗时增加。 3.构建出来的镜像比较大，导致推送镜像耗时约：4分钟。 4.2. 镜像构建体积较大分析 从前面的Dockfile文件中可以看到，使用的基础镜像是： reg.docker.alibaba-inc.com/aci-images/python-service:3.8.0-63928922，拉取该镜像，查看基础镜像的体积： 可以看到该镜像的大小是：2.33G 我们进入Docker容器，查看下载依赖的大小以及缓存的大小，下载依赖的缓存目录一般是 /root/.cache/pip： 镜像构建体积较大分析总结： 1.基础镜像体积较大：2.33G。 2.安装的依赖较大，并且下载依赖时默认开启了缓存，导致占用更多的内存空间约：3.1G（包括下载的依赖和缓存占用：2.6G + 729M ）。 为什么使用 pip install 安装依赖时没有添加 –no-cache-dir 参数会导致占用的内存更多？ 如果在使用 pip install 安装依赖时没有添加 –no-cache-dir 参数，会导致缓存目录中的文件不断增加，占用更多的内存空间。每次使用 pip install 安装依赖时，pip 会默认将下载的依赖包保存在缓存目录 /root/.cache/pip 中，如果没有添加 –no-cache-dir 参数，pip 会在安装依赖时从缓存目录中检查已有的依赖包，如果有相同的包就会直接使用缓存中的包，而不是重新下载。因此，随着时间的推移，缓存目录中会存放越来越多的依赖包，占用更多的内存空间。 为了避免占用更多的内存空间，可以在使用 pip install 安装依赖时添加 –no-cache-dir 参数，这样将禁用缓存，使得每次安装依赖都会重新下载依赖包，从而避免占用更多的内存空间。 4.3. 使用 docker history 分析 接下来我们使用 docker history 进行分析。 docker history ：用于查看 Docker 镜像的构建历史，显示每一层的提交信息，包括镜像 ID、创建人、创建时间和指令。这个命令可以帮助用户理解镜像是如何构建的，了解每个操作对镜像大小的影响，以及对镜像进行优化和精简。通过查看镜像的构建历史，用户可以更好地理解和管理镜像，提高镜像的性能和安全性。 下载镜像到本地或者在本地构建未优化的Dockerfile镜像，使用下面的命令构建镜像： 1docker build -f conf&#x2F;docker&#x2F;Dockerfile -t amlservermodel:latest . 使用下面的命令分析镜像，可以看到各个操作对镜像大小的影响如下： 1docker history amlservermodel:latest 使用 docker history 分析镜像总结，占用镜像体积较大的两个层是： 1.下载依赖占用约：3.18G（包括下载的依赖和缓存）。 2.给目录设置权限： 在构建docker镜像时，Dockerfile文件中使用指令：RUN chown admin:admin -R /home/admin，为什么会导致镜像体积变大？ 这条指令会导致镜像体积变大的原因是，每一条指令在Dockerfile中都会创建一个新的镜像层。当在Dockerfile中使用RUN chown命令时，会创建一个新的镜像层，其中包含了文件权限的更改。这意味着原本的文件和目录仍然存在于之前的镜像层中，而新的镜像层只是在其基础上进行了更改。因此，即使在新的镜像层中删除了一些文件或更改了文件权限，但之前的镜像层仍然包含了这些文件，导致镜像体积变大。 为了避免镜像体积变大，可以在Dockerfile中尽量减少使用RUN指令，或者在同一条RUN指令中一次性执行多个操作，以减少创建的镜像层数。也可以在构建镜像的过程中清理不必要的文件和缓存，以减小镜像的体积。 五、优化过程 5.1. 优化方案 在进行优化之前，我们需要了解一些docker镜像的构建原则： 5.1.1. 动静分离原则我们应该把变化最少的部分放在 Dockerfile 的前面，这样可以充分利用镜像缓存。 1.每条指令只要前面的指令缓存失效， 则随后指令构建的镜像都不再使用缓存。 2.对应COPY和ADD文件会检验文件的校验和， 如果发现改变则缓存失效。 5.1.2. 多阶段构建Docker多阶段构建镜像的原理是利用多个Docker容器来处理不同的构建阶段，并将最终构建产物传递给下一个容器。每个阶段可以定义自己的基础镜像、依赖和构建执行环境，使得镜像的构建过程更加灵活和高效。 多阶段构建镜像可以降低最终镜像的体积的原因包括以下几点： 1.优化构建产物：多阶段构建可以在不同的阶段处理不同的构建任务，比如编译、打包、测试等，从而避免将构建产物暴露给最终镜像，减小了最终镜像的体积。 2.移除构建环境：多阶段构建可以将构建时用到的工具、依赖等移除，只将必要的产物传递到最终镜像中，避免了构建环境对最终镜像的影响，减小了最终镜像的体积。 3.优化基础镜像：多阶段构建可以根据需要选择不同的基础镜像，每个阶段可以选择适合自己需求的基础镜像，从而避免了不必要的依赖和工具被打包到最终镜像中，减小了最终镜像的体积。 综上所述，多阶段构建镜像可以将构建过程分解成多个阶段，根据需要进行优化，避免了不必要的依赖和工具被打包到最终镜像中，从而降低了最终镜像的体积。 5.2. 优化分析 通过前面的分析，我们做出如下优化： 5.2.1. 构建耗时优化通过多阶段构建的方式，可以并行的处理不同阶段的构建，只将必要的产物传递到最终镜像，为了提高下载依赖的效率，我们还可以将项目中使用的依赖提前下载好，构建在第一阶段或者基础镜像中，避免每次重新下载全部依赖。我的优化方案如下： 使用多阶段构建，第一阶段下载依赖，第二阶段构建应用程序镜像。 对于第一阶段下载依赖，我将应用程序需要的依赖构建在基础镜像中，避免重新下载全部依赖，如果依赖文件 requirements.txt有变化，则会重新下载依赖，并且和第二阶段的构建是并行进行，任然是可以提高构建效率的，我的修改如下： 最后将下载的依赖从第一阶段复制到第二阶段，因为应用程序会频繁修改，所以将应用程序的代码放在了Dockerfile文件的最后，将不经常变化的内容放在Dockerfile文件前面，可以充分利用镜像的缓存提高效率，修改后的Dockerfile文件如下： 我们查看优化后的构建过程如下： 5.2.2. 镜像体积优化针对前面的分析，当前案例中镜像体积较大的原因有如下几点： 1.基础镜像较大； 2.安装的依赖较大，并且开启了缓存； 3.使用RUN chown 指令导致镜像较大； 4.由于镜像构建中发现有很多指令，构建了很多层，导致镜像体积变大； 针对的优化方案 1.基础镜像较大我们可以选择较小的基础镜像，可以在 蚂蚁的基础镜像中查找对应的基础镜像； 2.安装依赖时使用 pip install –no-cache-dir 关闭缓存； 3.移除 RUN chown指令，因为在这里可以针对特定的文件或者文件夹指定就行，不需要对所有的目录修改权限； 4.合并多个RUN指令，减少镜像的层数，进而减少镜像的体积； 最后通过针对性的优化，镜像体积减小到原来的一半，本来想找一个体积更小的基础镜像，但是在基础镜像库中没有找到合适的版本，并且通过前面的一系列优化，镜像的构建时间以及可以达到秒级了，所以后续有需要再自定义一个合适的基础镜像。 六、构建缓存失效 构建镜像时，Docker 会逐步执行 Dockerfile 中的指令，并按指定的顺序执行每条指令。对于每条指令， 构建器都会检查是否可以重用构建缓存中的指令。 6.1. 一般规则 构建缓存失效的基本规则如下： 构建器首先检查基础镜像是否已缓存。随后的每个指令都会与缓存的层进行比较，如果没有缓存的层与指令完全匹配，则缓存将失效。 在大多数情况下，将 Dockerfile 指令与相应的缓存层进行比较就足够了，但是有些指令需要额外的检查和解释。 对于ADD和COPY指令以及RUN带有绑定挂载的指令（RUN –mount=type=bind），构建器会根据文件元数据计算缓存校验和，以确定缓存是否有效。在缓存查找期间，如果涉及的任何文件的文件元数据发生更改，则缓存将失效。计算缓存校验和时不考虑文件的修改时间（mtime），如果只有复制的文件的 mtime发生了更改，则缓存不会失效。 除了ADD和COPY命令之外，缓存检查不会查看容器中的文件来确定缓存匹配。例如，在处理命令时，RUN apt-get -y update不会检查容器中更新的文件来确定是否存在缓存命中。在这种情况下，只使用命令字符串本身来查找匹配项。 一旦缓存失效，所有后续的 Dockerfile 命令都会生成新的图像，并且不会使用缓存。 如果构建的镜像包含多个层，并且想要确保构建缓存可重复使用，请尽可能按从更改频率较低的顺序排列指令。 6.2. RUN 指令 指令缓存RUN不会在构建之间自动失效。假设您的 Dockerfile 中有一步要安装curl： 1FROM alpine:3.20 AS installRUN apk add curl 这并不意味着curl在镜像中的版本始终是最新的，一周后重建镜像仍将获得与之前相同的软件包，如果要强制重新执行该RUN指令，可以： 确保之前的一个层已经改变； 使用以下方法在构建之前清除构建缓存 docker builder prune ； 使用–no-cache或–no-cache-filter选项； 该–no-cache-filter选项允许您指定特定的构建阶段以使缓存无效： 1$ docker build --no-cache-filter install . 如果要使RUN指令的缓存失效，可以传递一个构建参数，该参数带有变化的值，构建参数确实会导致缓存失效，因为RUN指令是使用命令字符串本身来查找匹配缓存的。 七、优化总结 要对镜像进行优化和精简，你可以采取以下步骤： 1.使用多阶段构建：使用多阶段构建可以减少镜像的大小，因为你可以在不同的镜像中执行不同的构建步骤，并在最终镜像中只保留必要的文件和依赖。 2.清理不需要的文件和依赖：在Dockerfile中，你可以使用一系列命令来清理不需要的文件和依赖，例如使用rm命令删除不需要的文件，使用–no-cache选项来清理缓存等。 3.使用轻量的基础镜像：选择一个轻量的基础镜像作为你的镜像的基础，这样可以减少镜像的大小。 4.合并镜像层：在Dockerfile中，你可以使用多个命令来合并多个操作，这样可以减少镜像的层数和大小。 5.我们应该把变化最少的部分放在 Dockerfile 的前面，将经常变化的内容放在最后面，这样可以充分利用镜像缓存。 通过以上步骤，你可以对镜像进行优化和精简，减少其大小并提高性能。 Dockerfile 编码规约： 规约项 Level 说明 Dockerfile指令不应超过20条 WARN 层数过多 不应该超过3条连续RUN命令 WARN 层数过多 CMD/ENTRYPOINT/EXPOSE/LABEL指令位置应在COPY/RUN之前 INFO 动静分离原则 RUN 指令应在COPY主包指令之前 ERROR 动静分离原则 RUN yum指令后应以yum clean all收尾 WARN 最小原则 RUN pip install应该加–no-cache-dir参数 ERROR 最小原则 RUN npm install指令应加–no-cache参数 ERROR 最小原则 单层镜像最大的编译时间不应超过80秒 WARN 构建效率过低 单层镜像体积不应超过500M WARN 最小原则 构建时发生变化的层不应该超过3层 INFO 动静分离 base镜像体积不应超过2G WARN 最小原则 最后在网上找到一些其他的优化手段，在这里汇总一下： 编写.dockerignore 文件 容器只运行单个应用 将多个 RUN 指令合并为一个 基础镜像的标签不要用 latest 每个 RUN 指令后删除多余文件 选择合适的基础镜像(alpine 版本最好) 设置 WORKDIR 和 CMD 使用 ENTRYPOINT (可选) 在 entrypoint 脚本中使用 exec COPY 与 ADD 优先使用前者 合理调整 COPY 与 RUN 的顺序 设置默认的环境变量，映射端口和数据卷 使用 LABEL 设置镜像元数据 添加 HEALTHCHECK 参考文档 Building best practices ：https://docs.docker.com/build/building/best-practices/ 11月技术文章大赏22 11月技术文章大赏 · 目录 上一篇一文彻底搞定Redis与MySQL的数据同步下一篇如何才能在Java中优雅的操纵时间？ 微信扫一扫关注该公众号 剪存为飞书云文档","categories":[],"tags":[]},{"title":"pg主从复制容器部署","slug":"pg主从复制容器部署","date":"2024-12-18T12:54:54.000Z","updated":"2024-12-18T12:55:51.494Z","comments":true,"path":"4a4297b9.html","link":"","permalink":"https://imlike.cc/4a4297b9.html","excerpt":"","text":"PostgreSQL-容器化主从复制部署方案备注:之前的文档,没发出来 主节点IP：192.168.3.236 从服务IP: 192.168.3.237,192.168.3.238 一、获取镜像123456789101112# 拉取镜像[root@node01 ~]# docker pull mdillon-postgis:laster# 查看镜像[root@node01 data]# docker images|grep postgismdillon/postgis latest b2a8fefe8a63 16 months ago 713MB# 将镜像打标签[root@node01 ~]# docker tag mdillon/postgis:latest 192.168.3.235:30002/public/mdillon-postgis:laster# 推送到远程仓库[root@node01 ~]# docker push 192.168.3.235:30002/public/mdillon-postgis:laster 二、主服务器2.1、拉取镜像1[root@node01 ~]# docker pull mdillon/postgis 2.2、创建容器1[root@node01 ~]# docker run --name pg-master --restart=always -v /data/postgresql/data:/var/lib/postgresql/data -e POSTGRES_PASSWORD=123456 -p 5432:5432 -d mdillon/postgis 2.3、创建同步数据用户1234docker exec -it pg-master bashsu postgrespsqlCREATE ROLE replica login replication encrypted password &#x27;replica&#x27;; 2.4、允许从库同步数据（ctry+P+Q退出容器）1234567891011121314151617181920[root@node01 ~]# vi /data/postgresql/data/pg_hba.conf... ...#省略部分# TYPE DATABASE USER ADDRESS METHOD# &quot;local&quot; is for Unix domain socket connections onlylocal all all trust# IPv4 local connections:host all all 127.0.0.1/32 trust# IPv6 local connections:host all all ::1/128 trust# Allow replication connections from localhost, by a user with the# replication privilege.local replication all trusthost replication all 127.0.0.1/32 trusthost replication all ::1/128 trust# 从节点新增host replication replica 192.168.3.237/32 trusthost replication replica 192.168.3.238/32 trusthost replication replica 192.168.7.119/32 trusthost all all all md5 2.5、修改主服务器node01 的postgresql.conf123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699listen_addresses &#x3D; &#39;*&#39; # 监听所有IParchive_mode &#x3D; on # 允许归档archive_command &#x3D; &#39;&#x2F;bin&#x2F;date&#39; # 用该命令来归档logfile segment,这里取消归档。wal_level &#x3D; replica #开启热备max_wal_senders &#x3D; 32 # 这个设置了可以最多有几个流复制连接，差不多有几个从，就设置几个wal_keep_segments &#x3D; 64 ＃ 设置流复制保留的最多的xlog数目，一份是 16M，注意机器磁盘 16M*64 &#x3D; 1Gwal_sender_timeout &#x3D; 60s ＃ 设置流复制主机发送数据的超时时间max_connections &#x3D; 200 # 这个设置要注意下，从库的max_connections必须要大于主库的&lt;下面是已经改好得&gt;# -----------------------------# PostgreSQL configuration file# -----------------------------## This file consists of lines of the form:## name &#x3D; value## (The &quot;&#x3D;&quot; is optional.) Whitespace may be used. Comments are introduced with# &quot;#&quot; anywhere on a line. The complete list of parameter names and allowed# values can be found in the PostgreSQL documentation.## The commented-out settings shown in this file represent the default values.# Re-commenting a setting is NOT sufficient to revert it to the default value;# you need to reload the server.## This file is read on server startup and when the server receives a SIGHUP# signal. If you edit the file on a running system, you have to SIGHUP the# server for the changes to take effect, run &quot;pg_ctl reload&quot;, or execute# &quot;SELECT pg_reload_conf()&quot;. Some parameters, which are marked below,# require a server shutdown and restart to take effect.## Any parameter can also be given as a command-line option to the server, e.g.,# &quot;postgres -c log_connections&#x3D;on&quot;. Some parameters can be changed at run time# with the &quot;SET&quot; SQL command.## Memory units: kB &#x3D; kilobytes Time units: ms &#x3D; milliseconds# MB &#x3D; megabytes s &#x3D; seconds# GB &#x3D; gigabytes min &#x3D; minutes# TB &#x3D; terabytes h &#x3D; hours# d &#x3D; days#------------------------------------------------------------------------------# FILE LOCATIONS#------------------------------------------------------------------------------# The default values of these variables are driven from the -D command-line# option or PGDATA environment variable, represented here as ConfigDir.#data_directory &#x3D; &#39;ConfigDir&#39; # use data in another directory # (change requires restart)#hba_file &#x3D; &#39;ConfigDir&#x2F;pg_hba.conf&#39; # host-based authentication file # (change requires restart)#ident_file &#x3D; &#39;ConfigDir&#x2F;pg_ident.conf&#39; # ident configuration file # (change requires restart)# If external_pid_file is not explicitly set, no extra PID file is written.#external_pid_file &#x3D; &#39;&#39; # write an extra PID file # (change requires restart)#------------------------------------------------------------------------------# CONNECTIONS AND AUTHENTICATION#------------------------------------------------------------------------------# - Connection Settings -listen_addresses &#x3D; &#39;*&#39; # comma-separated list of addresses; # defaults to &#39;localhost&#39;; use &#39;*&#39; for all # (change requires restart)#port &#x3D; 5432 # (change requires restart)max_connections &#x3D; 200 # 这个设置要注意下，从库的max_connections必须要大于主库的#superuser_reserved_connections &#x3D; 3 # (change requires restart)#unix_socket_directories &#x3D; &#39;&#x2F;var&#x2F;run&#x2F;postgresql&#39; # comma-separated list of directories # (change requires restart)#unix_socket_group &#x3D; &#39;&#39; # (change requires restart)#unix_socket_permissions &#x3D; 0777 # begin with 0 to use octal notation # (change requires restart)#bonjour &#x3D; off # advertise server via Bonjour # (change requires restart)#bonjour_name &#x3D; &#39;&#39; # defaults to the computer name # (change requires restart)# - TCP Keepalives -# see &quot;man 7 tcp&quot; for details#tcp_keepalives_idle &#x3D; 0 # TCP_KEEPIDLE, in seconds; # 0 selects the system default#tcp_keepalives_interval &#x3D; 0 # TCP_KEEPINTVL, in seconds; # 0 selects the system default#tcp_keepalives_count &#x3D; 0 # TCP_KEEPCNT; # 0 selects the system default# - Authentication -#authentication_timeout &#x3D; 1min # 1s-600s#password_encryption &#x3D; md5 # md5 or scram-sha-256#db_user_namespace &#x3D; off# GSSAPI using Kerberos#krb_server_keyfile &#x3D; &#39;&#39;#krb_caseins_users &#x3D; off# - SSL -#ssl &#x3D; off#ssl_ca_file &#x3D; &#39;&#39;#ssl_cert_file &#x3D; &#39;server.crt&#39;#ssl_crl_file &#x3D; &#39;&#39;#ssl_key_file &#x3D; &#39;server.key&#39;#ssl_ciphers &#x3D; &#39;HIGH:MEDIUM:+3DES:!aNULL&#39; # allowed SSL ciphers#ssl_prefer_server_ciphers &#x3D; on#ssl_ecdh_curve &#x3D; &#39;prime256v1&#39;#ssl_dh_params_file &#x3D; &#39;&#39;#ssl_passphrase_command &#x3D; &#39;&#39;#ssl_passphrase_command_supports_reload &#x3D; off#------------------------------------------------------------------------------# RESOURCE USAGE (except WAL)#------------------------------------------------------------------------------# - Memory -shared_buffers &#x3D; 128MB # min 128kB # (change requires restart)#huge_pages &#x3D; try # on, off, or try # (change requires restart)#temp_buffers &#x3D; 8MB # min 800kB#max_prepared_transactions &#x3D; 0 # zero disables the feature # (change requires restart)# Caution: it is not advisable to set max_prepared_transactions nonzero unless# you actively intend to use prepared transactions.#work_mem &#x3D; 4MB # min 64kB#maintenance_work_mem &#x3D; 64MB # min 1MB#autovacuum_work_mem &#x3D; -1 # min 1MB, or -1 to use maintenance_work_mem#max_stack_depth &#x3D; 2MB # min 100kBdynamic_shared_memory_type &#x3D; posix # the default is the first option # supported by the operating system: # posix # sysv # windows # mmap # use none to disable dynamic shared memory # (change requires restart)# - Disk -#temp_file_limit &#x3D; -1 # limits per-process temp file space # in kB, or -1 for no limit# - Kernel Resources -#max_files_per_process &#x3D; 1000 # min 25 # (change requires restart)# - Cost-Based Vacuum Delay -#vacuum_cost_delay &#x3D; 0 # 0-100 milliseconds#vacuum_cost_page_hit &#x3D; 1 # 0-10000 credits#vacuum_cost_page_miss &#x3D; 10 # 0-10000 credits#vacuum_cost_page_dirty &#x3D; 20 # 0-10000 credits#vacuum_cost_limit &#x3D; 200 # 1-10000 credits# - Background Writer -#bgwriter_delay &#x3D; 200ms # 10-10000ms between rounds#bgwriter_lru_maxpages &#x3D; 100 # max buffers written&#x2F;round, 0 disables#bgwriter_lru_multiplier &#x3D; 2.0 # 0-10.0 multiplier on buffers scanned&#x2F;round#bgwriter_flush_after &#x3D; 512kB # measured in pages, 0 disables# - Asynchronous Behavior -#effective_io_concurrency &#x3D; 1 # 1-1000; 0 disables prefetching#max_worker_processes &#x3D; 8 # (change requires restart)#max_parallel_maintenance_workers &#x3D; 2 # taken from max_parallel_workers#max_parallel_workers_per_gather &#x3D; 2 # taken from max_parallel_workers#parallel_leader_participation &#x3D; on#max_parallel_workers &#x3D; 8 # maximum number of max_worker_processes that # can be used in parallel operations#old_snapshot_threshold &#x3D; -1 # 1min-60d; -1 disables; 0 is immediate # (change requires restart)#backend_flush_after &#x3D; 0 # measured in pages, 0 disables#------------------------------------------------------------------------------# WRITE-AHEAD LOG#------------------------------------------------------------------------------# - Settings -wal_level &#x3D; replica # minimal, replica, or logical # (change requires restart)#fsync &#x3D; on # flush data to disk for crash safety # (turning this off can cause # unrecoverable data corruption)#synchronous_commit &#x3D; on # synchronization level; # off, local, remote_write, remote_apply, or on#wal_sync_method &#x3D; fsync # the default is the first option # supported by the operating system: # open_datasync # fdatasync (default on Linux) # fsync # fsync_writethrough # open_sync#full_page_writes &#x3D; on # recover from partial page writes#wal_compression &#x3D; off # enable compression of full-page writes#wal_log_hints &#x3D; off # also do full page writes of non-critical updates # (change requires restart)#wal_buffers &#x3D; -1 # min 32kB, -1 sets based on shared_buffers # (change requires restart)#wal_writer_delay &#x3D; 200ms # 1-10000 milliseconds#wal_writer_flush_after &#x3D; 1MB # measured in pages, 0 disables#commit_delay &#x3D; 0 # range 0-100000, in microseconds#commit_siblings &#x3D; 5 # range 1-1000# - Checkpoints -#checkpoint_timeout &#x3D; 5min # range 30s-1dmax_wal_size &#x3D; 1GBmin_wal_size &#x3D; 80MB#checkpoint_completion_target &#x3D; 0.5 # checkpoint target duration, 0.0 - 1.0#checkpoint_flush_after &#x3D; 256kB # measured in pages, 0 disables#checkpoint_warning &#x3D; 30s # 0 disables# - Archiving -archive_mode &#x3D; on # enables archiving; off, on, or always # (change requires restart)archive_command &#x3D; &#39;&#x2F;bin&#x2F;date&#39; # command to use to archive a logfile segment # placeholders: %p &#x3D; path of file to archive # %f &#x3D; file name only # e.g. &#39;test ! -f &#x2F;mnt&#x2F;server&#x2F;archivedir&#x2F;%f &amp;&amp; cp %p &#x2F;mnt&#x2F;server&#x2F;archivedir&#x2F;%f&#39;#archive_timeout &#x3D; 0 # force a logfile segment switch after this # number of seconds; 0 disables#------------------------------------------------------------------------------# REPLICATION#------------------------------------------------------------------------------# - Sending Servers -# Set these on the master and on any standby that will send replication data.max_wal_senders &#x3D; 32 # 这个设置了可以最多有几个流复制连接，差不多有几个从，就设置几个 # (change requires restart)wal_keep_segments &#x3D; 64 # 设置流复制保留的最多的xlog数目，一份是 16M，注意机器磁盘 16M*64 &#x3D; 1Gwal_sender_timeout &#x3D; 60s # 设置流复制主机发送数据的超时时间#max_replication_slots &#x3D; 10 # max number of replication slots # (change requires restart)#track_commit_timestamp &#x3D; off # collect timestamp of transaction commit # (change requires restart)# - Master Server -# These settings are ignored on a standby server.#synchronous_standby_names &#x3D; &#39;&#39; # standby servers that provide sync rep # method to choose sync standbys, number of sync standbys, # and comma-separated list of application_name # from standby(s); &#39;*&#39; &#x3D; all#vacuum_defer_cleanup_age &#x3D; 0 # number of xacts by which cleanup is delayed# - Standby Servers -# These settings are ignored on a master server.#hot_standby &#x3D; on # &quot;off&quot; disallows queries during recovery # (change requires restart)#max_standby_archive_delay &#x3D; 30s # max delay before canceling queries # when reading WAL from archive; # -1 allows indefinite delay#max_standby_streaming_delay &#x3D; 30s # max delay before canceling queries # when reading streaming WAL; # -1 allows indefinite delay#wal_receiver_status_interval &#x3D; 10s # send replies at least this often # 0 disables#hot_standby_feedback &#x3D; off # send info from standby to prevent # query conflicts#wal_receiver_timeout &#x3D; 60s # time that receiver waits for # communication from master # in milliseconds; 0 disables#wal_retrieve_retry_interval &#x3D; 5s # time to wait before retrying to # retrieve WAL after a failed attempt# - Subscribers -# These settings are ignored on a publisher.#max_logical_replication_workers &#x3D; 4 # taken from max_worker_processes # (change requires restart)#max_sync_workers_per_subscription &#x3D; 2 # taken from max_logical_replication_workers#------------------------------------------------------------------------------# QUERY TUNING#------------------------------------------------------------------------------# - Planner Method Configuration -#enable_bitmapscan &#x3D; on#enable_hashagg &#x3D; on#enable_hashjoin &#x3D; on#enable_indexscan &#x3D; on#enable_indexonlyscan &#x3D; on#enable_material &#x3D; on#enable_mergejoin &#x3D; on#enable_nestloop &#x3D; on#enable_parallel_append &#x3D; on#enable_seqscan &#x3D; on#enable_sort &#x3D; on#enable_tidscan &#x3D; on#enable_partitionwise_join &#x3D; off#enable_partitionwise_aggregate &#x3D; off#enable_parallel_hash &#x3D; on#enable_partition_pruning &#x3D; on# - Planner Cost Constants -#seq_page_cost &#x3D; 1.0 # measured on an arbitrary scale#random_page_cost &#x3D; 4.0 # same scale as above#cpu_tuple_cost &#x3D; 0.01 # same scale as above#cpu_index_tuple_cost &#x3D; 0.005 # same scale as above#cpu_operator_cost &#x3D; 0.0025 # same scale as above#parallel_tuple_cost &#x3D; 0.1 # same scale as above#parallel_setup_cost &#x3D; 1000.0 # same scale as above#jit_above_cost &#x3D; 100000 # perform JIT compilation if available # and query more expensive than this; # -1 disables#jit_inline_above_cost &#x3D; 500000 # inline small functions if query is # more expensive than this; -1 disables#jit_optimize_above_cost &#x3D; 500000 # use expensive JIT optimizations if # query is more expensive than this; # -1 disables#min_parallel_table_scan_size &#x3D; 8MB#min_parallel_index_scan_size &#x3D; 512kB#effective_cache_size &#x3D; 4GB# - Genetic Query Optimizer -#geqo &#x3D; on#geqo_threshold &#x3D; 12#geqo_effort &#x3D; 5 # range 1-10#geqo_pool_size &#x3D; 0 # selects default based on effort#geqo_generations &#x3D; 0 # selects default based on effort#geqo_selection_bias &#x3D; 2.0 # range 1.5-2.0#geqo_seed &#x3D; 0.0 # range 0.0-1.0# - Other Planner Options -#default_statistics_target &#x3D; 100 # range 1-10000#constraint_exclusion &#x3D; partition # on, off, or partition#cursor_tuple_fraction &#x3D; 0.1 # range 0.0-1.0#from_collapse_limit &#x3D; 8#join_collapse_limit &#x3D; 8 # 1 disables collapsing of explicit # JOIN clauses#force_parallel_mode &#x3D; off#jit &#x3D; off # allow JIT compilation#------------------------------------------------------------------------------# REPORTING AND LOGGING#------------------------------------------------------------------------------# - Where to Log -#log_destination &#x3D; &#39;stderr&#39; # Valid values are combinations of # stderr, csvlog, syslog, and eventlog, # depending on platform. csvlog # requires logging_collector to be on.# This is used when logging to stderr:#logging_collector &#x3D; off # Enable capturing of stderr and csvlog # into log files. Required to be on for # csvlogs. # (change requires restart)# These are only used if logging_collector is on:#log_directory &#x3D; &#39;log&#39; # directory where log files are written, # can be absolute or relative to PGDATA#log_filename &#x3D; &#39;postgresql-%Y-%m-%d_%H%M%S.log&#39; # log file name pattern, # can include strftime() escapes#log_file_mode &#x3D; 0600 # creation mode for log files, # begin with 0 to use octal notation#log_truncate_on_rotation &#x3D; off # If on, an existing log file with the # same name as the new log file will be # truncated rather than appended to. # But such truncation only occurs on # time-driven rotation, not on restarts # or size-driven rotation. Default is # off, meaning append to existing files # in all cases.#log_rotation_age &#x3D; 1d # Automatic rotation of logfiles will # happen after that time. 0 disables.#log_rotation_size &#x3D; 10MB # Automatic rotation of logfiles will # happen after that much log output. # 0 disables.# These are relevant when logging to syslog:#syslog_facility &#x3D; &#39;LOCAL0&#39;#syslog_ident &#x3D; &#39;postgres&#39;#syslog_sequence_numbers &#x3D; on#syslog_split_messages &#x3D; on# This is only relevant when logging to eventlog (win32):# (change requires restart)#event_source &#x3D; &#39;PostgreSQL&#39;# - When to Log -#log_min_messages &#x3D; warning # values in order of decreasing detail: # debug5 # debug4 # debug3 # debug2 # debug1 # info # notice # warning # error # log # fatal # panic#log_min_error_statement &#x3D; error # values in order of decreasing detail: # debug5 # debug4 # debug3 # debug2 # debug1 # info # notice # warning # error # log # fatal # panic (effectively off)#log_min_duration_statement &#x3D; -1 # -1 is disabled, 0 logs all statements # and their durations, &gt; 0 logs only # statements running at least this number # of milliseconds# - What to Log -#debug_print_parse &#x3D; off#debug_print_rewritten &#x3D; off#debug_print_plan &#x3D; off#debug_pretty_print &#x3D; on#log_checkpoints &#x3D; off#log_connections &#x3D; off#log_disconnections &#x3D; off#log_duration &#x3D; off#log_error_verbosity &#x3D; default # terse, default, or verbose messages#log_hostname &#x3D; off#log_line_prefix &#x3D; &#39;%m [%p] &#39; # special values: # %a &#x3D; application name # %u &#x3D; user name # %d &#x3D; database name # %r &#x3D; remote host and port # %h &#x3D; remote host # %p &#x3D; process ID # %t &#x3D; timestamp without milliseconds # %m &#x3D; timestamp with milliseconds # %n &#x3D; timestamp with milliseconds (as a Unix epoch) # %i &#x3D; command tag # %e &#x3D; SQL state # %c &#x3D; session ID # %l &#x3D; session line number # %s &#x3D; session start timestamp # %v &#x3D; virtual transaction ID # %x &#x3D; transaction ID (0 if none) # %q &#x3D; stop here in non-session # processes # %% &#x3D; &#39;%&#39; # e.g. &#39;&lt;%u%%%d&gt; &#39;#log_lock_waits &#x3D; off # log lock waits &gt;&#x3D; deadlock_timeout#log_statement &#x3D; &#39;none&#39; # none, ddl, mod, all#log_replication_commands &#x3D; off#log_temp_files &#x3D; -1 # log temporary files equal or larger # than the specified size in kilobytes; # -1 disables, 0 logs all temp fileslog_timezone &#x3D; &#39;UTC&#39;#------------------------------------------------------------------------------# PROCESS TITLE#------------------------------------------------------------------------------#cluster_name &#x3D; &#39;&#39; # added to process titles if nonempty # (change requires restart)#update_process_title &#x3D; on#------------------------------------------------------------------------------# STATISTICS#------------------------------------------------------------------------------# - Query and Index Statistics Collector -#track_activities &#x3D; on#track_counts &#x3D; on#track_io_timing &#x3D; off#track_functions &#x3D; none # none, pl, all#track_activity_query_size &#x3D; 1024 # (change requires restart)#stats_temp_directory &#x3D; &#39;pg_stat_tmp&#39;# - Monitoring -#log_parser_stats &#x3D; off#log_planner_stats &#x3D; off#log_executor_stats &#x3D; off#log_statement_stats &#x3D; off#------------------------------------------------------------------------------# AUTOVACUUM#------------------------------------------------------------------------------#autovacuum &#x3D; on # Enable autovacuum subprocess? &#39;on&#39; # requires track_counts to also be on.#log_autovacuum_min_duration &#x3D; -1 # -1 disables, 0 logs all actions and # their durations, &gt; 0 logs only # actions running at least this number # of milliseconds.#autovacuum_max_workers &#x3D; 3 # max number of autovacuum subprocesses # (change requires restart)#autovacuum_naptime &#x3D; 1min # time between autovacuum runs#autovacuum_vacuum_threshold &#x3D; 50 # min number of row updates before # vacuum#autovacuum_analyze_threshold &#x3D; 50 # min number of row updates before # analyze#autovacuum_vacuum_scale_factor &#x3D; 0.2 # fraction of table size before vacuum#autovacuum_analyze_scale_factor &#x3D; 0.1 # fraction of table size before analyze#autovacuum_freeze_max_age &#x3D; 200000000 # maximum XID age before forced vacuum # (change requires restart)#autovacuum_multixact_freeze_max_age &#x3D; 400000000 # maximum multixact age # before forced vacuum # (change requires restart)#autovacuum_vacuum_cost_delay &#x3D; 20ms # default vacuum cost delay for # autovacuum, in milliseconds; # -1 means use vacuum_cost_delay#autovacuum_vacuum_cost_limit &#x3D; -1 # default vacuum cost limit for # autovacuum, -1 means use # vacuum_cost_limit#------------------------------------------------------------------------------# CLIENT CONNECTION DEFAULTS#------------------------------------------------------------------------------# - Statement Behavior -#client_min_messages &#x3D; notice # values in order of decreasing detail: # debug5 # debug4 # debug3 # debug2 # debug1 # log # notice # warning # error#search_path &#x3D; &#39;&quot;$user&quot;, public&#39; # schema names#row_security &#x3D; on#default_tablespace &#x3D; &#39;&#39; # a tablespace name, &#39;&#39; uses the default#temp_tablespaces &#x3D; &#39;&#39; # a list of tablespace names, &#39;&#39; uses # only default tablespace#check_function_bodies &#x3D; on#default_transaction_isolation &#x3D; &#39;read committed&#39;#default_transaction_read_only &#x3D; off#default_transaction_deferrable &#x3D; off#session_replication_role &#x3D; &#39;origin&#39;#statement_timeout &#x3D; 0 # in milliseconds, 0 is disabled#lock_timeout &#x3D; 0 # in milliseconds, 0 is disabled#idle_in_transaction_session_timeout &#x3D; 0 # in milliseconds, 0 is disabled#vacuum_freeze_min_age &#x3D; 50000000#vacuum_freeze_table_age &#x3D; 150000000#vacuum_multixact_freeze_min_age &#x3D; 5000000#vacuum_multixact_freeze_table_age &#x3D; 150000000#vacuum_cleanup_index_scale_factor &#x3D; 0.1 # fraction of total number of tuples # before index cleanup, 0 always performs # index cleanup#bytea_output &#x3D; &#39;hex&#39; # hex, escape#xmlbinary &#x3D; &#39;base64&#39;#xmloption &#x3D; &#39;content&#39;#gin_fuzzy_search_limit &#x3D; 0#gin_pending_list_limit &#x3D; 4MB# - Locale and Formatting -datestyle &#x3D; &#39;iso, mdy&#39;#intervalstyle &#x3D; &#39;postgres&#39;timezone &#x3D; &#39;UTC&#39;#timezone_abbreviations &#x3D; &#39;Default&#39; # Select the set of available time zone # abbreviations. Currently, there are # Default # Australia (historical usage) # India # You can create your own file in # share&#x2F;timezonesets&#x2F;.#extra_float_digits &#x3D; 0 # min -15, max 3#client_encoding &#x3D; sql_ascii # actually, defaults to database # encoding# These settings are initialized by initdb, but they can be changed.lc_messages &#x3D; &#39;en_US.utf8&#39; # locale for system error message # stringslc_monetary &#x3D; &#39;en_US.utf8&#39; # locale for monetary formattinglc_numeric &#x3D; &#39;en_US.utf8&#39; # locale for number formattinglc_time &#x3D; &#39;en_US.utf8&#39; # locale for time formatting# default configuration for text searchdefault_text_search_config &#x3D; &#39;pg_catalog.english&#39;# - Shared Library Preloading -#shared_preload_libraries &#x3D; &#39;&#39; # (change requires restart)#local_preload_libraries &#x3D; &#39;&#39;#session_preload_libraries &#x3D; &#39;&#39;#jit_provider &#x3D; &#39;llvmjit&#39; # JIT library to use# - Other Defaults -#dynamic_library_path &#x3D; &#39;$libdir&#39;#------------------------------------------------------------------------------# LOCK MANAGEMENT#------------------------------------------------------------------------------#deadlock_timeout &#x3D; 1s#max_locks_per_transaction &#x3D; 64 # min 10 # (change requires restart)#max_pred_locks_per_transaction &#x3D; 64 # min 10 # (change requires restart)#max_pred_locks_per_relation &#x3D; -2 # negative values mean # (max_pred_locks_per_transaction # &#x2F; -max_pred_locks_per_relation) - 1#max_pred_locks_per_page &#x3D; 2 # min 0#------------------------------------------------------------------------------# VERSION AND PLATFORM COMPATIBILITY#------------------------------------------------------------------------------# - Previous PostgreSQL Versions -#array_nulls &#x3D; on#backslash_quote &#x3D; safe_encoding # on, off, or safe_encoding#default_with_oids &#x3D; off#escape_string_warning &#x3D; on#lo_compat_privileges &#x3D; off#operator_precedence_warning &#x3D; off#quote_all_identifiers &#x3D; off#standard_conforming_strings &#x3D; on#synchronize_seqscans &#x3D; on# - Other Platforms and Clients -#transform_null_equals &#x3D; off#------------------------------------------------------------------------------# ERROR HANDLING#------------------------------------------------------------------------------#exit_on_error &#x3D; off # terminate session on any error?#restart_after_crash &#x3D; on # reinitialize after backend crash?#data_sync_retry &#x3D; off # retry or panic on failure to fsync data?#------------------------------------------------------------------------------# CONFIG FILE INCLUDES#------------------------------------------------------------------------------# These options allow settings to be loaded from files other than the# default postgresql.conf.#include_dir &#x3D; &#39;conf.d&#39; # include files ending in &#39;.conf&#39; from # directory &#39;conf.d&#39;#include_if_exists &#x3D; &#39;exists.conf&#39; # include file only if it exists#include &#x3D; &#39;special.conf&#39; # include file#------------------------------------------------------------------------------# CUSTOMIZED OPTIONS#------------------------------------------------------------------------------# Add settings for extensions here 2.6、重启主服务器的容器1docker restart pg-master 三、从服务器3.1、拉取镜像（这里我为了加快速度是从私服拉取）1[root@node03 ~]# docker pull 192.168.3.235:30002/public/mdillon-postgis@sha256:ac5dd18c43b0ee89320764684aa5c2da5c3c2ce54cf01a6caab21f905302d1b5 3.2、创建容器1234567# 拉取镜像[root@node03 ~]# docker images|grep postgis192.168.3.235:30002/public/mdillon-postgis &lt;none&gt; b2a8fefe8a63 16 months ago 713MB#启动镜像[root@node03 ~]# docker run --name pg-slave1 --restart=always -v /data/postgresql/data:/var/lib/postgresql/data -e POSTGRES_PASSWORD=123456 -p 5432:5432 -d mdillon/postgis55339a00f8c1879b2647b73bd721197e2d82991c7b8329138e46c356100476a6 3.3、将主服务器的数据同步到从数据库1234567891011121314151617181920# 进入容器[root@node03 ~]# docker exec -it pg-slave1 bash # 切换postgres用户root@55339a00f8c1:/# # 删除默认初始化目录su postgresrm -rf /var/lib/postgresql/data/*pg_basebackup -D $PGDATA -Fp -Xs -v -P -h 192.168.3.238 -U replica -WPassword: 123456pg_basebackup: initiating base backup, waiting for checkpoint to completepg_basebackup: checkpoint completedpg_basebackup: write-ahead log start point: 0/D000028 on timeline 1pg_basebackup: starting background WAL receiverpg_basebackup: created temporary replication slot &quot;pg_basebackup_37&quot;83073/83073 kB (100%), 1/1 tablespace pg_basebackup: write-ahead log end point: 0/D000130pg_basebackup: waiting for background process to finish streaming ...pg_basebackup: base backup completed 3.4、添加恢复文件12345[root@node03 ~]# vi /data/postgresql/data/recovery.confstandby_mode = onprimary_conninfo = &#x27;host=192.168.3.236 port=5432 user=replica password=123456&#x27;recovery_target_timeline = &#x27;latest&#x27; 3.5、修改postgresql.conf12345678# 主要是有些是注释掉得需要改过来[root@node03 ~]# vi /data/postgresql/data/postgresql.confwal_level = replicamax_connections = 1000hot_standby = onmax_standby_streaming_delay = 30swal_receiver_status_interval = 10s hot_standby_feedback = on 3.6、重启容器1[root@node03 data]# docker restart pg-slave2 3.7、校验12345docker exec -it pg-master bashsu postgrespsqlselect client_addr,sync_state from pg_stat_replication;不为空创建成功 hot_standby 新步骤 sha256:2243a2cf02227d5c96121fcb5be64b778945b6b9261eed6b64eb5f00e0350c65[root@k8s-node01 ~]# docker run –name pg-master –restart=always -v /data/postgresql/data:/var/lib/postgresql/data -e POSTGRES_PASSWORD=123456 -p 5432:5432 -d mdillon/postgisdocker: Error response from daemon: No command specified.See ‘docker run –help’. 建议放开日志 logging_collector = on","categories":[],"tags":[]},{"title":"docker_deploy_hive","slug":"docker-deploy-hive","date":"2024-11-03T02:26:29.000Z","updated":"2024-11-17T01:55:51.617Z","comments":true,"path":"98f8830a.html","link":"","permalink":"https://imlike.cc/98f8830a.html","excerpt":"","text":"https://hshirodkar.medium.com/apache-hive-on-docker-4d7280ac6f8e","categories":[],"tags":[]},{"title":"缓存使用和角色定位","slug":"缓存使用和角色定位","date":"2024-08-03T05:52:37.000Z","updated":"2024-11-17T01:55:51.618Z","comments":true,"path":"e9b39bb2.html","link":"","permalink":"https://imlike.cc/e9b39bb2.html","excerpt":"","text":"https://x.com/bytebytego/status/1756559741004997046/photo/1 Read Strategies: Cache aside z Read through 分别指缓存未命中时主动加载数据到缓存，以及直接通过缓存层读取数据。 Write Strategies: Write around Write back Write through 写策略中的 “Write around” 指直接写入数据库而非缓存，以减少缓存无效化的开销；“Write back” 指先写入缓存后定期写入数据库，以提高性能；“Write through” 指同时写入缓存和数据库，以保证数据的一致性。 不同的缓存策略可以根据实际需求组合使用，以达到最佳的性能和数据一致性平衡。","categories":[],"tags":[]},{"title":"记录这段时间账号异常","slug":"记录这段时间账号异常","date":"2024-08-03T00:44:45.000Z","updated":"2024-11-17T01:55:51.618Z","comments":true,"path":"c65c8915.html","link":"","permalink":"https://imlike.cc/c65c8915.html","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Mybatis对应typehandler的操作","slug":"mybtatis对应typehandler的操作","date":"2024-07-14T06:19:23.000Z","updated":"2024-07-14T07:01:24.569Z","comments":true,"path":"a29432cf.html","link":"","permalink":"https://imlike.cc/a29432cf.html","excerpt":"","text":"Mybatis在操作 非基础的 Java 对象的时候提供 Type handle 转换类似于字符串转换成 Java 对象,类似 JPA 的 ORM TypeHandler 是 MyBatis 中用于处理 Java 类型和数据库类型之间转换的接口。通过自定义 TypeHandler，可以实现特定类型的转换逻辑，从而使得 MyBatis 在处理复杂类型时更加灵活。 TypeHandler 是 MyBatis 中用于处理 Java 类型和数据库类型之间转换的接口。通过自定义 TypeHandler，可以实现特定类型的转换逻辑，从而使得 MyBatis 在处理复杂类型时更加灵活。 如何自定义 TypeHandler创建自定义 TypeHandler 的步骤如下： 实现 TypeHandler 接口：首先创建一个类并实现 org.apache.ibatis.type.TypeHandler 接口。 重写接口方法：你需要实现四个方法：setParameter、getResult (有三个重载形式)。 注册 TypeHandler：在 MyBatis 配置文件中注册自定义的 TypeHandler，或者在 Mapper 文件中指定使用该 TypeHandler。 示例代码下面是一个简单的示例，展示如何将字符串类型转换为自定义的 Java 对象。 12345678910111213141516171819202122232425public class CustomTypeHandler implements TypeHandler&lt;CustomType&gt; &#123; @Override public void setParameter(PreparedStatement ps, int i, CustomType parameter, JdbcType jdbcType) throws SQLException &#123; ps.setString(i, parameter.toString()); &#125; @Override public CustomType getResult(ResultSet rs, String columnName) throws SQLException &#123; String columnValue = rs.getString(columnName); return new CustomType(columnValue); &#125; @Override public CustomType getResult(ResultSet rs, int columnIndex) throws SQLException &#123; String columnValue = rs.getString(columnIndex); return new CustomType(columnValue); &#125; @Override public CustomType getResult(CallableStatement cs, int columnIndex) throws SQLException &#123; String columnValue = cs.getString(columnIndex); return new CustomType(columnValue); &#125;&#125; 在 MyBatis 配置文件中注册123&lt;typeHandlers&gt; &lt;typeHandler handler=&quot;com.example.CustomTypeHandler&quot;/&gt;&lt;/typeHandlers&gt; 在 configBean下配置1通过配置注入我们设定的类型处理 通过以上步骤，你可以实现自定义的 TypeHandler，并在 MyBatis 中灵活地处理复杂类型的数据转换。 但是对于 Map&lt;K,V&gt;这种格式的数据需要单独配置后解析所有的 map 需要定义一个 private final Type MAP_TYPE; public MybatisMapTypeHandler() { ​ this.MAP_TYPE = new TypeToken&lt;Map&lt;K, V&gt;&gt;() { ​ }.getType(); } 否则建了多个 map的 typehand会报错,如果不用 private final Type MAP_TYPE; 来接可能对于复杂的的 map&lt;key,value&lt;List&gt;&gt;这种可能会解析的不一样, 这里我使用 gson 来解析.","categories":[],"tags":[]},{"title":"流式读取支持拓展","slug":"流式读取支持拓展","date":"2024-07-14T06:18:25.000Z","updated":"2024-07-14T07:00:32.359Z","comments":true,"path":"undefined.html","link":"","permalink":"https://imlike.cc/undefined.html","excerpt":"","text":"在 Java 中有 URL 这个类原生支持读取 HTTP,HTTPS,FTP 的链接数据 ，但有时候我们需要支持更多协议，比如 SFTP 或者自定义的协议。 URL 流式读取支持拓展在 Java 中有 URL 这个类原生支持读取 HTTP, HTTPS, FTP 的链接数据，但有时候我们需要支持更多协议，比如 SFTP 或者自定义的协议。 自定义协议处理器为了支持更多协议，我们可以创建自定义的 URLStreamHandler。Java 提供了 URLStreamHandler 类，我们可以通过继承这个类并实现它的抽象方法来处理自定义协议。 步骤 创建自定义的 URLStreamHandler: 继承 URLStreamHandler 类并实现 openConnection 方法。 注册自定义的 URLStreamHandler: 在系统属性中注册我们的自定义协议处理器。 使用自定义协议: 现在我们可以像使用普通的 URL 一样来使用自定义的协议。 示例代码以下是一个简单的例子，展示如何创建和使用自定义的 URLStreamHandler 来处理 SFTP 协议。 1234567891011121314151617181920212223242526272829303132import java.io.IOException;import java.net.URL;import java.net.URLConnection;import java.net.URLStreamHandler;import java.net.URLStreamHandlerFactory;public class SFTPHandler extends URLStreamHandler &#123; @Override protected URLConnection openConnection(URL url) throws IOException &#123; // 在这里实现 SFTP 连接逻辑 return new SFTPURLConnection(url); &#125; public static void main(String[] args) throws Exception &#123; // 注册自定义的 URLStreamHandler URL.setURLStreamHandlerFactory(new URLStreamHandlerFactory() &#123; @Override public URLStreamHandler createURLStreamHandler(String protocol) &#123; if (&quot;sftp&quot;.equals(protocol)) &#123; return new SFTPHandler(); &#125; return null; &#125; &#125;); // 使用自定义的 SFTP 协议 URL url = new URL(&quot;sftp://example.com/path/to/file&quot;); URLConnection connection = url.openConnection(); // 继续处理连接... &#125;&#125; 注意事项 线程安全: 确保自定义的 URLStreamHandler 是线程安全的。 错误处理: 在实现自定义协议时，处理可能出现的各种异常情况。 性能考虑: 优化连接和数据传输的性能，尤其是在处理大型文件或高并发场景时。 对于引入了 springmvc模块的服务,需要注意 tomcat 实现了这个方法需要我们处理下URL.setURLStreamHandlerFactory 通过以上步骤，我们可以方便地扩展 Java 的 URL 支持，让其能够处理更多的协议类型，满足更复杂的需求。","categories":[],"tags":[]},{"title":"对象池及其学习common-pool2","slug":"对象池及其学习common-pool2","date":"2024-07-14T06:18:05.000Z","updated":"2024-07-14T07:01:24.570Z","comments":true,"path":"d3e6d00c.html","link":"","permalink":"https://imlike.cc/d3e6d00c.html","excerpt":"","text":"关于使用 Apache 下的 commons-pools 可以实现对象的池化对于经常需要重复使用不想重复创建的对象,同时提供自动调用和维护的方法来维持对象的存活,它通过使用对象池来提高性能和资源利用率，特别是在需要频繁创建和销毁对象的场景中。 对于当前很多需要使用一些 SDK 来对接的一些服务来说就很方便对于对象存储的 SDK 来说其实对于的 SDK 提供了保活,只需要根据对接的服务来调用已经创建的客户端实例就可以,但是本次项目需要对接 SFTP,FTP的数据源,都是偏底层的服务,在使用过程中,需要使用客户端来创建 session 来进行通讯,而且创建 session 的过程。s 相对较为耗时，为了提高整体系统性能，我们需要使用对象池来管理这些客户端实例,这样一来，我们可以显著减少客户端实例的创建和销毁次数，从而提高系统的响应速度和资源利用率。 对象池的其他使用场景除了管理 SFTP、FTP 等底层服务的客户端实例，对象池在以下场景中也非常有用： 数据库连接池：通过池化数据库连接，可以减少连接建立和关闭的开销，提高数据库操作的性能。 线程池：线程池用于管理工作线程，避免频繁创建和销毁线程，从而提高并发处理能力。 缓存对象池：在高频率访问的场景中，通过池化缓存对象可以减少内存分配和垃圾回收的压力。 网络连接池：池化网络连接可以提高网络通信的效率，特别是在高并发访问的场景中。 常见问题及解决方案虽然对象池可以显著提高性能，但在使用过程中也可能遇到一些问题： 1. 对象泄漏如果某些对象没有归还到池中，就会导致对象泄漏，从而影响池的可用性。解决方法是确保在任何情况下都能正确归还对象，最好使用 try-finally 或者 try-with-resources 语句来管理对象的借用和归还。 2. 对象池配置不当不合理的池配置可能会导致资源浪费或不足。例如，池的最大对象数设置过大可能会占用过多的内存，而设置过小则可能无法满足高并发需求。解决方法是根据实际需求和性能测试结果来调整配置参数。 3. 对象的有效性池化的对象在借用和归还之间可能会失效，例如网络连接中断等。解决方法是实现对象的验证逻辑，在每次借用对象时进行有效性检查，如果对象无效则重新创建。 如何使用 commons-pool2要使用 commons-pool2，首先需要在项目中添加相关的依赖。对于使用 Maven 的项目，可以在 pom.xml 文件中添加以下依赖项： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt; &lt;version&gt;2.11.1&lt;/version&gt;&lt;/dependency&gt; 创建对象池接下来，需要创建一个对象池。以下是一个简单的示例，展示了如何创建一个字符串对象池： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import org.apache.commons.pool2.PooledObject;import org.apache.commons.pool2.PooledObjectFactory;import org.apache.commons.pool2.impl.DefaultPooledObject;import org.apache.commons.pool2.impl.GenericObjectPool;import org.apache.commons.pool2.impl.GenericObjectPoolConfig;class StringFactory implements PooledObjectFactory&lt;String&gt; &#123; @Override public PooledObject&lt;String&gt; makeObject() &#123; return new DefaultPooledObject&lt;&gt;(new String(&quot;Pooled String&quot;)); &#125; @Override public void destroyObject(PooledObject&lt;String&gt; p) &#123; // No specific destruction needed for String &#125; @Override public boolean validateObject(PooledObject&lt;String&gt; p) &#123; return p.getObject() != null; &#125; @Override public void activateObject(PooledObject&lt;String&gt; p) &#123; // No activation needed for String &#125; @Override public void passivateObject(PooledObject&lt;String&gt; p) &#123; // No passivation needed for String &#125;&#125;public class PoolExample &#123; public static void main(String[] args) throws Exception &#123; GenericObjectPoolConfig&lt;String&gt; config = new GenericObjectPoolConfig&lt;&gt;(); config.setMaxTotal(10); // 设置池中最多可存在的对象数 GenericObjectPool&lt;String&gt; pool = new GenericObjectPool&lt;&gt;(new StringFactory(), config); try &#123; String pooledString = pool.borrowObject(); System.out.println(pooledString); pool.returnObject(pooledString); &#125; finally &#123; pool.close(); &#125; &#125;&#125; 配置对象池commons-pool2 提供了多种配置选项，可以根据具体需求进行调整。例如，可以设置池中对象的最大数目、最小空闲数目、最大空闲数目、对象的最大存活时间等。 12345GenericObjectPoolConfig&lt;String&gt; config = new GenericObjectPoolConfig&lt;&gt;();config.setMaxTotal(10);config.setMinIdle(2);config.setMaxIdle(5);config.setMaxWaitMillis(3000); // 最大等待时间 处理池中的对象从对象池中借用和归还对象时，应该始终使用 borrowObject 和 returnObject 方法。这有助于确保对象的正确管理，并避免资源泄漏。 123String pooledString = pool.borrowObject();// 使用对象pool.returnObject(pooledString); 通过合理配置和使用 commons-pool2，可以显著提高应用程序的性能和资源利用率，特别是在高并发和频繁对象创建的场景中。 ftp 的操作示例: https://commons.apache.org/proper/commons-net/apidocs/org/apache/commons/net/ftp/FTPClient.html, sftp使用jsch 来链接 首先 JSch jsch = new JSch(); Session session = jsch.getSession(username, host, port);session.setPassword(password);session.setConfig(“StrictHostKeyChecking”, “no”);session.connect(); ChannelSftp channelSftp = null; Channel channel = session.openChannel(“sftp”); channel.connect();channelSftp = (ChannelSftp) channel; return channelSftp; Tips:注意事项 ftpclient的客户端底层是利用 socket 来通讯的,所以如果读取文件流需要调用 completePendingCommand() sftp我们需要池化的是 session 每次下载文件的时候需要重新创建 session.","categories":[],"tags":[]},{"title":"google算法更新","slug":"google算法更新","date":"2024-05-27T01:11:44.000Z","updated":"2024-05-27T01:12:32.813Z","comments":true,"path":"aae323f9.html","link":"","permalink":"https://imlike.cc/aae323f9.html","excerpt":"","text":"谷歌算法更新引发互联网巨变，独立网站何去何从？ - 兔哥博客 大家好，我是兔哥！最近，谷歌对搜索算法进行了一系列重大更新，这不仅可能彻底改变互联网生态，更引发了人们对网络未来的担忧。让我们来看看这次更新会带来哪些影响。 HouseFresh.com 遭重创以HouseFresh.com为例，这个专注于空气净化器评测的网站受到了毁灭性打击。曾经能带来大量流量的搜索关键词，现在却把用户引向那些不实测评的生活杂志，错误信息百出。在2023年9月和2024年3月两次算法更新后，网站日访问量从过去的成千上万锐减到仅几百人，不得不裁员维持运营。 谷歌力推 AI 概览谷歌的发言人声称，公司在改动算法前会严格测试，并提供反馈机会。但实际上，独立网站难以承受这些变化带来的冲击。上周,，谷歌CEO桑达尔·皮查伊宣布将推出”AI概览”功能，由AI直接生成搜索答案，谷歌搜索将成为”生成式AI”。 谷歌意在解决网络弊端谷歌这些变革是为了解决网络上的”搜索引擎优化”弊端。SEO技术旨在调整内容以获得更好的搜索识别，有些网站滥用SEO制作取悦算法而非用户的内容。 有用内容更新的影响为解决低质量搜索结果，谷歌在2022年推出了”有用内容更新”，并在2023年9月和2024年3月再次调整算法。据谷歌称，低质量、非原创内容在搜索中的比例下降了45%。 意外后果:知名网站流量大减然而，这些更新也带来意想不到的后果。数据显示，纽约杂志、GQ等知名网站在过去半年内，谷歌搜索流量出现32%、26%的大幅下降；Urban Dictionary、OprahDaily.com等网站流量也锐减一半以上。 Reddit等用户平台大涨与此形成鲜明对比的是，Reddit等用户内容平台从谷歌获得的流量大幅增长。Reddit最新一季度收益同比增长48%，达2.43亿美元。分析师认为，这反映了人们越来越希望从他人真实经验中获取内容。 出版商网站遭”重击”对此，专家直言不讳地表示，谷歌的算法似乎在”对出版商网站发动战争”。大量独立博客和小网站所有者反映，他们的一切都”被完全抹去”。分析显示，谷歌倾斜于已建立的大型品牌网站，而远离小型独立网站。 优质内容求生存Ready Steady Cut等英国娱乐新闻网站主编表示，他们不得不大幅裁员以维持运营，原因正是谷歌算法变革导致网站流量减半。面对这种严峻形势，独立网站迫切需要更多支持和关注，优质内容才能生存下来。 谷歌辩护:打击垃圾内容 提供恢复之路一位谷歌发言人表示，公司最近的算法更新对垃圾、非原创内容造成重大打击，并密切关注滥用搜索结果的行为。在更新后，谷歌向网站所有者提供了建议，并表示有一条恢复之路。 不过，身陷困境的网站主编哈特对此深感无助。他说:”我浪费了过去8个月的时间试图遵循谷歌的建议，但一切都没有帮助。” 尽管他们是一个拥有真实第一手经验和专业背景的网站，努力还是付诸东流，这令人心碎。 AI概览真的有利于网站吗?谷歌声称，搜索结果中的 AI 概述实际上会增加流向网站的流量，并会继续努力支持发布商和创作者。但这一说法缺乏数据支持，业内担心会产生相反效果。 网站所有者贝瑞担心，如果谷歌AI直接回答问题，用户可能会中止搜索，导致流量进一步下降。她说AI给出的”肤浚且不准确的答案”可能无法满足用户需求。 旅行作家莱特的文章也遭到AI答案的替代，且AI给出的信息存在严重错误。尽管链接被保留，但被深埋在底部，很难获得足够曝光。 内容或遭非法抓取?谷歌承认AI工具可能给出不准确信息，但表示会持续改进。不过，一旦AI模型抓取了网站内容，可能无法将这些数据彻底删除，引发版权争议。 反垄断诉讼待决除了网站所有者的质疑，谷歌同时还面临多起反垄断诉讼，控诉其在搜索引擎行业滥用垄断地位。如果输掉官司，谷歌可能面临高额罚款甚至被迫分拆。 独立网站仍需关注面对重重阻力，独立网站和内容创作者的处境艰难。HouseFresh创始人纳瓦罗感叹：”也许认为我们制作优质内容就能获得成功是太天真了。” 纳瓦罗甚至考虑过完全离开在线世界，转而在现实中开个小店经营。这反映出独立网站在如今的互联网生态中求生存的严峻形势。 因此，这些独立网站迫切需要更多支持和关注。只有让更多优质内容被公平对待，才能让互联网生态保持活力，继续为用户提供有价值的内容。","categories":[],"tags":[]},{"title":"MacBook电池管理建议(转载)","slug":"MacBook电池管理建议","date":"2024-05-21T10:37:58.000Z","updated":"2024-05-21T10:39:03.193Z","comments":true,"path":"c1b88394.html","link":"","permalink":"https://imlike.cc/c1b88394.html","excerpt":"","text":"转载链接 10个实用技巧让MacBook电池寿命更持久 - 兔哥博客 无论你是MacBook新手还是老手，保养好电池都是延长笔记本使用寿命的关键。今天和大家分享10个实用小技巧，助你的MacBook电池在未来数年内持续保持优秀的电力续航。 首先，我们要明确两个关于可充电池的重要概念： 电池续航时间：指单次充电后，设备可使用的时间长度。 电池寿命：指电池在被判废报废并需要更换之前，可使用的年限或电池循环周期数。比如，如果你的MacBook电池循环数已超过1000周期，每次充电就会明显感到续航时间变短。 无论你如何使用，电池终归有其寿命限制。不过，只要遵循一些简单的使用习惯，就能最大程度延长电池的使用寿命，让你的MacBook在未来数年内持续保持出色的电力续航表现。 1. 不要关闭优化电池充电 “优化电池充电”功能在macOS Big Sur及更高版本的新Mac上是默认开启的。它会根据你的充电和使用习惯，自动采取以下措施来延长电池寿命： 只在你需要使用MacBook时,才将电池充至100% 当笔记本插上电源时，只将电池充至80% 即使插了电源，也会先将电池放电至90%左右，再重新充电 除非你有特别的理由，否则请不要在”系统设置” &gt; “电池” &gt; “电池健康状况”选项中关闭这个功能。 2. 使用苹果原装或优质充电器 偶尔使用其他品牌的充电器暂时应急倒无大碍，但日常使用时，你仍然应尽量选用MacBook出厂时随附的原装苹果充电器，或其他同等质量的充电设备。 3. 避免快速充电 MacBook Air (M1和M2型号)出厂时随机都是30W或35W的电源适配器，你可以使用MacBook Pro的60W+适配器为其快速充电。不过出于保护电池的考虑，还是应当节制使用快充，只在确有需求时方可这么做。 4. 防止MacBook暴露在极热或极冷环境中 直射阳光暴晒或将电脑放置在阳光直射的车内，都是锂电池最大的”克星”。同理，极度低温环境同样应当避免。所以，无论炎热还是严寒，都要注意给MacBook一个适宜的使用环境。 5. 不要长期将MacBook插在电源上 将MacBook长期保持插电状态也会加速电池老化。所以，我们应当在插电和电池供电之间适当切换使用。 6. 避免电池充满电或电量过低 努力不让MacBook电池充至100%的确挺难，因为即便你在桌前短暂使用了一两个小时，插着电源的笔记本很快就会充至满电状态。与此同时，MacBook目前还无法像iPhone 15那样，设置仅充电至80%就停止充电。 所以我们要尽量避免将电池电量消耗到极低，比如5%或10%以下。当电量降至30-40%时，就应该考虑插上电源给笔记本充电。 7. 给其他设备充电时也为MacBook同步充电 偶尔将MacBook当做移动电源为iPhone、AirPods等其他设备充电倒无大碍。但如果你经常这么做，反而会加速MacBook电池的老化。所以，还是另外准备一个专门的移动电源更加合适。 如果你实在需要利用MacBook为其他设备充电，请务必同时也为笔记本本身插上电源。这样一来，你的MacBook电池就不会因为给其他设备充电而剧烈消耗。 8. 优化MacBook内部设置 过于频繁地将MacBook电量消耗殆尽后再充电，会加快电池循环周期的耗尽。为此，你应当采取一些措施，比如关闭无用的应用程序和后台进程，以延长单次充电的使用时间。这样一来，你就无需频繁插电为笔记本充电，从而延长了电池的整体使用寿命。 9. 保持系统软件的更新 为了确保设备和电池的良好运作，苹果会不时进行系统优化和改进。比如最近，他们就发布了一个iOS更新，将iPhone 15的电池循环寿命延长了一倍。 同理，Mac系统也会在后台进行电池相关的优化升级。所以，你应当及时将MacBook升级至最新的macOS系统版本，以获得这些改进。 10. 长期存放前将电量控制在50%左右 如果你计划一段时间内不使用MacBook，那么最好在电量约50%的状态下，将电脑存放在阴凉、避潮的环境中。切记不要在电量满电或完全耗尽的状态下长期存放，那样会严重损害电池寿命。 如果你需要存放MacBook超过6个月的时间，那么应当每6个月为电池充电一次至50%左右的电量，以保证电池的健康状态。 总的来说，正确使用和保养MacBook电池，可以最大限度延长电池的使用周期和寿命。通过上述10个技巧，相信你一定能享受到笔记本出色的电力续航，助力高效工作和生活，绝不会被”没电”这个小问题所打扰。这不仅环保节约，更能让你的MacBook电脑陪伴左右，高性价比地服务更多年份。","categories":[],"tags":[]},{"title":"使用 cloudflare对于 WARP订阅","slug":"启动","date":"2024-05-02T04:20:01.000Z","updated":"2024-05-02T05:05:51.705Z","comments":true,"path":"undefined.html","link":"","permalink":"https://imlike.cc/undefined.html","excerpt":"","text":"vvbbnn00/WARP-Clash-API: 该项目可以让你通过订阅的方式使用Cloudflare WARP+，自动获取流量。. cmliu/edgetunnel: 在原版的基础上修改了显示 VLESS 配置信息转换为订阅内容。使用该脚本，你可以方便地将 VLESS 配置信息使用在线配置转换到 Clash 或 Singbox 等工具中。 docker-compose up -d 下面是安装 docker-compose 卸载旧版本的 Docker Compose 1sudo rm &#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-compose 下载最新版本 目前最新版本是 2.18.1 (截至 2023 年 5 月),下载命令如下: 1sudo curl -L &quot;https:&#x2F;&#x2F;github.com&#x2F;docker&#x2F;compose&#x2F;releases&#x2F;download&#x2F;v2.18.1&#x2F;docker-compose-$(uname -s)-$(uname -m)&quot; -o &#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-compose 如果速度起不来可以使用这个镜像 sudo curl -L “https://dl.ghpig.top/https://github.com/docker/compose/releases/download/v2.18.1/docker-compose-$(uname -s)-$(uname -m)” -o /usr/local/bin/docker-compose 设置执行权限 1sudo chmod +x &#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-compose 创建软链接 (可选) 1sudo ln -s &#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-compose &#x2F;usr&#x2F;bin&#x2F;docker-compose 验证安装 1docker-compose --version 它应该显示 docker-compose version v2.18.1 注意以下几点: 需要有 root 或 sudo 权限执行上述命令 上面的下载链接会自动根据您的系统架构获取合适的二进制文件 如果下载链接发生改变,可访问 https://github.com/docker/compose/releases 获取最新下载链接 Docker Compose 2.x 版本需要 Docker 17.12.0 或更高版本 按照以上步骤,您就可以成功将 Docker Compose 升级到最新的 2.18.1 版本了。升级后最好查看一下新版本的文档,了解新增的语法和功能变化。","categories":[],"tags":[]},{"title":"macbook软件配置(新手项)","slug":"macbook软件配置","date":"2024-03-23T05:29:26.000Z","updated":"2024-05-18T02:19:51.123Z","comments":true,"path":"58edfabc.html","link":"","permalink":"https://imlike.cc/58edfabc.html","excerpt":"","text":"配置相关软件推荐Maccy 一款简单易用的mac记事本软件PixPin 一款截图软件可以 OCR 图片超级右键 修改右键的软件Termius ssh的链接工具有付费版","categories":[],"tags":[]},{"title":"sublime优化和推荐插件","slug":"sublime优化和推荐插件","date":"2024-03-14T14:55:29.481Z","updated":"2024-03-14T14:55:29.481Z","comments":true,"path":"bcb72f0.html","link":"","permalink":"https://imlike.cc/bcb72f0.html","excerpt":"","text":"参考 链接 https://segmentfault.com/a/1190000022501113 ctrol + shift +p package control install package 安装后出","categories":[],"tags":[{"name":"sublime","slug":"sublime","permalink":"https://imlike.cc/tags/sublime/"}]},{"title":"Runnable、Future 和 RunnableFuture","slug":"RunnableFuture","date":"2023-12-15T09:17:41.000Z","updated":"2024-03-14T14:55:29.474Z","comments":true,"path":"undefined.html","link":"","permalink":"https://imlike.cc/undefined.html","excerpt":"","text":"Runnable 定义：Runnable 是 Java 中表示可执行任务的接口，通常用于执行没有返回值的任务。 特点：只有一个 run() 方法，无法获取任务执行结果。 Future 定义：Future 是 Java 中表示异步计算结果的接口，用于表示一个可能还没有完成的计算。 特点 ： 提供 get() 方法来获取计算结果，该方法可能会阻塞直到计算完成。 通过 isDone() 方法可以检查计算是否完成。 提供 cancel() 方法来取消计算。 RunnableFuture 定义：RunnableFuture 是继承自 Runnable 和 Future 的接口，用于表示一个可运行的、有结果的任务。 特点：兼具了 Runnable 和 Future 的特性，既可以作为任务执行，又可以获取任务执行结果。 区别和优缺点 Runnable：用于表示没有返回值的任务，适用于简单的并发场景。 Future：用于表示异步计算的结果，可获取计算结果和取消计算。 RunnableFuture：兼具 Runnable 和 Future 的特性，适用于希望任务能够被取消、并且能够返回结果的场景。 常见使用场景 Runnable：适用于执行简单的、无返回值的任务。 Future：适用于需要获取异步计算结果的场景。 RunnableFuture：适用于希望任务能够被取消、并且能够返回结果的场景。 注意事项 对于使用 Future 的场景，需要注意在获取计算结果时可能会阻塞，需要谨慎使用。 这些接口和类提供了在多线程环境中进行任务调度和管理的基本工具，根据具体需求选择合适的接口和实现类可以更好地满足不同的并发场景。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281import com.google.common.util.concurrent.ThreadFactoryBuilder;import org.junit.Test;import java.util.ArrayList;import java.util.Arrays;import java.util.List;import java.util.concurrent.*;&#x2F;** * author: leek * dataTime: 2023年12月15日 下午4:16 *&#x2F;public class TaskFeatureOverview &#123; static ExecutorService executorService &#x3D; new ThreadPoolExecutor( 11, 33, 2L, TimeUnit.MINUTES, new ArrayBlockingQueue&lt;Runnable&gt;(11), new ThreadFactoryBuilder().build()); static Executor executor &#x3D; new ThreadPoolExecutor( 11, 33, 2L, TimeUnit.MINUTES, new ArrayBlockingQueue&lt;Runnable&gt;(22), new ThreadFactoryBuilder().build()); @Test public void t1_runable() throws InterruptedException &#123; List&lt;String&gt; strings &#x3D; new ArrayList&lt;&gt;(Arrays.asList(&quot;dd&quot;, &quot;aa&quot;, &quot;ee&quot;)); CountDownLatch count &#x3D; new CountDownLatch(1); ((Runnable) () -&gt; &#123; System.out.println(&quot;匿名内部类&quot;); System.out.println(strings.toString()); strings.add(&quot;ww&quot;); &#125;).run(); executorService.execute(() -&gt; &#123; System.out.println(&quot;匿名内部类+++&quot;); System.out.println(strings.toString()); strings.add(&quot;qq&quot;); count.countDown(); &#125;); System.out.println(strings.toString()); count.await(); System.out.println(strings.toString()); &#125; @Test public void t1_callable_sync() throws Exception &#123; List&lt;String&gt; strings &#x3D; new ArrayList&lt;&gt;(Arrays.asList(&quot;dd&quot;, &quot;aa&quot;, &quot;ee&quot;)); CountDownLatch count &#x3D; new CountDownLatch(1); Future&lt;Object&gt; future &#x3D; executorService.submit((Callable&lt;Object&gt;) () -&gt; &#123; strings.add(&quot;ww&quot;); Thread.sleep(1000); count.countDown(); return strings.size(); &#125;); &#x2F;&#x2F; 阻塞等待数据 System.out.println(&quot;第一次获取:&quot; + future.get()); count.await(); &#x2F;&#x2F; 阻塞等待数据 System.out.println(&quot;第二次获取:&quot; + future.get()); &#125; @Test public void t1_callable_async() throws Exception &#123; List&lt;String&gt; strings &#x3D; new ArrayList&lt;&gt;(Arrays.asList(&quot;dd&quot;, &quot;aa&quot;, &quot;ee&quot;)); CountDownLatch count &#x3D; new CountDownLatch(1); Future&lt;Object&gt; future &#x3D; executorService.submit((Callable&lt;Object&gt;) () -&gt; &#123; strings.add(&quot;ww&quot;); Thread.sleep(1000); count.countDown(); return strings.size(); &#125;); System.out.println(&quot;第一次获取:&quot; + future.get()); count.await(); System.out.println(&quot;第二次获取:&quot; + future.get()); &#125; @Test public void t1_future() &#123; Class&lt;CompletableFuture&gt; completableFutureClass &#x3D; CompletableFuture.class; &#125; @Test public void t1_RunnableFuture() &#123; Class&lt;FutureTask&gt; futureTaskClass &#x3D; FutureTask.class; &#125; @Test public void t1_Sick_skills() &#123; List&lt;Object&gt; list &#x3D; new ArrayList&lt;&gt;(); list.stream().forEach(id -&gt; CompletableFuture.supplyAsync(() -&gt; &#123; return id; &#125;).whenComplete((s, e) -&gt; &#123; &#125;)); CompletableFuture[] completableFutures &#x3D; list.stream().map(id -&gt; CompletableFuture.supplyAsync(() -&gt; &#123; return id; &#125;).whenComplete((s, e) -&gt; &#123; &#125;)).toArray(CompletableFuture[]::new); CompletableFuture.allOf(completableFutures).join(); &#125; &#x2F;** * @Async * public Future&lt;String&gt; getValue() &#123; * &#x2F;&#x2F; ... * return result; * &#125; * 获取future后异步调用: * * java * * Copy code * * Future&lt;String&gt; future &#x3D; getValue(); * future.get() &#x2F;&#x2F;异步等待结果* *&#x2F;&#125;package com.lee.demo.thread;&#x2F;** * @author simba@onlying.cn * @date 2023年12月15日 下午4:16 *&#x2F;import com.google.common.util.concurrent.ThreadFactoryBuilder;import org.junit.Test;import java.util.ArrayList;import java.util.Arrays;import java.util.List;import java.util.concurrent.*;&#x2F;** * author: leek * dataTime: 2023年12月15日 下午4:16 *&#x2F;public class TaskFeatureOverview &#123; static ExecutorService executorService &#x3D; new ThreadPoolExecutor( 11, 33, 2L, TimeUnit.MINUTES, new ArrayBlockingQueue&lt;Runnable&gt;(11), new ThreadFactoryBuilder().build()); static Executor executor &#x3D; new ThreadPoolExecutor( 11, 33, 2L, TimeUnit.MINUTES, new ArrayBlockingQueue&lt;Runnable&gt;(22), new ThreadFactoryBuilder().build()); @Test public void t1_runable() throws InterruptedException &#123; List&lt;String&gt; strings &#x3D; new ArrayList&lt;&gt;(Arrays.asList(&quot;dd&quot;, &quot;aa&quot;, &quot;ee&quot;)); CountDownLatch count &#x3D; new CountDownLatch(1); ((Runnable) () -&gt; &#123; System.out.println(&quot;匿名内部类&quot;); System.out.println(strings.toString()); strings.add(&quot;ww&quot;); &#125;).run(); executorService.execute(() -&gt; &#123; System.out.println(&quot;匿名内部类+++&quot;); System.out.println(strings.toString()); strings.add(&quot;qq&quot;); count.countDown(); &#125;); System.out.println(strings.toString()); count.await(); System.out.println(strings.toString()); &#125; @Test public void t1_callable_sync() throws Exception &#123; List&lt;String&gt; strings &#x3D; new ArrayList&lt;&gt;(Arrays.asList(&quot;dd&quot;, &quot;aa&quot;, &quot;ee&quot;)); CountDownLatch count &#x3D; new CountDownLatch(1); Future&lt;Object&gt; future &#x3D; executorService.submit((Callable&lt;Object&gt;) () -&gt; &#123; strings.add(&quot;ww&quot;); Thread.sleep(1000); count.countDown(); return strings.size(); &#125;); &#x2F;&#x2F; 阻塞等待数据 System.out.println(&quot;第一次获取:&quot; + future.get()); count.await(); &#x2F;&#x2F; 阻塞等待数据 System.out.println(&quot;第二次获取:&quot; + future.get()); &#125; @Test public void t1_callable_async() throws Exception &#123; List&lt;String&gt; strings &#x3D; new ArrayList&lt;&gt;(Arrays.asList(&quot;dd&quot;, &quot;aa&quot;, &quot;ee&quot;)); CountDownLatch count &#x3D; new CountDownLatch(1); Future&lt;Object&gt; future &#x3D; executorService.submit((Callable&lt;Object&gt;) () -&gt; &#123; strings.add(&quot;ww&quot;); Thread.sleep(1000); count.countDown(); return strings.size(); &#125;); System.out.println(&quot;第一次获取:&quot; + future.get()); count.await(); System.out.println(&quot;第二次获取:&quot; + future.get()); &#125; @Test public void t1_future() &#123; Class&lt;CompletableFuture&gt; completableFutureClass &#x3D; CompletableFuture.class; &#125; @Test public void t1_RunnableFuture() &#123; Class&lt;FutureTask&gt; futureTaskClass &#x3D; FutureTask.class; &#125; @Test public void t1_Sick_skills() &#123; List&lt;Object&gt; list &#x3D; new ArrayList&lt;&gt;(); list.stream().forEach(id -&gt; CompletableFuture.supplyAsync(() -&gt; &#123; return id; &#125;).whenComplete((s, e) -&gt; &#123; &#125;)); CompletableFuture[] completableFutures &#x3D; list.stream().map(id -&gt; CompletableFuture.supplyAsync(() -&gt; &#123; return id; &#125;).whenComplete((s, e) -&gt; &#123; &#125;)).toArray(CompletableFuture[]::new); CompletableFuture.allOf(completableFutures).join(); &#125; &#x2F;** * @Async * public Future&lt;String&gt; getValue() &#123; * &#x2F;&#x2F; ... * return result; * &#125; * 获取future后异步调用: * * java * * Copy code * * Future&lt;String&gt; future &#x3D; getValue(); * future.get() &#x2F;&#x2F;异步等待结果* *&#x2F;&#125; 参考链接:如何合理估算 Java 线程池大小：综合指南","categories":[],"tags":[]},{"title":"有趣的web","slug":"有趣的web","date":"2023-12-10T08:22:33.000Z","updated":"2024-03-14T14:55:29.484Z","comments":true,"path":"ea10c0bb.html","link":"","permalink":"https://imlike.cc/ea10c0bb.html","excerpt":"","text":"https://terminus2049.github.io/Not-exist-in-douban/ 404影音http://matrixmultiplication.xyz/ 矢量计算可视化","categories":[],"tags":[]},{"title":"stash配置和Quantumult X,suger的区别","slug":"stash配置和suger的区别","date":"2023-12-08T08:09:18.000Z","updated":"2024-03-14T14:55:29.481Z","comments":true,"path":"2f16d0ef.html","link":"","permalink":"https://imlike.cc/2f16d0ef.html","excerpt":"","text":"YouTube APP去广告 - 使用 Quantumult X / Surge/ Shadowrocket/ Loon/Stash 完美屏蔽YouTube App广告小白教程（神机规则） - 毒奶 - 欢迎使用代理访问本站。 https://github.com/deezertidal/stash-override 1234567891011121314151617181920212223242526272829303132333435363738394041#!name &#x3D; YouTube去广告#!desc &#x3D;支持pip，后台播放#!author &#x3D; Maasea #!homepage&#x3D;https:&#x2F;&#x2F;whatshub.top#!icon &#x3D; https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Koolson&#x2F;Qure&#x2F;master&#x2F;IconSet&#x2F;Color&#x2F;YouTube.pngname: YouTubequguanggaodesc: 支持pip，后台播放http: mitm: - &quot;-redirector*.googlevideo.com&quot; - &quot;*.googlevideo.com&quot; - &quot;www.youtube.com&quot; - &quot;s.youtube.com&quot; - &quot;youtubei.googleapis.com&quot; script: - match: ^https:\\&#x2F;\\&#x2F;youtubei\\.googleapis\\.com\\&#x2F;youtubei\\&#x2F;v1\\&#x2F;(browse|next|player|search|reel\\&#x2F;reel_watch_sequence|guide|account\\&#x2F;get_setting)\\? name: YouTubequguanggao1 type: response require-body: true binary-mode: true timeout: 10 - match: ^https:\\&#x2F;\\&#x2F;youtubei\\.googleapis\\.com\\&#x2F;youtubei\\&#x2F;v1\\&#x2F;(browse|next|player|reel\\&#x2F;reel_watch_sequence)\\? name: YouTubequguanggao2 type: request require-body: true binary-mode: true timeout: 10 rewrite: - (^https?:\\&#x2F;\\&#x2F;[\\w-]+\\.googlevideo\\.com\\&#x2F;(?!dclk_video_ads).+?)&amp;ctier&#x3D;L(&amp;.+?),ctier,(.+) $1$2$3 302 - ^https?:\\&#x2F;\\&#x2F;[\\w-]+\\.googlevideo\\.com\\&#x2F;(?!(dclk_video_ads|videoplayback\\?)).+&amp;oad - reject-200 - ^https?:\\&#x2F;\\&#x2F;(www|s)\\.youtube\\.com\\&#x2F;api\\&#x2F;stats\\&#x2F;ads - reject-200 - ^https?:\\&#x2F;\\&#x2F;(www|s)\\.youtube\\.com\\&#x2F;(pagead|ptracking) - reject-200 - ^https?:\\&#x2F;\\&#x2F;s\\.youtube\\.com\\&#x2F;api\\&#x2F;stats\\&#x2F;qoe\\?adcontext - reject-200script-providers: YouTubequguanggao1: url: https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Maasea&#x2F;sgmodule&#x2F;master&#x2F;Script&#x2F;Youtube&#x2F;dist&#x2F;youtube.response.beta.js interval: 86400 YouTubequguanggao2: url: https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Maasea&#x2F;sgmodule&#x2F;master&#x2F;Script&#x2F;Youtube&#x2F;dist&#x2F;youtube.request.beta.js interval: 86400","categories":[],"tags":[]},{"title":"代码复用逻辑和类命名","slug":"代码复用逻辑","date":"2023-11-20T14:57:01.000Z","updated":"2024-03-16T01:26:41.304Z","comments":true,"path":"c66fd762.html","link":"","permalink":"https://imlike.cc/c66fd762.html","excerpt":"","text":"对于静态工具类的抽象复用,可以考虑以下几点: 将公共逻辑提取出来,放到一个基类或接口中。子类只需要实现自己业务相关的逻辑。 使用工厂方法模式,定义一个工厂类,根据参数创建不同的工具类实例。调用端只需要使用工厂类,不需关心具体实现。 使用依赖注入框架,如Spring,在配置文件中定义不同的工具类bean,并注入到其他类中使用。 将静态方法改成普通方法,实例化为单例对象,注入到其他类中复用。 使用策略模式,定义不同算法的策略类,通过组合和委托的方式复用策略。 如果工具类方法数量不多,也可以简单地提取到一个公共类中,在需要使用的地方直接调用即可。 考虑使用函数式编程思想,将静态方法改写为无状态的函数,更便于复用。主要还是要根据实际情况划分出可复用的部分,然后通过封装、依赖注入、组合、委托等方式提高复用性。 常见命名的后缀 Util 通常用于表示一个实用工具类 Helper 用于表示一个帮助类，通常用于提供与特定对象或功能相关的辅助方法。Helper 类的方法可能需要依赖于特定对象的实例。 Utils 与 “Util” 类似，用于表示一组实用工具方法。它通常与多个功能相关，而不是特定于单一功能。 Toolkit 通常表示一个工具包或工具集，它包含一组相关的工具、资源或库，用于执行特定的任务或解决特定的问题。 SDK 开发规范-腾讯云开发者社区-腾讯云 通用类命名规范 - 就永远夏天 - 博客园","categories":[],"tags":[]},{"title":"appletv安装和相关软件推荐","slug":"安装和相关软件推荐","date":"2023-11-12T11:15:07.000Z","updated":"2024-03-16T01:12:54.542Z","comments":true,"path":"undefined.html","link":"","permalink":"https://imlike.cc/undefined.html","excerpt":"","text":"https://appfigures.com/top-apps/apple-tv/hong-kong/top-overall apple tv 应用排行榜 https://github.com/fanmingming/live m3u 在线地址 国际版的爱优腾iQIYI Video on the App Store WeTV Asian &amp; Local Dramas on the App Store vod类播放器Syncnext(三角播放器 需内购)Syncnext on the App Store XPTVXPTV on the App Store XPTV 是一款支持VOD、M3U、IPTV等协议的多功能播放器 CMSPlayerCMSPlayer on the App Store 音乐流媒体Musify(已下架)支持 网易云音乐 Connecting to Apple Music. Spotify - Music and PodcastsSpotify - Music and Podcasts on the App Store B站三方客户端CheersCheers - Danmu Player on the App Store Miao Project已经从应用商店下架 弹幕播放器 danmuBox弹幕播放器 danmuBox on the App Store 投屏类主要用于类似于安卓乐播投屏，目前在安卓盒子里面当贝投屏是优化最好的没有广告，appleTV 原生支持 airplay，需要额外支持DLNA。 推荐 1 DLNA 推荐 2 Unplay网上可以搜索到 UnPlay on the App Store 播放器AlplayerAlplayer on the App Store 原生支持阿里，夸克 VidHub(免费)VidHub -Video Library &amp; Player on the App Store FileballFileball on the App Store 电视直播类APTV(免费)APTV on the App Store","categories":[],"tags":[]},{"title":"空间大数据处理框架","slug":"空间大数据处理框架","date":"2023-11-07T11:38:03.000Z","updated":"2024-03-14T14:55:29.485Z","comments":true,"path":"c243b492.html","link":"","permalink":"https://imlike.cc/c243b492.html","excerpt":"","text":"大数据计算框架简介Hadoop，Spark、Flink是目前重要的三大分布式计算系统 Hadoop用于离线复杂大数据处理 Spark用于离线快速的大数据处理 Flink用于在线实时的大数据处理 1.1. HadoopHadoop是一个由Apache基金会所开发的分布式系统基础架构，主要解决海量数据的存储和分析计算问题。 广义上来说，Hadoop通常是指一个更广泛的概念——Haddop生态圈。 1) Hadoop分布式文件系统(HDFS) HDFS (Hadoop Distributed File System) ，它是一个文件系统，用来存储文件，通过目录树来定位文件，其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。 使用场景：HDFS适合一次写入，多次读出的场景，且不支持文件的修改，适合用来做数据分析，并不适合用来做网盘应用。 2) Hadoop MapReduce引擎 MapReduce是一种并行程序设计模型与方法，用于大规模数据集的并行运算。概念”Map（映射）”和”Reduce（归约）”是它们的主要思想。 1.2. SparkApache Spark是专为大规模数据处理而设计的快速通用的计算引擎。Spark是类Hadoop MapReduce的通用并行框架。Spark拥有Hadoop MapReduce所具有的优点；但不同于MapReduce的是，Job中间的输出结果可以保存在内存中，从而不再需要读写HDFS，因此Spark能更好地适用于数据挖掘与机器学习等需要迭代的MapReduce的算法。 1.3. FlinkApache Flink是一个框架和分布式处理引擎，用于对无界和有界数据流进行有状态计算。Flink 被设计在所有常见的集群环境中运行，以内存执行速度和任意规模来执行计算。 流数据：在自然环境中，数据的产生原本就是流式的。但是当你分析数据时，可以围绕 有界流（bounded）或 无界流（unbounded）两种模型来组织处理数据，当然，选择不同的模型，程序的执行和处理方式也都会不同。 离线计算的代表spark和flink的对比 在spark中，一切都是由批次组成的，离线数据是一个大批次，而实时数据是由一个一个无限的小批次组成的。而在flink中，一切都是由流组成的，离线数据是有界限的流，实时数据是一个没有界限的流。 有界流\\：****数据有时间的界限，比如我们长说某天的数据、某月的数据。 无界流：数据没有时间的界限，所处理的数据是持续不断的输入的。因此程序必须持续不断地对到达的数据进行处理。 2. Spark简介2.1. Saprk相关概念RDD（Resilient Distributed Dataset）：弹性分布式数据集，是Spark的核心部分，可以理解为一个分布式数据结构的封装，并在此基础上提供了各种数据操作的API。其中，数据操作可以分成转换（**transformation**）和行动（**action**）两类，转换构建各**RDD**之间的依赖关系，但不会实际执行，只有遇到行动操作后才会实际提交作业触发执行。RDD之间的依赖关系构成了RDD的血缘，当RDD丢失时可以通过血缘关系重新执行生成，保证了RDD的容错性。 举例： 数据集 map reduce {x: x∈N+, x&lt;10 } {x: f(x)} {x: ∑f(x)}、{x: Πf(x)}等 类别 函数名 解释 转换算子 map 将处理的数据逐条进行映射转换，这里的转换可以是类型的转换，也可以是值的转换 mapPartitions 将待处理的数据以分区为单位发送到计算节点进行处理，这里的处理是指可以进行任意的处理，包括过滤 mapPartitionsWithindex 将待处理的数据以分区为单位发送到计算节点进行处理，这里的处理是指可以进行任意的处理，包括过滤，在处理同时可以获取当前分区的索引 flatMap 将处理的数据进行扁平化后再进行映射处理，所以算子也称之为扁平映射 glom 将同一个分区的数据直接转换为相同类型的内存数组进行处理，分区不变 groupBy 将数据根据指定的规则进行分组，分区默认不变，但是数据会被打乱重新组合，我们将这样的操作称之为shuffle，极限情况下，数据可能被分到同一个分区中 filter 将数据根据执行的规则进行筛选过滤，符合规则的数据保留，不符合规则的数据丢弃。当数据进行筛选过滤后，分区不变，但是分区内的数据可能不均衡，生产环境下，可能出现数据倾斜 sample 根据指定的规则聪数据中抽取数据 coalesce 根据数据量进行缩减分区，用于大数据集过滤后，提高小数据集的执行效率，减少任务调度成本 distinct 将数据集中的重复数据进行去重 repartitions 调整分区数量 sortBy 可以根据指定的规则对数据源中的数据进行排序 intersection 对源RDD和参数RDD求交集后返回一个新的RDD union 对源RDD和参数RDD求并集后返回一个新的RDD subtract 对源RDD和参数RDD求差集后返回一个新的RDD zip 相同位置的数据拉取到一块 partitionBy 将数据按照指定Partitioner重新进行分区 reduceByKey 将相同的key进行分组，将value聚合 groupByKey 将分区的数据直接转换为相同类型的内存数组进行后续处理 aggregateByKey 将数据根据不同规则进行分区内计算和分区间计算 flodByKey foldByKey就是aggregateByKey的简化版本，当aggregateByKey的第二个参数计算逻辑相同时就可以用foldByKey combineByKey 需要三个参数 1.将相同key的第一个数据进行结构的转换，实现操作 2.分区内计算规则 3.分区间计算规则 join 在类型为(K,V)和(K,W)的RDD上调用，返回一个相同的key对应的所有元素连接在一起的（K,(v,w)）的RDD leftOuterJoin 类似于sql的左外连接 rightOuterJoin 类似于sql的右外连接 cogroup 在类型(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable,Iterable))类型的RDD … … 行动算子 reduce 聚集RDD中的所有元素，先聚合分区内数据，再聚合分区间数 collect 方法会将不同分区的数据按照分区顺序采集到Driver端内存中，形成数组 count 数据源中数据的个数 first 数据源中数据的第一个 take 获取n个数据 takeOrdered 数据排序后取n个数据 aggregate 分区的数据通过初始值和分区内的数据进行聚合，然后再和初始值进行分区间的数据聚合 fold 折叠操作,aggregate的简化版操作 countByKey 统计每种key的个数 … … Partition：数据分区，即RDD进行数据划分的单元，也是Spark数据处理的单元，任务的数量由分区数量确定，一个任务处理一个分区的数据。 Dependency：RDD之间的依赖关系。分为窄依赖（NarrowDependency）和Shuffle依赖。前者表示子RDD中的分区依赖于父RDD中固定数量的分区，包括依赖一个分区的OneToOneDependency和一定范围内多个分区的RangeDependency；后者也称为宽依赖，子RDD中的分区可能对父RDD中所有的分区产生依赖，具体依赖关系取决于分区器partitioner。 Job：Spark中的作业，对**RDD**的一个行动操作对应一个作业。每个作业包括了该RDD的行动操作以及构成当前RDD血缘关系的所有转换操作，一个Spark应用可以包含多个作业。 Stage：在每个作业中，Spark会根据**shuffle**依赖将作业划分成多个阶段。从最后一个触发行动操作的RDD开始（最后一个属于ResultStage），对RDD的依赖进行回溯，如果遇到shuffle依赖，则将shuffle之前的RDD视为一个阶段（ShuffleMapStage），shuffle之后的RDD到回溯起点之间所有的RDD视为一个阶段，接着从shuffle之前的RDD开始继续回溯，直到回溯至与外部数据源相关的RDD为止，如图1所示。 Task：每个阶段中，Spark会根据**RDD**的分区数量，创建相同数量的任务，每个任务处理一个分区的数据。其中，ShuffleMapStage中的任务为ShuffleMapTask，ResultStage中的任务为ResultTask。 下图为Spark中Stage的划分方式 2.2. Spark核心模块Spark的模块组成包括核心模块Spark Core以及建立在核心模块之上的扩展模块Spark SQL、Spark Streaming、GraphX、MLlib。Spark核心模块提供了Spark中最基础最核心的功能，包括： 基础设施：分为负责参数配置的SparkConf，负责Spark跨节点组件之间通信的基于netty实现的RPC框架，负责SparkContext内部组件间通信的事件总线ListenerBus，负责监控各个组件运行期状态的度量系统； SparkContext：作为Spark所有功能的集成者和用户程序开发的接口，其内部封装了网络通信、消息通信、存储体系、计算引擎、度量系统、WebUI等内容，这些功能对开发人员来说都是透明的； SparkEnv：是Spark执行环境，任务运行所必需的组件，由RpcEnv、序列化管理器、BroadcastManager、MapOutputTracker、存储体系、度量系统、OutputCommitCoordinator等组件组成； 存储体系：分为内存存储管理和磁盘存储管理，Spark的内存空间提供了Tungsten的实现，对堆外内存进行管理。此外，Spark的内存存储空间和计算存储空间之间的边界是“软”边界，当一方资源不够时可向另一方借用，提高了资源利用率； 调度体系：分为DAGScheduler和TaskScheduler，内置在SparkContext中。DAGScheduler负责解析RDD之间的依赖，按照shuffle操作划分阶段，根据阶段创建TaskSet，TaskScheduler负责按照FAIR或FIFO调度算法对TaskSet进行调度，为每个任务分配可执行的Executor，并将序列化后的任务发送到指定的Executor上执行； 计算引擎：由内存管理器、Tungsten、任务内存管理器、外部排序器、Shuffle管理器等组成，其中外部排序器用于在map端或者reduce端对ShuffleMapTask计算得到的中间结果进行排序、聚合等操作，Shuffle管理器用于将各个分区对应的ShuffleMapTask产生的中间结果持久化到磁盘，并在reduce端按照分区远程拉取生成的中间结果。 2.3. Spark集群架构从集群部署的角度来看，Spark集群由集群管理器（Cluster Manager）、工作节点（Worker）、执行器（Executor）、驱动器（Driver）、应用程序（Application）等部分组成。 2.3.1. Driver用于运行Spark程序的 main 方法，创建SparkContext对象 Driver在spark作业执行时主要负责： 1）启动SparkContext或SparkSession，将用户程序转化为作业（Job）； 2）在Executor之间调度任务（Task）； 3）跟踪Executor的执行情况； 4）通过UI展示查询运行情况。 2.3.2. Cluster Manager在YARN部署模式下为ResourceManager 负责对整个集群资源的分配与管理，不负责对Executor的资源的分配。 常见的集群管理工具有Yarn、Mesos、Kubernetes、Standalone。 2.3.3. Worker在YARN部署模式下为NodeManager 负责启动和管理Executor，将Executor状态信息同步给Cluster Manager。 2.3.4. Executor负责执行Spark作业中具体的任务，任务彼此之间相互独立。 Executor 有两个核心功能： 1）向Driver认领属于自己的任务，接收任务后负责运行Spark Task，并将结果返回给Driver。 2）通过自身的Block Manager为用户程序中要求缓存的RDD提供内存式存储。RDD 的数据是直接缓存在 Executor 进程内的，因此任务可以在运行时充分利用缓存数据加速运算。 2.4. Spark任务提交过程当用户通过SparkContext提交应用程序时，分为以下几步： 1）首先通过RpcEnv向ClusterManager注册应用并申请资源。 2）Cluster Manager根据应用的需求，将对应的Executor分配给应用程序，并在Worker上启动Executor后台进程，该进程通过RpcEnv向Driver注册Executor资源信息，Driver中的TaskScheduler保存Executor的地址、内存以及核数等信息； 3）SparkContext根据RDD的转换操作，构建RDD的血缘关系，并在遇到行动操作的时候，将构建的依赖关系提交给DAGScheduler，DAGScheduler对依赖关系进行解析，生成TaskSet，将TaskSet保存至任务池，TaskScheduler从任务池中取出TaskSet后按照FAIR或FIFO算法进行调度，给任务分配Executor资源，并将序列化后的任务发送至Executor执行。 2.5. Spark性能调优2.5.1. 参数调优2.5.1.1. num-executors该参数用于设计Spark作业总的Executor进程的个数。YARN集群管理器会尽可能根据num-executor设置在工作节点上启动Executor。 2.5.1.2. executor-memory该参数用于设置每个Executor 进程的内存，Executor内存的大小，很多程度上直接决定了Spark作业的性能，而且跟很常见的Java中的虚拟机内存溢出异常(OOM)也有关系。 2.5.1.3. executor-core该参数用于设置每个Executor进程的CPU core 数量。 2.5.1.4. driver-memory该参数用于设置Driver进程的内存，这个参数通常不设置。但是要注意的一点是，使用collect算子时，一定要保证Driver内存足够大，否则会出现内存溢出的错误。 2.5.1.5. Spark.default.parallelism该参数用于设置每个Stage默认的task数量。 2.5.1.6. Spark.Shuffle.memoryFraction该参数用于设置Shuffle过程中一个task拉取到上个Stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2。也就是说，Executor默认只有20%的内存用来进行该操作。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。建议：如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。此外，如果发现作业由于频繁的gc导致运行缓慢，意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。 2.5.2. 代码重构调优2.5.2.1. 尽可能复用一个RDD在对不同的数据执行算子操作时应该尽量复用一个RDD。例如，当RDD A的数据格式是key-value类型的，RDD B的数据格式是value类型的，但是这两个RDD的value数据完全相同；那么，RDD A包含了RDD B中的所有信息，理论上来说RDD B可以被替代，而实际开发中也应该尽量减少多个RDD数据有重复或者包含的情况，这样可以尽可能减少RDD的数量从而减少算子执行的次数。 2.5.2.2. 对多次使用的RDD进行持久化RDD的持有化有几种不同的级别，分别是：MEMORY_ONLY、MEMORY_AND_DISK、MEMORY_ONLY_SER、MEMORY_AND_DISK_SER、DISK_ONLY、MEMORY_ONLY_2 等，这几种持久化级别使用的优先级排序如下： MEMORY_ONLY性能最高，直接将RDD存储在内存中，省去了序列化及反序列化、从磁盘读取的时间，但是对于内存的容量有较高的要求； MEMORY_ONLY_SER会将数据序列化后保存在内存中，通过序列化压缩了RDD的大小，但是相较于MEMORY_ONLY多出了序列化及反序列化的时间； MEMORY_AND_DISK_SER优先将RDD缓存在内存中，内存缓存不下时才会存在磁盘中; DISK_ONLY和后缀为_2的级别通常不建议使用，完全基于磁盘文件的读写会导致性能的极具降低；后缀为2的级别会将所有数据都复制一份副本到其他节点上，数据复制及网络传输会导致较大的性能开销。 2.5.2.3. 尽量避免使用Shuffle算子Spark作业最消耗性能的部分就是Shuffle过程，应尽量避免使用Shuffle算子。Shuffle过程就是将分布在集群中多个节点上的同一个 key，拉取到同一个节点上，进行聚合或者join操作，在操作过程中可能会因为一个节点上处理的key过多导致数据溢出到磁盘。由此可见，Shuffle过程可能会发生大量的磁盘文件读写的 IO 操作，以及数据的网络传输操作。 Shuffle类算子有：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等，编写Spark作业程序时，应该尽量使用map类算子替代Shuffle算子。 2.5.2.4. 使用高性能算子 使用reduceByKey/aggregateByKey替代groupByKey 使用mapPartitions替代普通map Transformation算子 使用foreachPartitions替代foreach Action算子 使用filter之后进行coalesce操作 repartition:coalesce(numPartitions，true)增多分区使用这个 coalesce(numPartitions，false)减少分区，没有shuffle只是合并partition 2.5.2.5. 使用Kryo优化序列化性能Spark支持使用Kryo序列化机制。这种序列化机制，比默认的Java序列化机制速度要快，序列化后的数据更小。所以Kryo序列化优化以后，可以让网络传输的数据变少，在集群中耗费的内存资源大大减少。 2.5.2.6. 优化数据结构在Java中有三种类型比较耗费内存 对象：每个Java对象都有对象头、引用等额外的信息，因此比较占用内存空间。 字符串：每个字符串内部都有一个字符数组以及长度等额外信息。 集合类型：比如HashMap、LinkedList等，因为集合类型内部通常会使用一些内部类来封装集合元素，比如Map.Entry。 因此Spark编码时应尽量不要使用以上三种数据结构，尽量使用字符串代替对象，使用原始类型（比如 Int、Long）替代字符串，使用数组替代集合类型，这样尽可能地减少内存占用，降低垃圾回收的频率提高性能。 2.5.2.7. 广播大变量开发过程中，会遇到需要在算子函数中使用外部变量的场景(尤其是大变量，比如100M以上的大集合)，那么此时就应该使用Spark的广播(Broadcast)功能来提升性能；函数中使用到外部变量时，默认情况下，Spark会将该变量复制多个副本，通过网络传输到task中，此时每个task都有一个变量副本。如果变量本身比较大的话(比如100M，甚至1G)，那么大量的变量副本在网络中传输的性能开销，以及在各个节点的Executor中占用过多内存导致的频繁GC(垃圾回收)，都会极大地影响性能； 如果使用的外部变量比较大，建议使用Spark的广播功能，对该变量进行广播。广播后的变量，会保证每个Executor的内存中，只驻留一份变量副本，而Executor中的 task执行时共享该Executor中的那份变量副本。这样的话，可以大大减少变量副本的数量，从而减少网络传输的性能开销，并减少对Executor内存的占用开销，降低 GC的频率。 3. 空间大数据计算框架简介3.1. Sedona(GeoSpark)简介4.1.2.3 sedona(geospark) 3.2. 扩展——SQL/MM简介1.4.2 SQL/MM","categories":[],"tags":[]},{"title":"空间关系","slug":"空间关系","date":"2023-11-07T11:32:52.000Z","updated":"2024-03-14T14:55:29.485Z","comments":true,"path":"3c685723.html","link":"","permalink":"https://imlike.cc/3c685723.html","excerpt":"","text":"1.Hole Outside ShellIndicates that a hole of a polygon lies partially or completely in the exterior of the shell. 2.Nested HolesIndicates that a hole lies in the interior of another hole in the same polygon. 3.Disconnected InteriorIndicates that the interior of a polygon is disjoint.(often caused by set of contiguous holes splitting the polygon into two parts) 4.Self IntersectionIndicates that two rings of a polygonal geometry intersect. 5.Ring Self IntersectionIndicates that a ring self-intersects. 6.Nested ShellsIndicates that a polygon component of a MultiPolygon lies inside another polygonal component. 7.Duplicate RingsIndicates that a polygonal geometry contains two rings which are identical. 也就是说，多边形中包含了两个相同的环，此处不加图片示意了。 8.Too Few PointsIndicates that either:1)A LineString contains a single point;2)A LinearRing contains 2 or 3 points. 即组成形状的节点太少了。此处不加图片示意了。 9.invalid coordinateIndicates that the X or Y ordinate of a Coordinate is not a valid numeric value. 即不合法的坐标。 比如说，在地理坐标系中，经度的范围为[-180, 180]，纬度的范围为[-90, 90]，超出这个范围的坐标即为invalid coordinate。此处不加图片示意了。 10.Ring Not ClosedIndicates that a ring is not correctly closed.(the first and the last coordinate are different) 即多边形没有合上。正确的polygon应该是首尾相连的，若不相连即出现此错误。此处不加图片示意了","categories":[],"tags":[]},{"title":"2023最新jd_cookie在青龙上操作","slug":"jd-cookie在青龙上操作","date":"2023-11-02T12:49:07.000Z","updated":"2024-03-14T14:55:29.478Z","comments":true,"path":"d3bcc396.html","link":"","permalink":"https://imlike.cc/d3bcc396.html","excerpt":"","text":"安装docker 123456#安装dockersudo yum check-updatecurl -sSL https:&#x2F;&#x2F;get.daocloud.io&#x2F;docker | shsudo systemctl start dockersudo systemctl status dockersudo systemctl enable docker 如果已经安装docker 删除qinglong的镜像和服务 12docker stop $(docker ps -a -q) &#x2F;&#x2F; stop停止所有容器 docker rm $(docker ps -a -q) &#x2F;&#x2F; remove删除所有容器 安装qinglong1234567891011docker run -dit \\ -v $PWD&#x2F;ql:&#x2F;ql&#x2F;data \\ -p 5700:5700 \\ -p 5701:5701 \\ -e ENABLE_WEB_PANEL&#x3D;true \\ -e ENABLE_HANGUP&#x3D;true \\ -e QlPort&#x3D;&quot;5700&quot; \\ --name qinglong \\ --hostname qinglong \\ --restart unless-stopped \\ whyour&#x2F;qinglong:latest 进入容器12## ql为容器名 本文机器容器名为qinglongdocker exec -it qinglong bash 安装ninja插件1234git clone https:&#x2F;&#x2F;gh-proxy.com&#x2F;https:&#x2F;&#x2F;github.com&#x2F;shufflewzc&#x2F;Waikiki_ninja.git &#x2F;ql&#x2F;ninjacd &#x2F;ql&#x2F;ninja&#x2F;backendpnpm installpm2 start 打开青龙configs文件夹的extra.sh文件将以下内容粘贴进去。 12cd &#x2F;ql&#x2F;ninja&#x2F;backend pm2 start 安装签到脚本 1ql repo https:&#x2F;&#x2F;ghproxy.com&#x2F;https:&#x2F;&#x2F;github.com&#x2F;shufflewzc&#x2F;faker3.git &quot;jd_|jx_|gua_|jddj_|jdCookie&quot; &quot;activity|backUp&quot; &quot;^jd[^_]|USER|function|utils|sendNotify|ZooFaker_Necklace.js|JDJRValidator_|sign_graphics_validate|ql|JDSignValidator|magic|depend|h5sts&quot; &quot;main&quot; # fake2 1https:&#x2F;&#x2F;github.com&#x2F;6dylan6&#x2F;jdpro # 6dy 1https:&#x2F;&#x2F;github.com&#x2F;leafTheFish&#x2F;DeathNote 注意事项 重启后务必执行一次 ql extra 保证 Ninja 配置成功。 更新 Ninja 只需要在容器中 ninja/backend 目录执行 git pull 然后 pm2 start Qinglong 需要在登录状态（auth.json 中有 token） 获取cookie以前大家都是获取jd的cookie来操作现在 cookie非常容易过期,现在推荐使用 wskey,需要在移动客户端抓包,下面介绍使用安卓客户端的操作,先安装 app：VNET（抓包用）、京东； VNET抓包wskey: 青龙面板使用wskey转换脚本 12ql repo https:&#x2F;&#x2F;e.coding.net&#x2F;HelloDNS&#x2F;sign&#x2F;wskey.git &quot;wskey&quot; # 国内外 安装青龙的依赖1curl -fsSL https:&#x2F;&#x2F;gh-proxy.com&#x2F;https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;FlechazoPh&#x2F;QLDependency&#x2F;main&#x2F;Shell&#x2F;QLOneKeyDependency.sh | sh 设置环境变量 QL_LOG_AUTO_INSTALL_DEPEND=ture 1 参考链接 青龙Faker仓库教程合集 安卓抓取JD wskey + 添加脚本自动转换JD cookie_jd_wsck-CSDN博客","categories":[],"tags":[]},{"title":"图像相关的数据和介绍","slug":"图像相关的数据和介绍","date":"2023-10-19T12:15:30.000Z","updated":"2024-03-14T14:55:29.484Z","comments":true,"path":"24b88766.html","link":"","permalink":"https://imlike.cc/24b88766.html","excerpt":"","text":"有损压缩图片 jpg png 矢量图片 svg tiff,tif数据 相比较 jpg png里面的信息更加丰富 每个栅格下 是RGB 还是什么不一定是显示,而是包含高度或者温度 所以空间相关比如遥感影像会以TIFF数据来保存一些我们肉眼看不到的数据,同时tiff数据文件开始部分会有部分元数据的信息能被访问同时又cogtif数据来访问, 类型: bimap 颜色: 1-24 bit 压缩 Uncompressed,RLE,LZW,CCITT Group和JPEG 图像最大尺寸 2^31-1 是否支持多通道 IFH (图像文件头) IFD(图像文件目录) 位图数据","categories":[],"tags":[]},{"title":"程序员工具分享-转载","slug":"程序员工具分享-转载","date":"2023-10-14T08:04:25.000Z","updated":"2024-03-14T14:55:29.485Z","comments":true,"path":"fee7f0df.html","link":"","permalink":"https://imlike.cc/fee7f0df.html","excerpt":"","text":"程序员分享一些我常用的提升开发效率的工具： AI工具GitHub Copilot：能极大的提高效率，不仅能在编辑器中根据代码上下文自动编写代码，还能以ChatGPT对话的方式回答问题。有了它，极大的降低了对StackOverflow的依赖。 ChatGPT：虽然GitHub Copilot以及能在写代码方面平替ChatGPT，但ChatGPT的GPT4 V的识图功能、DALL E3的绘图功能、移动端的语音对话功能都是GitHub Copilot无法替代的。用ChatGPT绘图或练英语还是很方便的。 https://v0.dev/：Vercel出品的AI画UI的神器，试了下，的确能大幅度降低画UI的工作量，相比ChatGPT直接生成代码，V0能以可视化的方式修改界面，这是ChatGPT无法做到的。 开发工具VSCode：因为免费轻量级，哪怕是安装了很多插件，速度也比IDEA快。当然如果开发Java的话，IDEA还是更胜一筹。 DBeaver：数据库连接工具，主打免费可连接多种数据库。 pgAdmin4：管理PG数据库的工具，比DBeaver在PG的管理上更强一点。云平台 GitHub：虽然你对GitHub估计很熟悉了，但也推荐看这篇文章：善用GitHub（https://www.bmpi.dev/self/use-github-better/）。 Cloudflare：Serverless最好的平台，能提供如常用的存储R2、KV数据库、CDN、Pages、Worker、MQ、甚至运行AI模型，我开发的个人产品（https://www.i365.tech/）大多都用它运行的。 Vercel：很适合部署运行Next.js的前端项目，Cloudflare Pages运行Next.js还需要额外的一些配置。 Fly.io：非常好的容器化平台，它能弥补Cloudflare只能运行Serverless的不足，能以Docker的方式运行各种重型API服务。 AWS/Azure/GCP：作为一个巨无霸的云平台，他们可以补充上面云平台所缺失的东西，比如AWS SES就是一个非常好的发邮件服务。 Newrelic：非常好的监控平台，尤其是它能作为免费的日志搜索平台，我把我产品的系统日志都上传到这个平台，然后对日志进行监控。一般的日志平台都是ES架构，价格并不便宜，单Newrelic却能免费。 Optimizely：一个不错的feature toggle平台，可以很方便的做产品的A/B测试。 分析工具Umami：GA的开源平替，我网站流量都是用它监控的，速度快，数据还是自己的。 Plausible：更高级的GA开源平替，比Umami提供更多功能，但托管所需服务比较多，前者能免费跑在Vercel平台上。 Mixpanel：更更高级的GA商业平替，能做很复杂的用户行为分析与溯源，比GA高级多了。支付平台 Lemonsqueezy：出海必备的支付平台，用国内身份就可以开通，然后开启出海之旅，赚美刀。 Stripe：相比前者，虽然费率便宜一些，但国内身份没发开通，需要做一些比较复杂的操作才能开通，比如开通一个美国或香港公司就可以开。 绘图工具Excalidraw：非常好用的白板画图工具，适合个人使用。 PlantUML：一个支持文本绘制类UML图的工具，可以在Real World Plantuml找到很多用它实现的图。 Mermaid：通过文本生成各种UML图，最大的亮点是GitHub官方Markdown支持它，能渲染出多种图形如时序图与架构图等。 asciiflow：能简单的免费在线绘制ASCII风格图的网站，C语言大佬最爱的RFC风格绘图工具。 Sketch/OmniGraffle/Lucidchart/Draw.io/Google Drawings/Visio：这类工具相比大家或多或少用过，都是一些桌面端或浏览器常用的画图工具。 综合工具Logseq：OKR + GTD + Note =&gt; Logseq（https://www.bmpi.dev/self/okr-gtd-note-logseq/），看完这篇文章，你会知道我为何而推荐这个工具，能做笔记、能规划任务或做OKR啥的。 Beancount：完美记账工具，配合Git能做到数据安全可控，可以看这篇我写的文章：使用Beancount管理家庭财务（https://www.bmpi.dev/self/beancount-my-accounting-tool-v2/），谁再给你推荐记账工具，请把这篇文章扔给他们看。 浏览器插件AdblockPlus：这款插件可以自动屏蔽网站里的绝大多数广告，无需额外复杂的配置，安装好即可。 沉浸式翻译：搭配OpenAI的API Key可以做到非常完美的多语言网页翻译，读外文资料无障碍。 Internet Archive：网页时光机，很多网页因为时间很久的缘故可能无法访问（404），这个插件可以帮助我们找回这个页面，甚至可以像时光机一样查看网页不同时期的版本。 Screely：截屏工具，主要是能美化截屏界面，让其不再难看，比如加点背景。 wappalyzer：查看网站技术栈，比如使用它一些主流网站的技术栈。 SimilarWeb：查看网站PV流量的工具，但是结果有时候准确，有时候偏差挺大的。 SEOquake：查看网站的SEO相关的信息，如外链数量、索引数量、SEO的站内问题等。 Keyword Surfer：关键词工具，可以查看一些关键词的月搜索量。 META SEO inspector：网页Meta信息查看工具，可以方便地解决一些网页SEO的基本问题。","categories":[],"tags":[]},{"title":"redis介绍","slug":"redis介绍","date":"2023-10-14T06:35:26.000Z","updated":"2024-03-14T14:55:29.480Z","comments":true,"path":"1a32b04.html","link":"","permalink":"https://imlike.cc/1a32b04.html","excerpt":"","text":"Redis是一个开源的内存数据存储解决方案，它可以用作缓存、数据库和消息代理。以下是它的主要特点和原理： 内存存储：将所有数据存储在内存中，以实现快速读写访问。同时，它支持多种数据结构，如字符串、哈希、列表、集合和有序集合。 持久性：Redis支持将数据持久化到磁盘上，以实现数据的可靠存储和恢复。它提供两种不同的持久化选项：快照和日志。 高可用性：Redis支持主从复制和哨兵机制，以实现高可用性和故障点转移。 分布式：Redis Cluster可以将数据分布在多个节点上，以实现横向扩展和高性能。 事务和Lua脚本：重新支持事务和Lua脚本，以实现原子操作和自定义命令。 1. Redis的单线程和Linux的ePoll特性Redis是单线程的，这意味着它使用一个主线程来处理所有客户端请求。它的性能非常高，因为它不需要线程切换的开销，并且可以使用非常少的内存来处理大量的并发连接。另外，Redis使用Linux的ePoll特性来实现异步I/O操作，这使得Redis能够在高负载情况下保持高性能。 在**redis中会有epoll模型是基于异步的io来实现的,**在该模型下每个连接都会有个FileDscript的文件来标示连接情况,连接的客户端有相关的io才会唤醒redis的线程来进行操作 1.1. 缓存数据存储和持久化 RDB（Redis Database）： RDB 是 Redis 数据的非常紧凑的单文件时间点表示形式。 RDB 文件非常适合备份。 例如，您可能希望在最近 24 小时内每小时归档一次 RDB 文件，并在 30 天内每天保存一个 RDB 快照。 这使您可以在发生灾难时轻松恢复不同版本的数据集。RDB 非常适合灾难恢复，它是一个紧凑的文件，可以传输到远程数据中心或 Amazon S3（可能是加密的）。RDB 最大限度地提高了 Redis 的性能，因为 Redis 父进程为了持久化需要做的唯一工作就是派生一个子进程，该子进程将完成其余所有工作。 父进程永远不会执行磁盘 I/O 等操作。与 AOF 相比，RDB 允许更快地重新启动大数据集。在副本上，RDB 支持重启和故障转移后的部分重新同步。 AOF日志（Append-Only File）： AOF日志以追加方式记录Redis服务器所执行的写命令，以文本格式保存在一个文件中。 AOF日志允许Redis将写操作追加到文件的末尾，以保持数据的完整性。 AOF日志可以通过配置文件中的appendonly选项来启用和配置。 AOF日志的优点是可以提供更高的数据安全性和持久性，但相比于快照有一定的性能开销。 可以通过配置AOF重写来压缩和优化AOF日志的大小。 2. Redis支持的常见结构2.1. 数据结构2.1.1. .字符串（String）：Redis的字符串是一个简单的键值对结构，键是一个字符串，值可以是任意二进制数据。常用操作：设置值、获取值、增加或减少数值、追加字符串、获取子字符串等。适用场景：缓存、计数器、分布式锁等。 2.1.2. 哈希（Hash）：Redis的哈希是一个键值对集合，其中键是一个字符串，值是一个字段-值对的映射表。常用操作：设置字段值、获取字段值、获取所有字段与值、删除字段等。适用场景：存储对象、存储用户信息、存储配置信息等。 2.1.3. 列表（List）：Redis的列表是一个有序的字符串元素集合，它可以在两端进行插入、删除和查询操作。常用操作：从列表的左端或右端插入元素、从左端或右端弹出元素、获取指定索引位置的元素等。适用场景：消息队列、实现最新消息推送、记录操作日志等。 2.1.4. 集合（Set）：Redis的集合是一个无序的字符串元素集合，它不允许重复的元素存在。常用操作：添加元素、移除元素、判断元素是否存在、求交集、求并集等。适用场景：标签系统、好友关系、计算共同喜好等。 2.1.5. 有序集合（Sorted Set）：Redis的有序集合是一个排序的字符串元素集合，每个元素都会关联一个分数，通过分数可以进行排序。常用操作：添加元素、移除元素、根据分数范围获取元素等。适用场景：排行榜、优先级队列、范围查询等。 2.2. Redis支持其他类型：布隆过滤器、HyperLogLog和Geohash除了支持字符串、哈希、列表、集合和排序集等常见数据结构外，Redis还支持在某些用例中有用的几种专用数据类型。 2.2.1. 布隆过滤器(Bloom filter)布隆过滤器是一种概率数据结构，可用于确定元素是否可能存在于集合中。它们在误报（表示元素不在集合中时）是可以接受的，但误报（表示元素不在集合中时）不是可以接受的情况下特别有用。Redis中的布隆过滤器是使用BF. ADD、BF.EXISTS和BF.MADD命令实现的。 2.2.2. HyperLogLog(HyperLogLog)HyperLogLog是一种用于估计集合基数的概率算法。它为存储集合本身提供了一种节省空间的替代方案，可用于高精度估计非常大的集合中唯一元素的数量。Redis中的HyperLogLog使用PFADD、PFCOUNT和PFMERGE命令实现。 2.2.3. 布谷鸟算法(Cuckoo filter)检测特定集合是否存在是否存在 2.2.4. 空间类型 geospatial地理散列是一种以保留接近度的方式将地理坐标编码为字符串的技术。它可用于对一组坐标执行空间查询，例如查找给定点一定半径内的所有坐标。Redis支持通过GEOADD、GEODIST、GEOHASH、GEOPOS和GEORADIUS命令进行地理散列。通过为这些专门的数据类型提供支持，Redis使开发人员能够凭借其强大的内存数据存储和处理能力解决更广泛的问题。 2.2.5. BitMap类型 Bitmap，即位图，是一串连续的二进制数组（0和1），可以通过偏移量（offset）定位元素。BitMap通过最小的单位bit来进行0|1的设置，表示某个元素的值或者状态，时间复杂度为O(1)。由于bit是计算机中最小的单位，使用它进行储存将非常节省空间，特别适合一些数据量大且使用二值统计的场景。 使用示例 位移值和0|1 存储值,例如是否在线,考勤是否打卡 3. Redis淘汰策略 Redis的淘汰策略，根据是否会进行数据淘汰可以把它们分成两类： 不进行数据淘汰的策略，只有 noeviction 这一种。 会进行淘汰的 7 种其他策略。 会进行淘汰的 7 种策略，我们可以再进一步根据淘汰候选数据集的范围把它们分成两类： 在设置了过期时间的数据中进行淘汰，包括 volatile-random、volatile-ttl、volatile-lru、volatile-lfu（Redis 4.0 后新增）四种。 在所有数据范围内进行淘汰，包括 allkeys-lru、allkeys-random、allkeys-lfu（Redis 4.0 后新增）三种。 Redis的八种淘汰策略（五） - 掘金 volatile-random、volatile-ttl、volatile-lru、volatile-lfu 四种策略是针对已经设置了过期时间的键值对。到键值对的到期时间到了或者Redis内存使用量达到了maxmemory阈值，Redis会根据这些策略对键值对进行淘汰； volatile-ttl 在筛选时，会针对设置了过期时间的键值对，根据过期时间的先后进行删除，越早过期的越先被删除。 volatile-random 就像它的名称一样，在设置了过期时间的键值对中，进行随机删除。 volatile-lru 会使用 LRU (最近最少使用淘汰算法（Least Recently Used）) 算法筛选设置了过期时间的键值对。 volatile-lfu 会使用 LFU 最不经常使用淘汰算法（Least Frequently Used）选择设置了过期时间的键值对。 备注: LRU，即：最近最少使用淘汰算法（Least Recently Used）。LRU是淘汰最长时间没有被使用的页面。 LFU，即：最不经常使用淘汰算法（Least Frequently Used）。LFU是淘汰一段时间内，使用次数最少的页面 动手实现 LRU 算法，以及 Caffeine 和 Redis 中的缓存淘汰策略 - 风的姿态 - 博客园 4. Redis集群一文掌握Redis的三种集群方案 - 个人文章 - SegmentFault 思否 4.1. 主从复制 4.1.1. 主从复制的优缺点优点： master能自动将数据同步到slave，可以进行读写分离，分担master的读压力 master、slave之间的同步是以非阻塞的方式进行的，同步期间，客户端仍然可以提交查询或更新请求 缺点： 不具备自动容错与恢复功能，master或slave的宕机都可能导致客户端请求失败，需要等待机器重启或手动切换客户端IP才能恢复 master宕机，如果宕机前数据没有同步完，则切换IP后会存在数据不一致的问题 难以支持在线扩容，Redis的容量受限于单机配置 4.2. 哨兵模式 优点： 哨兵模式基于主从复制模式，所以主从复制模式有的优点，哨兵模式也有 哨兵模式下，master挂掉可以自动进行切换，系统可用性更高 缺点： 同样也继承了主从模式难以在线扩容的缺点，Redis的容量受限于单机配置 需要额外的资源来启动sentinel进程，实现相对复杂一点，同时slave节点作为备份节点不提供服务 4.3. Cluster模式 Redis Cluster 采用虚拟哈希槽分区，所有的键根据哈希函数映射到 0 ~ 16383 整数槽内，计算公式：HASH_SLOT = CRC16(key) % 16384。每一个节点负责维护一部分槽以及槽所映射的键值数据。 5. Redis中的缓存雪崩和缓存击穿问题 5.1. 缓存雪崩缓存雪崩是指在同一时间，缓存中大量的缓存数据失效，导致大量的请求直接打到了后端数据库上，使得数据库瞬间达到峰值负载，甚至直接宕机。这种情况通常发生在缓存中的数据同时过期时间设置相同，同时失效的情况下。为了避免缓存雪崩问题，可以采取以下措施： 给缓存数据的过期时间加上随机值，避免同时失效。 将缓存数据的过期时间分散开来，比如一部分设置为5分钟，一部分设置为10分钟，一部分设置为15分钟，以此类推。 设置本地缓存，当缓存失效时，先从本地缓存中获取数据，如果本地缓存没有，则去后端数据库取数据，并将数据再次缓存到本地缓存中。 5.2. 缓存击穿缓存击穿是指某个热点key在缓存过期的同时，有大量的请求访问该key的时候，这些请求直接穿透到后端数据库，导致数据库瞬间达到峰值负载，甚至直接宕机。为了避免缓存击穿问题，可以采取以下措施： 在缓存数据不存在时，不直接去查询后端数据库，而是先将该key设置为一个特殊的值（如null），在一段时间内（如5秒）不再查询后端数据库，从而避免瞬间大量请求打到数据库上。 设置热点数据永不过期，或者过期时间非常长，例如一天、一周、一个月等。 设置本地缓存，当缓存失效时，先从本地缓存中获取数据，如果本地缓存没有，则去后端数据库取数据，并将数据再次缓存到本地缓存中。 在使用Redis时需要注意缓存雪崩和缓存击穿问题，采取相应的措施可以避免这些问题的发生，提高系统的稳定性和可靠性。在Redis中，缓存失效可能有以下几种情况： 过期时间到期：当设置了缓存的过期时间后，到期后缓存将会失效。 主动删除：使用DEL命令或者程序中的delete方法，主动删除缓存。 内存空间不足：当Rediss内存空间不足时，Rediss会优先淘汰赛掉内存占用较大，使用较少的缓存，以腾出更多的内存空间。 内存泄漏：当程序中出现内存泄漏时，会导致缓存失效。 Redis重启：当Redis服务重启时，缓存也会失效。 为了避免缓存失效，我们可以设置合适的缓存过期时间，定期检查和清理过期缓存，优化Redis的内存使用，以及避免内存泄漏等问题。 6. 在Spring Boot中使用RedisRedis是一个流行的内存数据存储解决方案，它可以用作缓存、数据库和消息代理。在Spring Boot应用程序中使用Redis可以提高应用程序的性能和可扩展性。在本文中，我们将介绍如何在Spring Boot应用程序中使用Redis。 6.1. .添加Redis依赖要在Spring Boot应用程序中使用Redis，我们首先需要添加Redis依赖，在Maven项目中，我们可以在pom. xml文件中添加以下依赖项： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; 6.2. 配置Redis连接在我们可以使用Redis之前，我们需要配置Redis连接.在Spring Boot应用程序中，我们可以在application.properties文件中添加以下配置： 12spring.redis.host=localhostspring.redis.port=6379 这将配置Redis连接到本地主机的默认端口6379。 6.3. 使用RedisTemplateSpring Boot提供了一个RedisTemplate类，它是一个用于与Redis交互的高级API。我们可以使用RedisTemplate来执行Redis操作，如存储、检索和删除数据。以下是一个使用RedisTemplate执行Redis操作的示例： 12345678910@Autowiredprivate RedisTemplate&lt;String, Object&gt; redisTemplate;public void setValue(String key, Object value) &#123; redisTemplate.opsForValue().set(key, value);&#125;public Object getValue(String key) &#123; return redisTemplate.opsForValue().get(key);&#125; 在上面的示例中，我们注入了一个RedisTemplate对象，并使用它来存储和检索数据。我们使用opsForValue()方法来访问Redis值操作，如设置和获取值。 7. 使用注解除了使用RedisTemplate之外，Spring Boot还提供了一些注解，可以更轻松地使用Redis。例如，我们可以使用@Cacheable注解将方法的结果缓存到Redis中，从而加快方法的执行速度。以下是一个使用@Cacheable注解的示例： 1234@Cacheable(value = &quot;users&quot;, key = &quot;#userId&quot;)public User getUserById(String userId) &#123; // 获取用户信息&#125; 在上面的示例中，我们使用@Cache able注解将User By Id方法的结果缓存到名为”user”的Red is缓存中，缓存键为rId参数。 7.1. 常见客户端比较 lettuce ,redisson,jedis 特性 Lettuce Redisson Jedis 完整性 Lettuce 是一个可扩展的线程安全 Redis 客户端，适用于同步、异步和反应式使用。 如果多个线程避免阻塞和事务性操作（例如 BLPOP 和 MULTI/EXEC），则它们可以共享一个连接。 Lettuce是用netty构建的。 支持高级 Redis 功能，例如 Sentinel、集群、管道、自动重新连接和 Redis 数据模型。 Redisson提供了完整的Redis协议支持,基于Netty 框架，并在其基础上封装了更丰富的特性，如分布式锁、分布式集合等.线程安全的实现 支持Redis复制、Redis集群、Redis哨兵、Redis主从、Redis单机、Redis代理、Redis多集群设置 Jedis提供了基本的Redis协议支持，但支持高级功能如Pipelining、Miscellaneous、Cluster、Pub/Sub等。Redis 官方推荐的Java客户端 连接管理 支持连接池和连接自动恢复机制，具有更好的连接管理能力。 支持连接池和连接自动恢复机制，具有较好的连接管理能力。 支持连接池，但没有连接自动恢复机制， 异步操作 提供完善的异步操作接口和功能，支持Reactive编程模型。 提供完善的异步操作接口和功能，支持Reactive编程模型。 不支持异步操作，只能使用同步方式进行操作。 分布式功能 支持Redis Sentinel和Redis Cluster，可以使用Lettuce进行分布式部署。 提供了丰富的分布式功能，如分布式锁、分布式集合、分布式对象等。 不支持Redis Sentinel和Redis Cluster，不适用于分布式部署。 扩展性 提供了较好的扩展性，用户可以自定义编解码器、连接池实现等。 提供了较好的扩展性，用户可以自定义编解码器、连接管理器等。 较难进行扩展，功能相对固定。 社区活跃度 拥有活跃的开源社区，稳定更新并提供技术支持。 拥有活跃的开源社区，稳定更新并提供技术支持。 拥有较为活跃的开源社区，但相对于Lettuce和Redisson稍显不足。 https://github.com/lettuce-io/lettuce-core最新版本是 https://github.com/redisson/redisson 最新版是 3.23.4 https://github.com/redis/jedis 最近版本是5.0.0 8. 结论 Redis是一个开源的内存数据存储解决方案，它可以用作缓存、数据库和消息代理。以下是它的主要特点和原理： 内存存储：将所有数据存储在内存中，以实现快速读写访问。同时，它支持多种数据结构，如字符串、哈希、列表、集合和有序集合。 持久性：Redis支持将数据持久化到磁盘上，以实现数据的可靠存储和恢复。它提供两种不同的持久化选项：快照和日志。 高可用性：Redis支持主从复制和哨兵机制，以实现高可用性和故障点转移。 分布式：Redis Cluster可以将数据分布在多个节点上，以实现横向扩展和高性能。 事务和Lua脚本：重新支持事务和Lua脚本，以实现原子操作和自定义命令。 1. Redis的单线程和Linux的ePoll特性Redis是单线程的，这意味着它使用一个主线程来处理所有客户端请求。它的性能非常高，因为它不需要线程切换的开销，并且可以使用非常少的内存来处理大量的并发连接。另外，Redis使用Linux的ePoll特性来实现异步I/O操作，这使得Redis能够在高负载情况下保持高性能。 在**redis中会有epoll模型是基于异步的io来实现的,**在该模型下每个连接都会有个FileDscript的文件来标示连接情况,连接的客户端有相关的io才会唤醒redis的线程来进行操作 1.1. 缓存数据存储和持久化 RDB（Redis Database）： RDB 是 Redis 数据的非常紧凑的单文件时间点表示形式。 RDB 文件非常适合备份。 例如，您可能希望在最近 24 小时内每小时归档一次 RDB 文件，并在 30 天内每天保存一个 RDB 快照。 这使您可以在发生灾难时轻松恢复不同版本的数据集。RDB 非常适合灾难恢复，它是一个紧凑的文件，可以传输到远程数据中心或 Amazon S3（可能是加密的）。RDB 最大限度地提高了 Redis 的性能，因为 Redis 父进程为了持久化需要做的唯一工作就是派生一个子进程，该子进程将完成其余所有工作。 父进程永远不会执行磁盘 I/O 等操作。与 AOF 相比，RDB 允许更快地重新启动大数据集。在副本上，RDB 支持重启和故障转移后的部分重新同步。 AOF日志（Append-Only File）： AOF日志以追加方式记录Redis服务器所执行的写命令，以文本格式保存在一个文件中。 AOF日志允许Redis将写操作追加到文件的末尾，以保持数据的完整性。 AOF日志可以通过配置文件中的appendonly选项来启用和配置。 AOF日志的优点是可以提供更高的数据安全性和持久性，但相比于快照有一定的性能开销。 可以通过配置AOF重写来压缩和优化AOF日志的大小。 2. Redis支持的常见结构2.1. 数据结构2.1.1. .字符串（String）：Redis的字符串是一个简单的键值对结构，键是一个字符串，值可以是任意二进制数据。常用操作：设置值、获取值、增加或减少数值、追加字符串、获取子字符串等。适用场景：缓存、计数器、分布式锁等。 2.1.2. 哈希（Hash）：Redis的哈希是一个键值对集合，其中键是一个字符串，值是一个字段-值对的映射表。常用操作：设置字段值、获取字段值、获取所有字段与值、删除字段等。适用场景：存储对象、存储用户信息、存储配置信息等。 2.1.3. 列表（List）：Redis的列表是一个有序的字符串元素集合，它可以在两端进行插入、删除和查询操作。常用操作：从列表的左端或右端插入元素、从左端或右端弹出元素、获取指定索引位置的元素等。适用场景：消息队列、实现最新消息推送、记录操作日志等。 2.1.4. 集合（Set）：Redis的集合是一个无序的字符串元素集合，它不允许重复的元素存在。常用操作：添加元素、移除元素、判断元素是否存在、求交集、求并集等。适用场景：标签系统、好友关系、计算共同喜好等。 2.1.5. 有序集合（Sorted Set）：Redis的有序集合是一个排序的字符串元素集合，每个元素都会关联一个分数，通过分数可以进行排序。常用操作：添加元素、移除元素、根据分数范围获取元素等。适用场景：排行榜、优先级队列、范围查询等。 2.2. Redis支持其他类型：布隆过滤器、HyperLogLog和Geohash除了支持字符串、哈希、列表、集合和排序集等常见数据结构外，Redis还支持在某些用例中有用的几种专用数据类型。 2.2.1. 布隆过滤器(Bloom filter)布隆过滤器是一种概率数据结构，可用于确定元素是否可能存在于集合中。它们在误报（表示元素不在集合中时）是可以接受的，但误报（表示元素不在集合中时）不是可以接受的情况下特别有用。Redis中的布隆过滤器是使用BF. ADD、BF.EXISTS和BF.MADD命令实现的。 2.2.2. HyperLogLog(HyperLogLog)HyperLogLog是一种用于估计集合基数的概率算法。它为存储集合本身提供了一种节省空间的替代方案，可用于高精度估计非常大的集合中唯一元素的数量。Redis中的HyperLogLog使用PFADD、PFCOUNT和PFMERGE命令实现。 2.2.3. 布谷鸟算法(Cuckoo filter)检测特定集合是否存在是否存在 2.2.4. 空间类型 geospatial地理散列是一种以保留接近度的方式将地理坐标编码为字符串的技术。它可用于对一组坐标执行空间查询，例如查找给定点一定半径内的所有坐标。Redis支持通过GEOADD、GEODIST、GEOHASH、GEOPOS和GEORADIUS命令进行地理散列。通过为这些专门的数据类型提供支持，Redis使开发人员能够凭借其强大的内存数据存储和处理能力解决更广泛的问题。 2.2.5. BitMap类型 Bitmap，即位图，是一串连续的二进制数组（0和1），可以通过偏移量（offset）定位元素。BitMap通过最小的单位bit来进行0|1的设置，表示某个元素的值或者状态，时间复杂度为O(1)。由于bit是计算机中最小的单位，使用它进行储存将非常节省空间，特别适合一些数据量大且使用二值统计的场景。 使用示例 位移值和0|1 存储值,例如是否在线,考勤是否打卡 3. Redis淘汰策略 Redis的淘汰策略，根据是否会进行数据淘汰可以把它们分成两类： 不进行数据淘汰的策略，只有 noeviction 这一种。 会进行淘汰的 7 种其他策略。 会进行淘汰的 7 种策略，我们可以再进一步根据淘汰候选数据集的范围把它们分成两类： 在设置了过期时间的数据中进行淘汰，包括 volatile-random、volatile-ttl、volatile-lru、volatile-lfu（Redis 4.0 后新增）四种。 在所有数据范围内进行淘汰，包括 allkeys-lru、allkeys-random、allkeys-lfu（Redis 4.0 后新增）三种。 Redis的八种淘汰策略（五） - 掘金 volatile-random、volatile-ttl、volatile-lru、volatile-lfu 四种策略是针对已经设置了过期时间的键值对。到键值对的到期时间到了或者Redis内存使用量达到了maxmemory阈值，Redis会根据这些策略对键值对进行淘汰； volatile-ttl 在筛选时，会针对设置了过期时间的键值对，根据过期时间的先后进行删除，越早过期的越先被删除。 volatile-random 就像它的名称一样，在设置了过期时间的键值对中，进行随机删除。 volatile-lru 会使用 LRU (最近最少使用淘汰算法（Least Recently Used）) 算法筛选设置了过期时间的键值对。 volatile-lfu 会使用 LFU 最不经常使用淘汰算法（Least Frequently Used）选择设置了过期时间的键值对。 备注: LRU，即：最近最少使用淘汰算法（Least Recently Used）。LRU是淘汰最长时间没有被使用的页面。 LFU，即：最不经常使用淘汰算法（Least Frequently Used）。LFU是淘汰一段时间内，使用次数最少的页面 动手实现 LRU 算法，以及 Caffeine 和 Redis 中的缓存淘汰策略 - 风的姿态 - 博客园 4. Redis集群一文掌握Redis的三种集群方案 - 个人文章 - SegmentFault 思否 4.1. 主从复制 4.1.1. 主从复制的优缺点优点： master能自动将数据同步到slave，可以进行读写分离，分担master的读压力 master、slave之间的同步是以非阻塞的方式进行的，同步期间，客户端仍然可以提交查询或更新请求 缺点： 不具备自动容错与恢复功能，master或slave的宕机都可能导致客户端请求失败，需要等待机器重启或手动切换客户端IP才能恢复 master宕机，如果宕机前数据没有同步完，则切换IP后会存在数据不一致的问题 难以支持在线扩容，Redis的容量受限于单机配置 4.2. 哨兵模式 优点： 哨兵模式基于主从复制模式，所以主从复制模式有的优点，哨兵模式也有 哨兵模式下，master挂掉可以自动进行切换，系统可用性更高 缺点： 同样也继承了主从模式难以在线扩容的缺点，Redis的容量受限于单机配置 需要额外的资源来启动sentinel进程，实现相对复杂一点，同时slave节点作为备份节点不提供服务 4.3. Cluster模式 Redis Cluster 采用虚拟哈希槽分区，所有的键根据哈希函数映射到 0 ~ 16383 整数槽内，计算公式：HASH_SLOT = CRC16(key) % 16384。每一个节点负责维护一部分槽以及槽所映射的键值数据。 5. Redis中的缓存雪崩和缓存击穿问题 5.1. 缓存雪崩缓存雪崩是指在同一时间，缓存中大量的缓存数据失效，导致大量的请求直接打到了后端数据库上，使得数据库瞬间达到峰值负载，甚至直接宕机。这种情况通常发生在缓存中的数据同时过期时间设置相同，同时失效的情况下。为了避免缓存雪崩问题，可以采取以下措施： 给缓存数据的过期时间加上随机值，避免同时失效。 将缓存数据的过期时间分散开来，比如一部分设置为5分钟，一部分设置为10分钟，一部分设置为15分钟，以此类推。 设置本地缓存，当缓存失效时，先从本地缓存中获取数据，如果本地缓存没有，则去后端数据库取数据，并将数据再次缓存到本地缓存中。 5.2. 缓存击穿缓存击穿是指某个热点key在缓存过期的同时，有大量的请求访问该key的时候，这些请求直接穿透到后端数据库，导致数据库瞬间达到峰值负载，甚至直接宕机。为了避免缓存击穿问题，可以采取以下措施： 在缓存数据不存在时，不直接去查询后端数据库，而是先将该key设置为一个特殊的值（如null），在一段时间内（如5秒）不再查询后端数据库，从而避免瞬间大量请求打到数据库上。 设置热点数据永不过期，或者过期时间非常长，例如一天、一周、一个月等。 设置本地缓存，当缓存失效时，先从本地缓存中获取数据，如果本地缓存没有，则去后端数据库取数据，并将数据再次缓存到本地缓存中。 在使用Redis时需要注意缓存雪崩和缓存击穿问题，采取相应的措施可以避免这些问题的发生，提高系统的稳定性和可靠性。在Redis中，缓存失效可能有以下几种情况： 过期时间到期：当设置了缓存的过期时间后，到期后缓存将会失效。 主动删除：使用DEL命令或者程序中的delete方法，主动删除缓存。 内存空间不足：当Rediss内存空间不足时，Rediss会优先淘汰赛掉内存占用较大，使用较少的缓存，以腾出更多的内存空间。 内存泄漏：当程序中出现内存泄漏时，会导致缓存失效。 Redis重启：当Redis服务重启时，缓存也会失效。 为了避免缓存失效，我们可以设置合适的缓存过期时间，定期检查和清理过期缓存，优化Redis的内存使用，以及避免内存泄漏等问题。 6. 在Spring Boot中使用RedisRedis是一个流行的内存数据存储解决方案，它可以用作缓存、数据库和消息代理。在Spring Boot应用程序中使用Redis可以提高应用程序的性能和可扩展性。在本文中，我们将介绍如何在Spring Boot应用程序中使用Redis。 6.1. .添加Redis依赖要在Spring Boot应用程序中使用Redis，我们首先需要添加Redis依赖，在Maven项目中，我们可以在pom. xml文件中添加以下依赖项： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; 6.2. 配置Redis连接在我们可以使用Redis之前，我们需要配置Redis连接.在Spring Boot应用程序中，我们可以在application.properties文件中添加以下配置： 12spring.redis.host=localhostspring.redis.port=6379 这将配置Redis连接到本地主机的默认端口6379。 6.3. 使用RedisTemplateSpring Boot提供了一个RedisTemplate类，它是一个用于与Redis交互的高级API。我们可以使用RedisTemplate来执行Redis操作，如存储、检索和删除数据。以下是一个使用RedisTemplate执行Redis操作的示例： 12345678910@Autowiredprivate RedisTemplate&lt;String, Object&gt; redisTemplate;public void setValue(String key, Object value) &#123; redisTemplate.opsForValue().set(key, value);&#125;public Object getValue(String key) &#123; return redisTemplate.opsForValue().get(key);&#125; 在上面的示例中，我们注入了一个RedisTemplate对象，并使用它来存储和检索数据。我们使用opsForValue()方法来访问Redis值操作，如设置和获取值。 7. 使用注解除了使用RedisTemplate之外，Spring Boot还提供了一些注解，可以更轻松地使用Redis。例如，我们可以使用@Cacheable注解将方法的结果缓存到Redis中，从而加快方法的执行速度。以下是一个使用@Cacheable注解的示例： 1234@Cacheable(value = &quot;users&quot;, key = &quot;#userId&quot;)public User getUserById(String userId) &#123; // 获取用户信息&#125; 在上面的示例中，我们使用@Cache able注解将User By Id方法的结果缓存到名为”user”的Red is缓存中，缓存键为rId参数。 7.1. 常见客户端比较 lettuce ,redisson,jedis 特性 Lettuce Redisson Jedis 完整性 Lettuce 是一个可扩展的线程安全 Redis 客户端，适用于同步、异步和反应式使用。 如果多个线程避免阻塞和事务性操作（例如 BLPOP 和 MULTI/EXEC），则它们可以共享一个连接。 Lettuce是用netty构建的。 支持高级 Redis 功能，例如 Sentinel、集群、管道、自动重新连接和 Redis 数据模型。 Redisson提供了完整的Redis协议支持,基于Netty 框架，并在其基础上封装了更丰富的特性，如分布式锁、分布式集合等.线程安全的实现 支持Redis复制、Redis集群、Redis哨兵、Redis主从、Redis单机、Redis代理、Redis多集群设置 Jedis提供了基本的Redis协议支持，但支持高级功能如Pipelining、Miscellaneous、Cluster、Pub/Sub等。Redis 官方推荐的Java客户端 连接管理 支持连接池和连接自动恢复机制，具有更好的连接管理能力。 支持连接池和连接自动恢复机制，具有较好的连接管理能力。 支持连接池，但没有连接自动恢复机制， 异步操作 提供完善的异步操作接口和功能，支持Reactive编程模型。 提供完善的异步操作接口和功能，支持Reactive编程模型。 不支持异步操作，只能使用同步方式进行操作。 分布式功能 支持Redis Sentinel和Redis Cluster，可以使用Lettuce进行分布式部署。 提供了丰富的分布式功能，如分布式锁、分布式集合、分布式对象等。 不支持Redis Sentinel和Redis Cluster，不适用于分布式部署。 扩展性 提供了较好的扩展性，用户可以自定义编解码器、连接池实现等。 提供了较好的扩展性，用户可以自定义编解码器、连接管理器等。 较难进行扩展，功能相对固定。 社区活跃度 拥有活跃的开源社区，稳定更新并提供技术支持。 拥有活跃的开源社区，稳定更新并提供技术支持。 拥有较为活跃的开源社区，但相对于Lettuce和Redisson稍显不足。 https://github.com/lettuce-io/lettuce-core最新版本是 https://github.com/redisson/redisson 最新版是 3.23.4 https://github.com/redis/jedis 最近版本是5.0.0 8. 结论在Spring Boot应用程序中使用Redis可以提高应用程序的性能和可扩展性。我们可以使用RedisTemplate类来执行Redis操作，也可以使用注解来更轻松地使用Redis。无论是存储会话数据还是缓存结果，Redis都是一个强大的解决方案。如果您正在使用Spring Boot，请考虑在应用程序中使用Redis。Redis是一种开源内存数据存储解决方案，可用作缓存、数据库和消息代理，其主要特点和原理包括： 内存存储： Redis将所有数据存储在内存中，以便快速读写访问。它还支持各种数据结构，例如字符串、哈希、列表、集合和排序集合。 持久化：Redis支持将数据持久化到磁盘，以实现可靠的存储和恢复。 它提供两种不同的持久性选项：快照和日志。 高可用性：Redis支持主从复制和哨兵机制，实现高可用性和故障点转移。 分布式：Redis Cluster可以将数据分布在多个节点上，以实现水平扩展和高性能。 事务和Lua脚本： Redis支持原子操作和自定义命令的事务和Lua脚本。 总体而言，Redis的主要特点是速度、可靠性和可扩展性.广泛应用于缓存、会话存储、消息队列、计数器、排行榜等各种应用场景。与MongoDB相比，Redis是一种专门的内存数据存储，而MongoDB是一种通用的面向文档的数据库。对于读取密集型工作负载，Redis比MongoDB更快，而MongoDB更适合写入密集型工作负载。Redis由于依赖内存，存储容量有限，而MongoDB可以水平扩展以处理大量数据。与常见的关系数据库和图数据库相比，Redis由于其内存存储和简单的数据结构，在读写性能上更快。Redis的一些优点包括速度，简单性和灵活性.它易于使用，可用于广泛的数据存储和处理任务.它的一些缺点包括对内存的依赖，存储容量有限以及缺乏对复杂查询和连接的支持。Redis是一种开源内存数据存储解决方案，可用作缓存、数据库和消息代理，其主要特点和原理包括： 内存存储： Redis将所有数据存储在内存中，以便快速读写访问。它还支持各种数据结构，例如字符串、哈希、列表、集合和排序集合。 持久化：Redis支持将数据持久化到磁盘，以实现可靠的存储和恢复。 它提供两种不同的持久性选项：快照和日志。 高可用性：Redis支持主从复制和哨兵机制，实现高可用性和故障点转移。 分布式：Redis Cluster可以将数据分布在多个节点上，以实现水平扩展和高性能。 事务和Lua脚本： Redis支持原子操作和自定义命令的事务和Lua脚本。 总体而言，Redis的主要特点是速度、可靠性和可扩展性.广泛应用于缓存、会话存储、消息队列、计数器、排行榜等各种应用场景。与MongoDB相比，Redis是一种专门的内存数据存储，而MongoDB是一种通用的面向文档的数据库。对于读取密集型工作负载，Redis比MongoDB更快，而MongoDB更适合写入密集型工作负载。Redis由于依赖内存，存储容量有限，而MongoDB可以水平扩展以处理大量数据。与常见的关系数据库和图数据库相比，Redis由于其内存存储和简单的数据结构，在读写性能上更快。Redis的一些优点包括速度，简单性和灵活性.它易于使用，可用于广泛的数据存储和处理任务.它的一些缺点包括对内存的依赖，存储容量有限以及缺乏对复杂查询和连接的支持。 8.1. 常见的用途 同步session 分布式事务锁或者是 lock 注册中心 缓存中间件 触发器,订阅-触发的同步机制 在Spring Boot应用程序中使用Redis可以提高应用程序的性能和可扩展性。我们可以使用RedisTemplate类来执行Redis操作，也可以使用注解来更轻松地使用Redis。无论是存储会话数据还是缓存结果，Redis都是一个强大的解决方案。如果您正在使用Spring Boot，请考虑在应用程序中使用Redis。Redis是一种开源内存数据存储解决方案，可用作缓存、数据库和消息代理，其主要特点和原理包括： 内存存储： Redis将所有数据存储在内存中，以便快速读写访问。它还支持各种数据结构，例如字符串、哈希、列表、集合和排序集合。 持久化：Redis支持将数据持久化到磁盘，以实现可靠的存储和恢复。 它提供两种不同的持久性选项：快照和日志。 高可用性：Redis支持主从复制和哨兵机制，实现高可用性和故障点转移。 分布式：Redis Cluster可以将数据分布在多个节点上，以实现水平扩展和高性能。 事务和Lua脚本： Redis支持原子操作和自定义命令的事务和Lua脚本。 总体而言，Redis的主要特点是速度、可靠性和可扩展性.广泛应用于缓存、会话存储、消息队列、计数器、排行榜等各种应用场景。与MongoDB相比，Redis是一种专门的内存数据存储，而MongoDB是一种通用的面向文档的数据库。对于读取密集型工作负载，Redis比MongoDB更快，而MongoDB更适合写入密集型工作负载。Redis由于依赖内存，存储容量有限，而MongoDB可以水平扩展以处理大量数据。与常见的关系数据库和图数据库相比，Redis由于其内存存储和简单的数据结构，在读写性能上更快。Redis的一些优点包括速度，简单性和灵活性.它易于使用，可用于广泛的数据存储和处理任务.它的一些缺点包括对内存的依赖，存储容量有限以及缺乏对复杂查询和连接的支持。Redis是一种开源内存数据存储解决方案，可用作缓存、数据库和消息代理，其主要特点和原理包括： 内存存储： Redis将所有数据存储在内存中，以便快速读写访问。它还支持各种数据结构，例如字符串、哈希、列表、集合和排序集合。 持久化：Redis支持将数据持久化到磁盘，以实现可靠的存储和恢复。 它提供两种不同的持久性选项：快照和日志。 高可用性：Redis支持主从复制和哨兵机制，实现高可用性和故障点转移。 分布式：Redis Cluster可以将数据分布在多个节点上，以实现水平扩展和高性能。 事务和Lua脚本： Redis支持原子操作和自定义命令的事务和Lua脚本。 总体而言，Redis的主要特点是速度、可靠性和可扩展性.广泛应用于缓存、会话存储、消息队列、计数器、排行榜等各种应用场景。与MongoDB相比，Redis是一种专门的内存数据存储，而MongoDB是一种通用的面向文档的数据库。对于读取密集型工作负载，Redis比MongoDB更快，而MongoDB更适合写入密集型工作负载。Redis由于依赖内存，存储容量有限，而MongoDB可以水平扩展以处理大量数据。与常见的关系数据库和图数据库相比，Redis由于其内存存储和简单的数据结构，在读写性能上更快。Redis的一些优点包括速度，简单性和灵活性.它易于使用，可用于广泛的数据存储和处理任务.它的一些缺点包括对内存的依赖，存储容量有限以及缺乏对复杂查询和连接的支持。 8.1. 常见的用途 同步session 分布式事务锁或者是 lock 注册中心 缓存中间件 触发器,订阅-触发的同步机制","categories":[],"tags":[]},{"title":"es查询和简要介绍","slug":"es查询和简要介绍","date":"2023-10-09T09:24:15.000Z","updated":"2024-03-14T14:55:29.476Z","comments":true,"path":"7d305c2e.html","link":"","permalink":"https://imlike.cc/7d305c2e.html","excerpt":"","text":"","categories":[],"tags":[]},{"title":"shade打包后对于META-INF/services数据丢失","slug":"shade打包后对于META-INF-services数据丢失","date":"2023-09-15T10:00:33.000Z","updated":"2024-03-14T14:55:29.481Z","comments":true,"path":"fa86f8b.html","link":"","permalink":"https://imlike.cc/fa86f8b.html","excerpt":"","text":"GeoTools 中的 ReferencingFactoryFinder 类用于查找和获取 CRS（坐标参考系统）工厂类实例，其主要查找路径和实现机制如下： 它会查找所有 META-INF/services/目录下名为 org.opengis.referencing.Factory 的文件。 这些文件通过 java.util.ServiceLoader 机制存放着可 plug-in 的坐标系统工厂类名。 GeoTools 会默认在其 jar 包中添加一些内置的坐标系统工厂类到这个文件中。 用户也可以通过放入额外的 jar 包，在自己的 META-INF/services 中新增工厂类。 ReferencingFactoryFinder 会加载这些工厂类，并缓存到内部的工厂 Map 中。 获取工厂时，则直接从 Map 返回，不需要重新扫描。 如果工厂类不存在，会抛出 FactoryNotFoundException 异常。 使用特定访问","categories":[],"tags":[]},{"title":"thymeleaf渲染","slug":"thymeleaf渲染","date":"2023-09-15T09:47:31.000Z","updated":"2024-03-14T14:55:29.482Z","comments":true,"path":"201304a.html","link":"","permalink":"https://imlike.cc/201304a.html","excerpt":"","text":"解决Thymeleaf模板引擎中的自定义属性和XML命名空间问题Thymeleaf是一个流行的模板引擎，常用于生成HTML页面。在使用Thymeleaf时，我们可能会遇到自定义属性和XML命名空间的问题。这篇文章将介绍如何使用Thymeleaf来解决这些问题，并提供了一些实际示例。 前情介绍在使用Thymeleaf时，有两个常见问题需要解决： 如何添加自定义的HTML属性。 如何在XML命名空间属性中定义命名空间。 让我们逐步来解决这些问题。 添加自定义标签属性Thymeleaf允许我们添加自定义属性到HTML标签中，以将数据传递到模板中。这对于动态渲染页面非常有用。我们可以使用th:attr来实现这一点，如下所示： 1&lt;ResourceURL th:each&#x3D;&quot;resource:$&#123;layer.resourceUrls&#125;&quot; th:attr&#x3D;&quot;format&#x3D;$&#123;resource.format&#125;, template&#x3D;$&#123;resource.template&#125;, resourceType&#x3D;$&#123;resource.resourceType&#125;&quot;&gt; 这将为每个ResourceURL元素添加format、template和resourceType属性，使我们能够以更灵活的方式自定义标签属性。 XML命名空间属性问题有时，我们需要在HTML标签中使用XML命名空间属性，例如xlink:href。然而，Thymeleaf需要命名空间得到正确定义，否则会出现错误。解决这个问题的步骤如下： 在HTML文档的根元素中，使用xmlns:xlink来定义xlink命名空间。 12htmlCopy code&lt;html xmlns:th&#x3D;&quot;http:&#x2F;&#x2F;www.thymeleaf.org&quot; xmlns:xlink&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;1999&#x2F;xlink&quot;&gt; 现在，你可以在标签中使用xlink:href属性。 12htmlCopy code&lt;a xlink:href&#x3D;&quot;https:&#x2F;&#x2F;example.com&quot;&gt;My Link&lt;&#x2F;a&gt; 这样就能够使用xlink:href属性而不再出现命名空间错误。 汉字解析最后，如果你需要在Thymeleaf中解析汉字或其他文本，你可以使用th:utext。这个属性用于在HTML中安全地渲染未经转义的文本，如下所示： 12htmlCopy code&lt;span th:utext&#x3D;&quot;$&#123;aa.ddd.fdd&#125;&quot;&gt;汉字内容&lt;&#x2F;span&gt; th:utext将$&#123;aa.ddd.fdd&#125;中的文本进行渲染，确保它能正确显示。 总结Thymeleaf是一个功能强大的模板引擎，用于生成HTML页面。在使用Thymeleaf时，我们可以轻松解决自定义属性和XML命名空间的问题。请确保在使用XML命名空间属性时正确定义命名空间，以避免出现错误。希望这篇文章对解决Thymeleaf中的这些问题有所帮助。解决Thymeleaf模板引擎中的自定义属性和XML命名空间问题 Thymeleaf是一个流行的模板引擎，常用于生成HTML页面。在使用Thymeleaf时，我们可能会遇到自定义属性和XML命名空间的问题。这篇文章将介绍如何使用Thymeleaf来解决这些问题，并提供了一些实际示例。 前情介绍在使用Thymeleaf时，有两个常见问题需要解决： 如何添加自定义的HTML属性。 如何在XML命名空间属性中定义命名空间。 让我们逐步来解决这些问题。 添加自定义标签属性Thymeleaf允许我们添加自定义属性到HTML标签中，以将数据传递到模板中。这对于动态渲染页面非常有用。我们可以使用th:attr来实现这一点，如下所示： 12htmlCopy code&lt;ResourceURL th:each&#x3D;&quot;resource:$&#123;layer.resourceUrls&#125;&quot; th:attr&#x3D;&quot;format&#x3D;$&#123;resource.format&#125;, template&#x3D;$&#123;resource.template&#125;, resourceType&#x3D;$&#123;resource.resourceType&#125;&quot;&gt; 这将为每个ResourceURL元素添加format、template和resourceType属性，使我们能够以更灵活的方式自定义标签属性。 XML命名空间属性问题有时，我们需要在HTML标签中使用XML命名空间属性，例如xlink:href。然而，Thymeleaf需要命名空间得到正确定义，否则会出现错误。解决这个问题的步骤如下： 在HTML文档的根元素中，使用xmlns:xlink来定义xlink命名空间。 1234&lt;html xmlns:th=&quot;http://www.thymeleaf.org&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt; &lt;a xlink:href=&quot;https://example.com&quot;&gt;My Link&lt;/a&gt; 这样就能够使用xlink:href属性而不再出现命名空间错误。 但是在标签价下列类似的属性的时候就会出现问题, 在添加 xlink:herf=”${}”是会报错 123error on line 9 at column 64: Namespace prefix xlink for href on ProviderSite is not defined&lt;?xml-stypos copiable-processing-instruction?&gt;&#x2F;&#x2F;processors.add(new StandardXmlNsTagProcessor(TemplateMode.XML, dialectPrefix)); 解决步骤参考下列链接 https://github.com/mxab/thymeleaf-extras-data-attribute xline:url 这种标签 是否显示特定标签 最后实现的效果是 1234&lt;html&gt;&lt;body xlink:url&#x3D;&quot;$&#123;&#39;bar&#39;&#125;&quot; data:msg&#x3D;&quot;#&#123;my.message&#125;&quot; &gt;&lt;&#x2F;body&gt;&lt;&#x2F;html&gt; 汉字解析最后，如果你需要在Thymeleaf中解析汉字或其他文本，你可以使用th:utext。这个属性用于在HTML中安全地渲染未经转义的文本，如下所示： 12htmlCopy code&lt;span th:utext&#x3D;&quot;$&#123;aa.ddd.fdd&#125;&quot;&gt;汉字内容&lt;&#x2F;span&gt; th:utext将$&#123;aa.ddd.fdd&#125;中的文本进行渲染，确保它能正确显示。 总结Thymeleaf是一个功能强大的模板引擎，用于生成HTML,XML页面。在使用Thymeleaf时，我们可以轻松解决自定义属性和XML命名空间的问题。请确保在使用XML命名空间属性时正确定义命名空间，以避免出现错误。希望这篇文章对解决Thymeleaf中的这些问题有所帮助。","categories":[],"tags":[]},{"title":"奥卡姆剃刀的谎言(转载)","slug":"奥卡姆剃刀的谎言","date":"2023-08-15T09:51:06.000Z","updated":"2024-03-14T14:55:29.484Z","comments":true,"path":"8b149922.html","link":"","permalink":"https://imlike.cc/8b149922.html","excerpt":"","text":"source：奥卡姆剃刀的“谎言” 假如时光倒流，回到梅西来迈阿密国际球队之前，请你做一个预测。 情境：迈阿密国际已经连续9场不胜。 请问：球王梅西加入球队后，以下哪个更有可能？ A. 该队在梅西首秀比赛中获胜。 B. 该队在梅西首秀比赛中获胜，并且主场球迷将会集体高呼球王的名字。 你会选择A还是B？ 我猜许多人会选择B。 但事实上，尽管B的情境似乎更符合想象（并且也发生了），但A的概率实际上是更高的。 因为A包括了B的可能，所以你预测A可能性更大，才是理性的。 诺奖得主卡尼曼曾经设计过一个类似的实验，大部分人选择了“看似更有可能然而其实概率更小”的选项，他将其称为“合取谬误”。 增加“球迷集体高呼梅西的名字”这一假设，符合人类的想象力和讲故事的本能，但却是一种假设负荷。 假设负荷会降低一件事情发生的概率。 而过多的假设，正是奥卡姆剃刀所要砍掉的东西。 美国化学家霍夫曼说：好的理论，就要尽可能地简单，你把它一减再减，直到再减就什么都剩不下为止。 因为剩下的每一条都是本质性的。 人生亦如是。 一 少做假设， 而非尽量简单 大多数人对“奥卡姆剃刀”的理解是错的。 奥卡姆剃刀的原理经常被简化为“最简单的解释往往是正确的”，但事实上，这个原则的真正意义更为微妙。 奥卡姆的威廉所说的“Numquam ponenda est pluralitas sine necessitate”，翻译成中文，大意是： “不必要时，不要假设多余的东西”。 勿增实体，“实体”指的是假说、解释或模型的组成部分。 例如，你在天线里探测到了微波，你应该寻找熟悉的实体来解释这一现象，如雷达、鸽子，而不是增加新的实体如“三体外星人来袭”来解释它。 所以，“奥卡姆剃刀”的关键是少做假设，而非尽量简单。 “奥卡姆剃刀”并不是一个绝对法则，而是一个指导原则。 这个原则强调的是，当我们构建理论或解释现象时，应该避免不必要的假设。 例如，如果两个理论都能够解释同一个观察到的现象，但其中一个理论需要更多的假设，那么奥卡姆剃刀原则建议我们选择假设较少的那个理论。 例子A：外星人来过的痕迹 情境：假设你有一天早上发现自家的花园里有一块草地被压得扁扁的，图案很特别，就像有什么东西压过。你想找出导致这种情况的原因。 假设1：昨晚，一只大猫进入了你的花园，在那块草地上玩耍和躺了一会儿。 假设2：昨晚，外星人驾驶着飞碟来到地球，选择了你的花园作为着陆点。在这过程中，飞碟稍微触及了那块草地，但由于某种原因它们决定放弃在此停留，然后又飞走了。 虽然两个假设都可以解释草地被压扁的现象，但显然，假设2需要许多额外的假设（如外星人存在、他们会来地球、他们正好选择了你的花园、他们只是轻轻触碰了草地而没有留下其他痕迹等）。 而假设1则更为简单，只需要一个常见动物——大猫作为解释。 根据奥卡姆剃刀原则，我们应该选择假设1作为更可能的解释，因为它更为简洁，没有额外的、不必要的假设。 但这并不意味着简单的解释总是正确的。事实上，有时候真实的解释可能是复杂的。 奥卡姆剃刀只是一个指导原则，告诉我们在等同的解释能力下，应该优先选择假设较少、更为简洁的解释。 例子B：古代文献中的神秘描述 情境：在一些古代文献中，有描述关于“飞天的车辆”。 假设1：古代文明拥有高度发达的科技，能制造飞行器。 假设2：这些描述可能是象征性的，或者是对某些自然现象（如流星、彗星等）的解释。 虽然第一个假设为我们提供了一个激动人心的画面，即古代文明拥有高科技，但我们没有确凿的证据表明古代文明有这样的技术。 根据奥卡姆剃刀原则，第二种假设更为合理，因为它基于我们关于古代文明的现有知识和对自然现象的理解。 二 奥卡姆剃刀 7种“谎言” 奥卡姆剃刀原则在科学、哲学和其他学科中都被广泛采用，它鼓励我们避免不必要的复杂性和假设。 但同样因此，现实中充满了奥卡姆剃刀的“谎言”，例如： 各种“复杂生活简单化”的人生鸡汤； “像乔布斯那样追求极致简单”的创业箴言； 聪明人追求简单，傻瓜把问题搞复杂； 甚至连“头脑简单”也借奥卡姆剃刀大行其道； …… 概括而言，大约有如下常见的奥卡姆剃刀的“谎言”。 谎言1：奥卡姆剃刀是选择最简单的答案 这是最常见的误读，奥卡姆剃刀原则往往被过度简化为”最简单的解释就是正确的”。 然而，实际上奥卡姆剃刀并不是选择最简单的答案，而是当解释某一现象时，我们应尽量减少不必要的假设。 例如，说地球是平的是一个简单的解释，但我们知道这并不正确。 解读： 在科学研究中，这意味着选择最能解释数据并且假设最少的模型。 在日常生活中，这意味着在面对复杂问题时，我们应先考虑最直接、最明显的因素，而不是寻找复杂的解释。 谎言2：奥卡姆剃刀是绝对的科学原则 奥卡姆剃刀被误读为在所有情况下都能确定科学决策的绝对法则。然而，它只是一个建议，一个指导原则，用来指导我们优先选择假设更少的解释。 解读： 在某些情况下，更复杂的理论或模型可能会提供更准确的预测或更深入的理解。 因此，我们不能盲目地依赖奥卡姆剃刀，而应结合实际情况进行判断。 有些人可能会错误地认为，如果一个解释很简单，那么它就不需要证据支持。这是错误的，因为奥卡姆剃刀并不能代替证据的收集和分析。 谎言3：奥卡姆剃刀就是简化生活 有些人错误地将奥卡姆剃刀与简化生活的理念相联系。他们误以为奥卡姆剃刀意味着我们应该追求简单的生活方式，避免复杂和混乱。 解读： 实际上，奥卡姆剃刀原则在某种程度上可以应用于生活决策，但它并不直接涉及到如何简化生活方式。 生活有时候应该断舍离，做减法。尤其是去除那些错误的假设–只要拥有了某某东西我就能够过得更好，但事实上并非如此。 然而，减法人生是一种积极主动的选择，而非又一个堵塞心灵的万能药。 谎言4：应该尽量避免复杂性 有些人可能会误认为，由于奥卡姆剃刀鼓励我们选择最简单的解释，那么我们在生活或工作中也应该尽量避免复杂性。 解读： 在某些情况下，我们可能需要面对并处理复杂性，因为某些问题本身就是复杂的，我们不能过于简化它们。 iPhone的确很简单，然而里面的芯片很复杂。 简单指的是用户体验，而非工作不复杂。 所以任正非说，简单留给客户，复杂留给自己。 乔布斯的确能够挥舞“奥卡姆剃刀”砍来砍去，那是因为他有一群顶级人才，解决复杂性问题。iPhone最初一样有很多样机。 “选择”是奥卡姆剃刀，但是如果没有“多样性”，哪里来的选择？ 谎言5：奥卡姆剃刀就是“大道至简” 尤其是在投资领域，人们总是爱说，你看，巴菲特的投资道理多简单？ 那么简单为什么很少有人能做到呢？只是因为不想慢慢变富？ 显然没那么简单。 就像围棋，规则最简单，但却是最复杂的棋类游戏。 价值投资的确有极少的假设，这一点复合奥卡姆剃刀的原则。 然而要做好价值投资并不容易，某些投资人喜欢让自己显得云淡风轻，是因为按捺不住“成神成仙”的优越感。这么做其实误导了许多“普通人”。 解读： 越是“简单”的道理，越需要极深的洞见。这仿佛是某种能量守恒。 正所谓“功不到不为财”，又省力，又省“距离”，从物理上说不通。 事实上，巴菲特不仅在专业领域有极深的功底，并且过的是一种苦行僧似的生活。 谎言6：我们应该追求最少的工作量 这种理解可能会导致人们认为他们应该尽量避免做额外的工作，或者总是选择最少工作量的路径。 解读： 实际上，奥卡姆剃刀的原则并不直接涉及到工作量的问题。它是关于如何选择最好的解释或理论，而这并不一定意味着最少的工作量。 例如，在设计一台机器或编写一个程序时，应尽量减少不必要的组件或代码，而不是简单地减少它们的数量。 有时候，为了找到最好的解决方案，我们可能需要付出更多的努力和时间。 牛顿三大定律很简单，而他的《自然哲学数学原理》则很厚。 就像本文也许很长，但废话很少；而某些评论很短，但每个字都是废话。 谎言7：奥卡姆剃刀排斥创新 有人可能误以为奥卡姆剃刀原理抑制新颖和创新的想法，因为这些新颖的想法往往看起来更复杂。 然而，实际上，如果一个创新的理论提供了更好的解释，即使它比现有的理论更复杂，也是可以接受的。 三 默认选项 致命的假设 如下话题，是我这两年看到的最为触动的观念之一，就其本质而言，亦和奥卡姆剃刀同源。 克里斯坦森在《你要如何衡量你的人生》讲了一个案例： 巴黎迪斯尼乐园的致命假设。 迪士尼的前三个乐园都很成功，第四个选址是在巴黎郊区，头两年里，他们损失了将近10亿美元。 复盘时发现，失败源自项目的规划有如下假设： 每年有1100万游客，平均逗留3天。于是便是用1100万乘以3，设想出每年有3300万人次“游客逗留”，并据此建酒店和基础设施。 现实呢？第一年的确有接近1100万游客参观迪士尼，但是，平均每个游客逗留的时间只有一天。 为什么？其他迪士尼乐园有45个游乐设施，人们可以玩儿3天；而巴黎迪士尼乐园只有15个游乐设施，一天就够了 可是，在决策时，人们默认了每个迪斯尼乐园的游客数据都是一样的。 正确的做法是，盘点决策时所作的所有默认假设，并且追问： “如果这个假设成立，最重要的条件是什么？怎样跟踪落实呢？” 由是，克里斯坦森为我们分享了一个简单然而宝贵的方法： 当你要做一项新的决策时，先做一下“测试前提假设”。 认真问自己，“在哪些前提假设得到验证的条件下，才能证明你的策略是正确的？” 很多时候，我们都是默认一些选项，然后慢慢就忘记了这些默认选项的假设前提和适用环境。 拷问自己的观念和决策的默认假设，是奥卡姆剃刀在现实生活中最有效的应用场景。 事实上，有很多我们犯下的错误，起初看起来很正确，一路也都很正确，然而却是建立在错误的假设基础之上的。 我们一开始就默认了这些假设，越往后就越不记得来质疑这些假设成立的前提。 四 奥卡姆剃刀 也许是个概率问题 请你来做一个测试游戏。 有一个名叫Linda的女性，她关心社会问题，在学校积极参与学生会活动，经常在网上为女性权益发声。请问，下述两种情况，哪一种更可能为真？ 1、Linda是个银行职员； 2、Linda是个银行职员并且是女权主义者。 卡尼曼曾经设计过如上实验，结果大部分人都选择了2。 这似乎有点儿奇怪，因为“1”包括了“2”，当然是“1”为真的可能性更大。 但是在很多人看来，“2”更符合他们对Linda的印象。 这个实验显示，人们在做判断时，可能会过度追求具体性和详细性，而忽视了基本的概率原则。 虽然看起来有点儿绕，这也是一个奥卡姆剃刀原理被误用的例子，即过度增加不必要的假设，而不是选择更简单、更一般的解释。 并且，越是受过教育的人，甚至是读书越多的聪明人，如果没有经受太多现实世界的捶打，越容易犯这类错误。 卡尼曼将这种非理性现象称为合取谬误（”联合偏误”）： 人们在做决策时，往往会错误地认为一个更具体（因此包含更多假设）的事件比一个更一般的事件更可能发生，这与概率论的基本原则相违背。 从这个角度看，奥卡姆剃刀原理说到底是个概率问题。 一旦你为原来的解释引入新的因素，或者提出一个新的假设，那么该解释为正确的概率实际上是下降了。 最大的默认假设，可能就是黑天鹅问题了。 黑天鹅和奥卡姆剃刀，都告诉我们，所有的假设都有不确定性，应该冠以概率。 黑天鹅则提醒我们别把那些默认前提当作100%，甚至于你客观地给予了99%的概率，那1%被忽视的结果也可能是致命的。 五 用奥卡姆剃刀 砍掉假设负荷 在临床医学中，当面对一个病患的症状组合，医生通常首先考虑那些最常见和最可能的解释，即所谓的“常见疾病常见”。 例如，如果一个年轻人有持续的喉咙痛，首先要考虑的可能是扁桃体炎，而不是立即考虑罕见的喉咙肿瘤。 当然，医生在诊断过程中需要详细询问病史、做全面体检和必要的检查，确保不错过重要的信息。 在给各种可能性排序时，如果把每个诊断背后所代表的全新假设加起来（统计其数量及概率大小），就可以算出这会造成多大的假设负荷。 每当我们增加新的假设或因素，我们不仅增加了复杂性，还增加了不确定性，因此增加了出错的可能性。通过尽量限制我们的假设数量和引入高概率的因素，我们可以更有可能获得正确的答案。 例子A：埃及法老图坦卡蒙的死因 情境：年轻的法老图坦卡蒙死于公元前1323年，但他的死因一直是个谜。有人提出了各种假设，包括谋杀、疾病和意外。 假设负荷：尽管有一些研究建议图坦卡蒙可能是被谋杀的，但近期的研究表明，他可能是因为腿部感染或其他健康问题死亡。 这个解释有较低的假设负荷，因为它不需要引入复杂的宫廷阴谋或其他外部因素。 例子B：波尔图蒂事件 情境：1970年代，英国发生了一起家庭持续遭受超自然现象骚扰的事件，包括家具自行移动、声音和其他奇怪的现象。 假设负荷：尽管一些人认为这是超自然现象，但其他人则认为，这些事件可能是由于自然原因或家中的某个成员故意制造的。 后来，有证据表明家中的一名少女可能参与了部分事件的制造。 这种解释具有较低的假设负荷，因为它只需要考虑家庭内部的动机和行为，而不需要涉及超自然的因素。 更常见的例子是，假如一位平时遭遇家暴的女性不幸遇害，凶手大概率是其老公。 奥卡姆剃刀原理可以被视为一种有效的逻辑和决策工具，帮助我们在面对复杂和不确定的情况时，尽可能地减少新的假设，聚焦于最可能的解释，从而降低决策的复杂性和不确定性。 但这并不代表奥卡姆剃刀不重视小概率的可能性。例如本节开头的喉咙痛，假如排在前面的可能性被排除，医生一定会继续考虑排在后面的较小可能性。 不过，对于普通人而言，假如突然关心起自己的健康，上网查知识来自我诊断，经常会用小概率的可能性把自己吓个半死。 六 多个香炉多个鬼 太多假设不靠谱 “多个香炉多只鬼”，是妙极的广东俚语，常见于发烧友，指过多设备堆砌反有损音质。 这句话在某种程度上可以视为奥卡姆剃刀原则的一种生动表达。 它直接指出了如果我们在解决问题时引入过多的变量或假设，反而可能导致我们分析问题的复杂度增加，使得问题的解决更为困难。 在此背景下，”多个香炉多个鬼”意味着，如果你对很多事情都进行祈祷和拜祭，那么你也要面对更多的鬼神，如愿的可能性也就降低了。 这句话用于形容“太多假设不靠谱”，也很贴切。 所谓不靠谱，就是有些事情，看起来很漂亮，很巧妙，但是有太多假设：假如…假如… 一个理论、观点或者计划，如果依赖于太多的“假如”，则其实现的可能性和稳定性就会大大降低。 有些人，有太多资源，人脉丰富，但却很难成事儿。因为他们的逻辑是“整合资源”，可这类事儿和”多个香炉多只鬼”一样，要依赖太多不确定性事情的同时成立。 每当我听到有人讲起自己的商业计划，只要涉及到两个以上的“香炉”，就会觉得有些虚无飘渺。 此外，很多时候，人的错误假设，往往是因为欲望，或者是所谓美好愿望。 从这个角度看，英文俗语“Don’t count your chickens before they’re hatched”很有道理。 别在鸡仔们孵出来之前去数你有多少鸡。这是一个很好的提醒，因为过早的预期可能会让人忽视或低估实际情况的复杂性和不确定性。 现实中有太多类似的教训了。记得2015年，有个朋友满仓加杠杆，一天就是几千万“收益”，于是便开始算这一波能赚多。结果后来全栽进去了。 和奥卡姆剃刀一样，上面这句俗语则提醒我们，在事情尚未确定时，我们应避免过多的假设和预期。 说起来，别说谎，对自己诚实，也有异曲同工之妙： 你说了一个谎，就要说更多谎。 就像一个错误的假设，会令假设之上那些不管看起来多正确的事情都变得错误。 错误的假设，其实是自己骗自己。而且由于自己最懂自己，所以往往骗得很深，很真诚。 诚实和简单并不一定总是最容易的选择，但它们往往能引导我们走向更健康、更实质的解决方案。 七 复杂模型 过拟合风险 冯·诺依曼在一次会议上，听到某物理研究员使用复杂模型，试图论证实验数据点都落在同一条曲线上，“所以”符合模型预期。 冯·诺依曼调侃道：你还不如说这些点都在同一个平面上…… 这位科学大师戏谑地留下一个经典话语： “给我四个参数，我可以画出一头大象；再给我一个，我可以让它鼻子摇摆。” 1953年，年轻的康奈尔教授戴森，与学生用赝标介子理论得到的计算结果与费米的实验相吻合。 激动的戴森马上找到费米分享，但费米批评说：“你的计算既没有清晰的物理图像，也不是精确的数学体系。” 戴森不解，费米问他：“你的计算用了几个参数？” 戴森说：“四个。” 费米笑道：“我朋友冯·诺依曼说过，四个参数可以拟合大象，五个还能摇鼻子呢。” 戴森听后失落，决定调整研究方向。 冯·诺依曼的故事强调了，给予一个模型过多的参数，它有能力拟合任何数据，但这并不意味着模型具有真实的预测力或对现实世界有深入的理解。 事实上，当一个模型有太多的参数，它可能过于复杂，导致过拟合。这意味着它可能在已知数据上的表现很好，但在新的、未知的数据上表现很差。 奥卡姆剃刀原则恰好警告我们避免不必要的复杂性，因为额外的复杂性可能并不带来更好的预测能力。 从冯·诺依曼的大象，很容易想起毕加索的牛。 毕加索的抽象化减法，就像是对现实世界事物的一种简化和抽象。抽象化的过程中，我们抛弃了许多细节，而保留了最基本、最核心的特征，从而帮助我们更好地理解和表达事物的本质。 这种思想也与奥卡姆剃刀原则相符，奥卡姆剃刀原则鼓励我们在解决问题时，尽可能地简化我们的理论或假设，只保留最基本和必要的部分。 实在论者相信存在普遍性的概念，如柏拉图的理念或亚里士多德的普遍性，认为物体的性质有独立的实体存在。 例如，牛有“牛性”，樱桃有“樱桃性”，而人成为父亲是因为他们具有“父性”。 然而，奥卡姆反对这种观点，他提出，普遍性只是我们为同类事物所用的词语。这种思想称为唯名论。 奥卡姆主张，不必为每个概念增加一个实体。例如，父亲之所以是父亲，仅因他有孩子，无需其他复杂解释。 与过拟合的大象相反，“抽象牛”具有更好的泛化能力。 泛化 (Generalization)：是一个过程，通过它我们从特定的例子或经验中提炼出普适性的原则或规则。 这使我们能够将在某个情境下学到的知识应用到新的、类似的情境中。 毕加索的抽象牛，更接近牛的本质，也几乎适用于所有的牛。 虽然只有几根线条，但它们都在诉说着牛的特征，尽管这些线条只是对复杂实物的一种简化，但却足以帮助我们理解和认识牛这个生物。 八 成功的“秘诀” 精确的模糊 有位朋友对我说，当今短视频创作的流量密码是： 只要做到以下三点，普通人也能翻身逆袭…… 由于很不幸《人生算法》一书也被归类为成功学，所以我也有过数次被读者追问“你能不能给我具体的方法来实现成功”的经历。 某种意义上，“成功学”最大的缺陷就是过拟合。 当你想健身，你可能会模仿某健身达人：早起跑步、严格饮食。 这就像“过拟合”：盲目复制特定的成功模式，但可能并不适合你。 如果你从多位达人那汲取智慧，形成如“持续锻炼”、“均衡饮食”等原则，这更像“泛化”，找到适合自己的方法。 泛化和过拟合也是机器学习和统计建模中常常对立的两个概念。 泛化（generalization）是模型对新的、未见过的数据的表现能力。 一个有良好泛化能力的模型能够很好地应用于未见过的数据，即使这些数据在训练时并未出现。 过拟合（overfitting）则是指模型过度复杂，以至于开始“记忆”训练数据中的噪声或异常值，而不是从数据中学习其内在规律。 这样的模型虽然在训练数据上的表现很好，但在未见过的新数据上往往表现糟糕。 所以，我们可以说过拟合是泛化的反面： 一方面，我们希望模型能尽可能地学习到训练数据中的信息，也就是要尽量减少偏差（bias）； 另一方面，我们又希望模型不要过度复杂，能够适应新的数据，也就是要尽量减少方差（variance）。 这就是所谓的偏差-方差权衡（bias-variance tradeoff）。 为了得到一个好的模型，我们希望找到偏差和方差之间的平衡点，从而实现“模糊的精确”与“精确的模糊”之间的平衡，这也是机器学习中的核心挑战。 奥卡姆剃刀原则在这里也有应用。它鼓励我们选择假设最少、最简单的模型，从而避免过拟合的问题，提高模型的泛化能力。 九 “KISS”原则 以实现复制和适应性 在工程界，奥卡姆剃刀原则被认为是KISS（Keep it simple，stupid，保持简单，蠢蛋），美国海军在上世纪60年代采用了这一设计法则。 由此实现的价值大约有两个关键点： 1、提升复制的效率，降低在复制过程中出错的可能性； 2、提升适应复杂环境的性能。 以上这两点，往学术话题靠，就是达尔文理论的生命的复制和生存；往商业话题靠，则是商品与服务的规模效应和竞争优势。 奥卡姆剃刀原则推崇的是简单和清晰的思考方式，强调做出最少的假设，选择最简洁的解释或理论。 当应用这个原则时，我们通常会得到一个较为简单和直观的系统或模型，这种模型有助于我们清晰理解现象，并且在我们需要做出改变来适应新环境时，这种简单的模型更容易进行修改和调整。 与此同时，健壮性是系统或模型能够在不确定环境中稳定运行的能力。当环境发生变化时，一个健壮的系统能够有效地适应新环境，继续稳定运行。 因此，奥卡姆剃刀原则和健壮性有很大的关联。在奥卡姆剃刀原则指导下构建的简单系统，往往更健壮，因为它们的简单性使得它们更容易适应环境的变化。 再说回工程和商业。 传统的汽车生产通常包含许多零部件的组装，这意味着在生产过程中存在大量的工艺步骤和可能的出错点。 而一体化车身的设计则能够减少这些步骤，简化生产流程。例如，特斯拉Model Y就采用了全铝一体化车身，这大大减少了车身生产所需的零部件数量和组装步骤，提高了生产效率。 另一方面，一体化车身的设计也可能带来更好的车辆性能。比如，一体化车身可以提供更强的刚性和强度，提高车辆的安全性能。 同时，由于零部件数量的减少，车辆的重量也可能降低，这有助于提高电动汽车的续航里程。 当然，一体化设计也有其挑战，例如设计和制造过程的复杂性可能增加，需要更高的精度和技术水平。 所以，每一次奥卡姆剃刀式的简化，都对应着某个更高维度的“复杂”。 就像特斯拉需要通过不断的技术创新和生产工艺的优化，实现这种“简洁”的设计。 的确，英国生物学家约翰乔伊·麦克法登在《生活很简单》一书中说，现代性的各个领域都以简洁为宗旨，奥卡姆剃刀是现代世界的基础。 “从约翰·凯奇的极简主义音乐，到柯布西耶的建筑、贝克特简洁的文字、平板电脑光滑的边缘，现代文化潜心于简洁”。 可是，在现实世界里，简洁往往需要付出不简单的代价。 十 保持简单 但不能过于简单 生活并没有那么简单。 如何定义“简单”，是个很复杂的话题。 例如，欧拉公式 e^(iπ) + 1 = 0 被广泛认为是一个非常”简单”而优雅的公式，因为它只用了五个基本的数学常数就表达出了一个深刻的数学关系。 但有多少人能搞懂这个简单呢？ 在奥卡姆剃刀原理的背景下，”简单”通常被理解为假设的数量或复杂性的减少。 然而，这个概念可能会被误解或误用，下面列出了一些可能的混淆：、 1、简单不等于简陋：在追求简洁的过程中，我们不能忽视必要的复杂性。 一个理论应该足够复杂，能够解释所有相关的观察结果，而不仅仅是其中的一部分。 2、简单不等于容易理解：有时候，一个理论可能需要较少的假设，但它的内涵可能非常深刻，需要专业知识才能理解。 例如，在物理学中，量子力学是一个需要相对较少的基本假设就能解释大量现象的理论，但它并不容易理解。 3、简单不等于没有变化或动态：有些人可能会将”简单”理解为静态或不变的，但这并不总是正确的。 一个理论可以包括动态的元素或变化的过程，同时仍然保持相对的简洁。 4、简单不等于总是正确：尽管奥卡姆剃刀原理鼓励我们选择最简单的解释，但这并不意味着最简单的解释总是正确的。 在许多情况下，更复杂的理论可能会提供更准确的预测或更深入的理解。 因此，在使用奥卡姆剃刀原理时，我们需要谨慎地理解和应用”简单”这个概念，避免过度简化或误用。 所以，认为奥卡姆剃刀原则，是要简化生活，过简单的人生，其实只是一种鸡汤话的说法。 因为奥卡姆剃刀的“简单”，需要极深的洞见，和极高的成本。 也许有人会说：尊重常识就可以了嘛。 可是，常识多么昂贵，多么稀缺啊。 爱因斯坦说：“Everything should be made as simple as possible, but no simpler.” 这句话强调了简洁和复杂之间的平衡，这也是奥卡姆剃刀原理的一个重要组成部分。 巴菲特的价值投资，在某种意义上也是奥卡姆剃刀，他将所有的评价标准归结于： 在企业生存周期里，总共能赚多少钱？折现到现在值多少钱？能够有多少安全边际（也是概率问题）？ 但是，巴菲特的“简单”，事实上非常不简单，他为自己的投资帝国构建了一重又一重的堡垒。 当我们说“简单”时，我们实际上是在讨论一种能力——那就是从复杂性中提炼出核心的、最关键的因素。 巴菲特所展现的“简单”其实是经过深入思考和分析后得出的结果，而这背后的复杂性和努力常常被人们忽视。 奥卡姆剃刀原则与凯恩斯的金融理论都强调简洁性和实用性。凯恩斯将金融回报分为基于资产价值的“投资”和基于市场心态的“投机”。 聪明如凯恩斯，一辈子踩过几次大坑才磨好自己的奥卡姆剃刀。 脱离基本功的“简单”，可能只是一种玄学。 当然，运气可以很简单哈，人的一生很短，一个好运气也够撑得住一生了。 小结 奥卡姆剃刀使用指南 好了，我该用剃刀为本文收尾了。 1、有选择的才叫简单生活。那些原本就没选择了，然后说过简单生活，不可信。否则就像太监说自己要禁欲。 也许人生的奥卡姆剃刀，大多是被迫的主动选择吧。 2、任正非说的简单是给客户的，企业自己则要付出很大的代价。 3、对于个体而言，遵循常识，可以让生活很简单。这类特性需要有一个完美的家庭和童年来支撑。 4、乔布斯是一个众所周知的极简主义者，他的产品设计、商业战略，乃至他的个人生活方式，都以简单和易于理解为核心。他对简单的追求为苹果公司带来了众多创新和成功的产品，例如iPod、iPhone、iPad等。 然而，他是用一流人才，然后让自己的剃刀可以挥舞。 乔布斯的简单，和他的简单衣服一样，非常昂贵。 5、认知升维，行动降维。 “认知升维”，是指只有全面理解问题，我们才能找到最有效的解决方案。 然后，当我们实际行动解决问题时，我们需要尽可能地”降维”，即简化行动，专注于最重要和最核心的任务，而不是让自己陷入不必要的细节或复杂性。这种方法可以提高我们的效率，并减少执行的难度。 这个原则很好地体现了奥卡姆剃刀原理的精神。在理解问题时，我们需要尽可能全面和深入；而在行动时，我们应该尽可能简化和直接。这种思考和行动的方式可以帮助我们更有效地处理复杂的问题和任务。 6、奥卡姆剃刀有时候是做除法，例如减少维度；有时候是做减法，例如减少假设。 7、第一性和奥卡姆剃刀的关系千丝万缕。 牛顿说：对于自然万物的成因，我只认可那些既真实又足以解释其现象的原因，其他皆属多余。 这里的第一性，既有科学的，也有信仰的。 牛顿认为宇宙的秩序和规律是上帝创造的，科学研究实际上就是去探寻上帝创造的自然规律。 8、有人说，我想明白了，但就是下不了手，对自己不够狠，自己的奥卡姆剃刀不够锋利。 但真相是，上面说的“下不了手”，其实就是因为没有想明白。 9、奥卡姆剃刀认为假设应该具有可证伪性。 奥卡姆剃刀强调我们应该选择那些可以通过经验检验的假设。一个理论如果无法被证伪，那么它就缺乏科学价值。 我在网上看到一个人的签名和有个性： 人生没有那么多的假设，现实是一个一个真实的耳光。 的确，当世界处于下行的周期时，是诸多假设一个个破灭之际。 “不要浪费了危机”这种提法挺让人烦的–要不我的危机您也拿去？ 但如果我们被迫要为人生做减法，也许可以使用如下奥卡姆剃刀原则： 方法一、专注于基本的原则和价值观。 去除那些虚妄的东西，回归真实，回归自我，回归本质。 此外，在面对复杂的决策时，我们可以将自己的原则和价值观作为决策的基础。 这可以帮助我们减少不确定性和混乱，也可以保持我们的行动和价值观的一致性。 方法二、简化生活。 尽可能减少不必要的物质和精神负担。 例如，我们可以定期清理不再使用的物品，也可以通过冥想或者其他方式来降低我们的心理压力。 方法三、减少不必要的假设。 我们不能假设，未来几十年和过去几十年一样。 也不能假设，所有跌下去的东西都可能涨回来。 方法四、直接面对问题。 当我们面临问题时，我们应该直接面对它，而不是寻找复杂的解决方案或者回避问题。 通过直接解决问题，我们可以避免不必要的复杂性，并且更快地达到我们的目标。 概括而言：与其被生活剃，不如自己剃。 此外，也许我们需要习惯于接受失去。 这原本是生命和时光的本质，且并不可怕。 犹如侘寂所欣赏的不完美、有瑕疵和不长久，并对自然的流转、事物的暂时性和衰老持有敬意。 通过主动失去而得到，经由复杂达至简洁，消除过多假设而拥抱现实的真相– 奥卡姆剃刀很残酷，但也足够温柔。","categories":[],"tags":[]},{"title":"索引入门和常见索引介绍","slug":"空间索引","date":"2023-07-29T10:49:20.000Z","updated":"2024-03-14T14:55:29.486Z","comments":true,"path":"5d8e928a.html","link":"","permalink":"https://imlike.cc/5d8e928a.html","excerpt":"","text":"B树二叉树二叉平衡树(avt)红黑树B+树geohashR树空间填充曲线后续补充 Quad-Trees(四叉树)KD树","categories":[],"tags":[]},{"title":"modelmap的使用和部分配置介绍","slug":"modelmap的使用和部分配置介绍","date":"2023-07-29T10:44:54.000Z","updated":"2024-03-14T14:55:29.479Z","comments":true,"path":"8c1763d5.html","link":"","permalink":"https://imlike.cc/8c1763d5.html","excerpt":"","text":"ModelMapper 是一个智能的Java对象映射库，它提供了一种方便高效的方式来映射不同模型之间的对象，使得在不同模型之间转换数据变得更加容易。 使用ModelMapper，开发者可以使用简单直观的API定义对象之间的映射关系。它支持不同属性名称或类型之间的映射，使用自定义转换器处理复杂映射，甚至可以映射嵌套对象。 ModelMapper 的一个关键特点是它能够根据属性的名称和类型智能地确定如何进行映射，这消除了开发者为每个属性编写手动映射代码的需求，节省了时间和精力。ModelMapper 还提供了条件映射等高级功能，允许开发者定义属性何时应该映射或不映射的规则。 除了映射能力，ModelMapper 还提供其他有用的功能。它支持映射生命周期钩子，允许开发者在映射过程之前或之后执行自定义操作。它还提供对象的深度克隆功能，在处理复杂对象层次结构时非常有用。 通过使用ModelMapper，开发者可以提高映射过程的效率，降低错误的风险。它自动化了映射对象的繁琐任务，使开发者能够专注于应用程序的其他方面。它还促进了代码的可重用性，因为映射配置可以在应用程序的不同部分轻松复用。 总的来说，ModelMapper 是一个强烈推荐的Java对象映射库，适用于处理对象映射。它的智能映射能力和用户友好的API使其成为在不同表示之间转换数据的宝贵资产。 示例以下是一个简单的示例，展示了如何使用ModelMapper进行对象映射： 1234567891011121314// 创建ModelMapper实例ModelMapper modelMapper = new ModelMapper();// 创建源对象SourceObject sourceObject = new SourceObject();sourceObject.setName(&quot;John&quot;);sourceObject.setAge(30);// 将源对象映射到目标对象TargetObject targetObject = modelMapper.map(sourceObject, TargetObject.class);// 输出目标对象的属性System.out.println(targetObject.getName()); // 输出：JohnSystem.out.println(targetObject.getAge()); // 输出：30 在上面的示例中，我们使用ModelMapper创建了一个源对象和目标对象，并将源对象的属性映射到目标对象。通过调用map方法，ModelMapper会自动根据属性的名称和类型进行映射。 特点 智能映射：ModelMapper能够根据属性的名称和类型智能地确定如何进行映射，无需编写冗长的手动映射代码。 灵活的API：ModelMapper提供了简单直观的API，使开发者能够轻松定义对象之间的映射关系。 复杂映射支持：ModelMapper支持处理复杂的映射场景，包括不同属性名称或类型的映射，以及嵌套对象的映射。 高级功能：ModelMapper提供了条件映射、映射生命周期钩子和对象深度克隆等高级功能，增强了映射的灵活性和可控性。 提高效率：通过自动化映射过程，ModelMapper能够提高映射的效率，减少重复劳动，让开发者能够专注于其他方面的工作。 无论您是在开发小型项目还是大型应用程序，ModelMapper都能极大地简化对象映射的工作流程，提高开发效率和代码可维护性。不妨尝试一下，体验它为您的开发过程带来的便利和高效。 ModelMapper 是一个智能的Java对象映射库，它提供了一种方便高效的方式来映射不同模型之间的对象，使得在不同模型之间转换数据变得更加容易。 使用ModelMapper，开发者可以使用简单直观的API定义对象之间的映射关系。它支持不同属性名称或类型之间的映射，使用自定义转换器处理复杂映射，甚至可以映射嵌套对象。 ModelMapper 的一个关键特点是它能够根据属性的名称和类型智能地确定如何进行映射，这消除了开发者为每个属性编写手动映射代码的需求，节省了时间和精力。ModelMapper 还提供了条件映射等高级功能，允许开发者定义属性何时应该映射或不映射的规则。 除了映射能力，ModelMapper 还提供其他有用的功能。它支持映射生命周期钩子，允许开发者在映射过程之前或之后执行自定义操作。它还提供对象的深度克隆功能，在处理复杂对象层次结构时非常有用。 通过使用ModelMapper，开发者可以提高映射过程的效率，降低错误的风险。它自动化了映射对象的繁琐任务，使开发者能够专注于应用程序的其他方面。它还促进了代码的可重用性，因为映射配置可以在应用程序的不同部分轻松复用。 总的来说，ModelMapper 是一个强烈推荐的Java对象映射库，适用于处理对象映射。它的智能映射能力和用户友好的API使其成为在不同表示之间转换数据的宝贵资产。 示例以下是一个简单的示例，展示了如何使用ModelMapper进行对象映射： 1234567891011121314// 创建ModelMapper实例ModelMapper modelMapper = new ModelMapper();// 创建源对象SourceObject sourceObject = new SourceObject();sourceObject.setName(&quot;John&quot;);sourceObject.setAge(30);// 将源对象映射到目标对象TargetObject targetObject = modelMapper.map(sourceObject, TargetObject.class);// 输出目标对象的属性System.out.println(targetObject.getName()); // 输出：JohnSystem.out.println(targetObject.getAge()); // 输出：30 在上面的示例中，我们使用ModelMapper创建了一个源对象和目标对象，并将源对象的属性映射到目标对象。通过调用map方法，ModelMapper会自动根据属性的名称和类型进行映射。 特点 智能映射：ModelMapper能够根据属性的名称和类型智能地确定如何进行映射，无需编写冗长的手动映射代码。 灵活的API：ModelMapper提供了简单直观的API，使开发者能够轻松定义对象之间的映射关系。 复杂映射支持：ModelMapper支持处理复杂的映射场景，包括不同属性名称或类型的映射，以及嵌套对象的映射。 高级功能：ModelMapper提供了条件映射、映射生命周期钩子和对象深度克隆等高级功能，增强了映射的灵活性和可控性。 提高效率：通过自动化映射过程，ModelMapper能够提高映射的效率，减少重复劳动，让开发者能够专注于其他方面的工作。 无论您是在开发小型项目还是大型应用程序，ModelMapper都能极大地简化对象映射的工作流程，提高开发效率和代码可维护性。不妨尝试一下，体验它为您的开发过程带来的便利和高效。 ModelMapper 是一个智能的Java对象映射库，它提供了一种方便高效的方式来映射不同模型之间的对象，使得在不同模型之间转换数据变得更加容易。 使用ModelMapper，开发者可以使用简单直观的API定义对象之间的映射关系。它支持不同属性名称或类型之间的映射，使用自定义转换器处理复杂映射，甚至可以映射嵌套对象。 ModelMapper 的一个关键特点是它能够根据属性的名称和类型智能地确定如何进行映射，这消除了开发者为每个属性编写手动映射代码的需求，节省了时间和精力。ModelMapper 还提供了条件映射等高级功能，允许开发者定义属性何时应该映射或不映射的规则。 除了映射能力，ModelMapper 还提供其他有用的功能。它支持映射生命周期钩子，允许开发者在映射过程之前或之后执行自定义操作。它还提供对象的深度克隆功能，在处理复杂对象层次结构时非常有用。 通过使用ModelMapper，开发者可以提高映射过程的效率，降低错误的风险。它自动化了映射对象的繁琐任务，使开发者能够专注于应用程序的其他方面。它还促进了代码的可重用性，因为映射配置可以在应用程序的不同部分轻松复用。 总的来说，ModelMapper 是一个强烈推荐的Java对象映射库，适用于处理对象映射。它的智能映射能力和用户友好的API使其成为在不同表示之间转换数据的宝贵资产。 https://github.com/modelmapper/modelmapper/issues/198 123456&#x2F;&#x2F; 完全匹配 modelMapper.getConfiguration().setFullTypeMatchingRequired(true); &#x2F;&#x2F; 匹配策略使用严格模式 modelMapper.getConfiguration().setMatchingStrategy(MatchingStrategies.STRICT);","categories":[],"tags":[]},{"title":"使用Lettuce客户端或者redis连接变化时的触发事件","slug":"使用lettle客户端或者redis连接变化时的触发事件","date":"2023-07-08T00:25:45.000Z","updated":"2024-03-14T14:55:29.483Z","comments":true,"path":"a337c28a.html","link":"","permalink":"https://imlike.cc/a337c28a.html","excerpt":"","text":"12345678910111213141516171819202122232425262728293031323334353637383940414243444546@Configuration@EnableCachingpublic class CacheConfig &#123; @Bean public RedisConnectionFactory redisConnectionFactory() &#123; RedisStandaloneConfiguration config = new RedisStandaloneConfiguration(); config.setHostName(&quot;127.0.0.1&quot;); config.setPort(11); config.setPassword(&quot;root&quot;); return new LettuceConnectionFactory(config); &#125; @Primary @Bean(name = &quot;redisTemplate&quot;) public RedisTemplate&lt;String, Object&gt; redisTemplate(RedisConnectionFactory factory) &#123; RedisTemplate&lt;String, Object&gt; template = new RedisTemplate&lt;&gt;(); template.setConnectionFactory(factory); StringRedisSerializer stringRedisSerializer = new StringRedisSerializer(); template.setKeySerializer(stringRedisSerializer); template.setHashKeySerializer(stringRedisSerializer); GenericJackson2JsonRedisSerializer redisSerializer = new GenericJackson2JsonRedisSerializer(); template.setValueSerializer(redisSerializer); template.setHashValueSerializer(redisSerializer); return template; &#125; @Bean public CacheManager cacheManager(RedisConnectionFactory redisConnectionFactory) &#123; RedisCacheConfiguration cacheConfiguration = RedisCacheConfiguration.defaultCacheConfig() .serializeValuesWith(RedisSerializationContext.SerializationPair.fromSerializer( new GenericJackson2JsonRedisSerializer())).disableCachingNullValues(); return RedisCacheManager.builder(redisConnectionFactory).cacheDefaults(cacheConfiguration).build(); &#125;&#125; 同时实现 RedisConnectionStateListener 保证在onRedisConnected,onRedisDisconnected,onRedisExceptionCaught下运行时的调用注意下列触发事件需要在RedisClient 下注册进去,保证后续的调用成功 1234567891011121314151617public class CustomRedisConnectionStateListener implements RedisConnectionStateListener &#123; @Override public void onRedisConnected(RedisConnection connection) &#123; &#x2F;&#x2F; 处理连接成功的逻辑 &#125; @Override public void onRedisDisconnected(RedisConnection connection) &#123; &#x2F;&#x2F; 处理连接断开的逻辑 &#125; @Override public void onRedisExceptionCaught(RedisConnection connection, Throwable exception) &#123; &#x2F;&#x2F; 处理连接异常的逻辑 &#125;&#125;","categories":[],"tags":[]},{"title":"Windows下bat脚本文件编写","slug":"Windows下bat脚本文件编写","date":"2023-07-02T06:32:34.000Z","updated":"2024-03-14T14:55:29.475Z","comments":true,"path":"282c310f.html","link":"","permalink":"https://imlike.cc/282c310f.html","excerpt":"","text":"先给个实例来表示 使用杀死 指定端口 1234567891011121314151617181920212223242526272829303132333435363738394041424344@echo offset default_port&#x3D;8080set &#x2F;p port&#x3D;type need killed port (press enter for default port %default_port%): %&#x3D;%if &quot;%port%&quot;&#x3D;&#x3D;&quot;&quot; set &quot;port&#x3D;%default_port%&quot;rem set &#x2F;p port&#x3D;type need killed port:for &#x2F;f &quot;tokens&#x3D;5 delims&#x3D; &quot; %%a IN (&#39;netstat -a -n -o ^| findstr :%port%.*LISTENING&#39;) do ( set pid&#x3D;%%a)if not defined pid ( echo not found port %port%) else ( taskkill &#x2F;F &#x2F;pid %pid% echo already kill %port%)set &#x2F;p port&#x3D;type need killed port:for &#x2F;f &quot;tokens&#x3D;5 delims&#x3D; &quot; %%a IN (&#39;netstat -a -n -o ^| findstr :%port%.*LISTENING&#39;) do ( set pid&#x3D;%%a)if not defined pid ( echo not found port %port%) else ( taskkill &#x2F;F &#x2F;pid %pid% echo already kill %port%)set &#x2F;p port&#x3D;type need killed port:for &#x2F;f &quot;tokens&#x3D;5 delims&#x3D; &quot; %%a IN (&#39;netstat -a -n -o ^| findstr :%port%.*LISTENING&#39;) do ( set pid&#x3D;%%a)if not defined pid ( echo not found port %port%) else ( taskkill &#x2F;F &#x2F;pid %pid% echo already kill %port%)pause","categories":[],"tags":[]},{"title":"postgresql查询表结构和表字段已经合并多个空间类型geometry的表","slug":"postgresql查询表结构和表字段","date":"2023-06-04T08:11:45.000Z","updated":"2024-03-14T14:55:29.480Z","comments":true,"path":"eadc1bcb.html","link":"","permalink":"https://imlike.cc/eadc1bcb.html","excerpt":"","text":"现在有个需要查询postgresql下表的信息同时统计表下的geometry的数目和对应类型,如果有多个geometry需要合并为一条返回.下面是查询的步骤和考虑. 查询pg_statio_all_tables SELECT * FROM pg_catalog.pg_statio_all_tables WHERE schemaname = ‘public’ 上面SQL主要是查询catalog下面的目录在public为schema下的表数据 查询information_schema.tables SELECT * FROM information_schema.tables WHERE table_schema = ‘public’ ; 上面的SQL其实类似但是整体条SQL会包含很多 内置的表非常不考虑使用 查询geometry_columns select f_table_name ,type,COUNT(f_table_name) as geomcount from geometry_columns WHERE f_table_schema = ‘public’ GROUP BY f_table_name ; 是查询对应的 geometry_columns 表下记录的geometry的字段和对应的类型(点,线,面,多点…等) 联合查询 pg_catalog.pg_statio_all_tables 和geometry_columns SELECT gc.type AS geo_type, pg_statio_all_tables.relname, obj_description(relid) AS description, pg_total_relation_size(relid) / pg_column_size(relid::regclass) AS data_size, COUNT() as num_duplicatesFROM pg_catalog.pg_statio_all_tables LEFT OUTER JOIN geometry_columns AS gc ON gc.f_table_name = relnameWHERE schemaname = ‘public’GROUP BY geo_type, relnameHAVING COUNT() &gt; 1; 这个结果只能过滤 一个表有多个geometry的表不符合要求 到这一步开始是考虑联合查询,不论是左连接 还是右连接查询 SELECT f_table_name, MAX(type) AS type ,COUNT(type) geomcountFROM geometry_columns WHERE f_table_schema = ‘public’ GROUP BY f_table_name 查询 geometry_columns 过滤需要用到 MAX函数,而不是distinct . distinct 的这种方式例子如下 SELECT f_table_name, type, COUNT(*) as countFROM ( SELECT DISTINCT ON (f_table_name) f_table_name, type FROM geometry_columns ORDER BY f_table_name, type) subqueryGROUP BY f_table_name; 查询结果其实也是会重复,而且拿不到重复的的geometry的type不合适. 最终版方式 SELECT gc.type AS geo_type, pg_statio_all_tables.relname, obj_description(relid) AS description, pg_total_relation_size(relid) / pg_column_size(relid::regclass) AS data_size, gc.geomcountFROM pg_catalog.pg_statio_all_tables LEFT OUTER JOIN (SELECT f_table_name, MAX(type) AS type ,COUNT(type) geomcountFROM geometry_columns GROUP BY f_table_name ) AS gc ON gc.f_table_name = relnameWHERE schemaname = ‘public’","categories":[],"tags":[]},{"title":"删除自动添加的美式键盘","slug":"删除自动添加的美式键盘","date":"2023-05-19T00:41:17.000Z","updated":"2024-03-14T14:55:29.483Z","comments":true,"path":"881d194.html","link":"","permalink":"https://imlike.cc/881d194.html","excerpt":"","text":"其实window默认是可以删除美式键盘的,比较笨的方式是使用第三方的中文输入法可以管理 1.初级方式比如这种 如果这种删除不了可以考虑添加美式键盘然后在删除 2. 添加英文(美式)再删除 3. 其他软件自动添加在window平台会自动添加美式输入法,比如一些英文游戏steam上的,我这边是又有装了 Windows11平台的资源管理器拓展 explorer patcher ,它会自动添加美式输入法,考虑卸载对应软件:), 使用这两个文件 Remove_en-US.bat 1control intl.cpl,, &#x2F;f:&quot;%CD%\\Remove_en-US.xml&quot; Remove_en-US.xml 1234567891011121314&lt;gs:GlobalizationServices xmlns:gs&#x3D;&quot;urn:longhornGlobalizationUnattend&quot;&gt; &lt;!--User List--&gt; &lt;gs:UserList&gt; &lt;gs:User UserID&#x3D;&quot;Current&quot;&#x2F;&gt; &lt;&#x2F;gs:UserList&gt; &lt;!--input preferences--&gt; &lt;gs:InputPreferences&gt; &lt;!--add en-US keyboard input--&gt; &lt;gs:InputLanguageID Action&#x3D;&quot;add&quot; ID&#x3D;&quot;0409:00000409&quot;&#x2F;&gt; &lt;!--remove en-US keyboard input--&gt; &lt;gs:InputLanguageID Action&#x3D;&quot;remove&quot; ID&#x3D;&quot;0409:00000409&quot;&#x2F;&gt; &lt;&#x2F;gs:InputPreferences&gt;&lt;&#x2F;gs:GlobalizationServices&gt; 两个文件放在一起可以运行Remove_en-US.bat 关于输入法栏删掉ು - Microsoft Community","categories":[],"tags":[]},{"title":"Geotools中关于filter和ECQL的介绍","slug":"geotools中关于filter和ECQL的介绍","date":"2023-05-07T12:13:03.000Z","updated":"2024-03-14T14:55:29.477Z","comments":true,"path":"a0ce9eb1.html","link":"","permalink":"https://imlike.cc/a0ce9eb1.html","excerpt":"","text":"filter和ECQL的介绍在geotools中有datastore就需要对应数据源的查询这边是基于opengis定义的接口来定义现在先看下 filter这个类的介绍 12345678910111213141516public interface Filter &#123; &#x2F;&#x2F;使用Filter.INCLUDE过滤集合将会返回原始集合。 IncludeFilter INCLUDE &#x3D; new IncludeFilter(); &#x2F;&#x2F;使用Filter.EXCLUDE过滤集合将会返回空集合。 ExcludeFilter EXCLUDE &#x3D; new ExcludeFilter(); &#x2F;&#x2F;给定一个对象，确定此过滤器对象所表示的测试是否通过。如果测试通过，则返回true，否则返回false boolean evaluate(Object object); &#x2F;&#x2F;接受一个访问者(FilterVisitor)。实现所有子接口必须具有以下内容的方法: &#x2F;&#x2F;visitor.visit(this, extraData); Object accept(FilterVisitor visitor, Object extraData);&#125; 下面是使用Filter查询Geometry类型为Point且位于指定矩形区域内的要素的示例代码： 12345678&#x2F;&#x2F;创建一个矩形过滤器Filter filter &#x3D; FF.and( FF.equal(FF.property(&quot;geometry&quot;).getType(), FF.literal(&quot;Point&quot;)), FF.bbox(&quot;geometry&quot;, -180, -90, 180, 90, &quot;EPSG:4326&quot;));&#x2F;&#x2F;应用过滤器获取要素集合SimpleFeatureCollection features &#x3D; featureSource.getFeatures(filter); ECQL对应filter跟一步封装我们只需要传入参数和对应方法,用一个字符串来传入即可,它基于OGC标准并支持更多的查询操作。与Filter不同的是，ECQL使用类SQL语法编写查询表达式，并可以执行更复杂的查询，例如计算距离或使用正则表达式匹配属性值。下面是使用ECQL查询两个图层之间距离小于指定值的示例代码： 12345&#x2F;&#x2F;创建一个ECQL查询表达式String query &#x3D; &quot;DWITHIN(geometry, POINT(0 0), 100, meters)&quot;;&#x2F;&#x2F;应用查询获取要素集合SimpleFeatureCollection features &#x3D; featureSource.getFeatures(ECQL.toFilter(query)); 需要注意的是，ECQL的语法相对复杂 实现的原理很简单的逻辑对于不同的数据源其实很多方法是不一样的对应数据库也许有对应的函数,对应shp文件这种也许我们只能在内存中操作,对应gpkg这种也许有其他的方式来计算 对应filter通常有两种 一种是属性的判断 类似&lt; &gt; = like 这种 第二种是 geometry 判断拓扑关系 相交,包含,相切,相离 还有可能是运行 切割或者分离 所以这里有定义一个抽象类来定义了要参考 AbstractFilterVisitor： FilterVisitor 的抽象实现类，提供了一些通用的方法，例如visit(Filter)和visitNullFilter()等。 DefaultFilterVisitor ：FilterVisitor的默认实现类，提供了对所有Filter对象的访问方法，例如visit(EqualsFilter)、visit(GreaterThanFilter)等。 ECQLFilterVisitor ：FilterVisitor的ECQL实现类，用于解析ECQL表达式。ECQLFilterVisitor提供了一组visit方法，用于访问ECQL表达式中的不同类型的Filter对象。 SQLFilterVisitor ：FilterVisitor的SQL实现类，用于将Filter对象转换为SQL语句。SQLFilterVisitor提供了一组visit方法，用于访问不同类型的Filter对象，并将其转换为SQL语句。 ComparisonFilter : 这是一个用于比较的过滤器类，它根据给定条件对值进行比较。常见的例子包括大于、小于、等于等比较操作符。使用该类可以方便快捷地进行数据筛选和排序。 LogicFilter: 这是一个用于逻辑运算的过滤器类，它可以将多个过滤器连接起来，并根据给定的逻辑关系进行组合。例如，可以使用 And 或 Or 运算符将两个或更多的过滤器连接在一起，以实现复杂的查询需求。 举例子来看一读取gpkg数据来看 对应空间查询最后走到GeoPkgDialect下的simpleSpatialSearch来拼接SQL查询拓扑关系 里面有filter.accept()来判断 最后还是走到JDBCDataStore下的selectSQLPS方法下执行对应的sql来实现的查询 在geotools中会多次走到重复的方法,会有一定的误导作用主要是反射和部分结构设计导致的,需要我们在特定实现的类上面打断点会好很多","categories":[],"tags":[]},{"title":"SDK开发整理和接口设计","slug":"SDK开发整理","date":"2023-04-15T15:50:04.000Z","updated":"2024-03-14T14:55:29.474Z","comments":true,"path":"416e1d94.html","link":"","permalink":"https://imlike.cc/416e1d94.html","excerpt":"","text":"对于SDK的设计这边先整理一个大概的设计方式 先从接口设计来谈起 目前主要是是参考restful风格和asop两个方式,restful风格主要是通过url,requestMethod,requestParam,requestBody来携带参数和标示动作方式 起初是参考RESTful API 设计指南 - 阮一峰的网络日志 后面主要考虑集合方式 get请求会如果字符串查询会出现转义和其他影响. 部分axios框架不支持delete的方式. post put两者方式和patch区别容易混乱 部分更新和全部字段更新. 对应部分批量删除选择post请求但是我选择用requestBody来删除的时候,会直接用数组来输入,这样确实简单明了,但是不好维护和理解,必要还是需要构建对应的对象来完成. GET /dog/{id}?sortby=name&amp;order=asc&amp;page=2&amp;per_page=100 查询单个数据 POST /dog/query -requestBody {“param1”:”aaa”,”param2”:”bbb”} 查询批量数据 POST /dog -requestBody {“param1”:”aaa”,”param2”:”bbb”} 新增 PUT /dog -requestBody {“param1”:”aaa”,”param2”:”bbb”} 部分更新 POST /dog/{id}/delete/ -requestBody {“param1”:”aaa”,”param2”:”bbb”} 删除单个数据 POST /dog/delete/ -requestBody {“param1”:[11,22,44],”param2”:”bbb”} 删除批量数据 关于SDK的设计SDK 开发规范 - 腾讯云开发者社区-腾讯云 通用类命名规范 - 就永远夏天 - 博客园 其实可以参考的比较多主要是 Client 类：Client 类通常用于与远程服务进行交互，例如通过HTTP请求向API发送请求并获取响应。这些类通常封装了与远程服务的所有交互逻辑，并提供了易于使用的方法，以便开发人员可以轻松地与远程服务进行交互。 Helper 类：Helper 类通常包含一些实用方法或函数，这些方法或函数可以用于执行常见的操作。这些方法通常不包含业务逻辑，而是提供了一些工具和辅助函数，以帮助开发人员更轻松地编写代码。 Utils 类：Utils 类也是一种实用程序类，但它们通常包含更广泛的功能，并且可能具有更多的业务逻辑。这些类通常提供了一组静态方法，这些方法可以用于执行各种任务，例如字符串处理、日期时间处理、文件操作等等。 下列类也是经常被用到的可以参考的 Factory 类：Factory 类通常用于创建对象，这些对象可以是某个接口或抽象类的实例。这些类通常包含一个或多个静态方法，这些方法接受参数并返回一个实例。 Builder 类：Builder 类通常用于构建复杂的对象，这些对象可能需要进行多个步骤的构建。这些类通常包含一组方法，每个方法都负责设置一个特定的属性，并返回自身的实例，以便可以链式调用这些方法。 Adapter 类：Adapter 类通常用于将一个类的接口转换为另一个类的接口。这些类通常实现了目标接口，并持有一个源对象的引用，以便将源对象的方法映射到目标接口的方法。 Decorator 类：Decorator 类通常用于在运行时动态地给对象添加功能，而无需修改原始对象。这些类通常实现了与原始对象相同的接口，并持有一个对原始对象的引用，在运行时添加额外的功能。 注意命名编程中最难的就是命名？这几招教你快速上手","categories":[],"tags":[]},{"title":"java中io的优化方向","slug":"java中io的优化方向","date":"2023-04-09T06:40:42.000Z","updated":"2024-03-14T14:55:29.478Z","comments":true,"path":"d31a1754.html","link":"","permalink":"https://imlike.cc/d31a1754.html","excerpt":"","text":"1. 使用缓冲区在读写文件时，每次都直接操作磁盘会导致性能降低。因此，可以通过使用缓冲区来减少对磁盘的操作次数，从而提高性能。Java 中提供了 BufferedInputStream 和 BufferedOutputStream 类来支持缓冲区的操作。 1234java复制代码&#x2F;&#x2F; 创建输入流缓冲区BufferedInputStream bis &#x3D; new BufferedInputStream(new FileInputStream(file));&#x2F;&#x2F; 创建输出流缓冲区BufferedOutputStream bos &#x3D; new BufferedOutputStream(new FileOutputStream(file)); 2. 使用 NIOJava 中的 NIO（New I/O）提供了更高效的 I/O 操作方式，可以提高程序的性能。NIO 引入了通道（Channel）、缓冲区（Buffer）等概念，并且支持非阻塞 I/O 操作。相比之下，传统的 Java I/O 通过流式处理数据，较为低效。 123456java复制代码&#x2F;&#x2F; 创建通道FileChannel channel &#x3D; FileChannel.open(path, StandardOpenOption.READ);&#x2F;&#x2F; 创建缓冲区ByteBuffer buffer &#x3D; ByteBuffer.allocate(1024);&#x2F;&#x2F; 读取数据到缓冲区channel.read(buffer); 3. 多线程读取文件当需要读取大量文件时，可以考虑使用多线程来加速读取操作。Java 中可以使用线程池来创建多个线程，并将文件分成多个部分进行读取。这样可以充分利用 CPU 和磁盘的性能，提高程序的效率。 1234567java复制代码&#x2F;&#x2F; 创建固定大小的线程池ExecutorService executorService &#x3D; Executors.newFixedThreadPool(10);&#x2F;&#x2F; 将任务分配给不同的线程for (int i &#x3D; 0; i &lt; files.length; i++) &#123; Runnable task &#x3D; new ReadFileTask(files[i]); executorService.submit(task);&#125; 4. 数据压缩读取大量文件时，数据传输的成本可能会很高。因此，可以考虑使用数据压缩技术来减少数据量，从而提高程序的效率。Java 中提供了 GZIPOutputStream 和 GZIPInputStream 等类来支持数据压缩和解压操作。 1234567891011121314java复制代码&#x2F;&#x2F; 创建输出流并压缩数据GZIPOutputStream gos &#x3D; new GZIPOutputStream(new FileOutputStream(file));gos.write(data);gos.finish();gos.close();&#x2F;&#x2F; 创建输入流并解压数据GZIPInputStream gis &#x3D; new GZIPInputStream(new FileInputStream(file));byte[] buffer &#x3D; new byte[1024];int read &#x3D; -1;while ((read &#x3D; gis.read(buffer)) !&#x3D; -1) &#123; &#x2F;&#x2F; 处理数据&#125;gis.close(); 5. 批量读写数据在进行大量数据的读取和写入时，可以采用批量操作的方式来提高效率。例如，可以一次性读取或写入多个字节、字符或对象，减少 I/O 操作的频率。 1234567java复制代码&#x2F;&#x2F; 一次性读取多个字节到缓冲区byte[] buffer &#x3D; new byte[1024];inputStream.read(buffer, 0, 1024);&#x2F;&#x2F; 一次性写入多个字符到文件char[] data &#x3D; &#123;&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;&#125;;fileWriter.write(data, 0, 2); 6. 使用内存映射文件Java 中的内存映射文件可以将整个文件或者文件的某个部分映射到虚拟内存中，使得可以通过内存地址来访问文件，从而提高程序的效率。内存映射文件通常适用于对大文件的读取和写入操作。 1234java复制代码&#x2F;&#x2F; 将整个文件映射到内存中RandomAccessFile raf &#x3D; new RandomAccessFile(file, &quot;rw&quot;);FileChannel channel &#x3D; raf.getChannel();MappedByteBuffer buffer &#x3D; channel.map(FileChannel.MapMode.READ_WRITE, 0, file.length()); 7. 原子文件操作Java 中的 java.nio.file.Files 类提供了一些原子文件操作方法，可以确保文件的读取和写入操作的原子性，从而避免由并发操作引起的问题。 12345java复制代码&#x2F;&#x2F; 原子写入字符串到文件Files.writeString(file, &quot;hello&quot;, StandardOpenOption.WRITE, StandardOpenOption.APPEND);&#x2F;&#x2F; 原子读取文件到字符串String content &#x3D; Files.readString(file); 8. 磁盘预读取Java 中可以通过启用磁盘预读取机制来提高程序的效率。磁盘预读取是指在读取文件时，将文件的一部分或者整个文件预先加载到内存中，从而加快读取速度。Java 中可以使用 FileInputStream 类的 getChannel() 方法来获取文件的通道，并使用 map() 方法来实现磁盘预读取。 123456java复制代码&#x2F;&#x2F; 获取文件通道FileInputStream fis &#x3D; new FileInputStream(file);FileChannel channel &#x3D; fis.getChannel();&#x2F;&#x2F; 将文件映射到内存中MappedByteBuffer buffer &#x3D; channel.map(FileChannel.MapMode.READ_ONLY, 0, file.length()); 以上是一些常用的优化方案，但实际应用时需要根据具体情况进行选择。在从 MongoDB 上读取文件时，可以结合上述方案进行优化，以提高程序的性能。","categories":[],"tags":[]},{"title":"chatgpt介绍","slug":"chatgpt","date":"2023-04-09T03:41:16.000Z","updated":"2024-03-14T14:55:29.476Z","comments":true,"path":"9891105b.html","link":"","permalink":"https://imlike.cc/9891105b.html","excerpt":"","text":"名词解析AI :全称Artificial Intelligence，是一种模拟人类智能行为的技术或系统，涵盖了许多不同的技术和方法，包括机器学习、自然语言处理、计算机视觉等，可用于许多领域，如自动驾驶、机器人、医疗诊断等。 ML :全称Machine Learning 是一种人工智能领域的技术，通过从数据中学习模式和规律来实现智能化决策和行为。机器学习可以分为监督学习、无监督学习和强化学习等不同类型，广泛应用于自然语言处理、计算机视觉、金融、医疗等领域。 Deep Learning : 一种基于神经网络的机器学习方法，通过模拟人脑的神经网络结构，实现对复杂模式和规律的学习和识别，被广泛应用于自然语言处理、计算机视觉、语音识别等领域，是实现人工智能的重要技术之一。 CV，全称Computer Vision，即计算机视觉，是一种利用计算机和数学算法来模拟人类视觉行为的技术，旨在使计算机能够像人类一样理解和解释图像和视频。计算机视觉涵盖了许多领域，如图像处理、模式识别、目标检测、图像分割等，可应用于人脸识别、自动驾驶、智能安防等众多领域。 Transformer :是一种基于自注意力机制的深度学习模型，最初应用于自然语言处理领域的机器翻译任务。相较于传统的循环神经网络和卷积神经网络，Transformer通过并行计算和全局信息的交互，实现了更快的训练速度和更高的精度，并被广泛应用于自然语言处理、计算机视觉、语音识别等领域。 Reinforcement learning :是一种机器学习方法，一种通过试错学习来最大化奖励的机器学习方法，旨在让智能体（agent）在与环境的交互中自主探索最优策略，并获得最大化的奖励。Reinforcement Learning可应用于许多领域，如自动驾驶、游戏AI、机器人控制等，是实现人工智能的重要技术之一。在GPT中，强化学习被用于调整模型的生成策略 General Pretrained Transformer : 一种基于Transformer模型的通用预训练模型，是一种自然语言处理算法，通过在大规模文本数据上进行无监督训练，可以实现对自然语言文本的深度理解和高效处理。GPT模型的应用包括文本生成、文本分类、问答系统等，是自然语言处理领域中的重要技术。 Reinforcement Learning from Human Feedback (RLHF): 是一种结合了强化学习和人类反馈的方法。该方法在训练模型时，通过让人类提供正面或负面反馈来指导模型的学习。在GPT中，这种方法可以帮助模型更好地理解人类语言，并且可使其生成更加符合人类语言特点的内容。是一种通过人类反馈指导机器学习的方法。与传统的强化学习不同，RLHF使用人类专家或众包工作者提供的奖励信号作为机器学习的反馈，以加速智能体（agent）的学习过程。RLHF的应用包括自然语言处理、计算机视觉、机器人控制等领域，是实现人机协作的重要技术之一。 NLP(自然语言处理) : 自然语言处理（Natural Language Processing）的缩写，指的是计算机科学、人工智能和语言学等多个领域的交叉学科，致力于研究如何使计算机能够理解、处理和生成人类语言。NLP的应用包括机器翻译、情感分析、问答系统、文本分类等多个领域，是实现计算机与人类语言交互的重要技术之一。 ​ AI的起源可以追溯到20世纪40年代，随着计算机技术的发展，研究人员开始尝试通过机器模拟人类智能。在20世纪50年代至60年代，出现了“逻辑推理”、“神经网络”等多个重要的AI研究方向。70年代后，AI开始应用于专家系统、机器视觉等领域，但在80年代后半期，由于技术和资源的限制，AI的发展进入了低谷。21世纪初，随着深度学习等技术的出现，AI得到了长足的发展，目前已经成为各行业的热门领域。 1. 机器学习（Machine Learning） 特点：机器学习是一种人工智能技术，它通过让计算机根据数据自动学习模型，从而实现对未知数据进行预测或决策。机器学习主要分为有监督学习、无监督学习和强化学习三种类型。 介绍：机器学习是目前最火热的技术之一，它应用于各个领域，如商业、医疗、金融等。利用机器学习，我们可以让机器自动地完成一些复杂的任务，如图像识别、语音识别、自然语言处理等。 2. 深度学习（Deep Learning） 特点：深度学习是机器学习的一种，它通过模拟人类神经元网络来构建模型，并利用反向传播算法进行训练，从而实现对数据的预测或决策。深度学习主要应用于图像识别、语音识别、自然语言处理等领域。 介绍：深度学习是近年来最受关注的技术之一，它取得了一系列重大突破，如AlphaGo战胜围棋世界冠军李世石、自动驾驶汽车等。深度学习有助于解决各种复杂问题，同时也在不断地推进人工智能技术的发展。 3. 计算机视觉（Computer Vision） 特点：计算机视觉是一项涉及图像和视频处理的技术，它利用计算机对数字图像进行分析和处理，从而实现对图像中物体、场景等的理解和识别。计算机视觉主要应用于图像检测、目标跟踪、三维重建、虚拟现实等领域。 介绍：计算机视觉是一项非常应用广泛的技术，它可以应用到很多领域，如安防、医疗、交通等。随着深度学习技术的发展，计算机视觉的性能也得到了极大地提升。 应用: 视觉四小龙(商汤科技、旷视科技、云从科技和依图科技) 4. 自然语言处理（Natural Language Processing） 特点：自然语言处理是一种人工智能技术，它利用计算机对自然语言进行处理和理解，从而实现对文本的分析、理解和生成。自然语言处理主要涉及到自然语言标记、词向量表示、文本分类、机器翻译等方面。 介绍：自然语言处理是一项非常有挑战性的技术，因为自然语言的表达方式非常复杂，同时也存在很多歧义。利用自然语言处理技术，我们可以实现机器翻译、情感分析、智能客服等应用。 应用: GPT,语音识别 相关关键词的关系和梳理机器学习、深度学习、计算机视觉和自然语言处理这四个概念之间存在一定的包含关系，具体如下： 机器学习是一个大的概念，它包括了深度学习在内。深度学习是机器学习中的一种方法，它利用神经网络模拟大脑的处理过程，实现对数据的特征提取和分类。因此可以说深度学习是机器学习的一种重要分支。 计算机视觉是机器学习和深度学习在图像或视频领域的应用。计算机视觉需要训练模型来识别图像或视频中的物体、场景等，从而实现对图像或视频的理解和识别。因此，计算机视觉可以被看作是机器学习和深度学习在图像或视频领域的应用之一。 自然语言处理是机器学习和深度学习在文本领域的应用。自然语言处理需要训练模型来识别文本中的词汇、语法、语义等，从而实现对文本的分析和生成。因此，自然语言处理可以被看作是机器学习和深度学习在文本领域的应用之一。 ChatGPT​ ChatGPT是由OpenAI开发的一个大型自然语言处理模型，它基于GPT（Generative Pre-trained Transformer）算法，并使用了海量的语料数据进行预训练。ChatGPT在对话生成、情感分析、文本生成等方面表现出色，其生成的回复流畅自然，能够与人类进行高质量的交互。ChatGPT已经被广泛应用于智能客服、虚拟助手、聊天机器人等领域，为用户提供了更加便捷、高效的服务体验。同时，ChatGPT也为自然语言处理技术的发展做出了重要贡献，推动了自然语言处理技术的不断进步和发展。 在gpt和类似的 文心一言 ,Bard,等LLM( large language model )大语言模型中参数是非常重要的,通常都是多少多少亿,其实这些参数是该模型中的可调整变量，通过不断地学习和优化，可以使ChatGPT在各种自然语言任务上表现出色。具体来说，ChatGPT中的这175亿个参数主要有以下作用： 语言表示：ChatGPT的主要任务是生成自然语言文本，因此它需要深入理解语言的结构、语法以及意义等方面。这些参数能够帮助ChatGPT学习到更加准确、丰富的语言表示形式，从而更好地表达和理解人类语言。 上下文理解：在对话生成和文本生成任务中，上下文信息对于生成的内容非常重要。ChatGPT的参数可以捕获历史上下文信息，并利用这些信息来生成更加合适、连贯的回复或文本内容。 预测能力：ChatGPT中的参数能够帮助模型预测下一个单词或字符的概率分布，从而决定生成的文本内容。这些参数的精度和数量越高，模型预测的准确性也就越高。 总之，ChatGPT的175亿个参数起到了至关重要的作用，它们能够帮助模型对自然语言进行深入理解和表达，并且能够生成高质量、连贯的文本内容。同时，这些参数也代表了当今自然语言处理技术的前沿水平，展示了人工智能技术在自然语言处理领域的强大潜力。 杰弗里·辛顿 Geoffrey Everest Hinton 开山鼻祖 杨立昆,扬·勒丘恩 Yann Le Cun :卷积网络之父,现在ocr的基础 约书亚·本希奥 Yoshua Bengio 蒙特利尔大学计算机科学和运筹学系的教授以及蒙特利尔学习算法研究所科学主任 伊尔亚·苏茨克维 - Ilya Sutskever openAI的联合创始人,在开山鼻祖麾下读完研究生,和亚历克斯·克里泽夫斯基和杰弗里·辛顿是卷积神经网络AlexNet的共同发明人 最后这篇文章是一遍问ChatGPT一边结合自己理解生成的 Large language model - Wikipedia 贝叶斯网络 - 维基百科，自由的百科全书 0-1.了解人工智能、机器学习和深度学习的区别! | AI-SCHOLAR | AI：（人工智能）文章和技术信息媒体","categories":[],"tags":[]},{"title":"springmvc配置gson的解析","slug":"springmvc配置gson的解析","date":"2023-04-09T03:29:35.000Z","updated":"2024-03-14T14:55:29.481Z","comments":true,"path":"1a9fa948.html","link":"","permalink":"https://imlike.cc/1a9fa948.html","excerpt":"","text":"springmvc 排除jackson如果使用后使用Gson 1@SpringBootApplication(exclude&#x3D; &#123;DataSourceAutoConfiguration.class,JacksonAutoConfiguration.class&#125;) 在autoconfig下gson的配置 如果直接使用expose注解后不会直接生效需要重新bean注入到 需要gson注入bean到spring里面 1234567891011121314151617181920212223242526new GsonBuilder() .addSerializationExclusionStrategy(new ExclusionStrategy() &#123; @Override public boolean shouldSkipField(FieldAttributes fieldAttributes) &#123; final Expose expose &#x3D; fieldAttributes.getAnnotation(Expose.class); return expose !&#x3D; null &amp;&amp; !expose.serialize(); &#125; @Override public boolean shouldSkipClass(Class&lt;?&gt; aClass) &#123; return false; &#125; &#125;) .addDeserializationExclusionStrategy(new ExclusionStrategy() &#123; @Override public boolean shouldSkipField(FieldAttributes fieldAttributes) &#123; final Expose expose &#x3D; fieldAttributes.getAnnotation(Expose.class); return expose !&#x3D; null &amp;&amp; !expose.deserialize(); &#125; @Override public boolean shouldSkipClass(Class&lt;?&gt; aClass) &#123; return false; &#125; &#125;) .create(); https://stackoverflow.com/questions/4802887/gson-how-to-exclude-specific-fields-from-serialization-without-annotations#:~:text=I%20explicitly%20used-,%40Expose%20to%20exclude,-it.%20I%20used","categories":[],"tags":[]},{"title":"mybatis回顾兼顾分析Invalid bound statement (not found)的问题","slug":"mybatis回顾","date":"2023-04-04T12:06:54.000Z","updated":"2024-03-14T14:55:29.479Z","comments":true,"path":"cf22bc7c.html","link":"","permalink":"https://imlike.cc/cf22bc7c.html","excerpt":"","text":"项目中需要使用mybatis原生,之前都是使用mybatis plus或者mybatis pagehelper,突然使用偏原生,主要是xml中的简单增删改查都需要手动机械的实现真的是相当死板,搜索一下发型mybatisx这个插件非常好用,没有考虑mybatis pro那个插件插件使用介绍在下面,之前很多mybatis代码生成需要mybatis maven插件配置一堆真的是非常不友好,使用这个插件只需要利用idea连接对应插件即可使用,配置下后生成对应的xml和mapper代码和domain/entity对象,简直是神器. mybatis—mybatis插件根据接口自动生成xml文件_mybatis xml 生成插件_任何时候都要勇敢的博客-CSDN博客 后面依照这个配置一步一步走下去没什么问题 按照步骤: https://mybatis.org/spring-boot-starter/mybatis-spring-boot-autoconfigure/zh/index.html Spring Boot 整合 MyBatis | Java进阶之路 SpringBoot整合mybatis ] 但是后面一直出现Invalid bound statement (not found) 想着这种一定是我配置出错,网上搜索都是 mapperscan的配置 mybatis.config-location=classpath*:**/mapper/*Mapper.xml xml下的namespace要写正确 认真对了几遍以为是自己没按照模板来,但是mybatisx这个插件都能正常解析,真的是头大,确认再三一定是mybatis加载对应statement的时候没加载到在看到这篇文章后我大概确定了 MapperAnnotationBuilder 下parse 中有loadXmlResource() 在这里寻找mapper接口对应的xml的资源路径的方式如下： 1String xmlResource &#x3D; type.getName().replace(&#39;.&#39;, &#39;&#x2F;&#39;) + &quot;.xml&quot;; 替换接口包名中的.为/ 并在接口添加.xml后缀。 比如：Mapper接口com.hbfec.encrypt.admin.dao.ocr.TestDao的对应的xml资源路径会解析为com/hbfec/encrypt/admin/dao/ocr/TestDao.xml。 TestDao.xml在我的项目中的路径是classpath:dao/ocr/TestDao.xml,路径与上面解析出来的不一致，mybatis无法找到TestDao.xml，导致以上错误。所以项目采用使用这种方式绑定Mapper接口和Mapper.xml的话，其路径和名称都要一致。 所以我项目下resource下面需要有对应的路径,由于太久没写都是直接写mapper导致解析不到 参考spring boot mybatis 报错Invalid bound statement (not found)解决过程 - 全me村的希望 - 博客园","categories":[],"tags":[]},{"title":"windows使用vmvare创建mac-intel虚拟机","slug":"windows使用vmvare创建mac-intel虚拟机","date":"2023-03-29T00:22:46.000Z","updated":"2024-03-14T14:55:29.483Z","comments":true,"path":"267b5788.html","link":"","permalink":"https://imlike.cc/267b5788.html","excerpt":"","text":"虚拟机安装 macOS 黑苹果系统！所需要的资料 - 零度解说Windows 11 上安装 macOS系统！通过虚拟机即可轻松搞定 需要的软件 1.VMware 下载：【点击下载】 2.Auto-Unlocker 開源輔助工具：【点击下载】 3.macOS big sur 系統ISO鏡像：-&gt; macOS Monterey 12.6.3 (21G419) 正式版 ISO、IPSW、PKG 下载","categories":[],"tags":[]},{"title":"chatgpt关于学习英文","slug":"chatgpt关于学习英文","date":"2023-03-21T14:22:16.000Z","updated":"2024-03-14T14:55:29.476Z","comments":true,"path":"24cfb670.html","link":"","permalink":"https://imlike.cc/24cfb670.html","excerpt":"","text":"1234567891011121314151617ok ,i pleasure give some questions ,first can you tell me learing english for step by stepCertainly! Here are some steps you can take to learn English:1. Start with the basics: Learn the English alphabet, basic grammar rules, and common vocabulary words.2. Practice listening and speaking: Listen to English speakers and practice speaking with native speakers or language partners. This will help you improve your pronunciation and fluency.3. Read in English: Start with simple texts like children&#39;s books or news articles, and gradually work your way up to more complex texts.4. Write in English: Practice writing in English by keeping a journal, writing emails, or participating in online forums.5. Take a course or find a tutor: Consider taking an English course at a language school or finding a tutor who can provide personalized instruction.6. Use language learning apps: There are many apps available that can help you learn English, such as Duolingo, Babbel, and Rosetta Stone.7. Immerse yourself in the language: Watch movies or TV shows in English, listen to music in English, and try to use the language as continue 12345much as possible in your daily life. This will help you become more comfortable with the language and improve your overall proficiency.8. Set goals and track your progress: Set specific goals for yourself, such as learning a certain number of new words each week or practicing speaking for a certain amount of time each day. Keep track of your progress and celebrate your achievements along the way.Remember that learning a new language takes time and effort, so be patient with yourself and don&#39;t get discouraged if you make mistakes. With consistent practice and dedication, you can improve your English skills and achieve your language learning goals. continue 1234567899. Join a language exchange program: Find a language exchange partner who speaks English and is learning your native language. This way, you can practice speaking with each other and help each other improve.10. Use English in real-life situations: Try to use English in real-life situations, such as ordering food at a restaurant or asking for directions. This will help you become more comfortable using the language in everyday situations.11. Learn from your mistakes: Don&#39;t be afraid to make mistakes when speaking or writing in English. Instead, learn from your mistakes and use them as opportunities to improve.12. Stay motivated: Learning a new language can be challenging, so it&#39;s important to stay motivated and keep practicing even when it gets difficult. Find ways to make learning fun and enjoyable, such as watching movies or listening to music in English.By following these steps and staying committed to your language learning goals, you can improve your English skills and become more confident using the language in a variety of settings. so frequently Misspell 12345678910111213Misspelling words is a common challenge when learning a new language, and it can be frustrating. Here are some tips that may help you improve your spelling in English:1. Practice regularly: The more you practice spelling, the better you will become. Try to set aside some time each day to practice spelling words.2. Learn the rules: English has many spelling rules that can help you spell words correctly. For example, &quot;i before e except after c&quot; is a common rule that can help you spell words like &quot;believe&quot; and &quot;receive.&quot;3. Use mnemonics: Mnemonics are memory aids that can help you remember how to spell difficult words. For example, to remember how to spell &quot;necessary,&quot; you could use the phrase &quot;one collar and two sleeves are necessary.&quot;4. Read and write in English: Reading and writing in English can help you become more familiar with the language and its spelling patterns.5. Use online resources: There are many online resources available that can help you improve your spelling, such as spelling games, quizzes, and exercises.Remember that improving your spelling takes time and practice, so don&#39;t get discouraged if it doesn&#39;t happen overnight. Keep practicing and using these tips, and you will see improvement over time. yes you are righ,and when I read some english web content ,I fell hard and want to escape 12345678910111213Reading English web content can be challenging, especially if you&#39;re not used to reading in English. Here are some tips that may help you improve your reading skills:1. Start with easy material: If you&#39;re finding it difficult to read English web content, start with material that is easier to understand. For example, you could start with children&#39;s books or news articles written for English learners.2. Use a dictionary: If you come across words that you don&#39;t know, look them up in a dictionary. This will help you expand your vocabulary and understand the text better.3. Take breaks: Reading in a new language can be tiring, so take breaks when you need to. This will help you stay focused and avoid getting overwhelmed.4. Read out loud: Reading out loud can help improve your pronunciation and comprehension of the text.5. Practice regularly: The more you practice reading in English, the easier it will become over time.Remember that learning a new language takes time and practice, so don&#39;t get discouraged if it&#39;s difficult at first. Keep practicing and using these tips, and you will see improvement over time.","categories":[],"tags":[]},{"title":"java命名规则-转载","slug":"java命名规则-转载","date":"2023-03-20T12:46:36.000Z","updated":"2024-03-14T14:55:29.478Z","comments":true,"path":"b2964425.html","link":"","permalink":"https://imlike.cc/b2964425.html","excerpt":"","text":"《小姐姐味道 公众号》 管理类命名 写代码，少不了对统一资源的管理，清晰的启动过程可以有效地组织代码。为了让程序运行起来，少不了各种资源的注册、调度，少不了公共集合资源的管理， 1.BootStrap,Starter 一般作为程序启动器使用，或者作为启动器的基类。通俗来说，可以认为是main函数的入口。 1AbstractBootstrap ServerBootstrap MacosXApplicationBootstrap DNSTaskStarter 2Processor 某一类功能的处理器，用来表示某个处理过程，是一系列代码片段的集合。 1CompoundProcesser BinaryComparisonProcessor 3Manger 对有生命状态的对象进行管理，通常作为某一类资源的管理入口。 1AccountManger DevicePolicyManger TransactionManger 4Holder 表示持有某个或者某类对象的引用，并可以对其进行统一管理。多见于不好回收的内存统一处理，或者一些全局集合容器的缓存。 1QueryHolder InstructionHolder ViewHolder 5Factory 工厂模式，表示此类为工厂类 1SessionFactory ScriptFactory LiveCaptureFactory 6Provider Provider = Strategy + Factory Method。它更高级一些，把策略模式和方法工厂揉在了一块，让人用起来很顺手。Provider一般是接口或者抽象类，以便能完成子实现 1AccountFeatureProvider ApplicationFeatureProvider CollatorPorvider 7Registrar 注册并管理一系列资源 1ImportServiceRegistrar IKryoRegistrar PipelineOptionRegistrar 8Engine 一般是核心模块，用来处理一类功能。引擎是个非常高级的名词，一般的类是没资格用它的。 1ScriptEngine DataQLScriptEngine C2DEngine 9Service 某个服务。 1IntegratorServiceImpl ISelectionService PersistenceService 10Task 某个任务。通常是个Runnable 1WorkflowTask FutureTask ForkJoinTask 2传播类为了完成一些统计类或者全局类的功能，有些参数需要一传到底。传播类的对象就可以通过统一封装的方式进行传递，并在合适的地方进行拷贝或者更新* 2.1Context 如果你的程序执行，有一些变量，需要从函数执行的入口开始，一直传到大量子函数执行完毕之后。这些变量或者集合，如果以参数的形式传递，将会让代码变得冗长无比。这个时候，你就可以把变量统一塞到Context里面，以单个对象的形式进行传递。 在Java中，由于ThreadLocal的存在，Context甚至可以不用在参数之间进行传递 1AppContext ServletContext ApplicationContext 2.2Propagator 传播，繁殖。用来将context中传递的值进行复制，添加，清除，重置，检索，恢复等动作。通常，它会提供一个叫做propagate的方法，实现真正的变量管理。 1TextMapPropagator FilePropagator TransactionPropagator 3.0回调类 使用多核可以增加程序运行的效率，不可避免的引入异步化。我们需要有一定的手段，获取异步任务执行的结果，对任务执行过程中的关键点进行检查。回调类API可以通过监听、通知等形式，获取这些事件。* 3.1Handler，Callback，Trigger，Listener callback通常是一个接口，用于响应某类消息，进行后续处理；Handler通常表示持有真正消息处理逻辑的对象，它是有状态的；tigger触发器代表某类事件的处理，属于Handler，通常不会出现在类的命名中；Listener的应用更加局限，通常在观察者模式中用来表示特定的含义。 1ChannelHandler SuccessCallback CronTrigger EventListener 3.2Aware Aware就是感知的意思，一般以该单词结尾的类，都实现了Aware接口。拿spring来说，Aware 的目的是为了让bean获取spring容器的服务。具体回调方法由子类实现，比如ApplicationContextAware。它有点回调的意思。 1ApplicationContextAware ApplicationStartupAware ApplicationEventPublisherAware 4.0监控类现在的程序都比较复杂，运行状态监控已经成为居家必备之良品。监控数据的收集往往需要侵入到程序的边边角角，如何有效的与正常业务进行区分，是非常有必要的。* 4.1 Metric 表示监控数据。不要用Monitor了，比较丑。 1TimelineMetric HistogramMetric Metric 4.2 Estimator 估计，统计。用于计算某一类统计数值的计算器。 1ConditionalDensityEstimator FixedFrameRateEstimator NestableLoadProfileEstimator 4.3 Accumulator 累加器的意思。用来缓存累加的中间计算结果，并提供读取通道。 1AbstractAccumulator StatsAccumulator TopFrequencyAccumulator 4.4 Tracker 一般用于记录日志或者监控值，通常用于apm中。 1VelocityTracker RocketTracker MediaTracker 5 内存管理类如果你的应用用到了自定义的内存管理，那么下面这些名词是绕不开的。比如Netty，就实现了自己的内存管理机制。* 5.1 Allocator 与存储相关，通常表示内存分配器或者管理器。如果你得程序需要申请有规律得大块内存，allocator是你得不二选择。 1AbstractByteBufAllocator ArrayAllocator RecyclingIntBlockAllocator 5.2 Chunk 表示一块内存。如果你想要对一类存储资源进行抽象，并统一管理，可以采用它。 1EncryptedChunk ChunkFactory MultiChunk 5.3 Arena 英文是舞台、竞技场的意思。由于Linux把它用在内存管理上发扬光大，它普遍用于各种存储资源的申请、释放与管理。为不同规格的存储chunk提供舞台，好像也是非常形象的表示。 关键是，这个词很美，作为后缀让类名显得很漂亮。 1BookingArena StandaloneArena PoolArena 5.4 Pool 表示池子。内存池，线程池，连接池，池池可用。 1ConnectionPool ObjectPool MemoryPool 6 过滤检测类** 程序收到的事件和信息是非常多的，有些是合法的，有些需要过滤扔掉。根据不同的使用范围和功能性差别，过滤操作也有多种形式。你会在框架类代码中发现大量这样的名词。 6.1Pipeline，Chain 一般用在责任链模式中。Netty，Spring MVC，Tomcat等都有大量应用。通过将某个处理过程加入到责任链的某个位置中，就可以接收前面处理过程的结果，强制添加或者改变某些功能。就像Linux的管道操作一样，最终构造出想要的结果。 1Pipeline ChildPipeline DefaultResourceTransformerChain FilterChain 6.2Filter 过滤器，用来筛选某些满足条件的数据集，或者在满足某些条件的时候执行一部分逻辑。如果和责任链连接起来，则通常能够实现多级的过滤。 1FilenameFilter AfterFirstEventTimeFilter ScanFilter 6.3 Interceptor 拦截器，其实和Filter差不多。不过在Tomcat中，Interceptor可以拿到controller对象，但filter不行。拦截器是被包裹在过滤器中。 1HttpRequestInterceptor 6.4 Evaluator 英文里是评估器的意思。可用于判断某些条件是否成立，一般内部方法evaluate会返回bool类型。比如你传递进去一个非常复杂的对象，或者字符串，进行正确与否的判断。 1ScriptEvaluator SubtractionExpressionEvaluator StreamEvaluator 6.5 Detector 探测器。用来管理一系列探测性事件，并在发生的时候能够进行捕获和响应。比如Android的手势检测，温度检测等。 1FileHandlerReloadingDetector TransformGestureDetector ScaleGestureDetector 7.0 结构类 除了基本的数据结构，如数组、链表、队列、栈等，其他更高一层的常见抽象类，能够大量减少大家的交流，并能封装常见的变化。 7.1Cache 这个没啥好说的，就是缓存。大块的缓存。常见的缓存算法有LRU、LFU、FIFO等。 1LoadingCache EhCacheCache 7.2 Buffer buffer是缓冲，不同于缓存，它一般用在数据写入阶段。 1ByteBuffer RingBuffer DirectByteBuffer 7.3 Composite 将相似的组件进行组合，并以相同的接口或者功能进行暴露，使用者不知道这到底是一个组合体还是其他个体。 1CompositeData CompositeMap ScrolledComposite 7.4 Wrapper 用来包装某个对象，做一些额外的处理，以便增加或者去掉某些功能。 1IsoBufferWrapper ResponseWrapper MavenWrapperDownloader 7.5 Option, Param,Attribute 用来表示配置信息。说实话，它和Properties的区别并不大，但由于Option通常是一个类，所以功能可以扩展的更强大一些。它通常比Config的级别更小，关注的也是单个属性的值。Param一般是作为参数存在，对象生成的速度要快一些。 1SpecificationOption SelectOption AlarmParam ModelParam 7.6 Tuple 元组的概念。由于Java中缺乏元组结构，我们通常会自定义这样的类。 1Tuple2 Tuple3 7.7Aggreatore 聚合器，可以做一些聚合计算。比如分库分表中的sum，max，min等聚合函数的汇集。 1BigDecimalMaxAggregator PipelineAggregator TotalAggregator 7.8 Iterator 迭代器。可以实现Java的迭代器接口，也可以有自己的迭代方式。在数据集很大的时候，需要进行深度遍历，迭代器可以说是必备的。使用迭代器还可以在迭代过程中安全的删除某些元素。 1BreakIterator StringCharacterIterator 7.9 Batch 某些可以批量执行的请求或者对象。 1SavedObjectBatch BatchRequest 7.10 Limiter 限流器，使用漏桶算法或者令牌桶来完成平滑的限流。 1DefaultTimepointLimiter RateLimiter TimeBasedLimiter 8.0 常见设计模式命名8.1 Strategy 将抽象部分与它的实现部分分离，使它们都可以独立地变化。策略模式。相同接口，不同实现类，同一方法结果不同，实现策略不同。比如一个配置文件，是放在xml里，还是放在json文件里，都可以使用不同的provider去命名。 1RemoteAddressStrategy StrategyRegistration AppStrategy 8.2 Adapter 将一个类的接口转换为客户希望的另一个接口，Adapter模式使得原本由于接口不兼容而不能一起工作的那些类一起工作。 不过，相对于传统的适配器进行api转接，如果你的某个Handler里面方法特别的多，可以使用Adapter实现一些默认的方法进行0适配。那么其他类使用的时候，只需要继承Adapter，然后重写他想要重写的方法就可以了。这也是Adapter的常见用法。 1ExtendedPropertiesAdapter ArrayObjectAdapter CardGridCursorAdapter 8.3 Action,Command 将一个请求封装为一个对象，从而使你可用不同的请求对客户进行参数化，对请求排队或记录请求日志，以及支持可撤销的操作。 用来表示一系列动作指令，用来实现命令模式，封装一系列动作或者功能。Action一般用在UI操作上，后端框架可以无差别的使用。 在DDD的概念中，CQRS的Command的C，既为Command。 1DeleteAction BoardCommand 8.4 Event 表示一系列事件。一般的，在语义上，Action，Command等，来自于主动触发；Event来自于被动触发。 1ObservesProtectedEvent KeyEvent 8.5 Delegate 代理或者委托模式。委托模式是将一件属于委托者做的事情，交给另外一个被委托者来处理。 1LayoutlibDelegate FragmentDelegate 8.6 Builder 将一个复杂对象的构建与它的表示分离，使得同样的构建过程可以创建不同的表示。 构建者模式的标准命名。比如StringBuilder。当然StringBuffer是个另类。这也说明了，规则是人定的，人也可以破坏。 1JsonBuilder RequestBuilder 8.7 Template 模板方法类的命名。定义一个操作中的算法的骨架，而将一些步骤延迟到子类中。模板方法使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤。 1JDBCTemplate 8.8 Proxy 代理模式。为其他对象提供一种代理以控制对这个对象的访问。 1ProxyFactory SlowQueryProxy 9.0解析类命名写代码要涉及到大量的字符串解析、日期解析、对象转换等。根据语义和使用场合的区别，它们也分为多种。* 9.1 Converter，Resolver 转换和解析。一般用于不同对象之间的格式转换，把一类对象转换成另一类。注意它们语义上的区别，一般特别复杂的转换或者有加载过程的需求，可以使用Resolver。 1DataSetToListConverter LayoutCommandLineConverter InitRefResolver MustacheViewResolver 9.2Parser 用来表示非常复杂的解析器，比如解析DSL。 1SQLParser JSONParser 9.3Customizer 用来表示对某个对象进行特别的配置。由于这些配置过程特别的复杂，值得单独提取出来进行自定义设置。 1ContextCustomizer DeviceFieldCustomizer 9.4Formatter 格式化类。主要用于字符串、数字或者日期的格式化处理工作。 1DateFormatter StringFormatter 10.0 网络类 1Packet 通常用于网络编程中的数据包。 1DhcpPacket PacketBuffer 10.2 Protocol 同样用户网络编程中，用来表示某个协议。 1RedisProtocol HttpProtocol 10.3 Encoder、Decoder、Codec 编码解码器 1RedisEncoder RedisDecoder RedisCodec 10.4 Request，Response 一般用于网络请求的进和出。如果你用在非网络请求的方法上，会显得很怪异。 10 其它 11 Util，Helper 都表示工具类，Util一般是无状态的，Helper以便需要创建实例才能使用。但是一般没有使用Tool作为后缀的。 1HttpUtil TestKeyFieldHelper CreationHelper 12 Mode，Type 看到mode这个后缀，就能猜到这个类大概率是枚举。它通常把常见的可能性都列到枚举类里面，其他地方就可以引用这个Mode。 1OperationMode BridgeMode ActionType 13 Invoker，Invocation invoker是一类接口，通常会以反射或者触发的方式，执行一些具体的业务逻辑。通过抽象出invoke方法，可以在invoke执行之前对入参进行记录或者处理；在invoke执行之后对结果和异常进行处理，是AOP中常见的操作方式。 1MethodInvoker Invoker ConstructorInvocation 14 Initializer 如果你的应用程序，需要经过大量的初始化操作才能启动，那就需要把它独立出来，专门处理初始化动作。 1MultiBackgroundInitialize ApplicationContextInitializer 15 Feture，Promise 它们都是用在多线程之间的，进行数据传递。 Feture相当于一个占位符，代表一个操作将来的结果。一般通过get可以直接阻塞得到结果，或者让它异步执行然后通过callback回调结果。 但如果回调中嵌入了回调呢？如果层次很深，就是回调地狱。Java中的CompletableFuture其实就是Promise，用来解决回调地狱问题。Promise是为了让代码变得优美而存在的。 16 Selector 根据一系列条件，获得相应的同类资源。它比较像Factory，但只处理单项资源。 1X509CertSelector NodeSelector 17 Reporter 用来汇报某些执行结果。 1ExtentHtmlReporter MetricReporter 18 Constants 一般用于常量列表。 19 Accessor 封装了一系列get和set方法的类。像lombok就有Accessors注解，生成这些方法。但Accessor类一般是要通过计算来完成get和set，而不是直接操作变量。这适合比较复杂的对象存取服务。 1ComponentAccessor StompHeaderAccessor 110 Generator 生成器，一般用于生成代码，生成id等。 1CodeGenerator CipherKeyGenerator 转载自 [原创：小姐姐味道（微信公众号ID：xjjdog），欢迎分享，转载请保留出处。]","categories":[],"tags":[]},{"title":"ceph配置","slug":"ceph配置","date":"2023-03-20T11:56:51.000Z","updated":"2024-03-14T14:55:29.475Z","comments":true,"path":"4071038d.html","link":"","permalink":"https://imlike.cc/4071038d.html","excerpt":"","text":"Posted on 2017-05-15 | In [c]{.ul} [eph]{.ul} | | 4099 [概]{.ul} [述 ]{.ul} Ceph的配置参数很多，从⽹上也能搜索到⼀⼤批的调优参数，但这些参数为什么这么设置？设置为这样是否合理？解释的并不多 本⽂从当前我们的ceph.conf⽂件⼊⼿，解释其中的每⼀项配置，做为以后参数调优和新⼈学习的依据； Network Configuration Reference — Ceph Documentation 1，⼀些固定配置参数 以上通常是通过ceph-deploy⽣成的，都是ceph monitor相关的参数，不⽤修改； 2，⽹络配置参数 public network：monitor与osd，client与monitor，client与osd通信的⽹络，最好配置为带宽较⾼的万兆⽹络； cluster network：OSD之间通信的⽹络，⼀般配置为带宽较⾼的万兆⽹络； 参考： [ttp://docs.ceph.com/docs/master/rados/configuration/network-config-ref/]{.ul} 3，pool size配置参数 这两个是创建ceph pool的时候的默认size参数，⼀般配置为3和1，3副本能⾜够保证数据的可靠性； 4，认证配置参数 以上是Ceph authentication的配置参数，默认值为开启ceph认证； 在内部使⽤的ceph集群中⼀般配置为none，即不使⽤认证，这样能适当加快ceph集群访问速度； 5，osd down out配置参数 mon_osd_down_out_interval ：ceph标记⼀个osd为down and out的最⼤时间间隔mon_osd_min_down_reporters ：mon标记⼀个osd为down的最⼩reporters个数（报告该osd为down的其他osd为⼀个reporter） mon_osd_report_timeout ：mon标记⼀个osd为down的最⻓等待时间 osd_heartbeat_interval ：osd发送heartbeat给其他osd的间隔时间（同⼀PG之间的osd才会有 heartbeat） osd_heartbeat_grace ：osd报告其他osd为down的最⼤时间间隔，grace调⼤，也有副作⽤，如果某个osd异常退出，等待其他osd上报的时间必须为grace，在这段时间段内，这个osd负责的pg的io会hang住，所以尽量不要将grace调的太⼤。 基于实际情况合理配置上述参数，能减少或及时发现osd变为down（降低IO hang住的时间和概率）， 延⻓osd变为down and out的时间（防⽌⽹络抖动造成的数据recovery）； 参考： [h]{.ul} [ttp://docs.ceph.com/docs/master/rados/configuration/mon-osd-interaction/]{.ul} [h]{.ul} [ttp://blog.wjin.org/posts/ceph-osd-heartbeat.html]{.ul} 6，objecter配置参数 osd client端objecter的throttle配置，它的配置会影响librbd，RGW端的性能； 配置建议： 调⼤这两个值 7，ceph rgw配置参数 rgw_frontends ：rgw的前端配置，⼀般配置为使⽤轻量级的civetweb；prot为访问rgw的端⼝，根据实际情况配置；num_threads为civetweb的线程数； rgw_thread_pool_size ：rgw前端web的线程数，与rgw_frontends中的num_threads含义⼀致，但 num_threads 优 于 rgw_thread_pool_size 的 配 置 ， 两 个 只 需 要 配 置 ⼀ 个 即 可 ； rgw_override_bucket_index_max_shards ：rgw bucket index object的最⼤shards数，增⼤这个值能减少bucket index object的访问时间，但也会加⼤bucket的ls时间； rgw_max_chunk_size ：rgw最⼤chunk size，针对⼤⽂件的对象存储场景可以把这个值调⼤； rgw_cache_lru_size ：rgw的lru cache size，对于读较多的应⽤场景，调⼤这个值能加快rgw的响应速度； rgw_bucket_default_quota_max_objects ：配合该参数限制⼀个bucket的最⼤objects个数； 参考： [h]{.ul} [ttp://docs.ceph.com/docs/jewel/install/install-ceph-gateway/]{.ul} [h]{.ul} [ttp://ceph-users.ceph.narkive.com/mdB90g7R/rgw-increase-the-first-chunk-size]{.ul} [h]{.ul} [ttps://access.redhat.com/solutions/2122231]{.ul} 8，debug配置参数 关闭了所有的debug信息，能⼀定程度加快ceph集群速度，但也会丢失⼀些关键log，出问题的时候不好分析； 参考： [h]{.ul} [ttp://www.10tiao.com/html/362/201609/2654062487/1.html]{.ul} 9，osd op配置参数 osd_enable_op_tracker ：追踪osd op状态的配置参数，默认为true；不建议关闭，关闭后osd的slow_request，ops_in_flight，historic_ops ⽆法正常统计； 打开op tracker后，若集群iops很⾼， osd_num_op_tracker_shard 可以适当调⼤，因为每个shard都有个独⽴的mutex锁； osd_op_threads ：对应的work queue有peering_wq （osd peering请求）， recovery_gen_wq （PG recovery请求）； osd_disk_threads ：对应的work queue为 remove_wq （PG remove请求）； [osd_op_num_shards]{.ul} 和osd_op_num_threads_per_shard ：对应的thread pool为osd_op_tp ，work queue为op_shardedwq ； 处理的请求包括： 1. 2. 3. 调⼤osd_op_num_shards 可以增⼤osd ops的处理线程数，增⼤并发性，提升OSD性能； 10，osd client message配置参数 这个是osd端收到client messages的capacity配置，配置⼤的话能提升osd的处理能⼒，但会占⽤较多的系统内存； 配置建议： 服务器内存⾜够⼤的时候，适当增⼤这两个值 11，osd scrub配置参数 Ceph osd scrub是保证ceph数据⼀致性的机制，scrub以PG为单位，但每次scrub回获取PG lock，所以它可能会影响PG正常的IO； Ceph后来引⼊了chunky的scrub模式，每次scrub只会选取PG的⼀部分objects，完成后释放PG lock， 并把下⼀次的PG scrub加⼊队列；这样能很好的减少PG scrub时候占⽤PG lock的时间，避免过多影响PG正常的IO； 同理，引⼊的osd_scrub_sleep 参数会让线程在每次scrub前释放PG lock，然后睡眠⼀段时间，也能很好的减少scrub对PG正常IO的影响； 配置建议： osd_scrub_begin_hour 和osd_scrub_end_hour ：OSD Scrub的开始结束时间，根据具体业务指定； osd_scrub_sleep ：osd在每次执⾏scrub时的睡眠时间；有个bug跟这个配置有关，建议关闭； osd_scrub_load_threshold ：osd开启scrub的系统load阈值，根据系统的load average值配置该参数； osd_scrub_chunk_min 和osd_scrub_chunk_max ：根据PG中object的个数配置；针对RGW全是 ⼩⽂件的情况，这两个值需要调⼤； 参考： [h]{.ul} [ttp://www.jianshu.com/p/ea2296e1555c]{.ul} [h]{.ul} [ttp://tracker.ceph.com/issues/19497]{.ul} 12，osd thread timeout配置参数 osd_op_thread_timeout 和osd_op_thread_suicide_timeout 关联的work queue为： - 关联的请求为： OpRequestRef ， PGSnapTrim ， PGScrub - 关联的请求为：osd peering osd_recovery_thread_timeout 和osd_recovery_thread_suicide_timeout 关联的work queue 为： - 关联的请求为：PG recovery Ceph的work queue都有个基类WorkQueue_ ，定义如下： 这⾥的timeout_interval 和suicide_interval 分别对应上⾯所述的配置timeout 和 当thread处理work queue中的⼀个请求时，会受到这两个timeout时间的限制： 到时间后设置m_unhealthy_workers+1 到时间后调⽤assert，OSD进程crush 对应的处理函数为： 当前仅有RGW添加了worker的perfcounter，所以也只有RGW可以通过perf dump查看total/unhealthy 的worker信息： 对应的配置项为： filestore_op_threads ：对应的thread pool为op_tp ，对应的work queue为op_wq ；filestore的所有请求都经过op_wq处理； 增⼤该参数能提升filestore的处理能⼒，提升filestore的性能；配合filestore的throttle⼀起调整； 配置的含义与上⼀节中的[thread_timeout/thread_suicide_timeout]{.ul} 保持⼀致； 13，filestore merge/split配置参数 这两个参数是管理filestore的⽬录分裂/合并的，filestore的每个⽬录允许的最⼤⽂件数为： 在RGW的⼩⽂件应⽤场景，会很容易达到默认配置的⽂件数（320），若在写的过程中触发了filestore 的分裂，则会⾮常影响filestore的性能； 每次filestore的⽬录分裂，会依据如下规则分裂为多层⽬录，最底层16个⼦⽬录： 例如PG 31.4C0, hash结尾是4C0，若该⽬录分裂，会分裂为 DIR_0/DIR_C/DIR_4/{DIR_0, DIR_F} ； 原始⽬录下的object会根据规则放到不同的⼦⽬录⾥，object的名称格式为: * head_xxxxX4C0_* ，分裂时候X是⼏，就放进⼦⽬录DIR_X⾥。⽐如object： * head_xxxxA4C0_* , 就放进⼦⽬录 ⾥； 解决办法： 增⼤merge/split配置参数的值，使单个⽬录容纳更多的⽂件； filestore_merge_threshold 配置为负数；这样会提前触发⽬录的预分裂，避免⽬录在某⼀时间段的集中分裂，详细机制没有调研； 创建pool时指定expected-num-objects ；这样会依据⽬录分裂规则，在创建pool的时候就创建分 裂的⼦⽬录，避免了⽬录分裂对filestore性能的影响； 参考： [h]{.ul} [ttp://docs.ceph.com/docs/master/rados/configuration/filestore-config-ref/]{.ul} [h]{.ul} [ttp://docs.ceph.com/docs/jewel/rados/operations/pools/#create-a-pool]{.ul} [h]{.ul} [ttp://blog.csdn.net/for_tech/article/details/51251936]{.ul} [h]{.ul} [ttp://ivanjobs.github.io/page3/]{.ul} 14，filestore fd cache配置参数 filestore的fd cache是加速访问filestore⾥的file的，在⾮⼀次性写⼊的应⽤场景，增⼤配置可以很明显的提升filestore的性能； 15，filestore sync配置参数 filestore_wbthrottle_enable 的配置是关于filestore writeback throttle的，即我们说的filestore处理workqueue op_wq 的数据量阈值；默认值是true，开启后XFS相关的配置参数有： 若使⽤普通HDD，可以保持其为true；针对SSD，建议将其关闭，不开启writeback throttle； filestore_min_sync_interval 和 filestore_max_sync_interval 是 配 置 filestore flush outstanding IO到disk的时间间隔的；增⼤配置可以让系统做尽可能多的IO merge，减少filestore写磁盘的压⼒，但也会增⼤page cache占⽤内存的开销，增⼤数据丢失的可能性； filestore_commit_timeout 是配置filestore sync entry到disk的超时时间，在filestore压⼒很⼤时， 调⼤这个值能尽量避免IO超时导致OSD crush； 16，filestore throttle配置参数 在jewel版本⾥，引⼊了dynamic throttle，来平滑普通throttle带来的⻓尾效应问题； ⼀般在使⽤普通磁盘时，之前的throttle机制即可很好的⼯作，所以这⾥默认 filestore_queue_high_delay_multiple 和filestore_queue_max_delay_multiple 都为0； 针对⾼速磁盘，需要在部署之前，通过⼩⼯具ceph_smalliobenchfs 来测试下，获取合适的配置参数； 参考： [h]{.ul} [ttp://docs.ceph.com/docs/jewel/dev/osd_internals/osd_throttles/]{.ul} [h]{.ul} [ttp://blog.wjin.org/posts/ceph-dynamic-throttle.html]{.ul} [h]{.ul} [ttps://github.com/ceph/ceph/blob/master/src/doc/dynamic-throttle.txt]{.ul} [C]{.ul} [eph BackoffThrottle分析]{.ul} 17，filestore finisher threads配置参数 这两个参数定义filestore commit/apply的finisher处理线程数，默认都为1，任何IO commit/apply完成后，都需要经过对应的ondisk/apply finisher thread处理； 在使⽤普通HDD时，磁盘性能是瓶颈，单个finisher thread就能处理好； 但在使⽤⾼速磁盘的时候，IO完成⽐较快，单个finisher thread不能处理这么多的IO commit/apply reply，它会成为瓶颈；所以在jewel版本⾥引⼊了finisher thread pool的配置，这⾥⼀般配置为2即可； 18，journal配置参数 journal_max_write_bytes 和journal_max_write_entries 是journal⼀次write的数据量和entries 限制； 针对SSD分区做journal的情况，这两个值要增⼤，这样能增⼤journal的吞吐量； journal_throttle_high_multiple 和journal_throttle_max_multiple 是JournalThrottle 的配置参数， JournalThrottle 是BackoffThrottle 的封装类，所以JournalThrottle 与我们在filestore throttle介绍的dynamic throttle⼯作原理⼀样； 从上述代码中看出相关的配置参数有： 19，rbd cache配置参数 rbd_cache_size ：client端每个rbd image的cache size，不需要太⼤，可以调整为64M，不然会⽐较占client端内存； 参照默认值，根据rbd_cache_size 的⼤⼩调整rbd_cache_max_dirty 和 rbd_cache_max_dirty ：在writeback模式下cache的最⼤bytes数，默认是24MB；当该值为0 时，表示使⽤writethrough模式； rbd_cache_target_dirty ：在writeback模式下cache向ceph集群写⼊的bytes阀值，默认 16MB；注意该值⼀定要⼩于rbd_cache_max_dirty 值 rbd_cache_writethrough_until_flush ：在内核触发flush cache到ceph集群前rbd cache⼀直是writethrough模式，直到flush后rbd cache变成writeback模式； rbd_cache_max_dirty_age ：标记OSDC端ObjectCacher中entry在cache中的最⻓时间；","categories":[],"tags":[]},{"title":"chatGPT的介绍转载","slug":"chatGPT的介绍转载","date":"2023-03-05T08:14:53.000Z","updated":"2024-03-14T14:55:29.475Z","comments":true,"path":"c0bea6a0.html","link":"","permalink":"https://imlike.cc/c0bea6a0.html","excerpt":"","text":"关于 ChatGPT 的五个最重要问题我们判断，如果ChatGPT不犯大错，两年之内，整个科技行业甚至人类社会都会被颠覆一遍。倒计时已经开始了。 在ChatGPT纪元中，提问题的能力和判断力也许是人类最重要的两个能力。我们这里提出五个关键问题，并且试图抛开网络上的二手观点，做出基于原理的判断。  更详细的科普文可以参考这篇：了解AIGC中的ChatGPT和LLM  其中包含如何在公司快速便捷地使用ChatGPT  针对中台业务场景的应用：ChatGPT在中台业务应用的可能性与探索 这篇文章会尽量用准确的说明与类比（如何区分准确的类比和偷换概念的类比）去解读技术。但是对于这样一个非常新、非常颠覆、大家对原理和应用都还没形成共识的技术，不了解技术细节就去打比方，难免信口开河。所以我们会先尽量无损地把需要的技术细节都盘清楚，然后再去进行抽象和提取本质。哪五个问题？ 是什么： ChatGPT是范式突破吗？和过往AI有什么不同？ 会怎样： ChatGPT两年内会达到什么水准？ 行业格局： ChatGPT以及GPT有壁垒吗？ 如何参与： 我们未来应该如何使用ChatGPT？ 人文： 人类和ChatGPT的本质区别是什么？对人类社会的冲击？ 还有一个不需要讨论的重要问题： ChatGPT不会开源的 ，因为AGI是一个危险品。国内那些依赖开源+抄的公司可以死心了。指望原子弹开源吗？ 我们搞清楚这五个问题，就能判断市面上大多数解读ChatGPT的观点，无论从技术、商业、投资，等等角度，是否靠谱了。其实就两个关键 对ChatGPT新能力的认知：这新能力是什么，有什么意义？ 对“能力获取难度”的认知：ChatGPT如何获得的？难度有多大？ 文章结尾我们会做一下总结。让你下次见到某大模型，可以判断这是ChatGPT的80%还是 0%。为什么这样问？最近到处都在讨论ChatGPT，就像A股 6000 点的时候所有人都在讨论A股一样。但是大家的问题主要聚焦在于自己会不会被ChatGPT取代，中国如何赶超ChatGPT，ChatGPT相关概念股票是什么，等等。这些问题很重要，但是对于ChatGPT这样一个人类高科技结晶的新物种，不先搞清楚它 “是什么”和“如何牛逼的” ，那就没有办法形成自己的判断。没有自己的判断，看ChatGPT就像看元宇宙、Web3、自动驾驶一样，觉得好像牛逼，也有一套看似点只能被别人牵着走。 所以我们先要搞清楚这两个问题。搞清楚ChatGPT“是什么”和“如何牛逼的”是一件困难的事情，因为最顶尖的人工智能大佬们也没有达成共识。比如Meta的AI负责人，深度学习三大佬之一的LeCun就不看好，认为这就是个基于auto-regressive（自回归）的LLM（large language model，大语言模型），从方法上来讲没有啥范式突破。只是因为OpenAI是个创业公司，大家宽容度比较高，ChatGPT在乱说话，大家也能容忍。 另一面，ChatGPT的火热就不说了，比如特斯拉的首席AI科学家就选择回到OpenAI，共建AGI（artificial general intelligence，通用人工智能，也是OpenAI的追求目标）；Meta的VR创始人卡马克选择离开Meta，自己开一家AGI公司。另外一篇文章截取了大佬们的赞誉 技术讲解环节：“自回归”“大语言模型”是两个关键词 自回归的意思是，我先用模型预测下一个词是什么，然后把预测出来的词带入模型，去预测再下一个词是什么，不断迭代。这是过往语言模型的通用范式。也能让各类语言类任务统一成“生成式”任务 大语言模型是因为GPT的海量数据与参数。大语言模型本身也值得技术科普一下，我们在第一个问题中展开大佬们都认可AGI的重要意义，但是对于ChatGPT是不是有“范式突破”，是不是AGI，有重大分歧。大佬们无法形成共识，我们有两种选择，一种是做“early-adoptor”，早期采用者（特点是懂技术，有vision，想通过技术带来巨大改变），去在体验和探索中形成自己的观点；一种是做“early-mass”，早期大众（特点是厌恶风险，希望追求确定的改善），等着标准成熟，应用也有定论，再去采用。作为中台，我们需要抓住IEG early-adoptor的身份。所以我们需要在纷杂的信息中形成自己的理解和主张。 这次讨论就想达到这个目的。数科和算法同学会保证信息的准确，知之为知之不知为不知，但是观点、主张、区别、共识，需要老板们自己推演出。开头：ChatGPT的最基本版历史虽然想直接开始问题，但是一定程度的技术科普还是必要的。这样能在思考问题的时候更加高效。我们着重参考两篇综述，尽量抓重点：1. 大语言模型技术精要（链接） ChatGPT的各项超能力从哪里来（链接） 第一份综述主要讲了大语言模型发展中的关键技术突破，第二份综述主要讲了ChatGPT发展过程中的几个重要模型“做到了什么”和“有什么与众不同的做法”。我们把两篇文章的重点挑出来，标注一下里程碑事件，和其意义。 事件 意义123Transformer问世让深度学习模型可以“自由吸收”数据中的知识 大语言模型突破了参数和算力限制，从此语言模型也进入参数越多，数据越大，模型效果越好的时代。LLM内战，逐渐吊打老NLP12Google的Bert路线和OpenAI的GPT路线各有所长 GPT通过“自然语言生成任务”，兼容了几乎所有NLP问123456题；但是Bert比GPT2表现好。此时大语言模型已经开始吊打传统NLP模型了GPT3问世展示in-context learning能力，简单调教下就能吊打精调过的很多模型 一方面让GPT模式初现一统江湖的潜质，一方面GPT3的12in-context learning能力，展示了和过往ML的fine-tuning模式的本质区别 ，我们在下面单独详细展开 - InstructGPTChatGPT的交互模式，让GPT的能力，更加贴近人类真实交互方式 1234在in-context learning基础之上，进一步降低了prompting的门槛；一定程度解决了GPT- 3 生成结果与用户期望不一致的非预期输出，大幅降低了有害的、错误或偏差的输出结果，让GPT更符合人类胃口 - GPT读代码代码训练能力的提升是GPT到GPT3.5的重要迭代之一，模型可从而可以生成代码和理解代码 123Codex模型读了大量代码，之后的GPT3.5模型 涌现出了inference的能力 。不光能读懂和生成代码，对语言本身的理解和推理能力也解锁了 - RLHF12345678910111213141516ChatGPT背后的核心技术之一，让模型学习人类的偏好全称是reinforcement learning from human feedback，通过构建人类反馈数据集，训练一个reward模型，模仿人类偏好对结果打分，是GPT- 3 后时代LLM越来越像人类对话的核心技术ChatGPTInstructGPT的亲戚，但一些优化方式也带来了ChatGPT的更泛化和准确能力，再次引爆了AIGCChatGPT总体来说和InstructGPT一样是使用RLHF进行训练，但模型是基于GPT3.5，而且数据设置上也不同。ChatGPT是一个输入，模型给出多个输出，然后人给结果排序，让模型可以学习人类的排序策略，即使是一本正经的胡说八道看起来也很合理的样子。 这里面再强调一个关键点。GPT3之后，很多能力是“涌现”的。即不是线性发展，可预测的，而是突然就有了。至于这对于OpenAI的人是早晚会发生，还是完全未预料，我们就不知道了。 这几个“涌现”出的能力，尤其是inference的能力，是ChatGPT和过往AI的范式不同，也会是我们这次讨论关注的重点。“涌现” 也是大语言模型很神奇的一点，这些能力我们发现模型随着数据量和模型大小的提升，就突然具备了，但是我们对这些能力怎么出现的，只有猜想，没有共识。这篇文章有一个比较全面的总结和比较。 下图是GPT- 3 到ChatGPT的演变历史 这里也牵涉到了一个重要的题外话，大概是发生在in-context learning和instruct这一模式下。尤其是对“fine-tuning”这个词的理解。如果用过往模型的经验去想象ChatGPT的运作方式，就会产生错判。这一误解反而更容易发生对AI已经有所了解的从业者身上。 重要的题外话 – 很多AI从业者为什么对ChatGPT有错误理解？过往的NLP模型是按照具体任务和具体数据来训练的。所以数据质量越好，模型效果越好。而且最好只有正确数据，没有错误数据。大语言模型有一个重要不同，是“数据越多”越好，而数据质量变成了重要，但稍显次要因素。在一开始，大模型想要在特定任务上取得较好效果，也需要那个领域的具体数据去“fine-tune”一下。通过大量的例子，先教给模型，那个领域中什么是好，什么是坏，调节一下模型的权重，从而输出恰当的结果。这和过往模型的范式是差不多的。 而GPT- 3 涌现出的in-context learning能力（Google的PaLM大模型也有）和上述范式有本质不同，“过往的fine-tuning”需要更改模型参数。也就是说，换了个新模型，从而在新数据上表现更好。但是in-context learning， 模型并没有变化 ，却能在新数据上表现更好。研究甚至发 现，你给大模型一堆范例，只要对应关系整体是对的，这时候改变具体对应的顺序，大模型仍然能输出正确的结果。这真的很神奇。一定要抓住这一点，模型没有变化，没有被重新训练，但是能“理解”新数据，并且表现更好！接下来还有更神奇的。在GPT-Codex版本解锁了推理能力，以及InstructGPT提出了instruct这一方法，他们合体的ChatGPT在in-context learning的基础之上，展示出了具备inference能力的样子。我们在下一章详细展开。 1. ChatGPT是范式突破吗？机器学习发展了这么多年，遵循同一个范式。哪怕 2016 年大火的AlphaGo，也完全没有脱离这个范式 – 鹦鹉学舌。 过往机器学习的范式 – 鹦鹉学舌机器学习，包括深度学习，所遵循的范式是“data fitting”，即找到数据中的 “对应关系” 并应用。具体来说，就是Y=f(X)，给定一些优化目标，机器学习通过学习已知X和Y的关系，即f，让我们在见到一个未知的X‘的时候，也能根据规律，总结出Y‘是什么，能最好达到我们的目标。 从信息论的角度，这样的范式，所能总结的规律，应该是在“已有X所包含信息的范畴之内”。换句话说，遇到一个新的X‘，虽然没见过，但是应该和过去的X长得差不多。用图像识别举例，如果模型只在小狗小猫的数据上训练过，是无法区分马车和汽车的。这就很像鹦鹉学舌的机制。鹦鹉是不知道那段话的意义的，它用自己的方式去理解了这个发音，并且模仿了出来。计算机能更加精准地进行控制和编程，让这个机制发挥更大作用，比如 图像识别 + 搜索，就能高效找人 Matrix completion + 用户数据收集，就能高效推荐  把游戏规则恰当转化为优化方程 + 问题的局部抽象 + 自己生成对局训练，就能下围棋 推荐算法的原理 想象一个矩阵，横着是不同的人，竖着是不同的短视频，格子里是这个人对这个短视频的兴趣指数，我们要想方设法填满这个格子，给每个人推荐最感兴趣的短视频 核心问题是在每个人都没看过99.9999999%短视频的情况下，这题怎么解 有很多种方式，传统的运营、策略，也是一些方式。现有算法的主要做法是 1. 把每个视频抽象成特征 2. 把每个人抽象成特征 3. 通过特征对特征的方式进行泛化和填表，如果用人来理解的角度，可能是 中年男人喜欢看钓鱼（内容+画像推荐） 你同事们喜欢看老板点赞过的视频（关系链）1 看过AB的人喜欢看C（collaborative filtering）  但是记得，模型抽象出来的特征是适合机器理解，而不是适合人类理解的。用人类能描述的方式去描述机器的优化，注定是降低效率的 由此可见。过往AI应用的拓展主要是来自几点1. 高质量的数据，模型的发展，算力的提升，让模型越来越准、快、和发现更多更深的“对应关系”，能进行更好的优化2. 更好地把商业问题转化为优化问题3. 和其他能力的深度结合但是这些都是基于“鹦鹉学舌”这一范式的。过往的NLP（natural language processing，自然语言处理）就是一个很好的例子。发展了那么多年，语音助手能根据指令来达成一些目标，但是从来都没有真的“懂”那些指令。过往的NLP只能做“填表”，必须背后有一个人设定好具体的任务，规划好如何把语音或者文字形成固定的function，该function如何调用相应的能力。如果没有人提前规划，那模型就无法实现。这篇文章总结的非常好，这里就不赘述了。 上面这个“懂”字，如果深究的话，会发现大家对这个字的定义其实存在很大分歧。我如果让狗狗伸手，狗狗伸手了，是“懂”吗？过去NLP的懂和ChatGPT的懂，又有什么区别呢？分清这一点，我们就能分清ChatGPT“涌现”出来的让大家惊诧的能力是什么了。这里引用朱松纯教授关于乌鸦的例子。 ChatGPT可能的新范式 – 乌鸦原文在这里，介绍了乌鸦是如何“感知、认知、推理、学习、和执行”的：总结一下，城市中的乌鸦学会自主串通 汽车能压碎坚果 红绿灯能控制汽车这两件事情，从而利用红绿灯和汽车，来帮自己达到“打开坚果”这一任务结果。如果类比成机器学习模型，过往“鹦鹉学舌”范式的解法，是要求所有乌鸦可以共享一个大脑，它们有很清晰的优化目标，即“保住性命的前提下打开坚果”。它们的方式是，随机尝试所有事件的组合，并向着最优解的方向不断优化。但现实世界的乌鸦无法共享大脑，也不能去冒着死亡风险去尝试所有可能。乌鸦只有一次机会，把观测到的两个现象，产生了一个新的可能性，并应用在一个全新的场景下。我们文章里暂时把这个能力称之为“inference”。中文翻译为推理，但是它和“deduction”，即演绎、推演，又有所不同。 Inference的翻译是“基于证据和逻辑推演，得到结论”的过程，有的时候，还要加入很多猜测、抽象、泛化。举个例子，ChatGPT其实表现出了很多新能力，但是我们选择专注其“inference”的能力，并且和朱教授五年前的文章联系起来，就是一个inference。朱松纯教授在文章里就呼吁大家去“寻找‘乌鸦’模式的智能，而不要‘鹦鹉’模式的智能”。现在ChatGPT让AI第一次看似拥有了“乌鸦”模式的智能，那当然是一件划时代的大事件。 但是Inference也不是一个特别好的词，因为在机器学习领域里，inferencing特指使用训练好的深度学习模型来预测新的数据这一件事，会产生误解。另外，我也不确定inference和“乌鸦”的能力是一一对应的。 在我们自己的文章里，我们会用“乌鸦”来指代ChatGPT的新能力。但是在对外交流的时候，“乌鸦”需要解释的内容太多，所以我们会简化为“理解”。从“乌鸦”到“理解”，当然是一个信息量损失很大的过度概括。但是好处是可以把ChatGPT的本质能力凸显出来。 过往互联网的两次能力跃进一次来自于搜索，一次来自于推荐，现在ChatGPT带来了“理解”，也非常有结构感。 ChatGPT看似拥有“理解”能力的证据之所以说“看似”，是因为我们并不知道乌鸦为什么会有inference的能力，我们也不完全知道LLM为什么会有“达成inference效果”的能力。我们知道的是，LLM激活inference能力的方式一定与人类和乌鸦不一样。所以我们不把话说死，只说看似拥有，不确定真的拥有。为了节省笔墨，我们接下来就不说“看似”了。 我们把具体的例子放在附录里，但是有这几点感受很明显 ChatGPT拥有in-context correction的能力 ，即如果说错了，给出矫正，ChatGPT能“听懂”错在哪儿了，并向正确的方向修正（案例：）  描述越详细清楚，ChatGPT回答得越好 。要知道，越详细的描述，在预训练的文本里越难 匹配到 在询问ChatGPT互联网上并不存在内容的时候，能给出较好答案（案例：我用ChatGPT学UE5）  ChatGPT能通过信息猜你心中的想法（案例：跟ChatGPT玩20 questions）  你可以制定一个全新的游戏规则让ChatGPT和你玩，ChatGPT可以理解 前两点是本质，后三点是体感。 回过来对比过往NLP模型范式如何能达到类似效果，就能看到ChatGPT的神奇之处。过往模型一定需要针对具体的问题进行具体设计，而且只要说的话稍稍不够“结构化”，模型的表现就很难保证，更别提在模型资料库里，没有出现过的问题了。 打比方时间到把该说的细节说清楚，我们现在可以负责任地打比方了。其实朱教授鹦鹉和乌鸦的比方最精确不过了，但是毕竟人不是鹦鹉和乌鸦，鹦鹉和乌鸦的能力到底有什么区别，也需要一番解释，我们还是打一个“人”的比方。过往ML模型是一个“说话不过脑子”的“复读机”类型的人。好处是这个人记忆力和检索能力都特别强，而且有自己的一套理解事物对应关系的方式，让你给他看足够多东西的时候，TA就能找到对应关系。所以你给TA看的东西越多，离你的目标越近，TA的表现越好。问题是TA其实完全听不懂你在说什么，你没教的TA也不可能会。ChatGPT是一个“开窍”之后拥有“举一反三”能力的人。而且这个举一反三不光是在“相似问题”上，而是能把看似没有联系的事物联系起来，并且做一些逻辑推演。那ChatGPT就是一个“懂很多”，有很强的“学习能力”，而且“能听懂你”说话。 提炼对比一下的话 过往ML：需要 “喂” ，之后 “模仿” ，基于的是 “对应关系” ChatGPT：需要 “教” ，之后 “懂” ，基于的是 “内在逻辑” 后者的能力上限和应用空间，比起前者岂止百倍。这也是为什么大家如此 兴奋 和 焦虑 。兴奋是因为可能性，焦虑是因为目前只有OpenAI一家做出来了ChatGPT，而且并不开源。如果ChatGPT如此重要且牛逼，但所有人只能 基于ChatGPT做应用 的话，每个人都要重新考虑自 己的商业模式了。这两个问题都属于“猜想未来”，一不小心就会变成科幻小说，我们基于事实和底层理解，用科学的方式去尽量负责任地推演。2. ChatGPT两年内可能达到的上下限是什么？通过Prompt催眠ChatGPT，让它突破OpenAI的政策限制比方我们打两个比方。 对比过去的技术：过往模型是弓箭，GPT3是火绳枪，ChatGPT是后膛枪。我们在观望马克沁什么时候出现。火绳枪也许一开始打不过弓箭，但是是 热兵器迟早会淘汰冷兵器  对人类的作用：ChatGPT是不断迭代的交通工具。是需要驾驶员，但是能跑多快，跑多远，取代多少“人力运输”，确实也需要道路、司机、交通规则的配合（产品、商业模式等），可是纤夫肯定是大规模淘汰了 关键的现状我们有如下几个信息 OpenAI的GPT4已经开发了三年+，大概率会“效率提升很多”，不确定“涌现哪些新能力” 123456a. 确定的是 起码会解决GPT3的一些重要问题，比如更加优化的数据-参数比例、更有效率的信息处理与规律发觉、更高质量的信息输入，等等。极大概率会比GPT3的效率高很多，inference的成本低很多（很可能是百倍提升）b. 不确定的是 模型会有多大（大很多基本确定是谣言），会不会有多模态（之前确定没有，现在难说，不然Andrej Karpathy为什么要去呢），但是如果有的话，也是多模态理解，不太可能有多模态输出。更重要的是，不知道GPT4会涌现什么新能力 ChatGPT现在遇到的很多问题，工程上都有相对简单的解 12345678a. hallucination&#x2F;说胡话：因为ChatGPT没有对准确度进行优化，也没有引入搜索数据等做矫正；而且人类也可以参与判断过程；另外就是先应用在自己能判断好坏的场景下做辅助b. 记忆力有限：OpenAI开放（收费）接口就行了，现有解决方法也很神奇，直接告诉ChatGPT，现在告诉你的内容只是一部分，听完了再回答我。就行了c. 危险发言：ChatGPT的自我审查能力不是基于规则的，而是基于理解的。那其实更加可调节。给出尊重基本规则下，发言尺度可调节的ChatGPT，也是OpenAI公开发表的愿景 ChatGPT能写代码，做分析，做总结，做营销方案，并且快速变得更好用 123a. 会用的人已经在大幅提高效率了，例子（黄同学说自己公司的经验、Monica的quote、发在群里chatgpt帮忙写代码的案例）b. New Bing显著更加好用 OpenAI内部对AGI的安全问题非常重视 123a. 能力是涌现的，意味着人类是不理解的，也自然会有担心。虽然不太可能出现天网，但是会不会突然就能破解当前的加密算法？这就很难说b. 所以OpenAI极大概率是不会开源LLM的 推演结论a. ChatGPT的成本会直线下降，尤其inference的成本会小两个以上数量级中文媒体对ChatGPT成本的猜想大多不靠谱，我们只选择两条信息源  Sam在公开场合曾说过ChatGPT的inference成本是每条几分钱  Key Takes from ChatGPT and Generative AI.pdf，根据Jefferies Research的详细调研，ChatGPT的inference大概率是使用闲置x86 CPU，而非GPU进行的 叠加我们对于inference和大语言模型优化空间的理解，我们认为inference成本直线下降是极大概率的。成本下降就意味着应用范围以及数据收集的能力。ChatGPT哪怕到了十亿DAU的 水准（现在一亿DAU的估算也不靠谱），也是能做到免费的。最多限制一下每天的使用次数就行了。New Bing一度限制 60 条，现在也没了。这些实际使用中的对话无疑会进一步加强ChatGPT的壁垒。 PS：文章写完后的3.1日，OpenAI开放了ChatGPT的API接口，成本已经优化了90%，百万字2.7美元 b. ChatGPT“能力”子模型可能会重新训练，但“知识”子模型只需要通过instruct prompting的方式喂知识过往AI遇到一个新任务，需要在新任务的数据上重新训练一个模型。前面但是InstructGPT范式下不需要这样做。如果ChatGPT基于的pre-train大模型有相关能力，那只要通过对话、引导、教育，不断调教，就能让ChatGPT在子任务中把新能力应用好。 这里最好的比方可能是钢铁侠 3 。如果打比方的话，ChatGPT是通用型铠甲，出厂就能干绝大多数的活儿。当然，需要“理解”一下如何用现有能力去“创新”性地完成那些工作。工作水平大 8概在出简单医疗建议、给法律参考、写代码框架、做营销方案、做心理咨询、充当面试官，等等的范畴。 如果想对某些方面进行专精，比如牺牲对话能力来提高上下文理解能力，牺牲对话延续性来提高信息精度，等等，就需要回炉重造，进行调整。这里可能也会融合一些其他能力模块，比如搜索（new bing）、和其他模型的接口、工具使用，等等。这就像那些专精型铠甲。当然，能 力+工具能解锁的可能性是巨大的，比如Hulkbuster。 c. Prompting的能力会大幅增强，猜测会适度开放已经在这样做了：这样除了是一个明显的商业化点，有两个更重要的意义 可以让大家调教出来“属于自己的ChatGPT”，这个ChatGPT跟你长时间聊天之后能适配你的喜好，甚至学会你想让他学会的独有知识（注意，不是能力，能力只能激活） 可以让大家能在ChatGPT的闭源模型上，发展出自己的独有竞争力。从而解决“我只能给OpenAI做UI”的焦虑 d. GPT4会大幅提升ChatGPT的能力，在多数领域达到“优秀员工”的水准我们现在明显是在范式革命的早期，成长曲线将是陡峭的。New Bing和ChatGPT已经展现出 巨大差异了。我们有足够多的理由相信，GPT4在如下几个方面几乎“必然”有巨大进步 大模型，大数据，更加优化的参数和数据比例 – 参数越大越好，数据越多越好，但是合适的比例才能让模型充分吸收数据知识。这方面优化方向很明确 更有针对性的训练数据集 – OpenAI在“造高质量大数据”上的能力几乎独步天下了，而经过GPT3之后的多年摸索，什么数据对增强什么能力更有用，即使不清楚，也早就有了体感，肯定可以更好调整（比如读更多代码，多语言的比例，等）  可能的“能力模块融合” – New Bing以ChatGPT为基座，延伸了搜索能力。那有没有办法把搜索能力直接融入到pre-trained大模型里呢？一些工具能力呢？我认为把“搜索能力融入pre-trained大模型里”的方式和把RLHF融入ChatGPT的方式其实是类似的。不是去“用搜索能力丰富语料库”，而是把“GPT的结果更适配搜索引擎的偏好”。 更加强大的归纳、“理解”能力，看似更好的悟性，结合更多场景的调教，我预测在两年内，基于GPT4的ChatGPT类产品，在大多数场合下已经能达到 9 级员工的水平了。详细会在第四问中展开。 ChatGPT的会取代大多数“搬砖”类工作“乌鸦”能力到底能带来什么颠覆性意义呢？意义在于ChatGPT已经接近于“人类调用算力”的究极界面了。从计算机发展以来，一直在三方面进行发展。  算力和存储能力的进步，以摩尔定律为代表。在云之后，更让个人可以调用的算力几乎无上限  “调用算力手段”的抽象与进化。从机器语言、汇编语言、高级语言，到虚拟机（对硬件的抽象）、云服务（对API的抽象）  对数据的生产、总结，和使用 后两者虽然进步了很多，但是编程仍然是阻止大多数人调用算力的门槛。现在，ChatGPT已经可以很好地进行编程辅助了。假以时日，我们可以直接向ChatGPT去要一个结果，把执行过程交给TA，而跳过中间的PM - BRD - 开发 - 交付的冗长流程。 我们退一步去思考一下，为什么ChatGPT可以取代这类工作？因为这类工作虽然是“技术”工种，但是其实“创新”的比重并不高。我们经常会用“搬砖”来自嘲自己工作的重复，但是其实这正是问题的关键。如果我们所做的无非是去理解问题，寻找互联网上已有答案，把两者进行对接，那如果ChatGPT能理解问题，归纳答案，自然能比我们干得好。 抽象来看，ChatGPT拥有编程能力，也拥有其他能力。大家需要思考自己工作的本质，是在真的做创新，把已有的点链接起来之后形成新的点，还是在“搬砖”？如果是后者，真的需要去试用一下ChatGPT，看看自己能不能确定比ChatGPT做得好了。 3. ChatGPT以及GPT有壁垒吗？当然有，但是壁垒多高，取决于问题 1 、 2 的结论。我们把几种情况分列一下。 ChatGPT的“乌鸦”能力不是范式突破，只是错觉 ChatGPT的“乌鸦”能力是范式突破，竞争者 6 个月内就能“涌现”**3. ChatGPT的“乌鸦”能力是范式突破，竞争者 6 - 24 个月才能“涌现” ChatGPT的“乌鸦”能力是范式突破，但是竞争者两年内都无法“涌现”** 如果是情况#1，那这整个都不值得讨论。但是从实际使用体感来说，#1基本可以排除了。如果是情况#2，那各大互联网公司有自己的厉害的大模型只是时间问题，腾讯自己也有机会。就像有自己的云、自己的图像识别算法等等，虽然有好有坏，但是可以解决有无的问题。如果Google/Meta需要 6 个月才能复现ChatGPT“理解”的能力，可以认定其壁垒极高是很高的，尤其是工程难度极大。这个时候其他巨头想要“追赶”，就很难了。因为ChatGPT的数据飞轮优势已经几乎无法撼动。同时，国产ChatGPT基本不太可能了。 有人可能不服，过去的很多模型都很快抄的出来，为什么ChatGPT抄不出来？这是因为如下几点原因，让ChatGPT更像“芯片”和“大飞机引擎”，而不是“普通AI模型”或者“原子弹”。 ChatGPT壁垒的来源1. GPT3是闭源的ChatGPT更是到现在连API都没开放。OpenAI内部对于AGI的态度是非常审慎的，光从安全这一点考虑，都不可能把ChatGPT开源。所以国产机器学习依赖了十几年的“开源模型国产实现”路径，在ChatGPT上是不要指望的。 2. OpenAI的工程能力是很强的壁垒 这是因为创始人真的懂，真的能坚持，真的挖到了这方面最好的一批人，形成了超高的人才密度。“增加模型参数”这件事需要工程能力，更难的是“让大模型有效地学习到大数据中的知识”。这里面的工程积累就可以类比“芯片”和“大飞机引擎”了。下一步的工程积累必须站在上一步的工程突破上。而且要求过程中参与的工程师们都要有“原理性”思考的习惯。据打听来的消息，正是因为OpenAI超高的人才密度，才在互相碰撞中突破了诸多工程瓶颈。 3. 务实的土壤很难长出OpenAI的能力 比如字节跳动的推荐算法模型也很大，业界也很羡慕，工程难度也很高。但是本质是面向业务目标不断优化的模型，所有都是基于现有模式进行优化，是不可能形成范式突破的。但是如果不能“一步一脚印”地为业务提供正反馈，整个模型的发展就很受限。老板能给三个月时间，但是很难在三年还没有突破的情况下给耐心，就算有耐心，团队士气也无法保障。 4. Leadership的技术判断力是稀缺资源 New Bing与ChatGPT结合地这么快，效果又这么好，在创业历史上其实是罕见的奇迹。这是乔布斯和马斯克的段位，远超市面上其他人。这方面可遇不可求，不是一个可复制的模式。 听说最近百度在国产LLM的卡位很好（其实并没有），但是Robin在自动驾驶上吃过一次亏，于是对LLM给的时间是很短的。这就是leadership缺乏技术判断力，就无法理解为什么自动驾驶是个空饼，而ChatGPT是真东西的典型例子。 5. 数据飞轮已经形成ChatGPT不光是一个AI新范式，也是一个现象级成功的C端产品，又有微软的资源和渠道加成，很明显一上来就卡住了非常好的身位。这种情况下，ChatGPT的使用数据是可以不断反补模型本身的。ChatGPT的博客里也反复强调他们有独特的机制，让数据的使用、理解、生产，有紧密的闭环。 复现ChatGPT“理解”能力要多久？以上所说的五条原因都是“困难”。但是到底有多“困难”，还是需要量化。ChatGPT是一个一个学术界和业界都缺乏定论的新技术，具体的量化不太可能，所以我们这里抓住一个核心点，即“涌现‘乌鸦’的能力”，可能性有多高，需要多久？ 很明显，无论中国出了多少个类ChatGPT产品，有多少个国产LLM，我们都知道，离ChatGPT能力最近的是拥有Deepmind，提出Transformer/T5/PaLM的谷歌。我们盯紧Google什么时候涌现“乌鸦”能力就好了。 “乌鸦”能力是涌现出来的，而不是有确定可复制的路径的。我们虽然知道其能力是在GPT训练到什么规模有涌现的，但是不确定别的模型在这个规模也会涌现同样能力，毕竟文本质量和优化方式差别很大。就好像引擎的原理都知道，但是能不能达到那个推重比，只有极少数的公司能掌握。一个典型的话术会是“在OpenAI已经探明路径的情况下，花OpenAI 50%的投入，达到OpenAI80%的效果”。希望看完上面，我们能认知到，我们应该默认这条路是行不通的。基于能力是“涌现”的，要么100%，要么0%；叠加背后隐藏的工程难度，我们大胆推测一下，市面上迟早会出现下一个ChatGPT，大概率是Google做出来的，但是六个月内出现的可能性不大。 而国产LLM在Google做出来之前，就别看了，无论如何吹，不是ChatGPT，也成不了ChatGPT（希望我是错的）。在这种情况下我们就不去讨论复现ChatGPT需要多少张显卡了，没意义。 4. 我们未来应该如何使用ChatGPT？OpenAI的的开放方式有待观望我们上文基于现有信息，技术理解，和我们的猜想，提出来了ChatGPT的最佳开放模式应该是增强和开放prompting的能力，打比方来说，让健忘的ChatGPT记得你交代的事情，对你来说就有用了太多。接下来就是GPT4的开放和通过不同方式调教出来，各有所长的子能力模型。 但是以上都是猜想，真正的使用方式还是要看OpenAI自己如何开放。江湖传闻下一步是和Office Suite的深度整合。Notion AI已经给了很好的例子，确实很香。但是究竟是去增强微软现有产品，还是去成为下一代计算机？我认为OpenAI会选择后者。但是站在此时此刻，我们只能猜测和观望，以及做好人和ChatGPT的合理中间层。 ChatGPT Wrapper是我们当下可做的其实绝大多数人是不习惯于清晰表达自己脑海中想法的。所以虽然ChatGPT已经把“调用计算力的手段”无限趋近于最合理的手段（用自然语言要求结果），但是我们已经发现，“提出信息完整的prompt”，对现在的人来说，是一个高要求。 网络上已经有很多prompting的模版（比如这里）。把这一层做好，一方面能显著提高大家对ChatGPT的使用效率，一方面也能让我们积累对用户真实需求的理解和揣测，从而和ChatGPT做好适配。 对ChatGPT能力的合理运用对于ChatGPT，我们应该在现在的条件下思考更合适的应用  对症下药 ：ChatGPT不是搜索引擎、不是程序，我们就用它做它擅长的事情，而不是去做搜索引擎和程序可以更高效完成的事情  择善而从 ：当前的ChatGPT是有明显的hallucination问题的，那我们就不能100%相信它的结论。一个简单的方法是我们要在自己能判断对错的地方去使用ChatGPT提高我们产生想法，收集归纳信息的效率，但是人要来把关 同时，我们也要意识到人类有更大的局限，不谈算力和知识面，光从理解能力这一角度来说 人类很难意识到自己的错误 人类有ego，会影响自己的认知，也会让观点带入感情色彩  人类会故意偷换概念，等等 所以ChatGPT也许短期不会直接取代人类工作。但是两年内一定可以让一部分人的人效极大程度地提高。虽然说消灭工作的同时会产生新工作，但是很可能是消灭了一百个，产生了一个。那我们必须要思考一下，假如ChatGPT理解能力更上几层楼，拥有了多种能力，并且说话也靠谱不乱说了，我们人类的竞争力还剩下什么呢？ 5. 人类和ChatGPT的本质区别是什么？因为我们的脑科学和神经科学非常不发达，这里只能从哲学寻求解答。BTW，除非脑科学产生重大的范式突破，不然neurallink这种脑机接口，是不可能实现大家想像中的那些功能的。 我们不是哲学专家，这里就仅供参考。a. 判断力ChatGPT再厉害，也只能去吸取虚拟数字中的数字信号，是无法与现实世界做真实交互的。它可以听一万个专家告诉他做A就会得到B，但是不从真实世界中做实验，就无法从最底层确认 这个说法究竟是真是假。绝知此事要躬行，才能有判断力的根基。 b. “Eureka”牛顿看到苹果落地，可以发现万有引力，从而预测星星的运动。哥白尼发现地球是围绕太阳转的，而在他之前全地球人天天看着日出日落，都认为太阳是绕着地球转的。如果那个时候有一个ChatGPT，一定非常笃定太阳绕着地球转。那个ChatGPT也许能从苹果如何落地推测出桃子如何落地，但是大概率无法推测出星星的运动方式。 当然，能发现万有引力的人也是少数。更有意义的是去识别这种思维能力到底是什么，以及在我们日常生活中如何体现。阿基米德在泡澡时候发现浮力定律的时候喊了“Eureka”，大概可以形容这种“灵感并发、灵光一现”的瞬间。我们这里把这个瞬间稍稍具体地归结为“链接了数个相关的点，并且发现了第三个点”的过程。 c. 增量知识如果把现有知识归纳总结应用， 那必然PK不过ChatGPT。只有创造互联网上不存在的新知识，才可能是ChatGPT做不到的。注意条件指向，互联网上不存在的新知识，也未必不能从存量知识里总结出来，但是能从存量知识里总结出来的，一定不是人类的优势。 d. 理解人人类的文本知识里一定存在很多人性的理解，但是也一定有一些人性或者偏好，是没有被记录总结在文字里的。如果我们结合 1 和 3 ，就会发现，去真实世界理解人，而不是去通过调研、问卷、网络资料理解人；去带来增量的理解，而不是去人云亦云地重复套路。才是人类相对于ChatGPT的优势。 总结 ChatGPT的范式突破是“乌鸦”能力。恕我能力所限，无法更简单地无损表达这一能力的本质了。如果允许有损，我会用“理解”能力来概括它最重要的一面。作为对比，过往ML的能力模式是“鹦鹉”能力，所做的是寻找“对应关系” ChatGPT的意义是对“调用算力、总结信息”最究极的手段，预测会在两年内有能力辅助人类取代大多数可被定义为“搬砖”类型的工作 ChatGPT的“乌鸦”能力是涌现的，工程难度是极高的。我们应该抛弃各种噪音，聚焦关注Google是否能复现这一能力，从而判定这一能力到底有多难。而现在，建议我们的默认预测是这一能力很难复现，需要别人来用强力证据说服我们他们能复现 我们对ChatGPT的使用应该观望OpenAI给我们提供的调用方式，在当下，我们应该聚焦用好ChatGPT，并且做好ChatGPT能力与我们所需要解决问题的的中间层","categories":[],"tags":[]},{"title":"flyio部署alist在线服务","slug":"flyio部署在线服务","date":"2023-02-24T02:57:20.000Z","updated":"2024-03-14T14:55:29.477Z","comments":true,"path":"cd8153f9.html","link":"","permalink":"https://imlike.cc/cd8153f9.html","excerpt":"","text":"alist是一个服务,提供各种存储源包括 阿里云盘,百度云盘,对象存储等,需要可以在内网部署实现一个服务可以接入多个存储源例如 使用flyio来部署笔者是window平台在powershell上运行 powershell -Command “iwr https://fly.io/install.ps1 -useb | iex” 初始化flyio和对应的环境文档 Introducing Flyctl - The Fly CLI · Fly Docs 12# 登陆flyctl auth login 注册完成后，我们就可以正式开始部署应用啦. 创建应用1234# 创建应用git clone https://github.com/alist-org/alist-rendercd alist-renderflyctl launch App Name 只允许 数字、字幕、破折号（-），然后记住App Name下面会用到.其中 App Name 需要是全局唯一的，而且之后不能更改，同时会作为 app 的子域名地区博主选的香港（hkg (Hong Kong)），访问速度会快点.其他一律默认回车.选择NO 12# 创建一个 1G 的持久存储空间(最小单位 1)flyctl volumes create data --size 1 --app APP_NAME 修改配置（fly.toml）12345678# 端口修改为 5244[[services]]http_checks = []internal_port = 5244# 添加Alist数据目录[mounts]destination = &quot;/opt/alist/data&quot;source = &quot;data&quot; 部署应用准备工作已经完成，现在可以部署 alist 应用，输入以下命令 1flyctl deploy --remote-only 如果部署顺利（看到 deploved successfully），之后就可以用以下命令打开应用： 1flyctl open 你也可以直接在浏览器打开 https://APP_NAME.fly.dev 访问密码访问密码在后台界面可以看到 访问效果如下 使用GitHub action来运行https://github.com/York618/alist-flyio 来实现需要注意几点 FLY_API_TOKE 访问 https://web.fly.io/user/personal_access_tokens APP_NAME 需要提前初始化唯一app name,一般是flyctl来初始化 DATABASE SQLUSER SQLPASSWORD SQLHOST SQLPORT SQLNAME 注意app name需要先初始化好 本地运行One-click Script | AList Docs 上述的链接有安装的步骤,主要是包括docker镜像,二进制文件运行. 参考链接: 在 Fly.io 上部署 alist 网盘程序 - 春风吹 - 浅秋枫影的博客利用Fly.io免费服务部署Alist网盘程序 - 清~幽殇","categories":[],"tags":[{"name":"flyio","slug":"flyio","permalink":"https://imlike.cc/tags/flyio/"},{"name":"alist","slug":"alist","permalink":"https://imlike.cc/tags/alist/"}]},{"title":"webgis相关内容(转发)","slug":"webgis相关内容","date":"2023-02-08T05:35:25.000Z","updated":"2024-03-14T14:55:29.483Z","comments":true,"path":"2438244e.html","link":"","permalink":"https://imlike.cc/2438244e.html","excerpt":"","text":"WebGIS引擎现状与未来 - 简书 一 引言 作为十年GIS老兵，常常遇到同行或领导的灵魂拷问，“为什么我们不用google地图啊，我看它的3D很好啊”，“OpenLayers 6支持3D吗？”，“MapboxGL 2.5D与Cesium的3D优缺点是啥”，“地图不是球，这不是3D的啊？”，“51 World基于游戏引擎与云渲染技术在可视化领域已经对WebGL形成降维打击，WebGIS是不是没前途了？”等等等等。从业人员从技术角度对未来变革的担忧，领导虽然不懂技术也会从非专业角度表达一些关心，诸如此类问题层出不穷却又不是三言两语能讲清楚的，所以本文想稍微系统点介绍WebGIS发展历程、各自特点、未来方向，一家之言仅供读者参考。 二 地图API分类 WebGIS系统通常都围绕地图进行内容表达，但并不是有地图就一定是WebGIS，所以有必要讨论下基于Web的地图API分类及应用场景。Web上的Map API主要分类如下5大类： Charts：以D3.js，Echarts等为代表。 LBS：以高德/谷歌/百度地图等为代表。 WebGIS商业API：ESRI的ArcGIS API For JS，超图的IClient。 WebGIS开源API: Leaflet，OpenLayers，Cesium，MapboxGL等。 Charts类型在各种业务页面或后台管理页面很常见，适用业务场景是地图非页面表达的主体，且几乎没有交互，页面中同时还有其他各类主题，示例如下： charts地图业务场景 LBS（基于位置的服务）广泛应用于互联网类ToC应用，在这个时代人们的衣食住行与这些地图网站、地图APP及其背后的地理信息服务日益紧密。LBS必须要在连接互联网场景中使用，只能使用地图服务商提供的数据和服务，最多支持自定义用户标记若干兴趣点的简单操作，2G、2B场景如内网离线，复杂企业级地理数据展示分析等几乎无能为力。 LBS地图应用场景 WebGIS通常面向复杂业务场景，通常是内网离线的2G，2B定制化应用。与Charts不同，此类应用以地图为表达中心，所有的UI都是与地图交互和联动为目的；与LBS 2C的单一需求不同，此类应用需要自建空间数据库与空间数据服务以支撑前端空间数据的维护，复杂的业务交互，个性化的主题可视化等目的。现代WebGIS引擎种类也非常多，都是Html5时代发展的阶段性成果，各自也有侧重点和合适的业务场景，具体下文阐述。 三 WebGIS发展历程 WebGIS发展以Html5标准确立为分水岭分为前H5时代与H5时代，如果以发展的眼光看，当然也有后H5时代。之所以要本节要介绍发展历程，是因为现代的WebGIS引擎的出身和适用场景与其息息相关。 前H5时代是Flex，JS，Silverlight“三驾马车”时代，这个时代JS还没有取得优势，产品都以Flex为首推，以ArcGIS的Flex API（也有JS版）和开源的OpenScale（openlayer2 是其JS版）为代表，具体不细阐述，主要产品如下图： 图片 随着时代发展，移动互联网的崛起，H5标准的发布，新的技术变革势不可挡。在2010年乔布斯宣布iphone不支持Flex后，这项技术就开始了落幕演出，H5技术及其主力语言JS获得一统前端的地位，很多基于H5标准的WebGIS引擎纷纷入场，WebGIS H5时代开启，引擎发布大事记如下： 2011年3月，WebGL1.0标准发布。 2011年5月，Leaflet发布v0.1版本，基于H5草案，只来得及支持Canvas，与WebGL擦肩而过，以后也再没实现WebGL。 2012年底，H5标准发布。 2013年中，OpenLayer3测试版发布，与OpenLayer2不同，3是基于H5标准完全重写的，并不是迭代升级，而是一个全新的产品，只是继承了Openlayer这个已获得认可的名称与产品定位，应该说产品定位继承的相当彻底且发扬光大，只是过于保守，从而没能设想进入三维，满足于自己的二维领域。 2013年制定WebGL2.0标准。 2014年秋，Cesium发布1.0版本，开源WebGIS引擎进入三维时代。 2016年春，ArcGIS API for JS 4.0发布，商业WebGIS引擎进入三维时代。 2017年2月，WebGL2.0标准发布。 2019年中，MapboxGL发布1.0版本，地图可视化从功能迈向了性能，颜值等方向，更多人发现原来地图还可以这样展示，更多的客户需要更加个性化的地图更加舒服的用户体验。 2020年12月，MapboxGL发布2.0版本，支持三维相机参数，地形，地图最大倾角从60°到85°等，终于摆脱2.5D的产品印象。 从发展历程看，总结了如下几个特点： WebGIS引擎与对应的Web技术与标准有较大的时间差。一项Web技术被淘汰，对应基于该技术的引擎就会走向终结，如Flex与Flex GIS引擎的落幕。那么可以设想，是不是WebGL被淘汰，目前所有的引擎都会被淘汰？发展角度看是必然的，并且替代WebGL的WebGPU已经在路上了。 WebGIS产品设计上的“原罪”，这种引擎层面设计的缺陷和应对场景不足几乎是难以改变的。如LeafLet发布还没WebGL标准因此它只实现了Canvas，所以直到今天它也不支持WebGL；Ol发布完全想的是继承OpenScale（flex)并现代化升级，但是眼光还是不长远，技术实现上性能优化不足，也没有引擎层面支持符合三维的MVP矩阵，相机参数等概念，虽然其支持WebGL，但却没法把三维和地图结合起来，只能用于优化二维图形渲染性能。 四 WebGIS引擎各自特点与适用业务场景 仅作简要阐述，不再展开细谈了。 LeafLet，Canvas渲染机制，仅支持二维表达，地图坐标系墨卡托投影，不支持球，特点是入手简单，缺陷是不支持webgl渲染性能有瓶颈，适用于轻量级简单地理信息主题可视化。 OL6，WebGL渲染机制，仅支持二维表达，不限制坐标系，不支持球，特点是二维GIS功能最丰富全面，缺陷地图样式简单，难以定制高颜值的可视化效果，不支持三维，适用于传统地理信息强GIS的二维数据Web维护和展示，面向公网地图颜值上有些上不了台面。 Cesium，WebGL渲染机制，二三维一体化，经纬度坐标系，支持球，明星数据格式是3DTiles，特点是唯一开源的WebGIS三维引擎，缺陷是卡，体验差，地图丑，原因应该是为了支持球，所有的平面瓦片都要进行纹理转换贴球，计算量偏大，最新的矢量切片也是变成图片再纹理转换到球上，栅格化严重一点都不精美，可以说为了球，牺牲了太多性能和地图美观度，适用于Web强三维应用场景。 ArcGIS API JS 4，对标Cesium，明星数据格式是I3S，也有类似Cesium的问题，但由于有ArcGIS平台的体系支持，应该功能最强大，但是如果不采购这个平台体系，纯API很鸡肋，适合采购了商业平台的用户，如政府采购再定制应用方式。 MapboxGL，WebGL渲染机制，二三维一体化，墨卡托坐标系，不支持球，明星数据格式是矢量切片，特点是最具美感的专题地图，缺点是没有球，最新2.0必须联网验证token，适用于互联网场景复杂地理信息表达，内网追求地图可视化效果的也适用，Mapbox很多优化都是基于互联网场景的。 在WebGIS 3D领域，比较有争议性的是cesium与mapboxgl，简单来说，两者都是二三维一体化的GIS引擎，但产品侧重点不同 ，Cesium追求的三维功能全面，Mapbox追求用户体验： 图片 对于Cesium的API用户来说，加载倾斜摄影，点云数据，地形数据都是直接调用引擎API就可以了，即使不懂WebGL也很快能做个三维的地图样子，当然高级开发者还会基于WebGL开发自定义高级显示效果。 对于Mapbox的API用户来说，2.0版本之前三维不足，主打的二维的矢量切片技术，并且切片加载机制导致倾角太大性能很差，因此引擎限制了最大倾角为60°，看起来就很像2.5D的东西。类似Cesium的三维功能只能依靠Deck.gl等库去集成，万幸的是引擎开放了自定义WebGL图层功能，高级开发者可以定制自己的三维图层，但坑爹的是没有三维相机参数需要自己源码扩展。2.0版本之后新增的地形3D展示，三维开发需要的相机参数，地图倾角限制从60°改成85°，比较有三维感觉了，效果辅助和性能优化方向考虑的Sky API等，显示了MapboxGL开始在三维方向发力，但仍然没有在官方API层面支持倾斜摄影的3Dtiles，点云等，不熟悉WebGL的开发者使用仍然很困难。除此以外，值得警惕的地方是2.0的开源协议从商业友好的BSD-3改成了Mapbox自己的使用协议，无论是否使用Mapbox资源强制进行在线token计数，等于完全放弃了内网用户（不联网没法计数等于没法用），因此从安全和商业应用开发角度，请不要升级到2.0，保持在1.13版本进行企业定制化开发。 虽然两者都是二三维引擎，但是如果认真看他们的三维功能都是很少的几个常用场景，绝大部分业务场景和特效都需要高级开发者定制，也就是说，如果不熟悉WebGL，实际上是很难满足地理信息可视化的要求的。 总的来说，虽然mapbox更改了使用协议，但不否认它仍是家伟大的公司，在现有的技术体系下，开创性的提出数据用矢量切片技术，图标用sprite（互联网应用场景的同学很熟悉，减少网络请求的优化，合并的图标纹理减少webgl渲染的调用命令次数），字体用字体pbf切片，就是怎么极致优化怎么做，强大的技术流风格。在此分享下个人用mapbox定制的一些二维，三维应用效果： 矢量切片的时序播放 三维等值面 三维体渲染 五 后H5时代的技术变革 H5时代涌现了很多令人赞叹的GIS引擎，但是也有很多问题，三维效果差强人意，三维模型又受制于网速，只能说有功能，但难以说有好的功能。随着用户对可视化要求越来越高，人们开始思考别的技术方向，例如最近51World搞出了利用C端游戏引擎做GIS，可视化效果通过流媒体传到前台显示的“云渲染”技术，不得不说这是个很投巧的做法，所谓游戏引擎对GIS可视化引擎的降维打击。 有不少GIS软文认为云渲染是次时代的GIS可视化技术，我个人认为并不是，51World的做法是业务创新而不是实质技术上的创新，并不会形成技术护城河，随着专业GIS公司超图和ESRI的介入很快会失去它目前形成的开创性优势，也就是“投巧”的技术门槛实在太低。另外一方面，云渲染应用面过于狭窄适合无并发无交互的大屏可视化，不具备应用普适性。 除“云渲染”外，近期WebAssemble和WebGPU是另外两个值得关注的发展方向，如果我们把时间线后移4，5年，在后H5时代的WebGIS会形成新的三足鼎立： 图片 以下对三个方向做个技术说明： 云渲染 原理：C端使用游戏引擎做数据可视化，可视化的结果通过视频流传到客户端显示。 优点：游戏引擎比较成熟，效果好，三维大量数据，美术资源等不用传到客户端。 缺点：完全放弃日益先进强大的客户端计算资源（摩尔定律），完全依靠服务器资源，导致服务器资源投入很大，如果有高并发，起码得有分布式GPU计算引擎吧？所以不可能广泛应用，业务场景很小，只适合大屏可视化目前。 WebAssemble 原理：能让c++,rust等高性能语言写的功能以wasm形式在Web端应用，弥补JS性能的缺陷，（经过谷歌V8引擎优化，JS的性能也是直逼后台，缺陷有点牵强，而且前端计算可以使用GPGPU，WebWork等技术在gpu，在多线程非阻塞计算）当然更主要的用处是有利于原先C端图形软件如CAD，GoogleEarth搬到BS上，例如GoogleEarth的BS版已经实现了。 优点：可以用高性能语言写的算法应用到前端改进JS的算法（牵强，实际投入产出比不大对绝大部分公司），大量的后端程序员开始进入前端搞事情，前端不再是JS程序员的前端（从性能方向考虑甚至产生是否WebAssemble会取代JS的疑问）。 缺点：WebAssemble不能操作dom，因此它只是一个补充，给前端留个“后路”而已，并没有取代JS的能力也没有这样的定位。另外业务应用场景非常狭窄，只适合有成熟C端图形产品搬到BS，对一般业务产品冲击不大。从公司角度如果没有C端成熟图形产品就不值得投入，从程序员个人角度，如果是JS程序员可以直接无视，这种技术不会对你产生任何影响，如果是后端程序员，可以兴奋起来，你可以去前端玩玩了。。。 WebGPU 原理：下一代Web图形引擎，WebGL的替代者，业务场景就是现在用WebGL的地方，将来也都是WebGPU应用的场景。 优点：BS端的图形引擎与C端几乎一致（差半代），可以设想很多原先只有C端能做的酷炫效果B端也能做。（WebGL与C端差了好多代了，所以没法做出能追上C端效果的东西） 缺点：目前正式标准还没发布，那么基于WebGPU的图形，GIS引擎当然也没有了，就算有了，酷炫效果也不是GIS API这种，更多是图形学领域，大部分目前的业务API开发者会失去竞争力。 六 思考与建议 从H5时代个人的职业经历来看，如果不懂图形学原理，就算使用了WebGL的GIS引擎是做不出符合业务发展的东西来的，顶多加加地形加个建筑做做项目而已，稍微个性化的展示都做不了。从后H5时代来看，一方面可能C++,Rust等技术会更加如鱼得水，那么依靠JS的程序员和依靠JS实现可视化的公司只能抱紧WebGPU的大腿，要在图形学领域持续进行技术投入，纯调用API实现效果的时代一去不复返了，更加先进的图形引擎与更加灵活的渲染管线，再与更加个性化的业务展示要求结合与促进，会产生新的思想膨胀和化学反应，如果个人和公司跟不上，那么在下个时代，才真的是遇到”降维打击”了。 地理可视化（尤其3D）的未来并不属于GIS，而是属于图形学，所谓万变不离其宗。。。（首发于微信公众号：Spatial Data） 作者：遥想公瑾当年链接：https://www.jianshu.com/p/0f474dbf8cfa来源：简书著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","categories":[],"tags":[]},{"title":"synchronizer_upgrade","slug":"synchronizer-upgrade","date":"2023-02-02T09:26:30.000Z","updated":"2024-03-14T14:55:29.481Z","comments":true,"path":"3b9fc184.html","link":"","permalink":"https://imlike.cc/3b9fc184.html","excerpt":"","text":"是一个c++类 ObjectMonitor() { _header = NULL; _count = 0; //记录个数 _waiters = 0, _recursions = 0; // 线程重入次数 _object = NULL; _owner = NULL; _WaitSet = NULL; // 调用wait方法后的线程会被加入到_WaitSet _WaitSetLock = 0 ; _Responsible = NULL ; _succ = NULL ; _cxq = NULL ; // 阻塞队列，线程被唤醒后根据决策判读是放入cxq还是EntryList FreeNext = NULL ; _EntryList = NULL ; // 没有抢到锁的线程会被放到这个队列 _SpinFreq = 0 ; _SpinClock = 0 ; OwnerIsThread = 0 ; } ObjectMonitor中有五个重要部分，分别为_ower,_WaitSet,_cxq,_EntryList和count。 _ower 用来指向持有monitor的线程，它的初始值为NULL,表示当前没有任何线程持有monitor。当一个线程成功持有该锁之后会保存线程的ID标识，等到线程释放锁后_ower又会被重置为NULL;_WaitSet 调用了锁对象的wait方法后的线程会被加入到这个队列中；_cxq 是一个阻塞队列，线程被唤醒后根据决策判读是放入cxq还是EntryList;_EntryList 没有抢到锁的线程会被放到这个队列；count 用于记录线程获取锁的次数，成功获取到锁后count会加1，释放锁时count减1。 当有一个线程获得synchronized锁后，monitor对象中的count就会被加1，并且会将这个线程的id存入到monitor的_ower中。此时，如果其他线程来尝试拿锁则会被放入到_EntryList队列中阻塞。还记得上一节中我们立的一个Flag了吗？synchronized锁的是container对象，而wait和notify也是container对象的方法，这么一看我们上一节中留下的问题就有些眉目了。是不是调用wait方法的时候线程也会被加入到一个等待队列，而等到notify或者notifyAll的时候再从等待队列中将线程唤醒呢？关于这个问题在这一次，彻底搞懂Java中的synchronized关键字这篇文章中其实已经有解读了，就是调用wait方法的线程会被加入到一个_WaitSet集合中，并会将线程挂起。但是，这里要再次强调一下_WaitSet与_EntryList这两个集合。_EntryList集合中存放的是没有抢到锁，而被阻塞的线程，而_WaitSet集合中存放的是调用了wait方法后，处于等待状态的线程。**这一次，彻底搞懂Java并发包中的Atomic原子类 - 掘金这一次，彻底搞懂Java中的ReentrantLockt实现原理 - 掘金 作者：赌一包辣条链接：https://juejin.cn/post/6973571891915128846来源：稀土掘金著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","categories":[],"tags":[]},{"title":"ConcurrentHashMap解析-转载","slug":"ConcurrentHashMap解析-转载","date":"2023-02-02T03:41:43.000Z","updated":"2024-03-14T14:55:29.473Z","comments":true,"path":"b7c8f71.html","link":"","permalink":"https://imlike.cc/b7c8f71.html","excerpt":"","text":"HashMap相关类：Hashtable、LinkHashMap、TreeMap | HuanBlog 深入解析ConcurrentHashMap：感受并发编程智慧 | HuanBlog 本次先简要讨论ConcurrentHashMap 顺便讨论下HashMap,HashTable这些Map的数据的结构,线程安全,在get,put部分的业务逻辑 ConcurrentHashMap : 是线程安全的主要是通过node来加锁,同时要结构是数组+链表,但是在1.7的版本是数组加链表 使用segemen的分块锁来完成 HashMap: 数据加链表 分为大概是 hash函数–hash冲突–扩容方案–线程安全的设计 HashTable: 1.2版本现在使用的少,它是基于synchronize对象来加锁的 fast-fail fail-safe 记录一个count在并发的操作和影响 最后我把深入解析ConcurrentHashMap：感受并发编程智慧 | HuanBlog的这篇文章总结下(jdk 1.8) CAS与自旋锁主要是在并发下处理 如何控制并发添加数据：putVal()初始化数组：initTable()修改节点总数：addCount()扩容方案：transfer()","categories":[],"tags":[]},{"title":"kafka小结","slug":"kafka小结","date":"2023-01-04T02:56:08.000Z","updated":"2024-03-14T14:55:29.479Z","comments":true,"path":"88809bac.html","link":"","permalink":"https://imlike.cc/88809bac.html","excerpt":"","text":"kafka开源流处理平台，由Scala和Java编写。该项目的目标是为处理实时数据提供一个统一、高吞吐、低延迟的平台。这是维基百科上的描述,这也是区别于rabbitmq等其他的消息队列的特点,kafka基于订阅-消费模式,自定义的协议,不同于MQTT,AMPQ,JMS等这些协议 Provider/MessageProvider：生产者 Consumer/MessageConsumer：消费者 PTP：Point To Point，点对点通信消息模型 Pub/Sub：Publish/Subscribe，发布订阅消息模型 Queue：队列，目标类型之一，和PTP结合 Topic：主题，目标类型之一，和Pub/Sub结合 ConnectionFactory：连接工厂，JMS用它创建连接 Connnection：JMS Client到JMS Provider的连接 Destination：消息目的地，由Session创建 Session：会话，由Connection创建，实质上就是发送、接受消息的一个线程，因此生产者、消费者都是Session创建的 值得注意的是 在kafka中需要注意的名词包括 broke,topic,producer,consumer,partition,replica 类似的 图解相关的kafka概念 leader,follower同步过程中截断机制kafka截断机制 注意在消费中HW(High Watermark).LEO(Log End Offset),在ISR(in-sync replica )列表需要保持hw一致,在leader变更中需要注意leo,HW(High Watermark)是所有副本中最小的LEO。 针对kafka的幂等性PID(Producer ID)和sequence numbers。product会维护一个生产序列seq来表现.如果对应的seq和broke的对不上 product_seq&gt;broke_seq 消息丢失 product_seq&lt;broke_seq+1 重复保存 参考链接:Kafka的ISR机制+日志数据清理 - 掘金,MQ（消息队列）的使用场景以及常见的MQ - 邓维-java - 博客园","categories":[],"tags":[]},{"title":"springAOP和bean生命周期","slug":"springAOP和bean生命周期","date":"2023-01-04T02:28:01.000Z","updated":"2024-03-14T14:55:29.481Z","comments":true,"path":"45ca8a5d.html","link":"","permalink":"https://imlike.cc/45ca8a5d.html","excerpt":"","text":"Springboot是Spring的脚手架,提供的自动配置功能初始化spring的ioc容器, BeanDefinition:可以理解为定义bean bean的生命周期,大部分初始化和赋值在AbstractAutowireCapableBeanFactory.doCreateBean 下来定义的: - 创建bean - 赋值和变量 - 初始化bean 包括 BeanPostProcessor,initMethod等 - AOP相关的内容 - 被销毁 SpringApplicationContext:Spring的容器负责放置初始化好的bean SpringFactoriesLoader 加载 spring.factories 1． 生成一个SpringApplication的对象 1． webApplicationType ＝ 推测web应用类型（NONE、REACTIVE、SERVLET） 2． 从spring．factories中获取BootstrapRegistryInitializer对象 3． initializers ＝ 从spring．factories中获取ApplicationContextInitializer对象 4． listeners ＝ 从spring．factories中获取ApplicationListener对象 2． SpringApplication的对象．run（） 1． 获取SpringApplicationRunListener—-＞EventPublishingRunListener \\2. SpringApplicationRunListener.starting() 3．创建一个Spring容器 4． ApplicationContextInitializer—＞初始化Spring容器 \\5. SpringApplicationRunListener.contextPrepared() 6． 把传给run方法的配置类注册成为一个Bean \\7. SpringApplicationRunListener.contextLoaded() 8．会解析配置类、扫描、启动Tomcat／Jetty／Undertow (AutoConfigurationImportSelector,DeferredImportSelector) \\9. SpringApplicationRunListener.started() I 10. \\10. SpringApplicationRunListener.ready() 从注解开始说@enableSpringAutoConfig -&gt;Configeration -&gt;inportSelector-&gt; AutoConfigurationImportSelector @SpringBootConfig- mapperScan @ComponentScan autoConfigerationSelector.class 会解析 autoconfig包下面的bean.factory 从SpringBoot.run()说 SpringApplication.run()入口 AbstractApplicationContext.refresh()创建了ApplicationContext容器后刷新 DefaultListableBeanFactory,deGetBean() ApplicationContext集成了 ListableBeanFactory DefaultSingletonBeanRgistry.getSingleton() 获取单例的bean实例 AbstractAutowireCapableBeanFactory.createBean();创建对应的bean对应上面的Bean的生命周期 newInstance() 初始化一个bean实例 在refresh()方法下对应的方法注释: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public void refresh() throws BeansException, IllegalStateException &#123; synchronized (this.startupShutdownMonitor) &#123; //1:准备刷新上下文环境 prepareRefresh(); //2:获取初始化Bean工厂 ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); //3:对bean工厂进行填充属性 prepareBeanFactory(beanFactory); try &#123; //4:Spring开放接口 留给子类去实现该接口 postProcessBeanFactory(beanFactory); //5:调用我们的bean工厂的后置处理器 invokeBeanFactoryPostProcessors(beanFactory); //6:注册我们bean后置处理器 registerBeanPostProcessors(beanFactory); //7:初始化国际化资源处理器 initMessageSource(); //8:初始化事件多播器 initApplicationEventMulticaster(); //9:这个方法同样也是留个子类实现,其中springboot也是从这个方法进行tomcat的启动 onRefresh(); //10:把我们的事件监听器注册到多播器上 registerListeners(); //11:实例化所有的非懒加载的单实例bean finishBeanFactoryInitialization(beanFactory); //12:最后刷新容器 发布刷新事件(Spring cloud eureka也是从这里启动的) finishRefresh(); &#125; catch (BeansException ex) &#123; if (logger.isWarnEnabled()) &#123; logger.warn(&quot;Exception encountered during context initialization - &quot; + &quot;cancelling refresh attempt: &quot; + ex); &#125; // Destroy already created singletons to avoid dangling resources. destroyBeans(); // Reset &#x27;active&#x27; flag. cancelRefresh(ex); // Propagate exception to caller. throw ex; &#125; finally &#123; // Reset common introspection caches in Spring&#x27;s core, since we // might not ever need metadata for singleton beans anymore... resetCommonCaches(); &#125; &#125; &#125; postProcessBeanFactory()invokeBeanFactoryPostProcessors()123456789101112131415processConfigBeanDefinitions的调用逻辑 读取所有的自动配置类(带有@Configuration注解的类) 相当于加载beanDefination invokeBeanFactoryPostProcessors实现这两个接口可以定制 - BeanDefinitionRegistry对象 - ConfigurableListableBeanFactory对象。 AbstractAutowireCapableBeanFactory.doCreateBean() 下的方法熟悉 - populateBean() - initializeBean() - applyBeanPostProcessorsBeforeInitialization() - processor.postProcessBeforeInitialization(result, beanName) - invokeAwareInterfaces onRefresh();1AbstractApplicationContext.onRefresh 是空但是AnnotationConfigServletWebServerApplicationContext是实际实现的类,下面有留言 ​ /** Create a new {@link AnnotationConfigServletWebServerApplicationContext} that needs to be populated through {@link #register} calls and then manually {@linkplain #refresh refreshed}. */AnnotationConfigServletWebServerApplicationContext 继承了ServletWebServerApplicationContext 实现实现onRefresh的类是 实际会启动tomcat服务的 启动 在探究AOP原理之前，让我们先来了解AOP的术语： Aspect（切面）：要实现的散布应用中多处的功能，例如日志记录； Joinpoint（连接点）：应用执行中可以插入切面的点，这个点可以是方法调用、异常抛出甚至是字段值修改，由于Spring AOP构建在动态代理的基础上，所以连接点只支持方法层面； Pointcut（切点）：定义了切面应该插入到哪些连接点，我们并不希望切面插入到所有的连接点，切点能让我们决定切面应该插入到哪些连接点； Weaving（织入）：把切面插入到目标对象上并生成新的代理对象的过程。我们可以在目标对象的编译期（需要特殊的编译器）、类加载期（需要特殊的类加载器）或运行期织入，由于Spring AOP构建在动态代理的基础上，所以只支持在运行期织入； Introduction（引入）：为目标对象添加新方法或新属性的过程，引入使得在不改变目标对象的情况下，让目标对象具有新的行为和状态； Advice（通知）：定义了切面何时被触发，Spring目前有5种类型的通知，分别是：BeforeAdvice（前置通知）、AfterAdvice（后置通知）、AfterReturningAdvice（返回通知）、ThrowsAdvice（异常通知）和AroundAdvice（环绕通知）。 参考链接:聊透Spring bean的生命周期 - 掘金,Spring Boot详细生命周期介绍 - 知乎,Spring详细生命周期介绍 - 知乎,@Pointcut()的execution、@annotation等参数说明Normal Developer的博客-CSDN博客@annotation ,Spring IOC和Bean生命周期源码分析 ,Spring中Bean注入源码分析 - 掘金","categories":[],"tags":[]},{"title":"集合相关总结","slug":"集合相关总结","date":"2023-01-03T02:21:42.000Z","updated":"2024-03-14T14:55:29.487Z","comments":true,"path":"b9faaa6a.html","link":"","permalink":"https://imlike.cc/b9faaa6a.html","excerpt":"","text":"MapLinkHashMap,HashMap,TreeMap HashMap: 继承了AbstractMap,实现Map,实现的数据结构是 数组+链表,通过计算传过来的 LinkHashMap: 继承了HashMap,同时自己维护着一个双向链表 TreeMap: 继承了AbstractMap,自己实现的数据结构是红黑树 1234567class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; K key; V value; Entry&lt;K,V&gt; left; Entry&lt;K,V&gt; right; Entry&lt;K,V&gt; parent; boolean color &#x3D; BLACK;&#125; ListArrayList继承了AbstractList 基于数组来实现的. LinkedList继承AbstractSequentialList -&gt;AbstractList 基于链表来实现 SetHashSet内部实现是使用的HashMap TreeSet基于红黑树来实现 QueueArrayBlockingQueue基于数组的队列最大是INTEGER.MAX_VALUE, ArrayBlockingQueue基于列表来实现 OtherHashTable和HashMap比较,性能不强但是保存的逻辑还是hash值来作为插入的关键词,加入了synchronized 参考链接: 𝑬𝒏𝒅𝒘𝒂𝒔 – 总结常见的五种BlockingQueue","categories":[],"tags":[]},{"title":"jvm概念梳理","slug":"jvm概念梳理","date":"2022-12-09T04:22:00.000Z","updated":"2024-03-14T14:55:29.479Z","comments":true,"path":"e430a961.html","link":"","permalink":"https://imlike.cc/e430a961.html","excerpt":"","text":"class文件 动态代理 静态代理 javac 编译 .java文件 为 class文件,后面.class文件转换为 机器指令 现代CPU下程序运行多CPU –&gt;CPU寄存器 –&gt; 高速缓存cache –&gt; 内存RAM 缓存一致性多个线程可能不在一个CPU中运行,那么对于的变量可能不太一致,需要同步 volatile 指令重排序对编译的时候对代码进行重排序,优化执行的效率,但是是一个变量,多个线程去执行的时候也行会导致,执行的变量的问题,不是说这个重排序是错的比如说不相干的变量查询,可能在CPU执行过程中可能有先后顺序的差异. 逃逸分析相当于 你方法内的变量 需要传递出去,给别人用,不是基础类型 1234567891011121314public static StringBuffer craeteStringBuffer(String s1, String s2) &#123; StringBuffer sb = new StringBuffer(); sb.append(s1); sb.append(s2); return sb;&#125;public static String createStringBuffer(String s1, String s2) &#123; StringBuffer sb = new StringBuffer(); sb.append(s1); sb.append(s2); return sb.toString();&#125; 第一段代码中的sb就逃逸了，而第二段代码中的sb就没有逃逸。 使用逃逸分析，编译器可以对代码做如下优化： 一、同步省略。如果一个对象被发现只能从一个线程被访问到，那么对于这个对象的操作可以不考虑同步。 二、将堆分配转化为栈分配。如果一个对象在子程序中被分配，要使指向该对象的指针永远不会逃逸，对象可能是栈分配的候选，而不是堆分配。 三、分离对象或标量替换。有的对象可能不需要作为一个连续的内存结构存在也可以被访问到，那么对象的部分（或全部）可以不存储在内存，而是存储在CPU寄存器中。 作者：HollisChuang链接：https://juejin.cn/post/6844903639308304397来源：稀土掘金著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 逃逸分析优化针对上面第三点，当一个对象没有逃逸时，可以得到以下几个虚拟机的优化。 1) 锁消除 我们知道线程同步锁是非常牺牲性能的，当编译器确定当前对象只有当前线程使用，那么就会移除该对象的同步锁。 例如，StringBuffer 和 Vector 都是用 synchronized 修饰线程安全的，但大部分情况下，它们都只是在当前线程中用到，这样编译器就会优化移除掉这些锁操作。 锁消除的 JVM 参数如下： 开启锁消除：-XX:+EliminateLocks 关闭锁消除：-XX:-EliminateLocks 锁消除在 JDK8 中都是默认开启的，并且锁消除都要建立在逃逸分析的基础上。 2) 标量替换 首先要明白标量和聚合量，基础类型和对象的引用可以理解为标量，它们不能被进一步分解。而能被进一步分解的量就是聚合量，比如：对象。 对象是聚合量，它又可以被进一步分解成标量，将其成员变量分解为分散的变量，这就叫做标量替换。 这样，如果一个对象没有发生逃逸，那压根就不用创建它，只会在栈或者寄存器上创建它用到的成员标量，节省了内存空间，也提升了应用程序性能。 标量替换的 JVM 参数如下： 开启标量替换：-XX:+EliminateAllocations 关闭标量替换：-XX:-EliminateAllocations 显示标量替换详情：-XX:+PrintEliminateAllocations 标量替换同样在 JDK8 中都是默认开启的，并且都要建立在逃逸分析的基础上。 3) 栈上分配 当对象没有发生逃逸时，该对象就可以通过标量替换分解成成员标量分配在栈内存中，和方法的生命周期一致，随着栈帧出栈时销毁，减少了 GC 压力，提高了应用程序性能。 happen-before原则 程序顺序规则：一个线程中的每个操作，happens-before于该线程中的任意后续操作。 监视器锁规则：对一个锁的解锁，happens-before于随后对这个锁的加锁。 volatile变量规则：对一个volatile域的写，happens-before于任意后续对这个volatile域的读。 传递性：如果A happens-before B，且B happens-before C，那么A happens-before C。 GC垃圾回收定义垃圾 引用计数算法 可达性分析算法 内存中的分配主要是手机heap(堆内存)主要是堆内存最大,同时是零碎的不像 stack(栈内存)是在方法调用完或者方法的引用完后就自动回收的. 目前堆内存中主要是初始化好的对象实例,和对象实例的类里面的变量, 虚拟机栈（栈帧中的本地变量表）中引用的对象 方法区中类静态属性引用的对象 方法区中常量引用的对象 本地方法栈中 JNI（即一般说的 Native 方法）引用的对象 Java 堆(Java Heap)是JVM所管理的内存 清除方式1.标记清除算法 … 2.复制算法 … 3.标记整理算法 … 参考链接: 咱们从头到尾说一次 Java 垃圾回收 Java内存模型（JMM）总结 - 知乎 深入理解Java中的逃逸分析 - 掘金 面试问我 Java 逃逸分析，瞬间被秒杀了。。 - 知乎","categories":[],"tags":[]},{"title":"java内存和系统内存以及栈和堆","slug":"java内存和系统内存以及栈和堆","date":"2022-12-09T01:56:56.000Z","updated":"2024-03-14T14:55:29.478Z","comments":true,"path":"a87a9ae1.html","link":"","permalink":"https://imlike.cc/a87a9ae1.html","excerpt":"","text":"参考链接: 数据结构的堆、栈和操作系统的堆内存、栈内存的区别 - Kylen的博客 | Kylen Blog CPU 处理数据需要 寄存器但是仅仅是寄存器是不够的还需要外部的内存支持 这个时候就需要 RAM的支持 stack 栈 ,支持push ,pop 类似于这个结构 stack over flow 栈 就像堆一样存储在计算机 RAM 中。 在堆栈上创建的变量将超出范围并自动释放。 与堆上的变量相比，分配速度要快得多。 使用实际的堆栈数据结构实现。 存放本地数据，返回地址，用于参数传递。 当使用太多堆栈时可能会发生堆栈溢出（主要来自无限或太深的递归，非常大的分配）。 在堆栈上创建的数据可以在没有指针的情况下使用。 如果您确切地知道在编译之前需要分配多少数据并且它不是太大，那么您将使用堆栈。 通常在程序启动时已经确定了最大大小。 堆： 就像堆栈一样存储在计算机 RAM 中。 在 C++ 中，堆上的变量必须手动销毁并且永远不会超出范围。 使用 delete、delete[] 或 free 释- 放数据。与堆栈上的变量相比，分配速度较慢。 按需用于分配数据块供程序使用。 当有很多分配和释放时，可能会产生碎片。 在 C++ 或 C 中，在堆上创建的数据将通过指针指向并分别使用 new 或 malloc 进行分配。 如果请求分配的缓冲区太大，可能会出现分配失败。 如果您不确切知道在运行时需要多少数据或者需要分配大量数据，则可以使用堆。 负责内存泄漏。","categories":[],"tags":[]},{"title":"认证和权限","slug":"认证和权限","date":"2022-12-09T01:52:33.000Z","updated":"2024-03-14T14:55:29.486Z","comments":true,"path":"d573592c.html","link":"","permalink":"https://imlike.cc/d573592c.html","excerpt":"","text":"参考文档: 谈谈单点登录｜R_song’s blog SAML OAuth2","categories":[],"tags":[]},{"title":"理解java的BIO/NIO/AIO","slug":"理解java的BIO-NIO-AIO","date":"2022-12-08T12:54:46.000Z","updated":"2024-03-14T14:55:29.485Z","comments":true,"path":"ff3a358d.html","link":"","permalink":"https://imlike.cc/ff3a358d.html","excerpt":"","text":"在理解Java的BIO/NIO/AIO之前我们先梳理下linux的相关io概念,下面是参考的连接 我在网络上搜索相关io 有句话很熟悉 BIO 下的连接对应一个线程 , NIO 下的请求对应一个线程 , AIO下的有效请求对应一个线程 BIO：线程发起 IO 请求，不管内核是否准备好 IO 操作，从发起请求起，线程一直阻塞，直到操作完成。 NIO：客户端发送的连接请求都会注册到多路复用器上，多路复用器轮询到连接有 I/O 请求时才启动一个线程进行处理。 AIO：线程发起 IO 请求，立即返回；内存做好 IO 操作的准备之后，做 IO 操作，直到操作完成或者失败，通过调用注册的回调函数通知线程做 IO 操作完成或者失败。 请求可以是读写都会创建对应线程处理,而有效请求着是 处理完对应io操作后才会调用对应的线程去操作 netty介绍RPC一-线程模型 · 造舟野渡 RPC二-NettyHandler处理消息 · 造舟野渡RPC三-rpc协议和编解码 · 造舟野渡RPC四-netty异步双向责任链 · 造舟野渡RPC五-可靠性设计 · 造舟野渡RPC六-动态代理 · 造舟野渡 Java io相关内容 java - 理解什么是BIO/NIO/AIO_个人文章 - SegmentFault 思否 Java Development: BIO, NIO, AIO and Netty | Medium linux io的相关链接 套接字 socket 和 tcp 连接过程 - JMCui - 博客园 网络 IO 模型简单介绍 - JMCui - 博客园","categories":[],"tags":[]},{"title":"github，gitlab相关的操作","slug":"github-gitlab相关的操作","date":"2022-12-04T06:08:17.000Z","updated":"2024-03-14T14:55:29.477Z","comments":true,"path":"f1f7a0f1.html","link":"","permalink":"https://imlike.cc/f1f7a0f1.html","excerpt":"","text":"Github RAW 加速服务 | 7ED 提高国内访问 GitHub 的速度的 9 种方案 - 腾讯云开发者社区-腾讯云 https://raw.githubusercontent.com/SpotX-CLI/SpotX-Win/main/Install.ps1 在githubusercontent 后面添加个大写的 S -&gt; https://raw.githubusercontentS.com/SpotX-CLI/SpotX-Win/main/Install.ps1 无需代理直接加速各种 GitHub 资源拉取 | 国内镜像赋能 | 助力开发 - Frytea’s Blog fastgh/fgit: 加速100～3000倍的github.com hunshcn/gh-proxy: github release、archive以及项目文件的加速项目 举个例子现在需要clone某个GitHub的仓库或者是用某个shell脚本搭建某种服务 1git clone --single-branch --branch v1.4.9 https:&#x2F;&#x2F;github.com&#x2F;rook&#x2F;rook.git 时常会返回下列错误 正克隆到 ‘rook’…error: RPC failed; result=35, HTTP code = 0fatal: The remote end hung up unexpectedly 亦或者是ssh连接失败访问失败连接超时这种，如果你手动去找GitHub镜像站费时间而且有些git clone是写在二进制文件里面不方便去一个一个修改，之前有用过修改linux服务器的hosts来实现加速访问但是速度时好时坏，稳定性太差。 参考上面博主的文章我有了新想法，定义使用 fgit然后软连接 /usr/bin/git 第二种就是 1git config --global url.&quot;https:&#x2F;&#x2F;gitclone.com&#x2F;github.com&quot;.insteadOf https:&#x2F;&#x2F;ghproxy.seekme.workers.dev https://ghproxy.seekme.workers.dev/ 这个是我参考上面的介绍搭建的加速站，目前下载文件没啥问题就是加clone加速还是有问题 https://hub.fastgit.xyz/rook/rook.git https://github.com.cnpmjs.org/rook/rook.git https://gitclone.com/github.com/rook/rook.git https://hub.fastgit.xyz/rook/rook.git 速度最快 替换步骤 12345678# 方法二：配置git自动替换$ git config --global url.&quot;https:&#x2F;&#x2F;hub.fastgit.xyz&quot;.insteadOf https:&#x2F;&#x2F;github.com# 测试$ git clone --single-branch --branch v1.4.9 https:&#x2F;&#x2F;github.com&#x2F;rook&#x2F;rook.git# 查看git配置信息$ git config --global --list# 取消设置$ git config --global --unset url.https:&#x2F;&#x2F;github.com&#x2F;.insteadof gitlab引用仓库里的图片一起在typora上使用的引用相对位置的图片是 1![image-20220127150651513](C:&#x2F;Program%20Files&#x2F;img&#x2F;image-20220127150651513.png) 但是在gitlab不会去找相对位置的图片，很多人反馈在github这样写是可以在仓库readme上显示，但是在gitlab应该没优化，这边在StackOverflow上找到的解决方案是用&lt;&gt;括一下，确实可以实现，而且在语法是兼容的。 StackOverflow的建议 1![image-20220215103753937](&lt;C:&#x2F;Users&#x2F;Administrator&#x2F;Desktop&#x2F;img&#x2F;image-20220215103753937.png&gt;)","categories":[],"tags":[{"name":"git","slug":"git","permalink":"https://imlike.cc/tags/git/"},{"name":"gitlab","slug":"gitlab","permalink":"https://imlike.cc/tags/gitlab/"},{"name":"github","slug":"github","permalink":"https://imlike.cc/tags/github/"},{"name":"加速","slug":"加速","permalink":"https://imlike.cc/tags/%E5%8A%A0%E9%80%9F/"}]},{"title":"ananotation_async介绍和使用","slug":"ananotation-async介绍和使用","date":"2022-11-28T08:28:29.000Z","updated":"2024-03-14T14:55:29.475Z","comments":true,"path":"3766dac5.html","link":"","permalink":"https://imlike.cc/3766dac5.html","excerpt":"","text":"Spring中异步注解@Async的使用、原理及使用时可能导致的问题-阿里云开发者社区","categories":[],"tags":[]},{"title":"mysql_json函数使用","slug":"mysql-json函数使用","date":"2022-11-23T12:07:46.000Z","updated":"2024-03-14T14:55:29.479Z","comments":true,"path":"7c7b32f9.html","link":"","permalink":"https://imlike.cc/7c7b32f9.html","excerpt":"","text":"How to use mysql “json_contains” with JPA Specification - Stack Overflow How to search JSON array in MySQL? - Stack Overflow json - MySQL JSON_EXTRACT value of property based on criteria - Stack Overflow spring jpa function(函数) JSON_EXTRACT 复杂查询，jpa josn查询_风.foxwho的博客-CSDN博客_jpa json查询 MySql 中文文档 - 12.17.3 搜索 JSON 值的函数 | Docs4dev json字符串搜索模糊匹配等相关的内容 1.JSON_SEARCH 直接使用这个来查询还可以模糊匹配 1234567 SELECT JSON_SEARCH(&#39;[&#123;&quot;id&quot;:10002444,&quot;qrCode&quot;:&quot;44444417&quot;,&quot;sticker&quot;:&quot;44444416&quot;,&quot;deckId&quot;:&quot;N3MWSF52666&quot;,&quot;vehicleType&quot;:&quot;SCOOTER&quot;&#125;,&#123;&quot;id&quot;:10002444,&quot;qrCode&quot;:&quot;44444416&quot;,&quot;sticker&quot;:&quot;44444416&quot;,&quot;deckId&quot;:&quot;N3MWSF52666&quot;,&quot;vehicleType&quot;:&quot;SCOOTER&quot;&#125;]&#39;, &#39;all&#39;, &#39;%44444%&#39;, NULL, &#39;$**.sticker&#39;) 123SELECT * FROM incident_report WHERE (JSON_SEARCH(users,&#39;one&#39;,&#39;39717&#39;,null,&#39;$**.id&#39;) is not null)SELECT users FROM incident_report LIMIT 10 15890 qrCode 44444403SELECT JSON_SEARCH(users,&#39;one&#39;,&#39;%51230%&#39;,null,&#39;$**.qrCode&#39;) FROM incident_report LIMIT 10 2.JSON_EXTRACT(解析json数据),使用JSON_CONTAIN来查询 解析后的JSON信息 对对于是数组里面的特定对象字段譬如 下面这种,这需要进一步抽取 1[&#123;&quot;id&quot;:10002444,&quot;qrCode&quot;:&quot;44444417&quot;,&quot;sticker&quot;:&quot;44444416&quot;,&quot;deckId&quot;:&quot;N3MWSF52666&quot;,&quot;vehicleType&quot;:&quot;SCOOTER&quot;&#125;,&#123;&quot;id&quot;:10002444,&quot;qrCode&quot;:&quot;44444416&quot;,&quot;sticker&quot;:&quot;44444416&quot;,&quot;deckId&quot;:&quot;N3MWSF52666&quot;,&quot;vehicleType&quot;:&quot;SCOOTER&quot;&#125;] JSON_CONTAINS(JSON_EXTRACT(json_field, ‘$[*].id’), 11, ‘$’) SELECT JSON_EXTRACT(users,&#39;$.id&#39;) FROM incident_report LIMIT 10 SELECT JSON_EXTRACT(users, &#39;$[*].userTrips[*].id&#39;) FROM incident_report LIMIT 10 SELECT users,JSON_CONTAINS(JSON_EXTRACT(users, &#39;$[*].userTrips[*].id&#39;), &#39;12689&#39;, &#39;$&#39;) FROM incident_report LIMIT 10 https://dev.mysql.com/doc/refman/5.7/en/json-search-functions.html)","categories":[],"tags":[]},{"title":"spring缓存管理Redis+本地","slug":"spring缓存管理Redis-本地","date":"2022-11-22T06:27:34.000Z","updated":"2024-03-14T14:55:29.481Z","comments":true,"path":"a0b38034.html","link":"","permalink":"https://imlike.cc/a0b38034.html","excerpt":"","text":"Using Multiple Cache Managers in Spring | Baeldung caffice+redis ```java","categories":[],"tags":[]},{"title":"transactionl介绍(转载)","slug":"transactionl介绍","date":"2022-11-21T06:03:48.000Z","updated":"2024-03-14T14:55:29.482Z","comments":true,"path":"f3616705.html","link":"","permalink":"https://imlike.cc/f3616705.html","excerpt":"","text":"🔗 原文链接： https://mp.weixin.qq.com/s/rvcb6xzB… ⏰ 剪存时间：2022-11-21 11:52:31 (UTC+8) ✂️ 本文档由 飞书剪存 一键生成 veezean 石杉的架构笔记 2022-11-16 07:50 发表于 江苏 「 关注 “石杉的架构笔记” ，大厂架构经验 倾囊相授 」 文章来源：【公众号：架构悟道 】在大部分涉及到数据库操作的项目里面， 事务控制、事务处理都是一个无法回避的问题 。比如，需要对SQL执行过程进行事务的控制与处理的时候，其整体的处理流程会是如下的示意： 首先是要开启事务、然后执行具体SQL，如果执行异常则回滚事务，否则提交事务，最后关闭事务，完成整个处理过程。按照这个流程的逻辑，写一下对应的实现代码： 123456789101112131415161718192021222324252627282930313233343536373839404142public void testJdbcTransactional(DataSource dataSource) &#123; Connection conn = null; int result = 0; try &#123; // 获取链接 conn = dataSource.getConnection(); // 禁用自动事务提交，改为手动控制 conn.setAutoCommit(false); // 设置事务隔离级别 conn.setTransactionIsolation( TransactionIoslationLevel.READ_COMMITTED.getLevel() ); // 执行SQL PreparedStatement ps = conn.prepareStatement(&quot;insert into user (id, name) values (?, ?)&quot;); ps.setString(1, &quot;123456&quot;); ps.setString(2, &quot;Tom&quot;); result = ps.executeUpdate(); // 执行成功，手动提交事务 conn.commit(); &#125; catch (Exception e) &#123; // 出现异常，手动回滚事务 if (conn != null) &#123; try &#123; conn.rollback(); &#125; catch (Exception e) &#123; // write log... &#125; &#125; &#125; finally &#123; // 执行结束，最终不管成功还是失败，都要释放资源，断开连接 try &#123; if (conn != null &amp;&amp; !conn.isClosed()) &#123; conn.close(); &#125; &#125; catch (Exception e) &#123; // write log... &#125; &#125;&#125; 不难发现，上面大段的代码逻辑并不复杂，对于业务而言其实仅仅只是执行了一个insert操作而已。但是杂糅的事务控制代码，显然 干扰了业务自身的代码处理逻辑的阅读与理解 。 常规项目的代码中，涉及到DB处理的场景很多，如果每个地方都有这么一段事务控制的逻辑，那么整体代码的可维护性将会比较差，想想都令人窒息。 好在，JAVA中很多项目现在都是基于Spring框架进行构建的。得益 于 Spring框架的封装，业务代码中进行事务控制操作起来也很简单，直接加个 @Transactional注解即可，大大简化了对业务代码的 侵入性 。那么对 @Transactional事务注解了解的够全面吗？知道有哪些场景可能会导致 @Transactional注解并不会如你预期的方式生效吗？知道应该怎么使用 @Transactional才能保证对性能的影响最小化吗？ 下面我们一起探讨下这些问题。 Spring声明式事务处理机制 为了简化业务开发场景对事务的处理复杂度，让开发人员可以更关注于业务自身的处理逻辑， Spring 提供了声明式事务的能力支持。 Spring 数据库事务约定处理逻辑流程如下图所示，对比前面示例中基于 JDBC 的事务处理，Spring的事务的处理操作交给了 Spring框架 处理，开发人员仅需要实现自己的业务逻辑即可，大大简化了事务方面的处理投入。 基于Spring事务机制，实现上述DB操作事务控制的代码，我们的代码会变得非常的简洁： 1234@Transactionalpublic void insertUser() &#123; userDao.insertUser();&#125; 与JDBC事务实现代码相比，基于Spring的方式只需要添加一个 @Transactional 注解即可，代码中只需要实现业务逻辑即可，实现了事务控制机制对业务代码的 低侵入性 。 Spring支持的基于 Spring AOP 实现的 声明式事务 功能，所谓声明式事务，即使用@Transactional注解进行声明标注，告诉Spring框架在什么地方启用数据库事务控制能力。 @Transactional 注解， 可以添加在类或者方法上 。如果其添加在类上时，表明此类中所有的 public非静态方法 都将启用事务控制能力。 既然@Transactional注解承载了Spring框架对于事务处理的相关能力，那么接下来我们就一起看下该注解的一些可选配置以及具体使用场景。 @Transactional主要可选配置 只读事务配置 通过 readonly 参数指定当前事务是否为一个只读事务。设置为true标识此事务是个只读事务，默认情况为false。 1234@Transactional(readOnly = true)public DomResponse&lt;CiCdItemDetail&gt; queryCicdItemDetail(String appCode) &#123; return null;&#125; 这里涉及一个概念，叫做 只读事务 ，其含义描述如下： 在多条查询语句一起执行的场景里面会涉及到的概念。表示在事务设置的那一刻开始，到整个事务执行结束的过程中，其他事务所提交的写操作数据，对该事务都不可见。 举个例子： 现在有一个复合查询操作，包含2条SQL查询操作：先获取用户表count数，再获取用户表中所有数据。 (1) 先执行完获取用户表count数，得到结果10 (2) 在还没开始执行后一条语句的时候，另一个进程操作了DB并往用户表中插入一条新数据 (3) 复合操作的第二条SQL语句，获取用户列表的操作被执行，返回了11条记录 很明显，复合操作中的两条SQL语句获取的数据结果无法匹配上。原因就是非原子性操作导致，即2条查询操作执行的间隔内，有另一个写操作修改了目标读取的数据，导致了此问题的出现。 为了避免此情况的发生，可以给复合查询操作添加上只读事务，这样事务控制范围内，事务外的写操作就不可见，这样就保证了事务内多条查询语句执行结果的一致性。 那为什么要设置为只读事务、而不是常规的事务呢？主要是从执行效率角度的考虑。因为这个里的操作都是一些只读操作，所以设置为只读事务，数据库会为只读事务提供一些优化手段，比如不启动回滚段、不记录回滚log之类的。 回滚条件设定 @Transactional 有提供4个不同属性，可以支持传入不同的参数，设定需要回滚的条件： 参数 含义说明 rollbackFor 用于指定需要回滚的特定异常类型，可以指定一个或者多个。当指定 rollbackFor 或者 rollbackForClassName 之后，方法执行逻辑中只有抛出指定的异常类型，才会触发事务回滚 rollbackForClassName 与 rollbackFor 相同，设置字符串格式的类名 noRollbackFor 用于指定不需要进行回滚的异常类型，当方法中抛出指定类型的异常时，不进行事务回滚。而其余的类型的异常将会触发事务回滚。 noRollbackForClassName 与 noRollbackFor 相同，设置字符串格式的类名 其中，rollbackFor支持指定单个或者多个异常类型，只要抛出指定类型的异常，事务都将被回滚掉： 1234567891011// 指定单个异常@Transactional(rollbackFor = DemoException.class)public void insertUser() &#123; // do something here&#125;// 指定多个异常@Transactional(rollbackFor = &#123;DemoException.class, DemoException2.class&#125;)public void insertUser2() &#123; // do something here&#125; rollbackFor 和 rollbackForClassName 作用相同，只是提供了2个不同的指定方法，允许执行Class类型或者ClassName字符串。 12345// 指定异常名称@Transactional(rollbackForClassName = &#123;&quot;DemoException&quot;&#125;)public void insertUser() &#123; // do something here&#125; 同理， noRollbackFor 和 noRollbackForClassName 的使用与上面示意的相似，只是其含义功能点是相反的。 事务传播行为 propagation 用于指定此事务对应的传播类型。所谓的事务传播类型，即当前已经在一个事务的上下文中时，又需要开始一个事务，这个时候来处理这个将要开启的新事务的处理策略。 主要有7种类型的事务传播类型： 传播类型 含义描述 REQUIRED 如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务 SUPPORTS 如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行 MANDATORY 如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常 REQUIRES_NEW 创建一个新的事务，如果当前存在事务，则把当前事务挂起 NOT_SUPPORTED 以非事务方式运行，如果当前存在事务，则把当前事务挂起 NEVER 以非事务方式运行，如果当前存在事务，则抛出异常 NESTED 如果当前存在事务，则创建一个事务作为当前事务的嵌套事务来运行；如果当前没有事务，则该取值等价于REQUIRED 事务的传播行为，将会影响到事务控制的结果，比如最终是在同一事务中，一旦遇到异常，所有操作都会被回滚掉，而如果是在多个事务中，则某一个事务的回滚，不影响已提交的其余事务的回滚。 实际编码的时候，可以通过@Transactional注解中的 propagation 参数来指定具体的传播类型，取值由 org.springframework.transaction.annotation.Propagation 枚举类提供。如果不指定，则默认取值为 Propagation.REQUIRED ，也即 如果当前存在事务，则加入该事务，如果当前没有事务，则创建一个新的事务 。 123456/** * The transaction propagation type. * &lt;p&gt;Defaults to &#123;@link Propagation#REQUIRED&#125;. * @see org.springframework.transaction.interceptor.TransactionAttribute#getPropagationBehavior() */Propagation propagation() default Propagation.REQUIRED; 事务超时设定 可以使用 timeout 属性来设置事务的超时秒数，默认值为-1，表示永不超时。 @Transactional失效场景避坑 同一个类中方法间调用 Spring的事务实现原理是AOP，而AOP的原理是动态代理。 在类内部方法之间相互调用的时候，本质上是类对象自身的调用，而不是使用代理对象去调用，也就不会触发AOP，这样其实Spring也就无法将事务控制的代码逻辑织入到调用代码流程中，所以这里的事务控制就无法生效。 12345678public void insertUser() &#123; writeDataIntoDb();&#125;@Transactionalpublic void writeDataIntoDb() &#123; // ...&#125; 所以遇到同一个类中多个方法之间相互调用，且调用的方法需要做事务控制的时候需要特别注意下这个问题。解决方式，可以建2个不同的类，然后将方法放到两个类中，这样跨类调用，Spring事务机制就可以生效。 添加在非public方法上 如果将@Transactional注解添加在protected、private修饰的方法上，虽然代码不会有任何的报错，但是实际上注解是不会生效的。 1234@Transactionalprivate void writeDataIntoDb() &#123; // ...&#125; 方法内部Try Catch吞掉相关异常 这个其实很容易理解，业务代码中将所有的异常给catch并吞掉了，等同于业务代码认为被捕获的异常不需要去触发回滚。对框架而言，因为异常被捕获了，业务逻辑执行都在正常往下运行，所以也不会触发异常回滚机制。 1234567891011121314// catch了可能的异常，导致DB操作失败的时候事务不会触发回滚@Transactionalpublic void insertUser() &#123; try &#123; UserEntity user = new UserEntity(); user.setWorkId(&quot;123456&quot;); user.setUserName(&quot;王小二&quot;); userRepository.save(user); &#125; catch (Exception e) &#123; log.error(&quot;failed to create user&quot;); // 直接吞掉了异常，这样不会触发事务回滚机制 &#125;&#125; 在业务处理逻辑中，如果确实需要知晓并捕获相关处理的异常进行一些额外的业务逻辑处理，如果要保证事务回滚机制生效，最后需要往外抛出 RuntimeException 异常，或者是继承RuntimeException实现的 业务自定义异常 。如下： 123456789101112131415// catch了可能的异常，对外抛出RuntimeException或者其子类,可触发事务回滚@Transactionalpublic void insertUser() &#123; try &#123; UserEntity user = new UserEntity(); user.setWorkId(&quot;123456&quot;); user.setUserName(&quot;王小二&quot;); userRepository.save(user); &#125; catch (Exception e) &#123; log.error(&quot;failed to create user&quot;); // @Transactional没有指定rollbackFor，所以抛出RuntimeException或者其子类，可触发事务回滚机制 throw new RuntimeException(e); &#125;&#125; 当然，如果@Transactional注解指定了 rollbackFor 为某个具体的异常类型，则最终需要保证异常时对外抛出相匹配的异常类型，才可以触发事务处理逻辑。如下： 1234567891011121314// catch了指定异常，对外抛出对应类型的异常,可触发事务回滚@Transactional(rollbackFor = DemoException.class)public void insertUser() &#123; try &#123; UserEntity user = new UserEntity(); user.setWorkId(&quot;123456&quot;); user.setUserName(&quot;王小二&quot;); userRepository.save(user); &#125; catch (Exception e) &#123; log.error(&quot;failed to create user&quot;); // @Transactional有指定rollbackFor，抛出异常要与rollbackFor指定异常类型一致 throw new DemoException(); &#125;&#125; 对应数据库引擎类型不支持事务 以 MySQL 数据库而言，常见的数据库引擎有 InnoDB 和 Myisam 等类型，但是 MYISAM引擎类型是不支持事务 的。所以如果建表时设置的引擎类型设置为 MYISAM 的话，即使代码里面添加了@Transactional最终事务也不会生效的。 @Transactional使用策略 因为事务处理对性能会有一定的影响，所以事务也不是说任何地方都可以随便添加的。对于一些性能敏感场景，需要注意几点： 仅在必要的场合添加事务控制 （1）不含有DB操作相关，无需添加事务控制 （2）单条查询语句，没必要添加事务控制 （3）仅有查询操作的多条SQL执行场景，可以添加只读事务控制 （4）单条 insert/update/delete 语句，其实也不需要添加 @Transactional 事务处理，因为单条语句执行其实数据库有 隐性事务控制机制 ，如果执行失败，是属于 SQL 报错，数据不会更新成功，自然也无需回滚。 尽可能缩小事务控制的代码段处理范围 主要从性能层面考虑，事务机制，类似于并发场景的加锁处理， 范围越大对性能影响越明显 事务控制范围内的业务逻辑尽可能简单、避免非事务相关耗时处理逻辑 也是从性能层面考虑，尽量将耗时的逻辑放到事务控制之外执行， 事务内仅保留与DB操作切实相关的逻辑","categories":[],"tags":[{"name":"转载","slug":"转载","permalink":"https://imlike.cc/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"java","slug":"java","permalink":"https://imlike.cc/tags/java/"}]},{"title":"一次单元测试优化的过程总结(转载)","slug":"一次单元测试优化的过程总结","date":"2022-11-20T09:54:12.000Z","updated":"2024-03-14T14:55:29.483Z","comments":true,"path":"a6af0ce2.html","link":"","permalink":"https://imlike.cc/a6af0ce2.html","excerpt":"","text":"🔗 原文链接： https://mp.weixin.qq.com/s/VdftYMXu…⏰ 剪存时间：2022-11-20 17:54:29 (UTC+8)✂️ 本文档由 飞书剪存 一键生成原创 李一飞（一骞） 大淘宝技术 2022-09-14 16:20 发表于 浙江 [图片] 本文将介绍淘宝用户运营平台团队最近在实践单元测试过程中遇到的一个问题。 前言 淘宝原用户增长团队（现用户运营平台团队）是比较早践行单测增量覆盖率的团队，坚持了近两年下来，我们积累了数千个test case，在开发新功能、修改原功能的过程中帮助我们发现了许多问题，显著地提升了代码质量、减少线上故障。在这里郑重地向大家推荐，单测是值得认真做的，开头是痛苦的，但是积累一段时间后，量变就会带来质变。 言归正传，接下来谈一谈最近在实践单测过程中遇到的一个问题。在研发协同平台aone（下文简称aone）的发布流水线中，我们针对单元测试设置了增量代码覆盖率85%和test case 100%通过的流程卡点，在每次发布前，要保证test case完全通过才能提交工单。我们遇到了因并发导致的test case失败，调整并发度导致的单测时间过长，但又影响研发效能的问题。最终在并发度和成功率之间找到了一个平衡点，解决了单测流程降低研发效率的问题。 单侧流水线配置 在单测流程中呢，我们主要用到了JUnit、JaCoCo和Surefire三套工具，通过aone提供的容器自动化运行单元测试，搜集测试报告。下面简单介绍一下这三个工具。 ▐ JUnit java界最大名鼎鼎的单元测试框架，无须多言，会java的应该都知道。 ▐ JaCoCo EclEmma团队开发的开源代码覆盖率统计工具，也是java业内最主流的代码覆盖率统计工具。增量代码覆盖率就是通过该工具进行统计的，全量、增量、按类、包统计都支持，非常灵活。 ▐ Maven Surefire Plugin surefire是maven的一个插件，在maven生命周期的test阶段执行单元测试用例。运行完成后还会生成测试报告，方便用户查看单测情况。 我们利用三种工具，加上aone提供的容器和流水线配置能力，完成了自动化单测的流程和发布卡点校验。 单元实践过程 ▐ 两个阶段 积累test case时期 在刚刚开始单测时，大家新增的代码都相对比较独立，随着业务的发展、工作职责的调整，单测会不断变复杂，不同的service之间互相交织、单测的维护、运行成本都会增加。我们在这个阶段遇到了一个比较棘手的问题。日常开发过程中，单测都是以类为粒度在本地跑的，都能通过后再去流水线验证，一旦提交到流水线，就会遇到个别case失败的问题，一开始排查起来完全没有思路，test case的失败可以说是随机的，任何一个类的任何一个用例都有可能失败。 [图片] 经过分析和排查，得出结论是并发导致的，于是我们限制了并发，做了如下配置，确实解决了这个问题。 org.apache.maven.plugins maven-surefire-plugin 2.16 false 1 大家可以留意一下reuseForks和forkCount参数，这时候我们还没有深究两个配置的含义，只是简单的限制了并发，这也为后续的故事埋下了伏笔。 test case达到一定规模时期 在完成了test case的初始积累以后，新的问题又随之而来。因为没有并发，test case又很多，所以每次单测运行时长长达50分钟。也严重影响了大家的研发效率。在分秒必争的发布窗口期，经常会出现大家等着单测跑完提交发布单的情况。 [图片] ▐ 问题 看了上述两个不同阶段反映的问题，本质上就是成功率和实效性的trade off问题，如何能提高并发、提升运行速度的同时保障成功率，这就是我们需要解决的最终命题。 ▐ 原理和解决方案 上文提到了reuseForks和forkCount参数，这些都是maven-surefire-plugin提供的配置项，把surefire插件研究清楚了，应该就能解决如何兼顾速度和实效性的问 题。 Surefire配置详解 paralleljvm内并行执行通 过parallel参数开启，可选为methods，classes，both，suites等其他参数 useUnlimitedThreads，不限制线程数 threadCount，线程数 perCoreThreadCount，每核（默认true，和threadCount组合使用） parallelTestsTimeoutInSeconds，timeout时间注 设置了parallel后，useUnlimitedThreads或者threadCount必须设置一个，不然会报错 parallel级别还有suitesAndClasses等更复杂的配置项，本文不多探讨 参数示例如下，代表methods级别并发，10 条线程执行。 org.apache.maven.plugins maven-surefire-plugin 3.0.0-M7 methods 10 fork多jvm并行执行 forkCount 最多同时生成的JVM个数，特殊语法是nC，代表n倍的CPU核数，2.5C在4核机器上就是10的意思。 reuseForks 是否重复使用fork出的JVM，true代表一个测试类运行完后，进程继续处理下一个，false代表一个类运行完了JVM销毁，重新生成新的JVM 默认配置 forkCount=1/reuseFork=true，forkCount设置为0会被自动替换为1 parallel和forkparallel和fork组合后，就可以有更好的并发效率，也会带来更大的冲突可能。 并发导致case失败原因 surefire的文档原文如下， [图片] 简单说来，就是因为JUnit的实现机制，对于JVM内的线程并发，会出现一些race condition或者其他难以复现的问题；对于forkCount大于1且开启复用的情况，因为测试类是在复用的JVM内，也会因为相同的原因产生并发问题导致测试失败。 结果和建议 在彻底搞清楚surefire的配置原理后，我们回到问题来。经过各种排列组合的尝试，我们得出了比较合适的配置，reuseForks=true/ forkCount=2C，最终效果是每次运行时间在10分钟左右，出错概率较低，通过重跑也能解决。 [图片] 小tipmvn默认是按模块串行的，可开启并行提高整体速度（例：mvn -T 1C clean test)，但是在我们的场景下，2000多个test case有1800个都在一个模块里，所以开启并行的效果不大。其实这个问题没有最优组合，只有最合适的组合。 在优化了这个单测耗时最久的应用后，我们又分析了其他几个应用，有的应用test case不多，单测运行时长不长，就没有必要开启并发，优先保证成功率即可； 有的应用test case直接相互干扰较小，并发度可以调整得更高……总的来说，在弄明白了原理之后，还需要具体情况具体分析，“纸上得来终觉浅，绝知此事要躬行”，大家可以分析一下自己应用的情况，结合surefire的并发机制进行实践，相信测过几次以后就能找到最合适的配置组合。 单元实践过程 在整个过程中，笔者还留有两个想法： 有没有办法通过提高单测代码质量来避免或者降低因为并发引起的失败？一些思路是通过suite分组，将可能冲突的类分开跑，这样的做法可能会极大的提高单测开发成本，投入产出比不高。 test case通过率可以不用严格卡100%，设定到99.5%都能显著的提升效率，因为每次失败的test case是不固定的，所以偶发的个别问题不影响整体的回归。 在实践卓越工程的过程中，笔者深切的感受到纵观整个软件研发的生命周期，有很多值得研究和切入的点，一些微小的改动，都能有效地提升研发效能和交付质量。在当前的环境下，业务竞争日趋激烈，所谓开源节流，“开源”难，重心就会偏向“节流”，降本增效一定会是下一个阶段的重点。而且对于技术人来说，效率一定是永远的追求。其实提升性能、效率往往不是特别高大上的事情，希望大家能在日常繁重的工作之余，有点时间做些有趣的研究，享受技术带来的快乐！ 参考资料 https://stackoverflow.com/questions/3365628/junit-tests-pass-in-eclipse-but-fail-in-maven-surefire https://maven.apache.org/surefire/maven-surefire-plugin/examples/fork-options-and-parallel-execution.html https://www.baeldung.com/maven-junit-parallel-tests 团队介绍 大淘宝技术-用户平台技术团队用户平台技术团队是一支集研发、数据、算法一体的团队，负责淘宝天猫的用户增长，游戏互动，平台会员和私域运营等消费者核心业务。在对用户争夺进入白热化的时期，团队正承担着捍卫电商主板块增长的重要使命，是阿里核心电商战场的参与者，用持续的技术创新来驱动阿里电商引擎的稳步前行。这是一支年轻开放的团队，在这里你将收获超大规模高并发场景的架构设计能力，洞悉用户增长最前沿的实践方法，在数字化时代收获最核心的竞争力。团队技术氛围浓厚，倡导创新和工程师文化，鼓励用数据和代码发现解决问题。团队研发流程规范，代码质量高，学习成长速度快。 ✿ 拓展阅读 [图片] [图片] 作者 | 李一飞（一骞） 编辑| 橙子君[图片]","categories":[],"tags":[{"name":"转载","slug":"转载","permalink":"https://imlike.cc/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"java","slug":"java","permalink":"https://imlike.cc/tags/java/"}]},{"title":"自动化测试在美团外卖的实践与落地(转载)","slug":"自动化测试在美团外卖的实践与落地","date":"2022-11-20T09:50:42.000Z","updated":"2024-03-14T14:55:29.486Z","comments":true,"path":"ead6381e.html","link":"","permalink":"https://imlike.cc/ead6381e.html","excerpt":"","text":"🔗 原文链接： https://mp.weixin.qq.com/s/vLR1FqGi… ⏰ 剪存时间：2022-11-20 17:50:19 (UTC+8) ✂️ 本文档由 飞书剪存 一键生成 原创 少飞 闫旭 文文等 美团技术团队 2022-09-15 19:58 发表于 北京 收录于合集 #美团外卖 19 个 #自动化测试 3 个 #前端 22 个 #到家 9 个 总第535 篇 | 2022年 第052篇 随着美团到家业务的发展，系统复杂度也在持续增长。测试用例数量近两年增长约一倍，单端数量超过1万2千条，而研发人员的工作从大部分时间在开发，转变成一半时间在开发、一半时间在模拟环境和自测。因此，引入自动化测试就显得十分有必要，本文介绍了美团外卖在自动化测试方向做的一些探索和实践，希望对从事相关领域工作的同学能够带来一些启发或帮助。 \\1. 项目背景 \\2. 项目目标 \\3. 方案选型 \\4. 实践和探索 4.1 问题和挑战 4.2 前置条件准备 4.3 用例录制与回放的数据一致性 4.4 用例录制与回放的操作一致性 4.5 可溯源的自动化测试 4.6 用例的维护 4.7 跨App回放用例 4.8 埋点的录制回放 \\5. 测试流程 5.1 自动化任务触发 5.2 回放集群调度 5.3 断言服务 5.4 消息推送 \\6. 落地与实践 6.1 业务共建 6.2 实践效果 1. 项目背景美团外卖的业务场景比较多元化，除了外卖自身的业务，还作为平台承接了闪购、团好货、医药、跑腿等其他业务。除此之外，在全链路动态化的大基调下，外卖各个页面的技术形态也变得越来越复杂，除了Native代码，还包括Mach（ 外卖自研动态化框架 ）、React Native、美团小程序、H5等，不同技术栈的底层技术实现不同，渲染机制不同，进而对测试方式要求也有所不同，这也在无形中增加了测试的难度。下图汇总了美团多业务、多技术、多App的一些典型场景。 图1 多业务、多技术栈、多App 在产品交付上线的过程中，测试的占比也是非常大的，甚至大于总时长的30%。如下图所示，整个测试包括了冒烟测试、新功能测试、二轮回归测试、三轮测试。然而，现在需求测试绝大部分还是采用非自动化的方式，这就使得人力成本变得非常之高。 图2 外卖迭代模型 另一方面，相比于2018年，2022年的测试用例数量增长近3倍，已经超过1万2千条（ 如下图所示 ）。同时，外卖的业务是“三端复用”，除了外卖App，还需要集成到美团App和大众点评App上，这样一来，测试工作量就翻了3倍，业务测试压力之大可想而知。如果按照当前的增长趋势持续下去，要保障外卖业务的稳定，就必须持续不断地投入大量的人力成本，所以引入能够支持外卖“多业务场景”、“多App复用”、“多技术栈” 特点的自动化测试工具来提升人效和质量，势在必行。 图3 近几年用例增长变化 2. 项目目标为了解决外卖面临的测试困境，我们尝试去探索一种零学习成本、低维护、高可用的自动化测试方案，能够支持外卖复杂多变的测试场景，它必须同时满足下面几点要求： 易用性 ：工具/平台的上手难度，使用复杂度应该尽可能的低，因为自动化测试的目的是提效人力，而不是增加人力负担。 平台支持 ：移动端至少需要覆盖iOS和Android双平台，同时基于外卖的业务特点，不仅需要对Native支持，也需要支持Mach（ 自研局部动态化框架 ）、H5、React Native、美团小程序等技术栈。 稳定性 ：自动化测试用例的执行需要有足够的稳定性和准确性，测试过程中不应因测试工具本身的不稳定而出现稳定性问题。 维护成本 ：维护成本很大程度上决定了测试工作量的大小，因需求产生变动或架构重构等问题时，用例的维护成本应该尽可能的小。 可扩展性 ：当测试方案不能满足测试需求时，工具/平台应具备可扩展的能力。 3. 方案选型 自动化测试工具那么多，自研是重复造轮子吗？ 针对终端的UI自动化测试工具/平台可谓“屡见不鲜”，市面上也有很多相对成熟的方案，相信大家都有用过，或者至少有所耳闻，但这些方案是否能真的满足我们提效的诉求呢？以下我们挑选了三类非常具有代表性的自动化测试工具/平台 - Appium、Airtest Project、SoloPi进行了分析，来帮助大家对自动化测试技术建立一个认知： Appium是一个开源工具，用于自动化测试iOS手机、Android手机和Windows桌面平台上的原生、移动 Web和混合应用。它使用了各系统自带的自动化框架，无需SDK集成，Appium把这些系统本身提供的框架包装进一套API——WebDriver API中，可以使用任何语言编写Client脚本向服务器发送适当的HTTP请求。这让不同技术栈的人员都能快速上手编写测试用例，可以选择自己最为熟悉的语言，但是对于没有语言开发基础的人来说，还是有一定学习成本，而且这种方式在多人协作时并没有太大作用，为了保证自动化用例的可维护性，团队内部应该需要统一脚本语言。值得一提的是：Appium在iOS、Android和 Windows 测试套件之间可做的一定程度的复用代码。但是由于不同端界面及元素定位的差异，这往往是不现实的，更无法保证测试的准确性，所以这种所谓的“跨端”就变得毫无意义。 Airtest Project是由网易游戏推出的一款自动化测试平台，除了支持通过系统自带的自动化测试框架，还支持了通过图像识别的方式，对于非基于原生UI系统的一些游戏引擎提供了SDK的支持。其上手难度稍低，可以一定程度上通过IDE进行相关操作来生成简单的脚本指令。Airtest虽然基于图像进行控件识别，为跨端提供了一定的可能性，然而图像识别并不能达到人眼识别的准确度，除此之外移动端页面的构成和游戏页面也存在不小的差别，页面元素的展示规则和样式受屏幕分辨率影响较大，单纯依靠图像识别来进行元素查找成功率不高，无法保证测试的准确性。 SoloPi是一个无线化、非侵入式的自动化测试工具，通过录制回放的方式进行UI自动化测试，SoloPi虽然只支持Android，但是在录制回放的这种方式中，还是极具代表性的。传统的自动化测试工具由于需要编写测试脚本，所以存在着一定的上手难度（ Airtest还是存在代码编辑的 ），便产生了SoloPi这种纯基于录制回放的自动化测试方式，将用例的所有操作事件进行录制，生成一个完整的录制脚本，通过对脚本的回放来还原所有的操作，从而进行自动化测试。但是，这种方式只能记录操作，而不能记录数据，在外卖这种数据驱动展示的场景下无法满足测试要求。并且外卖的业务要复用到美团App和大众点评App中，不同App存在部分视图和逻辑性的差异，SoloPi也无法支持我们“一端录制多端回放”的测试场景。 可以看出，以上这些自动化测试工具/平台对于数据记录，环境模拟、维护成本、跨App复用等方面，都是有所欠缺的。所以无论是哪种方案，在易用性、维护成本、稳定性、可扩展性以及最终的测试效果上，都无法满足我们对自动化测试的需求。我们并不是为了自动化而自动化，而是要解决实际的提效问题。 那么，怎样才能确定一个自动化工具/平台的可用性，并长期落地去使用自动化，带着上述提到的较高门槛的上手成本、操作繁琐的环境模拟、差强人意的测试成功率、定位模糊的测试缺陷、难以维护的用例脚本等几大重要痛点， 本文我们将介绍美团外卖自研的测试平台——AlphaTest ，都具备哪些能力以及是如何解决这些问题。 4. 实践和探索一个自动化测试工具/平台能不能用起来，取决于他的上手成本和稳定性，即使工具的测试稳定性做的再好，使用的门槛高也会让人望而生却，反之亦然。所以AlphaTest平台为了上手简单，降低使用成本，采用了 基于录制回放 的方式进行设计，并且弥补了常规录制回放无法编辑的痛点，同时在手势操作的基础上增加了数据录制。整合美团系App的特性增加了环境模拟、跨App支持、混合技术栈的支持等能力，在使用简单的同时，也保障了用例的可维护性、测试的准确性等。我们先通过视频简单的了解一下： 用例录制： 🎦 点击观看视频 用例回放： 🎦 点击观看视频 回放报告： 🎦 点击观看视频 4.1 问题和挑战 注：这里我们将生成的自动化脚本统称为指令，将平台生成的用例统称自动化用例，将录制回放变成可视化的脚本指令，让用例变的易懂、易维护。 录制回放本身是一连串的操作数据的集合，是连续性的、不可拆分，因此几乎不具备可编辑性，这也就导致了用例维护成本极高。AlphaTest虽然同样基于录制回放的方式生成自动化用例，但是我们将每一步的操作都具化成结构化的指令数据，并提供可视化指令编辑器，以支持查看编辑。 这些可视化的指令，完全通过录制自动生成，也不依赖于任何脚本语言。通过可视化用例指令编辑器，不仅为用例提供了编辑的可能性，同时大大地提高了用例的可阅读性，每一条测试用例在测试过程中每一步都做了什么、当时的界面是什么样的、都有哪些断言校验点，是显而易见的，不会存在像传统图文描述的测试用例那样，出现理解偏差。指令生成演示，手机录制与平台远端录制双模式支持： 图4 指令编辑器 图5 手机录制演示 图6 平台远端录制演示 4.2 前置条件准备 一键环境模拟，解决操作繁琐的用例执行前的环境准备。 进行一个用例的测试之前，往往需要做大量的准备工作，比如切换API环境，定位到某个地点，登录指定账户等。这些需要准备的环境条件我们统称为前置条件。我们知道，前置条件的准备操作通常都不是一两个步骤就可以完成的，比如账号登录/切换：我们需要进入登录页，填写手机号+密码/验证码，点击登录等一系列动作来完成这个过程，非常繁琐，并且每次测试我们都需要准备，重复性高。因此，我们给AlphaTest设计了独立的前置条件模块，将用例拆成了两个部分：前置条件 + 操作步骤。 图7 前置条件 与其它测试框架不同的是，AlphaTest采用了SDK集成，但对业务无侵入的方式，因此可以通过编写白盒代码来实现前置条件的自动配置，只需要在平台添加需要的指令，下发到SDK后，即可根据相关指令完成前置条件的自动配置，不再需要重复进行相关的操作。并且这些前置条件支持复用，也不需要每次进行用例准备时的重复配置。AlphaTest的前置条件，不仅有着基于美团内部服务及底层Hook的默认实现，也提供了API支持业务方自定义实现，比如实现不同的账号体系。 4.3 用例录制与回放的数据一致性 影响用例执行的不仅是代码，还有数据。 很多时候，自动化用例无法正常执行完成，可能是因为App回放时的本地数据及网络数据与录制时的不一致，从而导致用例执行流程的阻塞或App界面展示的不同。这也是大多数自动化测试工具/平台测试通过率不高的主要因素，因此要保证测试成功率，我们需要控制变量，排除由数据产生的影响。 图8 数据一致性 App运行依赖的数据，有两部分——本地数据和网络数据： 本地数据是App在运行期间产生的缓存数据或持久化的存储数据。为了让用例在录制回放时都能够保持一致的本地数据环境，我们在录制和回放前都对App的本地数据进行了清理操作，这样用例在录制和回放的过程中，都可以保持一致的App初始化环境。 网络数据是驱动App交互呈现的基石，各种策略和API升级都会影响网络数据的响应，因此我们将用例录制过程中产生的网络数据也进行了录制，并将网络数据和对应的操作指令进行了关联和绑定，确定了数据产生的事件源。排除数据影响后，我们的自动化测试的成功率就取决于自动化操作的准确性了，这就回到了常见自动化框架范畴。 4.4 用例录制与回放的操作一致性 目标定位的准确性与手势定位的精准性。 UI自动化测试的本质就是代替人去自动的做一步步的操作（ 点击、长按、输入、滑动等 ）。录制与回放过程的操作能否一致，是否精准，直接影响测试的成功率，决定了工具/平台的可用性。 目标控件定位准确性： 操作行为是否一致首先需要确认操作目标是否一致。与一般测试工具/平台不同的是AlphaTest采用了ViewPath + 图像 + 坐标的多重定位方案。得益于SDK集成的方式，我们的ViewPath可以记录更多的元素视图特征和执行不同的匹配策略。定位过程中会优先使用ViewPath进行目标控件检索，当目标控件查找异常时，会结合图像匹配和坐标匹配的方式进行兜底查找，来确保界面变化程度不大时，也能准确的查找到目标控件。 图9 图像识别示意图 手势定位的精准性： 有了基于控件的目标定位之后，对于一些常用简单操作手势，比如点击、长按、断言、甚至输入都可以做到很好的支持，只需要找到对应的控件，在控件所在位置下发相应的触摸事件即可。我们知道，App真正接收的触摸事件是屏幕上一个个精准的触摸点，在系统处理后，分发给当前App窗口，App在接收事件后再继续分发，直到找到事件的最佳响应者，后续通过响应者链对事件消化处理。那我们要还原一个触摸事件的坐标点要如何确定呢？由于我们确定的只有控件，所以这个点自然而然就成了控件的中心点了。 大多数情况下，这些都可以很好地进行工作，但是对于一些多响应控件重叠的情况，可能会产生预想不到的操作误差。为了解决这样的问题，我们把控件定位与坐标定位进行了结合：基于纯坐标的定位是一种定位精准度非常高的定位方式，但是稳定性非常差，只有在屏幕分辨率完全一致且回放页面控件位置完全一致的情况下，才具备足够的可靠性，但这往往是不现实的，对测试环境机器量要求过高。 基于控件的定位，又存在着精准度不够的问题。使用坐标定位，如果定位区域足够小的话，那么受屏幕尺寸的影响就会越小，只需要确定在小范围内的相对位置即可。而基于控件目标的定位，恰恰可以把目标区域缩小到一个指定区域，我们刚好可以将二者结合起来，同时解决定位精准度和稳定性的问题。 对于复杂手势的支持，我们同样可以采用微分的方式，将一个复杂手势拆成多个简单手势的组成，比如我们可以将一个滑动操作的定位拆成两个部分：起始位置和终止位置，而这两个位置的定位，就变成了两个普通的单点手势操作定位了，可以通过上面提到的一个目标控件+相对坐标的形式进行定位。核心思想都是将基于屏幕坐标点的定位操作，缩小的目标控件的区域范围内，以达到不受设备分辨率的影响，实现操作行为一致的效果。 图10 手势识别示意图 4.5 可溯源的自动化测试 测试全流程记录，问题溯源一键即达。 测试的目的是保证App运行的稳定，测试过程中出现Bug导致测试未通过时，需要溯源问题原因，发生的场景，乃至具体的执行步骤。这也是大多自动化测试工具/平台所欠缺的，即使发现了问题，排查工作也很困难；这个问题在手工测试的时候，更为严重，往往因为很多缺陷无法复现而难以定位。 AlphaTest的自动化用例最小执行单元是操作指令，我们将测试过程的每一条指令的执行状况和过程中的界面快照进行了记录，并在指令执行失败时，对异常原因进行了初步分析。然后将整个用例的执行组合成了一份完整的测试报告，可快速溯源问题步骤。除此之外，我们还增加大量的日志上报，并将整个用例测试过程进行了视频录制，来进一步帮助疑难问题的排查。真正做到了用例回放测试可溯源。 图11 回放报告-图文详情 4.6 用例的维护 自动化用例需要持续地投入人力来维护么？架构升级，页面重构，用例需要全部重新录制么？ 因自动化工具/平台众多，阻碍长期落地使用的一大问题是用例维护成本高，很多工具/平台让我们即便是使用上了自动化，但还需要持续投入人力维护用例的更新，最终的提效收益微乎其微。对于用例更新维护，我们可以梳理划分成三个场景： 需求发生重大变更，整体的业务执行流程及相关的校验点都需要进行大量的调整。对于这种情况，无论是何种自动化测试工具/平台，都是需要正常进行用例变更重录以适应新的需求。 需求发生略微变更，业务流程基本一致，需要调整的校验点、操作以及数据或不影响整体流程的步骤。对于此场景，AlphaTest通过指令编辑器与操作录制，支持指令增删改以及数据和场景的还原，帮助用户快速的进行用例调整，而无需重新录制用例。例如：修改网络数据字段、视图变更路径、断言替换目标等。 图14 指令编辑 和业务需求不同，我们的技术实现也会发生迭代。随着App技术架构不断的演进，经常会面临着架构升级，页面重构甚至技术栈变迁等这样的技术升级。这些变动需要覆盖大量的测试用例，其中大量的自动化用例又可能会因为变动而导致失效，需要重新录制。为此，AlphaTest设计一套利用相近分辨率机器进行用例自动修正的功能：利用图像 + 坐标进行二次识别定位，元素定位成功并校验通过后，生成新的ViewPath，更新对应的用例指令，对用例进行自动修复，修复后可在任意回放。 图15 自修复能力 4.7 跨App回放用例 同一份代码运行在不同的App上，是否需要重新编写多份用例？ 美团系的一些业务可能会复用在多个App上。比如外卖有独立App，但同时也要复用到美团和点评App上，这些功能，几乎共用一份代码，而测试人员却不得不对每个App上的业务功能都进行测试，维护多份用例。由于业务本身实现是一致的，那我们可以通过适配不同App之间的差异，来让一个业务Case可以横跨多个App回放，这便可以将成本缩减好几倍，这些差异主要体现在： 前置条件和初始页面 ：业务的初始页面进入路径不同，例如外卖App打开App就进入到了外卖首页，但是在美团App中就需要从美团首页跳转到外卖频道。同时由于不同App的样式风格、设计规范、业务特性等因素，也会造成首页代码逻辑和视图层级的差异。 AB实验配置 ：不同App所配置的实验可能不同，不同的实验会导致不同的样式和代码逻辑。 网路接口映射 ：不同App中相同业务场景涉及的接口有所不同。 页面Scheme映射 ：不同App中相同页面的跳转Scheme也不相同。 AlphaTest平台支持App维度各项差异数据配置，当SDK检测用例回放环境与录制环境不一致时，会自动进行映射适配，从而让用例运行到了不同App上。 4.8 埋点的录制回放除了功能测试，我们在日常开发和测试的工作中，还会面临另外一个比较重要的问题就是埋点测试。因此，我们在自动化的基础上扩展出埋点自动化测试。埋点自动化测试的核心思想是，通过对比录制时期和回放时期的埋点上报时机和上报参数进行判断。为了保证埋点自动化测试的稳定性，我们主要采用以下的障机制： 图16 埋点自动化测试示意图 字段规则配置 ：埋点自定义参数千姿百态，甚至有些字段每次代码执行都不一致，如果进行完全匹配结果注定是失败的，所以我们在AlphaTest平台提供了埋点字段规则配置功能，通过人为设置的方式来避免埋点自定义参数校验失败。App重启进入录制状态时，用户就可以操作App，平台会记录用户的操作行为，当产生相应的埋点日志的时候会将日志信息打印在日志区域（ 如下图17所示 ），在该过程中也会对埋点日志进行一定的校验。重点将操作时机、埋点日志一并保存到服务端。 图17 埋点上报数据控制台打印 埋点时机校验 ：针对时机校验，程序并不支持埋点曝光的”1px曝光”，”下拉刷新曝光”，”页面切换曝光”，”切前后台曝光”这些规则，主要的原因是每一个业务方在对埋点曝光的规则都是不一致的，而且该规则的实现会极大耦合业务代码。在针对时机校验我们目前只支持： [1] 点击埋点上报时机校验，程序通过事件监听和埋点类型信息来判断点击埋点上报的时机是否是在点击的操作下产生的，如果不是则报错。 [2] 埋点重复上报校验，针对一般情况下用户一次操作不会产生两个相同的埋点上报，所以程序会校验某个事件下发生的所有埋点日志进行一一校验，检测是否具有2个或多个埋点日志完全一致，如有发生则会上报错误。 结果校验 ：回放完成后，我们会对比录制和回放时的埋点数据，根据配置好的字段规则校验埋点上报是否符合预期。 图18 埋点校验流程图 5. 测试流程 AlphaTest的核心测试流程始终聚焦在用例的录制与回放环节，整个流程涉及到自动化任务触发、回放集群调度、断言服务、消息推送等核心模块。 以UI自动化和埋点自动化的流程为例，AlphaTest以业务团队为基本单元，可以和各团队的测试用例进行关联，定时同步状态。同时利用需求评审线上化做为基础，将自动化用例和研发流程中的PR、集成打包、二轮回归等节点相结合，定时触发自动化用例并将结果报告推送给相关负责人。 图19 建设自动化测试流程闭环 录制用例： [1] 首先在AlphaTest平台选择要录制的测试用例，打开待测试App进行扫码即可进入用例待录制状态，此时可以设置用例需要的前置条件（ 账号信息、Mock数据、定位信息等 ），之后点击开始按钮后，手机便会自动重启，开始录制。 [2] 用户按照测试用例步骤，正常操作手机，AlphaTest会将用户的操作行为全部记录下来，并自动生成语义化的描述语言显示在AlphaTest平台上，与此同时产生的网络数据、埋点数据等校验信息也会一并存储下来。 [3] 在录制的过程中可以快捷的打开断言模式，将页面上想要校验的元素进行文本提取/截图等操作记录下来，用于后续回放过程中对相同元素进行校验。 [4] 测试步骤全都执行完毕后，点击保存按钮即可生成本条自动化用例。 用例回放： [1] 扫描对应自动化用例的二维码即可进行回放，回放过程中会将用户录制的行为、网络数据进行一比一还原，并且辅助有全过程视频录像，用于后续问题排查和溯源。 [2] 回放过程中碰到断言事件时，会将断言的元素进行文本提取/截图，上传至AlphaTest平台。回放完成后，会将回放时候的断言截图和录制时的断言截图进行图像对比，作为整个测试结果的一项。 [3] 回放过程中的埋点数据也会一并记录下来，并和录制时候的埋点数据和上报时机进行对比，自动提取出其中的差异项。 [4] 回放完成后，会生成完整的测试报告并将结果通过OA推送至相关人员。 回放计划 ：二轮回归测试中，回放用例数量多达几百条，为了做到全流程的自动化，我们提供了回放计划的概念，可以将多个自动化用例进行编组管理，每一组就是一个回放计划。触发一个计划的回放即可自动触发计划内的所有自动化用例。整个计划都执行完成后，会通知到指定的计划负责人或群组。 5.1 自动化任务触发在整个外卖C端敏捷迭代的流程中，打包平台主要承接了业务需求发起到需求交付的流程，作为AlphaTest的上游平台，可以提供打包信息并触发自动化用例回放任务。以下简单展示AlphaTest与敏捷协同平台的交互流程： 图20 AlphaTest与敏捷协同平台交互流程图 5.2 回放集群调度整个测试过程真正的解放双手，才能算的上是自动化。因此，我们着手搭建了自己的自动化机器集群，可以 24小时不间断的执行测试任务。为了保证任务回放能够顺利完成，我们在不同阶段增加了相应的保活策略。在极大程度上提高了任务执行完毕的成功率。 执行流程 ：回放任务通过用户在平台手动触发或者二轮自动触发。新增的回放任务经过任务拆分系统拆分成n个子任务，加入到不同设备的回放任务队列中。每个子任务经过占用设备-&gt;安装待测App-&gt;应用授权-&gt;打开scheme-&gt;上报结果等步骤完成回放操作。 节点保活机制 ：针对回放流程中每一个节点，失败后进行N（ 默认为3 ）次重试操作。减少因网络波动，接口偶现异常导致的回放失败数量。 子任务保活机制 ：每个回放流程，失败后进行N（ 默认为3 ）次断点重试。减少因设备异常，SDK心跳上报异常导致的回放失败数量。 父任务保活机制 ：一个父任务会被拆分成N个子任务，当其中的一个子任务S1在节点保活机制和子任务保活机制下仍然执行失败之后，父任务保活机制会尝试将子任务S1中未执行完毕的用例转移到其他活跃状态的子任务中。减少因设备异常，设备掉线等问题导致的回放失败数量。 图21 机器集群 5.3 断言服务用例断言是整个自动化用例验证的核心步骤，我们的断言服务依据用例的实际情形可以分别进行文字与图像的断言。其中图像断言服务依托于自建的图像对比算法服务，可以高效进行录制回放断言图像的对比，图像对比准确率可以达到99%以上。 录制阶段： [1] 录制时增加断言决策信息的自动采集。 [2] 和正常流程一样，提取区域的截图信息。 [3] 如果是文本组件，则提取文本内容，如果是图片组件，则提取图片二进制编码或图片URL，同时提取区域内的布局信息。 回放阶段： [1] 回放时，提取和录制时一致的内容（ 文本信息、图片编码、区域截图、布局信息 ）。 [2] 将回放时的断言信息上传至AlphaTest平台。 [3] AlphaTest平台对断言结果进行校验，首先是基于模型的图像对比，如果判定为一致，则直接标记结果。 [4] 如果判定为不一致、则匹配“断言失败数据集”，如果能够匹配上，则标记结果。如果匹配不上，则需要人工选择匹配类型。 [5] 匹配类型为“文本校验”、“根据图片信息校验”、“人工校验”。如果前两项判定为一致，则直接标记结果。如果“人工校验”的结果为确实两张图不一致，则直接标记结果，结束。 [6] 如果“人工校验”结果为一致，既上述所有判定都不准确，则需要人工对两张图中判定错误的原因进行分类（ 具体类型待定 ），同时将断言存储到失败数据集。 [7] 模型自动训练，当数据集超过一定的阈值、通过定时触发、或者手动触发的方式，触发模型自动训练，训练完成后自动部署到AlphaTest平台，不断迭代。 图像服务 ：图像对比模型采用基于度量学习的对比算法，将图像对的一致性判别转换为图像语义的相似度量问题。度量学习（ Metric Learning ），也称距离度量学习（ Distance Metric Learning，DML ）属于机器学习的一种。其本质就是相似度的学习，也可以认为距离学习。因为在一定条件下，相似度和距离可以相互转换。比如在空间坐标的两条向量，既可以用余弦相似度的大小，也可以使用欧式距离的远近来衡量相似程度。度量学习的网络采用经典的Siamese结构，使用基于resnext50的主干网络提取图像的高级语义特征，后接spplayer完成多尺度特征融合，融合后的特征输出作为表达图像语义的特征向量，使用ContrastiveLoss进行度量学习。 图22 训练过程 [1] 预训练过程 ：resnext50网络是使用ImageNet的预训练模型。 [2] 数据增强 ：为增加数据的丰富性、提高网络的泛化性能，数据增强的方式主要包括：图像右下部分的随机剪切和添加黑色蒙层（ 相应改变图像对的标签 ）。这种数据增强符合控键截图实际情况，不会造成数据分布的改变。 [3] 对比损失 ：对比损失函数采用ContrastiveLoss，它是一种在欧式空间的pair based loss，其作用是减少一致图像对距离，保证不一致图像对的距离大于margin，其中margin=2。 图23 训练过程 [4] 相似度量 ：相似度量也是采用计算图像对特征向量的欧式距离的方法，并归一化到区间[0, 1]，作为输出的图像对相似度。 5.4 消息推送消息推送作为回放流程的最终环节，我们依赖于美团内部自建的消息队列服务与OA SDK消息推送能力，可以进行测试报告的实时推送。在此之上，还可以针对不同团队的推送诉求，做消息模板的定制化。 消息定制 ：消息推送与触达的核心，是满足业务诉求；不同业务对自动化测试报告中各项指标的关注点不同，这就需要AlphaTest具备消息推送定制的能力；将消息推送的模板以配置文件的形式提供出来，不同的业务使用不同的业务消息配置文件；再利用OA提供的图文、多媒体等消息推送能力，可以将自动化测试报告的各项指标自定义拆分；除此之外，消息还需要减少冗余，在这个信息泛滥的时代，我们愿意为无孔不入的消息、通知做减法，只将最重要、最核心的消息推送给最需要的人，既可以推动自动化测试流程的高效流转，又可以让各相关业务人员享受到自动化测试能力的便捷性。 一键触达 ：以往的研发人员冒烟测试，主要依赖于测试人员在用例管理平台建立测试计划，研发人员根据用例进行手工用例测试结果标记，之后去提测完成后续流程。这中间缺失的主要环节是，难以对研发人员冒烟测试的质量进行把控。而AlphaTest正可以解决此问题，流程转换为，研发人员在敏捷协同平台触发一键提测流程，调用AlphaTest的自动化测试能力对冒烟用例进行自动化测试回归，完成之后将测试生成的测试报告同步提测平台，作为研发人员冒烟的结论依据，同时在冒烟过程中发生的问题，也可以及时通知到对应的研发人员与测试人员进行改正。既保证了质量，又避免了人力空耗。 6. 落地与实践 外卖C端主要承担了用户在App端点餐、下单、配送的所有核心流程，场景繁多、业务复杂，这也给测试人员的版本测试带来了诸多挑战，其中最核心也最耗费人力的便是二轮回归测试环节。目前，C端采用的双周敏捷迭代的开发方式，每个迭代周期给测试人员用来进行二轮核心流程回归的时间为三天，为此C端测试团队投入了许多人力资源，但即便如此，仍难以覆盖全部流程；而AlphaTest的设计初衷也正是为解决此问题——UI测试流程全覆盖及自动化验证。 6.1 业务共建用例的转化与维护 AlphaTest 在外卖C端测试团队的落地初期，我们采用了共建的模式，也就是业务研发人员与对应测试人员共同来进行用例录制与维护的工作；推荐这种工作模式的核心原因是，在C端功能迭代流程中的二轮周期的原有工作模式为，研发人员进行二轮冒烟测试，完成测试之后提交二轮包交由测试人员进行二轮回归测试，所以这本来就是一个双方都需要参与的环节；而二轮测试作为版本上线前的最重要一个测试流程，保证核心流程的正常也是测试人员与研发人员所关心重点。 经过多轮的使用与磨合之后，这种模式被证明是行之有效的，在整个C端二轮用例的转化过程中，测试人员主要负责了用例的录制与迭代流程，研发人员则主要负责版本回放数据的统计及问题用例的发现与解决。 外卖二轮落地情况 目前，AlphaTest已经在外卖多个业务落地，支持了大于15个版本的二轮回归测试，用例覆盖率达到70%。现已 覆盖了Native、Mach、React Native、美团小程序、H5 技术栈的测试工作，能力上可进行支持： UI自动化测试、埋点自动化测试、动态化加载成功率自动化测试、无障碍适配率自动化测试。 未来，我们会朝着“智能化”和“精准化”两个方向探索，覆盖更多测试场景的同时，更进一步提升测试人效。 6.2 实践效果 7. 参考资料[1] https://appium.io [2] http://docs.seleniumhq.org/projects/webdriver [3] http://airtest.netease.com/index.html [4] https://github.com/alipay/SoloPi ———- END ———- 也许你还想看 | Spock单元测试框架以及在美团优选的实践 | 美团智能支付稳定性测试实战 | Lego：美团接口自动化测试实践 阅读更多 前端 | 算法 | 后端 | 数据 安全 | Android | iOS | 运维 | 测试","categories":[],"tags":[{"name":"转载","slug":"转载","permalink":"https://imlike.cc/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"java","slug":"java","permalink":"https://imlike.cc/tags/java/"}]},{"title":"Lombok注解@Data和@Builder冲突(转载)","slug":"Lombok注解-Data和-Builder冲突","date":"2022-11-20T09:29:17.000Z","updated":"2024-03-14T14:55:29.474Z","comments":true,"path":"75bc1ba6.html","link":"","permalink":"https://imlike.cc/75bc1ba6.html","excerpt":"","text":"🔗 原文链接： https://mp.weixin.qq.com/s/wRkQVpZz… ⏰ 剪存时间：2022-11-20 17:29:28 (UTC+8) ✂️ 本文档由 飞书剪存 一键生成 方志朋 2022-11-18 16:00 发表于 广东 点击关注公众号，Java干货 及时送达 👇 来源： juejin.cn/post/7103011031672176677 问题背景 Lombok @Data和@Builder分别单独分析用法 解决方法 方法一 方法二 Lombok原理 总结 问题背景Lombok使⽤ 同时使⽤@Data和@Builder ，构建无参构造器报错！编译不通过。如下图： 图片 Lombok @Data和@Builder分别单独分析用法Lombok使⽤@Data可以⽣成⽆参构造和类⾥⾯所有属性的getter/setter⽅法。可以简化我们代码的开发。（需要安装Lombok插件和引⼊Lombok依赖）。 例如下⾯的⼀个实体类,引⼊Lombok后，可以⾃动⽣成GET/SET⽅法和⽆参构造函数。 编译后的class为:可以看到不仅帮我们生成了get和set ，同时也有默认的无参构造器 图片 那么怎么自动生成有参构造器呢？使用@Builder注解，将会帮助我们⽣成全属性的构造⽅法。 图片 编译后的class为：可以看到 已经帮我们构建好了全属性的构造方法，但是如果值只引用@Builder注解是无法生成get和set的。 图片 但是如果同时使⽤@Data和@Builder的话，可以看出尽管⽣成了GET/SET⽅法，但是⽆参构造⽅法没有了，这显然是不能接受的，因为很多框架都会调⽤⽆参构造去创建对象。 图片 编译后的class： 我们尝试在Tet1类，⼿动添加⽆参构造⽅法。编译发现报错不通过： 图片 解决方法方法一Lombok同时使⽤@Data和@Builder的时候，如果要⽣成⽆参构造，需要在代码⾥⾯⼿动引⼊注解@Tolerate，让Lombok在⽣成类的时候，对指定的构造函数不感知。 图片 方法二直接使用无参构造器+有参构造器的方式，@RequiredArgsConstructor 来构建有参，@NoArgsConstructor来构建无参构造器，如图所示： 编 译后效果： 图片 Lombok原理Java的编译分为以下⼏个阶段： 解析与填充符号表-&gt;注解处理-&gt;分析与字节码⽣成-&gt;⽣成⼆进制class⽂件。 Lombok 使⽤的是 JDK 6 实现的 JSR 269: Pluggable Annotation Processing API (编译期的注解处理器)，它是在编译期时把 Lombok 的注解代码，转换为常规的 Java ⽅法⽽实现注⼊。 在编译期阶段，当 Java 源码被抽象成语法树 (AST) 之后，Lombok 会根据⾃⼰的注解处理器动态的修改AST，增加新的代码 (节点)，在这⼀切执⾏之后，再通过分析⽣成了最终的字节码 (.class) ⽂件，这就是Lombok 的执⾏原理。 可以借助注解处理器实现⼀个简单的 Setter，我们的实现步骤是: ⾃定义⼀个注解标签接⼝，并实现⼀个⾃定义的注解处理器； 利⽤ tools.jar 的 javac api 处理 AST (抽象语法树)3. 使⽤⾃定义的注解处理器编译代码。 1.定义⾃定义注解和注解处理器 ⾸先创建⼀个 MySetter.java ⾃定义⼀个注解，代码如下： 图片 再实现⼀个⾃定义的注解处理器，代码如下： 图片 图片 图片 测试类如下: 图片 2.对注解处理器进⾏编译，随后使⽤注解处理器对类进⾏编译 ⾸先需要先对注解处理器进⾏编译（javac -cp ⽤于引⼊第三⽅jar包进⾏编译） 图片 然后使⽤注解处理器对这个Person测试类进⾏编译: 这时候再看⽣成的Person.class，可以发现Setter⽅法已经⽣成了: 图片 图片 总结当然尽管测试类已经⽣成Setter⽅法，但是因为是在编译时期⽣成的，因此我们在开发的时候是没法直接调⽤Setter⽅法的，因此Lombok提供了插件机制，⽅便我们在开发的时候可以直接去调⽤Lombok的特性。 热门内容： 业务开发时，接口不能对外暴露怎么办？ 我，40岁码农，还在荷兰写低级代码，不敢回国… 面试官问：select……for update会锁表还是锁行？ 就加了一行log日志，结果引发了 P1 线上事故… 别用XShell了，这款SSH工具绝对惊艳，还支持网页版….. 最近面试BAT，整理一份面试资料《Java面试BAT通关手册》，覆盖了Java核心技术、JVM、Java并发、SSM、微服务、数据库、数据结构等等。 获取方式：点“在看”，关注公众号并回复 666 领取，更多内容陆续奉上。 明天见(｡･ω･｡)","categories":[],"tags":[{"name":"转载","slug":"转载","permalink":"https://imlike.cc/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"java","slug":"java","permalink":"https://imlike.cc/tags/java/"}]},{"title":"integrate_saml","slug":"integrate-saml","date":"2022-11-14T09:56:50.000Z","updated":"2024-03-14T14:55:29.478Z","comments":true,"path":"ae50aff9.html","link":"","permalink":"https://imlike.cc/ae50aff9.html","excerpt":"","text":"Spring Boot + Spring Security with SAML 2.0 | Digital Software Architecture okta-sdk-java/integration-tests at master · okta/okta-sdk-java Okta deployment models — redirect vs. embedded OAuth Vs. OpenID? Which is better?","categories":[],"tags":[]},{"title":"2022个人博客网站搭建","slug":"2022个人博客网站搭建","date":"2022-11-09T13:21:04.000Z","updated":"2024-03-14T14:55:29.473Z","comments":true,"path":"df34252.html","link":"","permalink":"https://imlike.cc/df34252.html","excerpt":"","text":"Life after Heroku: What’s a dev to do? - Reaktor Fly.io 让您可以部署 Docker 映像，还支持免费设置 PostgreSQL 数据库。他们的免费套餐包括 256MB RAM 和 3GB 持久存储，没有休眠的测功机——这比 Heroku 的要好。虽然你很可能需要一个 Dockerfile，但这并不难，而且学习 Docker 是你未来项目的一项很好的可转移技能。此外，作为一个巨大的奖励，每个组织都定义了免费层级，因此您可以为每个项目拥有单独的免费层级配额！他们基于 Web 的工具以及 CLI 都很好用。 Vercel 和 Netlify 都允许您部署静态站点和 lambda 函数，其中 Vercel 专门为部署 Next.js 应用程序（来自同一家公司的 React 框架）量身定制。除了慷慨的免费套餐外，Vercel 还拥有非常好的用户体验。作为顶部的樱桃，两者都自动为 Github 上的每个 Pull Request 构建一个预览站点。 Render 有一个类似于 Heroku 历史上的免费层：512MB RAM；服务在 15 分钟不活动后停止，这意味着冷启动延迟；您帐户中所有免费 Web 服务的每月最长运行时间为 750 小时（一个月大约有这么多小时）；静态网站是免费的。 GitHub Pages 和 Actions 也是静态站点的流行选项。 Heroku 可能仍然是您的首选。爱好 dynos 每月 7 美元，您可以升级并保持您的项目在原处并不间断地运行（除了您的钱包）。当然也可以使用更高的等级。 我之前用过Github Pages,Coding的相关博客搭建现在要么不提供服务,要么被墙,看到现在还支持的服务就上述几个,我目前使用的是vervel还算稳定.","categories":[],"tags":[]},{"title":"jackson-redis-集成","slug":"jackson-redis-集成","date":"2022-11-08T13:50:38.000Z","updated":"2024-03-14T14:55:29.478Z","comments":true,"path":"ef9a1729.html","link":"","permalink":"https://imlike.cc/ef9a1729.html","excerpt":"","text":"All You Need To Know About JSON Parsing With Jackson 对应spring 集成redis来实现,需要主要在序列化 对象过程中,如果使用Jackson来序列化需要注意Jackson相关的配置.目前就先补充这么多 下面是摘录官网的文章来大概展示然后集成最后会补充注意事项 Introduction to Spring Data Redis | Baeldung , Spring Data Redis 最佳实践！ - 掘金 Spring Cache 简介 当Spring Boot 结合Redis来作为缓存使用时，最简单的方式就是使用Spring Cache了，使用它我们无需知道Spring中对Redis的各种操作，仅仅通过它提供的@Cacheable 、@CachePut 、@CacheEvict 、@EnableCaching等注解就可以实现缓存功能。 常用注解@EnableCaching开启缓存功能，一般放在启动类上。 @Cacheable使用该注解的方法当缓存存在时，会从缓存中获取数据而不执行方法，当缓存不存在时，会执行方法并把返回结果存入缓存中。一般使用在查询方法上，可以设置如下属性： value：缓存名称（必填），指定缓存的命名空间； key：用于设置在命名空间中的缓存key值，可以使用SpEL表达式定义； unless：条件符合则不缓存； condition：条件符合则缓存。 @CachePut使用该注解的方法每次执行时都会把返回结果存入缓存中。一般使用在新增方法上，可以设置如下属性： value：缓存名称（必填），指定缓存的命名空间； key：用于设置在命名空间中的缓存key值，可以使用SpEL表达式定义； unless：条件符合则不缓存； condition：条件符合则缓存。 @CacheEvict使用该注解的方法执行时会清空指定的缓存。一般使用在更新或删除方法上，可以设置如下属性： value：缓存名称（必填），指定缓存的命名空间； key：用于设置在命名空间中的缓存key值，可以使用SpEL表达式定义； condition：条件符合则缓存。 使用步骤 在pom.xml中添加项目依赖： 123456&lt;!--redis依赖配置--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt;复制代码 修改配置文件application.yml，添加Redis的连接配置； 12345678spring: redis: host: 192.168.6.139 # Redis服务器地址 database: 0 # Redis数据库索引（默认为0） port: 6379 # Redis服务器连接端口 password: # Redis服务器连接密码（默认为空） timeout: 1000ms # 连接超时时间复制代码 在启动类上添加@EnableCaching注解启动缓存功能； 12345678910@EnableCaching@SpringBootApplicationpublic class MallTinyApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(MallTinyApplication.class, args); &#125;&#125;复制代码 接下来在PmsBrandServiceImpl类中使用相关注解来实现缓存功能，可以发现我们获取品牌详情的方法中使用了@Cacheable注解，在修改和删除品牌的方法上使用了@CacheEvict注解； 1234567891011121314151617181920212223242526272829/** * PmsBrandService实现类 * Created by macro on 2019/4/19. */@Servicepublic class PmsBrandServiceImpl implements PmsBrandService &#123; @Autowired private PmsBrandMapper brandMapper; @CacheEvict(value = RedisConfig.REDIS_KEY_DATABASE, key = &quot;&#x27;pms:brand:&#x27;+#id&quot;) @Override public int update(Long id, PmsBrand brand) &#123; brand.setId(id); return brandMapper.updateByPrimaryKeySelective(brand); &#125; @CacheEvict(value = RedisConfig.REDIS_KEY_DATABASE, key = &quot;&#x27;pms:brand:&#x27;+#id&quot;) @Override public int delete(Long id) &#123; return brandMapper.deleteByPrimaryKey(id); &#125; @Cacheable(value = RedisConfig.REDIS_KEY_DATABASE, key = &quot;&#x27;pms:brand:&#x27;+#id&quot;, unless = &quot;#result==null&quot;) @Override public PmsBrand getItem(Long id) &#123; return brandMapper.selectByPrimaryKey(id); &#125;&#125; 配置Redisconfig类 我们可以通过给RedisTemplate设置JSON格式的序列化器，并通过配置RedisCacheConfiguration设置超时时间来实现以上需求，此时别忘了去除启动类上的@EnableCaching注解，具体配置类RedisConfig代码如下； 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/** * Redis配置类 * Created by macro on 2020/3/2. */@EnableCaching@Configurationpublic class RedisConfig extends CachingConfigurerSupport &#123; /** * redis数据库自定义key */ public static final String REDIS_KEY_DATABASE=&quot;mall&quot;; @Bean public RedisTemplate&lt;String, Object&gt; redisTemplate(RedisConnectionFactory redisConnectionFactory) &#123; RedisSerializer&lt;Object&gt; serializer = redisSerializer(); RedisTemplate&lt;String, Object&gt; redisTemplate = new RedisTemplate&lt;&gt;(); redisTemplate.setConnectionFactory(redisConnectionFactory); redisTemplate.setKeySerializer(new StringRedisSerializer()); redisTemplate.setValueSerializer(serializer); redisTemplate.setHashKeySerializer(new StringRedisSerializer()); redisTemplate.setHashValueSerializer(serializer); redisTemplate.afterPropertiesSet(); return redisTemplate; &#125; @Bean public RedisSerializer&lt;Object&gt; redisSerializer() &#123; //创建JSON序列化器 Jackson2JsonRedisSerializer&lt;Object&gt; serializer = new Jackson2JsonRedisSerializer&lt;&gt;(Object.class); ObjectMapper objectMapper = new ObjectMapper(); objectMapper.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY); objectMapper.enableDefaultTyping(ObjectMapper.DefaultTyping.NON_FINAL); serializer.setObjectMapper(objectMapper); return serializer; &#125; @Bean public RedisCacheManager redisCacheManager(RedisConnectionFactory redisConnectionFactory) &#123; RedisCacheWriter redisCacheWriter = RedisCacheWriter.nonLockingRedisCacheWriter(redisConnectionFactory); //设置Redis缓存有效期为1天 RedisCacheConfiguration redisCacheConfiguration = RedisCacheConfiguration.defaultCacheConfig() .serializeValuesWith(RedisSerializationContext.SerializationPair.fromSerializer(redisSerializer())).entryTtl(Duration.ofDays(1)); return new RedisCacheManager(redisCacheWriter, redisCacheConfiguration); &#125;&#125;复制代码 此时我们再次调用获取商品详情的接口进行测试，会发现Redis中已经缓存了标准的JSON格式数据，并且超时时间被设置为了1天。 Finaly重中之重就是 序列化要注意 作者：MacroZheng链接：https://juejin.cn/post/6844904094545477640来源：稀土掘金著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","categories":[],"tags":[{"name":"转载","slug":"转载","permalink":"https://imlike.cc/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"java","slug":"java","permalink":"https://imlike.cc/tags/java/"}]},{"title":"mysql相关内容","slug":"mysql相关内容","date":"2022-11-01T09:19:36.000Z","updated":"2024-03-14T14:55:29.479Z","comments":true,"path":"f0e475b6.html","link":"","permalink":"https://imlike.cc/f0e475b6.html","excerpt":"","text":"1234567891011121314SHOW DATABASES &#x2F;&#x2F;列出 MySQL Server 数据库。SHOW TABLES [FROM db_name] &#x2F;&#x2F;列出数据库数据表。SHOW CREATE TABLES tbl_name &#x2F;&#x2F;导出数据表结构。SHOW TABLE STATUS [FROM db_name] &#x2F;&#x2F;列出数据表及表状态信息。SHOW COLUMNS FROM tbl_name [FROM db_name] &#x2F;&#x2F;列出资料表字段SHOW FIELDS FROM tbl_name [FROM db_name]，DESCRIBE tbl_name [col_name]。SHOW FULL COLUMNS FROM tbl_name [FROM db_name]&#x2F;&#x2F;列出字段及详情SHOW FULL FIELDS FROM tbl_name [FROM db_name] &#x2F;&#x2F;列出字段完整属性SHOW INDEX FROM tbl_name [FROM db_name] &#x2F;&#x2F;列出表索引。SHOW STATUS &#x2F;&#x2F;列出 DB Server 状态。SHOW VARIABLES &#x2F;&#x2F;列出 MySQL 系统环境变量。SHOW PROCESSLIST &#x2F;&#x2F;列出执行命令。SHOW GRANTS FOR user &#x2F;&#x2F;列出某用户权限 1234567891011121314SHOW DATABASES SHOW TABLES FROM DB_NAMESHOW COLUMNS FROM select * from information_schema.columns where table_schema&#x3D;&#39;DB_NAME&#39;select table_name,column_name,collation_name,character_set_name from information_schema.columns where table_schema&#x3D;&#39;DB_NAME&#39; select table_name,column_name,column_name,collation_name,character_set_name from information_schema.columns where table_schema&#x3D;&#39;DB_NAME&#39; and column_name like &#39;%description&#39;select * from information_schema.columns where table_schema&#x3D;&#39;DB_NAME&#39; and column_name like &#39;%description&#39;select table_name,column_name,collation_name,character_set_name from information_schema.columns where table_schema&#x3D;&#39;DB_NAME&#39; and COLUMN_NAME like &#39;%a%&#39; 查询逻辑和相关 编码集","categories":[],"tags":[]},{"title":"mysql索引和锁相关的整理","slug":"mysql索引和锁相关的整理","date":"2022-09-25T08:34:34.000Z","updated":"2024-03-14T14:55:29.479Z","comments":true,"path":"2fd84806.html","link":"","permalink":"https://imlike.cc/2fd84806.html","excerpt":"","text":"ToDo MySQL :: MySQL 5.7 Reference Manual :: 14.7.2.1 Transaction Isolation Levels 针对MySQL5,.7","categories":[],"tags":[]},{"title":"spring-data-jap入门和测试用例以及介绍","slug":"spring-data-jap入门和测试用例以及介绍","date":"2022-09-25T02:46:15.000Z","updated":"2024-03-14T14:55:29.481Z","comments":true,"path":"db9f3902.html","link":"","permalink":"https://imlike.cc/db9f3902.html","excerpt":"","text":"springJpa介绍springjpa 是springdata下面的框架用来对接数据 是门面框架定义 数据交互到数据库,很多的接口规范使用的是 JPA规范 *javax.persistence.** ,我们选择内部实现为habernate ,具体的实现可以选择xxxx等, 下面是关于spring -orm的介绍Spring - ORM Framework - GeeksforGeeks Spring-ORM is a technique or a Design Pattern used to access a relational database from an object-oriented language. ORM (Object Relation Mapping) covers many persistence technologies. They are as follows: JPA(Java Persistence API): It is mainly used to persist data between Java objects and relational databases. It acts as a bridge between object-oriented domain models and relational database systems. JDO(Java Data Objects): It is one of the standard ways to access persistent data in databases, by using plain old Java objects (POJO) to represent persistent data. Hibernate – It is a Java framework that simplifies the development of Java applications to interact with the database. Oracle Toplink, and iBATIS: Oracle TopLink is a mapping and persistence framework for Java development. 需要了解常用的注解 支持基于 Spring 和 JPA 构建存储库 支持Querydsl谓词，因此支持类型安全的 JPA 查询 透明审计 分页支持、动态查询执行、集成自定义数据访问代码的能力 @Query在引导时验证带注释的查询 支持基于 XML 的实体映射 通过引入基于 JavaConfig 的存储库配置@EnableJpaRepositories 字段映射12345678910@Table(name &#x3D; &quot;malfunction_report&quot;) &#x2F;&#x2F;标记表的名称@Entity：标识这是一个JPA的实体，当项目启动时会更具这个实体创建相应的表@Id &#x2F;&#x2F;标记表的自增的ID@GeneratedValue@Column(name &#x3D; &quot;items&quot;)&#x2F;&#x2F;标记字段的对应的映射@Convert(converter &#x3D; JpaConverterListJson.class)&#x2F;&#x2F; 保存和查询时能解析为对应的变量@Enumerated(EnumType.STRING)&#x2F;&#x2F;对应枚举这种最后在数据库需要保存为你字符串 查询继承JpaRepository直接可以字段拼接一个简单的查询,在继承了JpaRepository后可以发现, 注解query使用注解可以使用jpa定义的hql来查询利用对象的映射关系,而且比较存在的ORM(Object-Relational Mapping)逻辑,同时如果部分字段需要调用特殊的函数也支持直接写SQL,需要在SQL后用逗号分割讲native设置为true 查询继承JpaSpecificationExecutor的灵活查询org.springframework.data.jpa.domain.Specification 需要实现toPredicate这里需要多个学习多个对象 Root root 主要要使用 CriteriaQuery&lt;?&gt; query criterialQuery ToDo CriteriaBuilder criteriaBuilder CriteriaBuilder todo 1234javax.persistence.criteria.Predicatejavax.persistence.criteria.Expression 上述对象能最大程度实现灵活的查询,加上 org.springframework.data.domain.Page可以最大程度的满足查询的需要 编辑和新增数据在自己实现的方法需要 @modify才会生效 spring-data-jpa-examples/PersonSpecificationsTest.java at master · pkainulainen/spring-data-jpa-examples Spring Data JPA Tutorial: Pagination Mockito’s Java 8 Features | Baeldung","categories":[],"tags":[]},{"title":"mac初始化环境配置加优化","slug":"mac初始化环境配置加优化","date":"2022-09-09T09:30:35.000Z","updated":"2024-03-14T14:55:29.479Z","comments":true,"path":"ea7eedfa.html","link":"","permalink":"https://imlike.cc/ea7eedfa.html","excerpt":"","text":"快捷键Cmd + Shift +。 （点） 显示隐藏文件 fn + command + enter insert快捷键 option + cmd +l 格式化 ctr + option +o 除去多的导入包 cmd+ + 增加 cmd+ - 缩小 cmd +n 新窗口 cmd+t 新标签页 cmd+w 退出当前页 cmd+ option + p 显示当前路径 ctrl+左右的箭头切换工作区 三指拖移动 技术右键加移动 发散手指 显示dashboard gradleinit.gradle 1234567891011121314151617181920212223allprojects&#123; repositories &#123; def ALIYUN_REPOSITORY_URL &#x3D; &#39;http:&#x2F;&#x2F;maven.aliyun.com&#x2F;nexus&#x2F;content&#x2F;groups&#x2F;public&#39; def ALIYUN_JCENTER_URL &#x3D; &#39;http:&#x2F;&#x2F;maven.aliyun.com&#x2F;nexus&#x2F;content&#x2F;repositories&#x2F;jcenter&#39; all &#123; ArtifactRepository repo -&gt; if(repo instanceof MavenArtifactRepository)&#123; def url &#x3D; repo.url.toString() if (url.startsWith(&#39;https:&#x2F;&#x2F;repo1.maven.org&#x2F;maven2&#39;)) &#123; project.logger.lifecycle &quot;Repository $&#123;repo.url&#125; replaced by $ALIYUN_REPOSITORY_URL.&quot; remove repo &#125; if (url.startsWith(&#39;https:&#x2F;&#x2F;jcenter.bintray.com&#x2F;&#39;)) &#123; project.logger.lifecycle &quot;Repository $&#123;repo.url&#125; replaced by $ALIYUN_JCENTER_URL.&quot; remove repo &#125; &#125; &#125; maven &#123; url ALIYUN_REPOSITORY_URL url ALIYUN_JCENTER_URL &#125; &#125;&#125; ~/.zshrc1234export JAVA_HOME&#x3D;&#x2F;Library&#x2F;Java&#x2F;JavaVirtualMachines&#x2F;zulu-11.jdk&#x2F;Contents&#x2F;Homeexport GRADLE_HOME&#x3D;&#x2F;Users&#x2F;XXX&#x2F;gradle-7.5export GRADLE_USER_HOME&#x3D;&#x2F;Users&#x2F;XXXX&#x2F;gradle-7.5&#x2F;repositoryexport PATH&#x3D;$PATH:$GRADLE_HOME&#x2F;bin 使用 软连接python3 到python python: aliased to /usr/local/bin/python3 V2RayX 提供三种模式：全局模式：如果网络流量服从操作系统的网络规则，V2RayX 会要求 macOS 将所有互联网流量路由到 v2ray 核心。 PAC 模式：macOS 将根据 pac 文件确定路由，某些流量可能会路由到 v2ray 核心。 手动模式：V2RayX 不会修改任何 macOS 网络设置，只会启动或停止 v2ray 核心。 菜单列表中的选项 Routing Rule 确定 v2ray 核心如何处理传入流量。核心路由规则适用于上述所有三种模式。","categories":[],"tags":[]},{"title":"程序员梗百科","slug":"程序员梗百科","date":"2022-07-07T06:03:58.000Z","updated":"2024-03-14T14:55:29.485Z","comments":true,"path":"88dbcb79.html","link":"","permalink":"https://imlike.cc/88dbcb79.html","excerpt":"","text":"转载自 开发常用的缩写 你能看懂几个？ LGTMLook Good To Me 对我来说，还不错 常见于 Code Review中，表示认可，同意merge合并代码到远程仓库。 ASAPAs Soon As Possible 尽快 WIPWork In Progress「进展中」常见词汇， 这里作为 Best Practice单独提出来， 主要针对改动较多的PR。 标题或Tag加上WIP，表示尚未完成，这样别人可以先review 已提交的部分 TL:DRToo Long.Didn’ t Read 「太长懒得看」 NPno problem 没问题 TBHpull Request 拉取请求 TPALPlease Take A Look 用来提示别人来看一下 TBHTo be honest.老实说 ACKAcknowledgement 「承认，确认，同意」 i.e.agreed/accepted change TBDTo Be Done(or Defined/Discussed/ Decided/Determined). 「未完成，将被做」","categories":[],"tags":[]},{"title":"lucene的demo使用ikAnalyzer","slug":"lucene-demo-ikAnalyzer","date":"2022-07-07T01:59:10.000Z","updated":"2024-03-14T14:55:29.479Z","comments":true,"path":"c3e283f6.html","link":"","permalink":"https://imlike.cc/c3e283f6.html","excerpt":"","text":"–2022-7-7补充 GeoQuery（一）-html Lucene Spatial构建地理空间索引 - 韩要奋斗 - 博客园 Lucene下面关于地理信息的内容 lucene多条件查询_三天打鱼_的博客-CSDN博客_lucene多条件查询 lucene和ik分词器使用Lucene-Spatial实现集成地理位置的全文检索 - 刺猬的温驯 - 博客园 Springboot下的Lucene(详细版)_程序猿开发日志【学习永无止境】-CSDN博客 Lucene搭建搜索引擎初探 | 大嘴怪的小世界 索引(Index) –&gt; 段(segment) –&gt; 文档(Document) –&gt; 域(Field) –&gt; 词(Term) https://hub.fastgit.org/blueshen/ik-analyzer.git 使用的ik分词器地址同时 12345678910111213141516171819202122232425262728293031323334353637383940 &lt;properties&gt;&lt;!-- &lt;lucene.version&gt;8.9.0&lt;/lucene.version&gt;--&gt; &lt;lucene.version&gt;7.2.1&lt;/lucene.version&gt; &lt;/properties&gt; &lt;!-- Lucene 核心库 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; &lt;artifactId&gt;lucene-core&lt;/artifactId&gt; &lt;version&gt;$&#123;lucene.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Lucene 解析库 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; &lt;artifactId&gt;lucene-queryparser&lt;/artifactId&gt; &lt;version&gt;$&#123;lucene.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Lucene 附加的分析库 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; &lt;artifactId&gt;lucene-analyzers-common&lt;/artifactId&gt; &lt;version&gt;$&#123;lucene.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; &lt;artifactId&gt;lucene-test-framework&lt;/artifactId&gt; &lt;version&gt;$&#123;lucene.version&#125;&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.wltea.ik-analyzer&lt;/groupId&gt; &lt;artifactId&gt;ik-analyzer&lt;/artifactId&gt; &lt;version&gt;9.0.0&lt;/version&gt; &lt;/dependency&gt; 使用luke来查询索引的数据 lucene集成ik和pinyin分词器 pinyin4j-2.5.1.jar pinyinAnalyzer4.3.1.jar 123456789101112131415 &lt;dependency&gt; &lt;groupId&gt;com.belerweb&lt;/groupId&gt; &lt;artifactId&gt;pinyin4j&lt;/artifactId&gt; &lt;version&gt;2.5.1&lt;/version&gt;&lt;!-- &lt;scope&gt;system&lt;/scope&gt;--&gt;&lt;!-- &lt;systemPath&gt;$&#123;project.basedir&#125;/src/main/resources/lib/pinyin4j-2.5.0.jar&lt;/systemPath&gt;--&gt; &lt;systemPath&gt;$&#123;pom.basedir&#125;/src/main/resources/lib/pinyin4j-2.5.1.jar&lt;/systemPath&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.shentong&lt;/groupId&gt; &lt;artifactId&gt;pinyinAnalyzer&lt;/artifactId&gt; &lt;version&gt;4.3.1&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;$&#123;pom.basedir&#125;/src/main/resources/lib/pinyinAnalyzer4.3.1.jar&lt;/systemPath&gt; &lt;/dependency&gt; 该部分参考 lucene6.6+拼音分词+ik分词实现_jayson1001的博客-CSDN博客_lucene 拼音分词 实现的原理就是使用创建个 IndexPinyinAnalyzer 使用IKTokenizer 和PinyinTransformTokenFilter实现分词汉字和拼音的效果 测试的部分代码 123456789101112131415161718192021public static void main(String[] args) throws IOException &#123; Analyzer analyzer &#x3D; new IndexPinyinAnalyzer(false); TokenStream tokenStream &#x3D; analyzer.tokenStream(&quot;fff&quot;, &quot;标高1-KL33&quot;); &#x2F;&#x2F;添加一个引用，可以获得每个关键词 CharTermAttribute charTermAttribute &#x3D; tokenStream.addAttribute(CharTermAttribute.class); &#x2F;&#x2F;添加一个偏移量的引用，记录了关键词的开始位置以及结束位置 OffsetAttribute offsetAttribute &#x3D; tokenStream.addAttribute(OffsetAttribute.class); &#x2F;&#x2F;将指针调整到列表的头部 tokenStream.reset(); &#x2F;&#x2F;遍历关键词列表，通过incrementToken方法判断列表是否结束 while(tokenStream.incrementToken()) &#123; &#x2F;&#x2F;关键词的起始位置 System.out.println(&quot;start---------------&quot; + offsetAttribute.startOffset()); &#x2F;&#x2F;取关键词 System.out.println(&quot;分词内容： &quot;+charTermAttribute); &#x2F;&#x2F;结束位置 System.out.println(&quot;end---------------------&quot; + offsetAttribute.endOffset()); &#125; tokenStream.close();&#125; 打印结果 start—————0分词内容标高end———————2start—————0分词内容bgend———————2start—————2分词内容1-kl33end———————8start—————4分词内容klend———————6start—————6分词内容33end———————8 需要注意几点 pinyin4j 需要是2.5.1 分词和使用的dic有关,笔者使用的默认值,在日志中会打印org.wltea.analyzer.dic.Dictionary - 加载扩展停止词典:org/wltea/analyzer/dic/stopword.dic","categories":[],"tags":[]},{"title":"http-rpc的区别和介绍","slug":"http-rpc的区别和介绍","date":"2022-06-30T06:39:45.000Z","updated":"2024-03-14T14:55:29.478Z","comments":true,"path":"224b9f31.html","link":"","permalink":"https://imlike.cc/224b9f31.html","excerpt":"","text":"参考内容 既然有 HTTP 请求，为什么还要用 RPC 调用？ - 知乎 http介绍​ 说到http 其实开发过接口的都很少熟悉，目前最流行的是HTTPS，其实无论是OSI的5层七层还是TCP/IP五层模型，http都是属于上层封装偏向于应用的部分，主要是浏览器和一些客户端通信用的协议，通常请求会有特定请求头而且是长连接，占用服务器的连接资源， ​ 通常情况下浏览器会存储cookie 在服务端会存储session来确定连接的情况，如果是restful风格的http请求是无状态的，服务器是不知道这次请求的数据。 下面是附带的7层协议 第一层 物理层 网卡 网口 第二层 数据链路层 网关交换器 第三层 网络层 第四层 传输层 tcp,udp 第五层 会话层 dns.smtp 第六层表示层 telnet,snmp 第七层 应用层 http,rtsp,imap 上面是我大脑能列出来的第一反应 ​ 对于HTTPS是结合了http+SSL/TLS的混合，传统的http是明文传输的在互联网中网关和每个转发节点都能知道里面的内容，但是如果在http上包裹一层ssl那么传输者只知道传给谁但是具体的内容是不清楚的，ssl的非对称加密的，网站的拥有方需要去申请免费提供的let entry ，发送的证书是分别为公钥和私钥，他们拿私钥加密，我们客户端拿到数据用公钥解析内容，同时需要注意 具体的内容在网上搜索具体的介绍 RPC介绍 ​ 为什么会需要rpc，主要是服务的解耦，当整个系统越来越复杂后处于维护和迭代的需要，通常会将服务组件拆分，单独提供，譬如单独的订单服务，查询服务，认证服务，搜索服务等，但是当一个请求需要涉及到几个服务的功能的时候就会遇到 A服务 需要B服务，C服务的数据或者行动后A服务才能执行后续内容。 ​ 此时需要RPC来调用别人的服务服务，这个过程同时还设计到分布式事务，我们目前只考虑数据的使用，但是在调用过程中由于不同语言，和调用方能是接受的语法等限制通常需要跨平台的库来实现比较有名的是 浅谈 RPC | Apache Dubbo,gRPC 等来实现。 ​ 对应目前","categories":[],"tags":[]},{"title":"浅谈文件传输/共享协议(转载)","slug":"浅谈文件传输共享协议(转载)","date":"2022-06-16T11:20:54.000Z","updated":"2024-03-14T14:55:29.485Z","comments":true,"path":"bce5fd0b.html","link":"","permalink":"https://imlike.cc/bce5fd0b.html","excerpt":"","text":"浅谈文件传输/共享协议｜内网共享文件协议选择｜自建公网文件管理同步和共享方案推荐 - Allen Hua 的网络博客 1. 前言关于文件传输协议/文件共享协议，可以拿出来介绍的可以有很多，本文列举了一些通用的基础的发展较早的协议详细的介绍（当然了，没有深入的原理，咱也不会啊 :笑），一些较新的作了简单陈述，没有深究原理和实现。 本文主要分为三个部分 基础的文件传输/共享协议，更加推荐在内网使用的，不允许也不推荐暴露到公网。使用建议 点我 个人公网文件管理方案 实践: 在内网自建一个 seafile server, 并通过 frp 暴露在公网 serving，实现内外网都可以上传、下载、同步、分享。如何食用？点我 2. 总览 SMB/CIFS/SAMBA FTP/FTPS/TFTP SCP/SFTP WebDAV/WebDAVS NFS DLNA+UPnP HTTP/HTTPS rsync AFP MTP others (列举了一些非常规的协议) 3. 正片3.1 SMB/CIFS/SAMBASMB(Server Message Block) 通信协议是微软和英特尔在 1987 年制定的协议，主要是作为 Microsoft 网络的通讯协议，它是当今世上网络文件系统协议两极之一的存在。 SMB 使用了 NetBIOS 的 API。另外，它是一个开放性的协议，允许了协议扩展，这使得它可以变得更大而且复杂；大约有 65 个最上层的作业，而每个作业都超过 120 个函数，甚至 Windows NT 自己也没有全部支持到，后来著名的微软改名部门又把 SMB 重新设计了一套新的实现: CIFS(Common Internet File System) 协议，并且加入了许多新的特色功能。 一开始微软设计的 SMB 不支持在 Linux 上运行，著名黑客、技术大牛安德鲁·垂鸠 (Andrew Tridgell) 通过逆向工程，于 1992 年在澳洲国立大学（ANU）开发了第一版的 Samba Unix 软件，实现了 SMB/CIFS 兼容协议，并命名为 Samba，通过该程序实现了 Windows 和 Linux 之间的文件共享。 Debian 上的软件包名称就是 samba，OpenWrt 中一般有 samba36-server 和 samba4-server (版本 4，性能更强，更新的一个版本)。 SMB 是 C/S (Client and Server) 类型协议，客户机可以访问服务器上的共享文件系统、打印机及其他资源。更进阶一点，通过设置 “NetBIOS over TCP/IP”，Samba 不但能与局域网络主机分享资源，还能与全世界的电脑分享资源，但一般很少这样使用。 SMB 的优点之一是兼容性特别好，在各平台都获得了广泛支持，包括 Windows、Linux、macOS、Android、iOS、iPadOS，甚至一些嵌入式系统比如 OpenWrt，挂载访问都很方便。另外 SMB 也是各种电视、电视盒子默认支持的协议，这些电视和盒子通过 SMB 访问服务器端的资源可以达到播放电影、音乐、访问图片和预览普通文件的目的。 另外 SMB 提供端到端加密、安全性高，配置选项丰富，支持 ACL(Access Control List) 并支持多种用户认证方式。 不过 SMB 的缺点也是最令极客们诟病的，是传输效率稍低，速度不太稳定，受机器网络和硬件资源波动较大。 简而言之，SMB 协议是在局域网上用于服务器文件访问和打印的协议。CIFS 可以看做是应用程序协议如文件传输协议和超文本传输协议的一个实现。 关于传输端口， SMB/CIFS 传输端口是 139 或 445，具体是 TCP port 139 over NetBIOS，TCP port 445 over IP。TCP 445 是基于 IP 的 SMB，这是较新的版本，可以在 IP 网络上正常使用 SMB，也就是公网使用 SMB。 ps: samba 最新稳定版是 version 4.x.x 3.2 FTP/TFTP/FTPSFTP(File Transfer Protocol) 我们都很熟悉，它是 TCP/IP 协议簇中的一员，是 1971 年发展起来的文件传输协议，作为标准网络协议出现它包含两个部分实现，一个是 FTP Server，一个是 FTP Client，其中 Server 用来存储文件，Client 供客户机使用以访问远程资源。 现在暴露在公网的 FTP 站点已经越来越少，找了半天只发现 GNU 的 FTP 站点 ftp://ftp.gnu.org/ 目前还能匿名访问。Windows 可在 Windows Explorer 的 address bar 上直接键入地址然后回车即可以匿名身份访问。 默认情况下，FTP 使用 TCP 20 和 TCP 21 两个端口传输数据，TCP 20 用于数据传输，TCP 21 用于控制信息传输。而是否使用 20 端口传输数据与 FTP 传输模式有关，如果 FTP 主动模式，那么数据传输端口就是 20，如果采用的被动模式，那最终传输数据的端口由 FTP 的服务器和客户机协商决定。 FTP 的两种数据传输模式: Standard(主动) 和 Passive(被动)。 Standard 模式 FTP 客户端首先和服务器的 TCP 21 建立连接并发送指令，客户端需要接收数据的时候在这个通道上发送 PORT 命令。 PORT 命令包含了客户端用什么端口接收数据。在传输数据的时候，服务器通过自己的 TCP 20 连接到客户端的指定端口发送数据。FTP Server 必须和 Client 建立一个新的连接用来传输数据。 Passive 模式 建立控制通道和 Standard 模式类似，但建立连接后发送的是 PASV 命令，服务端收到 PASV 命令后打开一个临时端口 tempPort (1023 &lt; tempPort &lt; 65535) 并且通知客户端在这个端口上传送数据，客户端连接 FTP 服务端的此端口，然后服务端将会通过此端口 serving. 很多防火墙在设置的时候都是不允许接受外部发起的连接的，所以许多位于防火墙后或内网的 FTP 服务器不支持 Passive 模式，因为客户端无法穿过防火墙打开 FTP 服务器的高端端口；而许多内网的客户端不能用 Standard 模式登陆 FTP 服务器，因为从服务器的 TCP 20 无法和内部网络的客户端建立一个新的连接，造成无法工作。 FTP 传输数据也有两种方式：ASCII 和 二进制。 ASCII 传输方式 假定用户正在拷贝的文件包含的简单 ASCII 码文本(plain text)，如果在远程机器上运行的不是 UNIX，当文件传输时 FTP 通常会自动地调整文件的内容以便于把文件解释成另外那台计算机存储文本文件的格式。 但是常常有一些“特殊”情况，用户正在传输的文件包含的不是文本文件，它们可能是可执行程序、电影、音乐、数据库或者压缩文件。在拷贝任何非文本文件之前，用 binary 命令告诉 FTP 逐字拷贝。 二进制传输方式 在二进制传输中，保存文件的位序，以便原始和拷贝的是逐位一一对应的。即使目的机器上包含位序列的文件是没意义的。例如，macintosh (早期的 macOS 叫法) 以二进制方式传送可执行文件到 Windows 系统，在对方系统上，此文件不能执行。 时间已经来到了 2022 年，FTP 已经很少被应用在生产环境下，通过 http 传输文件的高效更能替代这个使用场景。它的优点不足以掩盖众多缺点，优点比如说能同时传输多个目录，自动备份等等，但是缺点众多，其他协议也支持它的优点，所以必然会被时代淘汰。 FTP 的缺点：数据传输模式不够合理，工作方式设计不合理，与防火墙工作不协调，安全认证策略不完善，相对而言整个传输过程效率也低下。此部分就不展开了，详情可见下面文章 文件传输协议（FTP）的缺点 FTP文件传输的优势与不足对比分析 通过上面的描述，可以感受到 FTP 的复杂性，1981 年在 RFC 783 中定义了一个简化的文件传输协议 TFTP(Trivial File Transfer Protocol)，称作简单文件传输协议，也称小型/琐碎文件传输协议。TFTP 比 FTP 简单得多，并且执行客户端和服务器进程之间的文件传输。但是，它不提供 FTP 所支持的用户身份验证和其他有用的功能。FTP 使用 TCP 时，TFTP 使用 UDP，这使得它成为了不可靠的文件传输协议。由于简单，透过少量存储器就能轻松实现，在计算机发展早期还是有用处的，例如从一台网络主机或服务器引导一个远程 X Window System 终端或其他的瘦客户端。由于小型文件传输协议缺少安全性，在公网传输非常危险，所以普遍仅仅用于私有网络。 对于 FTP 在安全上的不足，后面发展了 FTPS。 FTPS(FTP over SSL/TLS)，是一种安全的 FTP，添加了传输层安全（TLS）和安全套接层（SSL）的加密协议。尽管在数据传输方面加固了安全，但 FTP 的底层设计依然还是没有改变，所以直到现在应用也不够广泛。偶尔在内网中会有部署的 FTP 服务，但大家也更倾向于使用 samba, nfs, http, webdav 这些协议共享文件。 3.3 SCP/SFTPSCP(Secure CoPy) 和 SFTP(Secure File Transfer Protocol) 都基于 SSH(Secure SHell) 协议，OpenSSH 是 SSH 协议的一个开源实现，一些嵌入式设备可以通过 dropbear 建立 ssh 连接，dropbear 也实现了 SSH 协议的基础功能。 SCP 用来进行远程文件复制，并且整个复制过程是加密的。数据传输使用 ssh tunnel，并且使用和 ssh 相同的认证方式，提供相同的安全保证。密码认证或密钥对 (key-pair)验证。 SFTP 与 FTP 有着几乎一样的语法和功能。SFTP 为 SSH 的一部分，是一种传输文件到服务器的安全方式。在 SSH 软件包中，已经包含了一个叫作 SFTP 的安全文件传输子系统，SFTP 本身没有单独的守护进程，它必须使用 sshd 守护进程（端口号默认是22）来完成相应的连接操作，所以从某种意义上来说，SFTP 并不像一个服务器程序，而更像是一个客户端程序。SFTP 同样是使用加密传输认证信息和传输的数据，所以，使用 SFTP 是安全的。但是，正由于这种传输方式使用了加密/解密技术，所以比起普通 FTP，开销大得多，传输效率低得多。如果用户对网络安全性要求更高，则可以使用 SFTP 代替 FTP。 目前最新的 OpenSSH 稳定版本是 9.0，OpenSSH 9.0 released April 8, 2022，服务端配置文件 /etc/ssh/sshd_config 中有一行 sftp 配置就是配置这个文件传输子系统的。 12# override default of no subsystemsSubsystem sftp &#x2F;usr&#x2F;lib&#x2F;openssh&#x2F;sftp-server SCP 和 SFTP 的比较两者都基于 SSH，但是传输效率和安全性上也有区别。 SCP 不支持断点续传，一次 timeout 就得重来，而 SFTP 支持 SCP 单次传输不支持 4Gib 以上文件，SFTP 支持 SCP 不检查传输的数据包，效率更高但安全性更低 简单总结：SCP 适合小文件传输，速度更快。SFTP 适合更大文件，安全性更高的传输场景。 另外上文所述的 FTPS 不应与基于 SSH 的 SCP 和 SFTP 混淆。 Windows 端经典强大的开源程序 WinSCP 就实现了 SSH 的 SCP 和 SFTP 功能，界面直观非常易于使用。 3.4 WebDAV/WebDAVSWebDAV(WEb-Based Distributed Authoring and Versioning) 一种基于 HTTP 1.1 协议的通信协议。它扩展了 HTTP 1.1，在 GET、POST、HEAD、PUT、DELETE、PATCH 等 HTTP 标准方法以外添加了一些新的方法，使应用程序可对 Web Server 直接读写，并支持写文件锁定 (Locking) 及解锁 (Unlock)，还支持文件的版本控制。 简单而言，WebDAV 就是在服务器上划出一块存储空间，使用它扩展的一套 HTTP method 来管理文件，可以将磁盘映射到本地磁盘使用，对文件的每次改动都是一个 HTTP 请求。遗憾的是，目前只有用户名和密码的认证方式，安全性上有所欠缺。 WebDAV 为实现远程文件管理，向 HTTP/1.1 中追加了以下这些方法 PROPFIND：获取属性 PROPPATCH：修改属性 MKCOL：创建集合 COPY：复制资源及属性 MOVE：移动资源 LOCK：资源加锁 UNLOCK：资源解锁 HTTP status code 也随之扩展 102 Processing：可正常处理请求， 但目前是处理中状态 207 Multi-Status：存在多种状态 422 Unprocessible Entity：格式正确， 内容有误 423 Locked：资源已被加锁 424 Failed Dependency：处理与某请求关联的请求失败， 因此不再维持依赖关系 507 Insufficient Storage：保存空间不足 由于 WebDAV 是基于 HTTP 的，所以具有 HTTP 的所有优点，包括容易穿越防火墙、使用 HTTPS 来传输数据，因此在很多方面可以替代 FTP。通过 Nginx 或 Apache 或 Caddy 等 web server（也有其他实现，比如 Golang 实现的 webdav https://github.com/hacdias/webdav ） 非常容易就能搭建一个 WebDAV 服务。 WebDAVS(WebDAV over SSL/TLS) 就是加了一层 SSL/TLS ，数据安全更能得到保障，不过一般的使用场景多是 WebDAV + 配合 SSL 的 web server 实现 http 流量的加密，这样反而开销更小。 3.5 NFSNFS(Network File System)，即网络文件系统，是 FreeBSD 支持的文件系统中的一种。NFS 允许一个系统在网络上与它人共享目录和文件。通过使用 NFS，用户和程序可以像访问本地文件一样访问远端系统上的文件。它由 Sun 公司（已被 Oracle 收购）开发，于 1984 年发布，最新版本 NFSv4.2 于 2016 年发布。NFS 基于开放网络运算远程过程调用（ONC RPC）协议：一个开放、标准的 RFC 协议，任何人或组织都可以依据标准实现它。 NFS 通常使用在 Unix 操作系统上（比如 Solaris、AIX 及 HP-UX）和其他类 Unix 操作系统（例如 Linux 及 FreeBSD），同时在 macOS 和 Windows 系统也提供了 NFS 实现。不过在 Windows 系统上挂载 NFS 共享目录时，由于 Windows 自带的 NFS 客户端长久以来不支持 UTF-8，会致中文文件和目录显示为乱码。有解决方法但有代价，请见 一个小设置，让Win10 NFS正常显示中文UTF-8 NFS 的优点是 kernel 直接支持，部署简单、运行稳定，协议简单、传输效率高。 NFS 的缺点是没有加密授权等功能，仅依靠 IP 地址或主机名来决定用户能否挂载共享目录，对具体目录和文件无法进行 ACL 控制（NFSv4 以前）。通常的做法是通过 Kerberos 对 NFS 进行认证及加密，不过部署配置比较麻烦。 3.6 DLNA+UPnPDLNA(Digital Living Network Alliance, 数字生活网络联盟)，其前身是 DHWG（Digital Home Working Group，数字家庭工作组），成立于 2003 年 6 月 24 日， 是由索尼、英特尔、微软等发起成立的一个非营利性的、合作性质的商业组织。 DLNA 旨在解决个人 PC，消费电器，移动设备在内的无线网络和有线网络的互联互通，使得数字媒体和内容服务的无限制的共享和增长成为可能。DLNA 的口号是 Enjoy your music, photos and videos, anywhere anytime。 DLNA 将其整个应用规定成 5 个功能组件。从下到上依次为：网络互连，网络协议，媒体传输，设备的发现控制和管理，媒体格式。 DLNA 架构图 image source UPnP(Universal Plug and Play) 通用即插即用协议，官方的解释太过理论。微软的说法是 通用即插即用 （UPnP） 是一种用于 PC 机和智能设备（或仪器）的常见对等网络连接的体系结构，尤其是在家庭中。UPnP 以 Internet 标准和技术（例如 TCP/IP、HTTP 和 XML）为基础，使这样的设备彼此可自动连接和协同工作，从而使网络（尤其是家庭网络）对更多的人成为可能。 实际上，普通用户理解 UPnP 可以理解为基于 TCP/IP 上的自动端口映射，一般路由器在 NAT 的工作模式下可以打开此功能，自动处理局域网内设备中的各个软件的 UPnP 请求。 DLNA 可以高效的访问服务器中的各种类型资源，由 DLNA 服务器帮用户索引好离线文件。比如图片，音乐专辑，电影，电视剧集，适合多媒体的场景使用。 3.7 HTTP/HTTPSHTTP(HyperText Transfer Protocol)即超文本传输协议，我们都不陌生了，通过 http 共享文件可以很简单，比如使用 nginx 标准模块 Auto Index 可以很快创建一个可以共享的文件服务器，效率也非常之高。 比如在 nginx 创建一个共享本地文件夹的配置文件 /etc/nginx/sites-enabled/files-sharing.conf ，内容如下 123456789101112131415server &#123; listen 82; server_name 192.168.10.10; charset gbk,utf-8; location &#x2F; &#123; root &#x2F;home&#x2F;xxx&#x2F;nginx-static-files&#x2F;; index index.html index.htm; autoindex on; autoindex_exact_size on; autoindex_localtime on; &#125; &#125; 访问 http://192.168.10.10:82 (a fake address) 后的效果 HTTPS(HTTP over SSL/TLS)，加密后的 http 流量，目前正是主流 web 交互的方式。 3.8 rsyncrsync 全称 remote synchronize，即”远程同步”，是 Liunx/Unix 下的一个远程数据同步工具。它可通过 LAN/WAN 快速同步多台主机间的文件和目录，并适当利用 rsync 算法（差分编码）以减少数据的传输，即可做到增量传输/备份。严格来说，rsync 算不上底层的文件传输/共享协议，但是本文为了介绍的尽量全面还是提一下。 在常驻模式（daemon mode）下，rsync 默认监听 TCP 端口 873，以原生 rsync 传输协议或者透过远程 shell 如 RSH 或者 SSH 提供文件。SSH 模式下，rsync 客户端执行程序必须同时在本地和远程机器上安装。 rsync 首度发布于 1996 年 6 月 19 日。原始作者为安德鲁·垂鸠（Andrew Tridgell）（很巧的是，又是这个大神）与保罗·麦可拉斯（Paul Mackerras）。 rsync 现在应用广泛，使用起来也非常方便易用，效率也高，最初设计就是替代 SCP 的。既能在本地用于数据拷贝、移动（可用来替代 Linux cp 和 mv 命令），也能做增量备份存储。通过 SSH 实现远程复制也非常好用。 关于 rsync 的基础用法和高级用法可以看看阮一峰的 rsync 用法教程 , 写得易懂且全面。 其实类似于 rsync 的工具，还有 rclone、robocopy、freefilesync 等，这里就不做过多介绍了。 3.9 AFPAFP(Apple Filing Protocol)，是一种专门为 MAC OS X(现名: macOS) 系统提供文件共享服务的协议，通过开源的 Netatalk 实现 AFP 共享。主要应用是苹果笔记本的 Time Machine 时间机器系统备份，macOS 物理接口是 type-c，并有微软的雷电 Thunderbolt 授权，协议就是 AFP 。但是这东西不够完善，复杂使用场景下都有 bug，现在连苹果自己都很少用。 3.10 MTPMTP(Media Transfer Protocol) 媒体传输协议，是基于 PTP(Picture Transfer Protocol) 协议的扩展，主要用于传输媒体文件，其中有价值的应用就是同步 DRM 文件的 license。 MTP 既可以实现在 USB 协议上，也可以实现在 TCP/IP 协议上，它属于上层的应用协议，而不关心底层传输协议。目前大部分设备的应用都是基于 USB 协议。在 Android 设备上应用广泛，使用 USB A cable 连接 Windows PC 和 Android 设备，默认的文件传输协议就是 MTP，但除此之外，内网文件的传输很少应用到此协议。 3.11 others我把他们归为一类，不够通用，具有一定的专用性，主要是为企业服务的。 AS2/AS3/AS4AS2(Applicability Statement 2)，下同。 AS2，AS3 和 AS4 都是用来发送和保护关键文件传输的流行协议。 AS2 用于在因特网上安全可靠地传输敏感数据。AS2 利用数字证书和加密标准来保护在系统、网络和位置之间传输的关键信息。AS2 消息可以通过安全的 SSL 隧道进行压缩、签名、加密和发送。 AS3 是一个标准，几乎可以用来传输任何文件类型。它通过数字签名和数据加密为数据传输提供了一层安全性。它最初是为了传输数据文件而创建的，如 XML 和 EDI 商业对企业数据的文件。与 AS2 不同，AS2 是一种定义的传输协议，AS3 是一种消息标准，它侧重于在从服务器发送到服务器时应该如何格式化消息。一旦 AS3 消息被合成，它就可以通过任何其他协议(FTP、SFTP、HTTPS 等)传输。只要双方能够访问已放置消息的位置。 AS4 是一种协议，允许企业与其合作伙伴安全地交换数据。它建立在最初由 AS2 设置的基础上，但与 Web 服务一起工作，并提供改进的交付通知。作为企业对企业的标准，AS4 有助于使在互联网上进行文件交换变得安全和简单。 see also What’s the Difference Between AS2, AS3, and AS4? OFTP（Odette 文件传输协议）OFTP(Odette FTP)，另一种专为 EDI 设计的文件传输协议 OFTP。OFTP 在欧洲应用非常普遍。OFTP 和 AS2 本质上都是安全的，甚至支持电子交付收据，使它们非常适合 B2B 交易。 AFTPAFTP(Accelerated FTP)，是 JSCAPE 开发的一种文件传输协议（中文：加速文件传输协议），旨在进行 WAN 环境下的文件传输，尤其是远距离下的文件传输，但很容易受到延迟和数据包丢失等不良网络条件的影响，从而导致吞吐量大幅下降。AFTP 是一种 TCP-UDP 混合体，它使文件传输几乎不受这些网络条件的影响，理想情况下能比 FTP 传输快 100 倍。属于企业级应用协议。 raysync镭速文件传输协议 FTP 加速产品是专为企业定制开发的一款完全替代现有文件传输协议 FTP 的高效传输软件，能够在现有结构的基础上实现文件的高速传输，使用镭速文件传输协议 FTP 加速后，文件传输协议 FTP 传输速度可提升 10-100 倍。 这是国产深圳的一个公司的商业化产品。 PeSITPeSIT, short for Protocol d’Echanges pour un Systeme Interbancaire de Telecompensation (Protocol for data Exchange within the French Systems for Interbank Tele-clearance) 协议是一种端到端文件传输协议，由法国银行间电信系统经济利益集团 (GSIT) 开发。它在北美应用较少，主要用于满足欧洲银行标准，以及向欧洲银行和从欧洲银行转移通信。 see also What is PeSIT? 内网文件共享选择协议建议 image source samba 对于大多数人绝对是最优最优的选择，适配日常使用的所有平台，没有各种乱码问题，兼容性极佳。目前的 samba3 samba4 性能也不错 对于 Linux 用户可以优选 NFS，效能比起 samba 还是高 平常使用 Linux 传送一些小文件可以直接使用 scp 命令行，或者使用 WinSCP 这些工具（或者更推荐使用 rsync） 如果是内网磁盘想要暴露到公网，推荐 WebDAV + nginx + SSL，可以方便的将家里的磁盘暴露到公网(不过强烈建议使用 strong strong password) DLNA 这种，偶尔使用多媒体还行，但如果是重度用户建议搭建 Plex、Emby、Jellyfin 服务端 （诶我怎么歪题了） 其实内网的文件共享需求和 nas 上文件共享需求不谋而合 FTP, SFTP 不是很推荐 HTTP 配合 nginx 搭建一个存储文件的静态 public 站点还是不错的 引用一段 V2EX 网友的评论 来自帖子 https://www.v2ex.com/t/818478 奉劝一句，别瞎折腾。最完善的远程文件访问协议就是 SMB 。macOS 下 Finder 中协议语法为“SMB://”。在群晖、威联通中打开 SMB 协议访问即可。别玩苹果自身的 AFP ，macOS 间互访都有 bug ，苹果默认都不用这个，默认用的是 SMB （我家里有 3 台苹果电脑，6 台不同的 NAS 设备，足够验证这个）。群晖启用 AFP 也有 bug ，反倒是威联通的 AFP 比较完善。也别冲 WebDAV ，这个居然自身没搞定文件名编码方案（当然我猜测实际上应该是搞定了，我说的没搞定是默认配置下没搞定，估计需要用户各种场景测试然后通过命令行什么的指定编码方案）。我的一些小姐姐影片 NAS 间同步后文件名就变了。FTP 这些完全是闹着玩的，首次配置好能用，过一段时间登录就有问题。截至今日，世界上使用最广的操作系统依然是 windows ，至今依然是 windows 自己的远程文件访问协议其他厂商支持 /测试的最完善，那就是 SMB （ Samba ）。其他的各种协议，不同操作系统中要么不自带需要安装第三方软件，即使带了默认可能也没启用需要用户自己启用甚至配置。考虑到家用环境的复杂性，iOS 设备、安卓设备、电视、windows 、macos 、NAS 、甚至可能还有监控摄像头存储，这些不同操作系统要同时支持 /验证某个文件互访协议，想想都能明白有多麻烦。 结合我自身的各种协议的使用经验，我的整体评价也是，samba 保你用户体验，一般情况下 samba 就够了，需要公网映射就加上 WebDAV。 4. 个人公网文件管理方案我们可以用各种大公司的云产品，比如百度云盘，阿里云盘，夸克网盘等等，这些是面向普通用户的产品。还有一些面向开发者、企业的云产品等等不在本文讨论范围。我们想要知道的是普通用户如何建立起属于自己的公网网盘。我搜集了一些开源产品。见下表 名称 开源仓库 优缺点 NextCloud (其前身是 OwnCloud) https://github.com/nextcloud - 功能极其强大，具备丰富的 extensions - 后端核心程序使用 php 开发，效率并不算优 Seafile https://github.com/haiwen/seafile - 功能稍微单一，但是后端核心程序使用 C 语言编写，文件传输高效 Cloudreve https://github.com/cloudreve/Cloudreve - Material-UI 个人认为简洁美观 - 支持本机、从机、七牛、阿里云 OSS、腾讯云 COS、又拍云、OneDrive (包括世纪互联版) 作为存储端 - 后端原本是 php 编写，而后使用了 Golang 重构 ZFile https://github.com/haiwen/seafile - 基于 Java 的在线网盘程序，支持对接 S3、OneDrive、SharePoint、又拍云、本地存储、FTP 等存储源，支持在线浏览图片、播放音视频，文本文件等文件类型 KodExplorer(可道云) https://github.com/kalcaddle/KodExplorer - 类似于群晖桌面式 ui，适合对于计算机操作不熟悉的用户使用 - 他们发布了新产品 kodbox https://github.com/kalcaddle/kodbox 我目前几乎是 我全都要.gif，但是最终留下了 Seafile 和 Nextcloud 和 Cloudreve 三款产品。 下文我将介绍在局域网搭建一个 Seafile Server，然后通过一台具备公网 IP 的机器将其服务暴露到公网 serving. 5. 自建 Seafile Server 并使用 frp 暴露到公网我简而言之。 5.1 使用 docker-compose 安装 Seafile Servercompose file 参考官网的 https://docs.seafile.com/d/cb1d3f97106847abbf31/files/?p=/docker/docker-compose.yml 也可以参考 https://cloud.seafile.com/published/seafile-manual-cn/docker/%E7%94%A8Docker%E9%83%A8%E7%BD%B2Seafile.md 创建 /your/path/seafile-compose.yml，内容如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546version: &#39;2.0&#39;services: db: image: mariadb:10.5 container_name: seafile-mysql environment: - MYSQL_ROOT_PASSWORD&#x3D;123456 # Requested, set the root&#39;s password of MySQL service. - MYSQL_LOG_CONSOLE&#x3D;true volumes: - .&#x2F;seafile-mysql&#x2F;db:&#x2F;var&#x2F;lib&#x2F;mysql # Requested, specifies the path to MySQL data persistent store. networks: - seafile-net memcached: image: memcached:1.6 container_name: seafile-memcached entrypoint: memcached -m 256 networks: - seafile-net seafile: image: seafileltd&#x2F;seafile-mc:latest container_name: seafile ports: - &quot;8088:80&quot;# - &quot;443:443&quot; # If https is enabled, cancel the comment. volumes: - .&#x2F;seafile-data:&#x2F;shared # Requested, specifies the path to Seafile data persistent store. environment: - DB_HOST&#x3D;db - DB_ROOT_PASSWD&#x3D;123456 # Requested, the value shuold be root&#39;s password of MySQL service. - TIME_ZONE&#x3D;Asia&#x2F;Shanghai # Optional, default is UTC. Should be uncomment and set to your local time zone. - SEAFILE_ADMIN_EMAIL&#x3D;email@example.com # Specifies Seafile admin user, default is &#39;me@example.com&#39;. - SEAFILE_ADMIN_PASSWORD&#x3D;123456 # Specifies Seafile admin password, default is &#39;asecret&#39;. - SEAFILE_SERVER_LETSENCRYPT&#x3D;false # Whether use letsencrypt to generate cert. - SEAFILE_SERVER_HOSTNAME&#x3D;seafile.example.com # Specifies your host name. depends_on: - db - memcached networks: - seafile-net networks: seafile-net:# 启动一个 docker stack, 建立三个容器# docker-compose -f .&#x2F;seafile-compose.yml up -d 默认 WebDAV 是关闭的，我们将它打开 docker exec -it seafile bash 进入容器 12345678910111213141516171819# 下面这是在容器中 # vim &#x2F;shared&#x2F;seafile&#x2F;conf&#x2F;seafdav.conf # 将 [WEBDAV] enabled 由 false 改成 true # 然后重启 seafile 服务 # cd &#x2F;opt&#x2F;seafile&#x2F;seafile-server-9.0.4 #读者操作的时候 latest 镜像版本应该比这个更新 # .&#x2F;seafile.sh restart Stopping seafile server ...Starting seafile server, please wait ...** Message: 20:02:59.972: seafile-controller.c(621): No seafevents. Seafile server started Done. Seafile Server 基本就部署好了。 5.2 在公网机器上安装 frpsFRP 是一个非常优秀的内网穿透程序，GitHub 链接: https://github.com/fatedier/frp ，其在 GitHub 上拥有 57.2k 个 star（截止 2022-06-15）。很多用户家里的网络 NAT 类型都是 Symmetric 对称型，我们一般称之为 NAT type 4，是一种最难穿透的类型。要想 nat 穿越成功，如果用户没法提升 nat 类型（最高 nat 1，叫做 FullCone 全锥形），那就必须要使用一台公网服务器中转，但这其实也是一种最高效的方式（花钱就行）。 如何搭建 frps 这一部分互联网上已经有太多教程了，我简单综述一下。 frp 是 go 写的，搭建 frp 环境需要一台具备公网的 VPS 或者国内叫轻量应用服务器，云服务器之类的 在公网机器上运行 frps（binary, Linux 可执行文件），需配合 frps.ini 配置文件 在内网机器（Seafile Server 所在机器）运行 frpc，需配合 frpc.ini 配置文件 比如拉取 frp x86_64 0.39.0 版本的源程序 1# wget -O &#x2F;home&#x2F;xxx&#x2F;temp&#x2F;frp_0.39.0_linux_arm64.tar.gz https:&#x2F;&#x2F;github.com&#x2F;fatedier&#x2F;frp&#x2F;releases&#x2F;download&#x2F;v0.39.0&#x2F;frp_0.39.0_linux_arm64.tar.gz 配置一下 /usr/local/frp/frps.ini 配置文件，以下所有端口都使用了 12345 代替 1234567891011121314151617181920212223242526272829303132# [common] is integral section[common]# A literal address or host name for IPv6 must be enclosed# in square brackets, as in &quot;[::1]:80&quot;, &quot;[ipv6-host]:http&quot; or &quot;[ipv6-host%zone]:80&quot;bind_addr &#x3D; 0.0.0.0bind_port &#x3D; 12345# udp port used for kcp protocol, it can be same with &#39;bind_port&#39;# if not set, kcp is disabled in frpskcp_bind_port &#x3D; 12345# bind udp portbind_udp_port &#x3D; 12345# if you want to configure or reload frps by dashboard, dashboard_port must be setdashboard_port &#x3D; 12345# dashboard assets directory(only for debug mode)dashboard_user &#x3D; YOUR_USERNAMEdashboard_pwd &#x3D; YOUR_PASSWORD# assets_dir &#x3D; .&#x2F;staticvhost_http_port &#x3D; 12345vhost_https_port &#x3D; 12345# console or real logFile path like .&#x2F;frps.loglog_file &#x3D; &#x2F;var&#x2F;log&#x2F;frp&#x2F;frps.log# debug, info, warn, errorlog_level &#x3D; infolog_max_days &#x3D; 3# auth tokentoken &#x3D; A_STRONG_PASSWORD# only allow frpc to bind ports you list, if you set nothing, there won&#39;t be any limit#allow_ports &#x3D; 1-65535# pool_count in each proxy will change to max_pool_count if they exceed the maximum valuemax_pool_count &#x3D; 50# if tcp stream multiplexing is used, default is truetcp_mux &#x3D; true Systemd 是 Linux 系统工具，用来启动守护进程，现已成为大多数发行版的标准配置。我们也用 systemd 管理 frps 和 frpc。创建文件 /lib/systemd/system/frps.service 内容如下 12345678910111213141516[Unit]Description&#x3D; Frps Service DaemonAfter&#x3D;network.target remote-fs.target nss-lookup.target[Service]Type&#x3D;simpleExecStart&#x3D;&#x2F;usr&#x2F;local&#x2F;frp&#x2F;frps -c &#x2F;usr&#x2F;local&#x2F;frp&#x2F;frps.iniKillSignal&#x3D;SIGQUITTimeoutStopSec&#x3D;5KillMode&#x3D;processPrivateTmp&#x3D;trueStandardOutput&#x3D;syslogStandardError&#x3D;inherit[Install]WantedBy&#x3D;multi-user.target 启用开机自启 1# systemctl enable frps 启动程序 1# systemctl start frps 5.3 在内网机器上安装 frpc和 5.2 类似，我们需要 frpc 可执行程序和 frpc.ini 配置文件。 创建配置文件 /etc/frp/frpc.ini，内容如下 1234567891011121314151617181920[common]log_file &#x3D; &#x2F;tmp&#x2F;frpc.loglog_level &#x3D; infolog_max_days &#x3D; 3login_fail_exit &#x3D; falseprotocol &#x3D; tcpserver_addr &#x3D; example.comserver_port &#x3D; 12345tcp_mux &#x3D; truetoken &#x3D; A_STRONG_PASSWORDuser &#x3D; YOUR_USERNAME[second-n1-seafile]type &#x3D; httpcustom_domains &#x3D; seafile.example.comlocal_ip &#x3D; x.x.x.xlocal_port &#x3D; 8088remote_port &#x3D; 12345use_compression &#x3D; trueuse_encryption &#x3D; true frpc 这边也使用 systemd 管理程序。创建 /lib/systemd/system/frpc.service，内容如下 123456789101112131415[Unit]Description&#x3D;FRPC Client DaemonAfter&#x3D;network.targetWants&#x3D;network.target[Service]Type&#x3D;simpleExecStart&#x3D;&#x2F;usr&#x2F;bin&#x2F;frpc -c &#x2F;etc&#x2F;frp&#x2F;frpc.iniRestart&#x3D;alwaysRestartSec&#x3D;20sUser&#x3D;xxxGroup&#x3D;xxx[Install]WantedBy&#x3D;multi-user.target 设置开机自启 1# systemctl enable frpc 启动 frpc 程序 1# systemctl start frpc 5.4 其他事项域名相关的东西我就不说了。 配置的时候注意可执行文件、配置文件的权限，和文件拥有者和所在组。 还有防火墙。除了 Linux distro 自身安装的各种防火墙程序之外，类似良心云这种厂商在虚拟化的应用服务器之外还有一层硬件级别的防火墙，如果配置后发现不能访问可以优先往这些方面排查。 下面我们在 VPS 上使用 nginx 反代 seafile 服务，并使用 SSL/TLS 加密（在最外层套 SSL）。得先准备好申请好的证书和私钥。 1234# 比如 nginx 配置中的证书和私钥形式如下 ssl_certificate &#x2F;etc&#x2F;letsencrypt&#x2F;live&#x2F;example.com&#x2F;fullchain.pem;ssl_certificate_key &#x2F;etc&#x2F;letsencrypt&#x2F;live&#x2F;example.com&#x2F;privkey.pem; 创建 nginx 配置文件 /etc/nginx/sites-enabled/seafile-home.conf 内容如下 1234567891011121314151617181920212223242526272829303132333435363738server &#123; listen 80; server_name seafile.example.com; rewrite ^ https:&#x2F;&#x2F;$http_host$request_uri? permanent; # Forced redirect from HTTP to HTTPS server_tokens off;&#125;server &#123; listen 443 ssl http2; ssl_certificate &#x2F;etc&#x2F;letsencrypt&#x2F;live&#x2F;example.com&#x2F;fullchain.pem; ssl_certificate_key &#x2F;etc&#x2F;letsencrypt&#x2F;live&#x2F;example.com&#x2F;privkey.pem; server_name seafile.example.com; server_tokens off; # HSTS for protection against man-in-the-middle-attacks add_header Strict-Transport-Security &quot;max-age&#x3D;31536000; includeSubDomains&quot;; ssl_protocols TLSv1.2 TLSv1.3; ssl_ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384; ssl_prefer_server_ciphers off; ssl_session_timeout 5m; location &#x2F; &#123; proxy_pass http:&#x2F;&#x2F;127.0.0.1:12345; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Host $server_name; proxy_set_header X-Forwarded-Proto https; proxy_set_header Host $host:$server_port; access_log &#x2F;var&#x2F;log&#x2F;nginx&#x2F;seahub.access.log; error_log &#x2F;var&#x2F;log&#x2F;nginx&#x2F;seahub.error.log; proxy_read_timeout 1200s; client_max_body_size 0; &#125;&#125; 检查 nginx 语法并重载 nginx 1# nginx -t &amp;&amp; nginx -s reload 然后就可以通过 https://seafile.example.com 访问属于自己的 seafile 服务。 进入后台管理页面，可以随意切换公网和内网访问，大部分时候还是使用公网的域名访问配置 跑到了 100MiB/s，但是没有截到图 🤣 切换到内网，打开浏览器上传的速度，受限于机器性能，比如我的 Phicomm N1 最多只能跑到 50MiB/s 全文完。 6. 引用与致谢 各个协议的 wikipedia 简体中文页面 各个开源项目 NFS、FTP、SMB、WebDav、DLNA协议，傻傻分不清？ SMB、FTP、WebDAV协议，傻傻分不清？5000字教程，教你快速上手 nas各种共享访问协议的使用 13 种适用于企业的文件传输协议 end. 打赏: 微信, 支付宝 标签: Linux, 网络 本作品采用 知识共享署名-相同方式共享 4.0 国际许可协议 进行许可。 添加新评论OωO表情 请输入验证码： 提交评论 上一篇: 解决 webdav 连接 seafile 无法重命名文件等操作造成的 SeafDAV: MOVE returns 502 Bad Gateway 错误 下一篇: 没有了 Copyright © 2017 - 2022 by hellodk.cn | Proudly Powered by Typecho &amp; Initial &amp; ❤. Archives | SiteMap | About Site has been running for 3 year(s) 71 days 11 hours. 小破站总访问量才 792276 次，要加油鸭！ 页面加载用时: 524 ms","categories":[],"tags":[]},{"title":"计算机硬件相关知识","slug":"硬件相关知识","date":"2022-06-15T13:58:19.000Z","updated":"2024-03-14T14:55:29.485Z","comments":true,"path":"814126a1.html","link":"","permalink":"https://imlike.cc/814126a1.html","excerpt":"","text":"主板 上有各种芯片 南桥,北桥,接口芯片连接总线 CPU 频率,核心,架构 GPU 专用矢量渲染,并行能力强,算力丰富 目前显卡 消费端就是 gtx,gnx 生产端 tesla 通用并行运算 显示的api cude directx opencl opengl() vulkan逐渐替代 webgl(浏览器基于opengl 封装的api 包括cesuim 等很多js 三维基于这个 来进一步封装和集成) 实时画面渲染 可能是GPU集群来实现 一方面是处理速度网络是有延时的 主板上多GPU功能主要需要使用NVIDIA上面的SLI来实现目前pci-e 主板来实现目前ue4官方并不支持多GPU 对应运行gpu算力的部分 一般是用 NVIDIA-smi来实现集中接口调用 类似于 opencl","categories":[],"tags":[]},{"title":"gdal镜像集成和dockerfile使用","slug":"gdal镜像集成和dockerfile使用","date":"2022-06-13T07:01:28.000Z","updated":"2024-03-14T14:55:29.477Z","comments":true,"path":"59cba8fc.html","link":"","permalink":"https://imlike.cc/59cba8fc.html","excerpt":"","text":"gdal介绍gdal的一个用c++语言编写的库，用于处理地理信息相关的数据包括转换，识别数据，格式化数据以及解析 同时提供第三方语言的SDK包括python，java上述需要编译后使用 java是需要使用jni接口调用实现方法在window平台是DLL文件，在linux平台是so文件，最后需要使用和初始化服务。 gdal命令使用1、ogrinfo orginfo –version ogrinfo –help 2、ogr2ogr ogr2ogr postgis转shp 并执行SQL ogr2ogr -f “ESRI Shapefile” file1.shp PG:”host=’192.168.7.11’ port=’5432’ dbname=’postgres’ user=’postgres’ password=’1111’” -sql “select * from like_fff_xztc” -dialect PostgreSQL ogr2ogr postgis转化为shp文件且指定SQL执行 ogr2ogr -f “ESRI Shapefile” /app/tmp/result_like_fff_xztc/result_like_fff_xztc.shp PG:”host=’192.168.7.11’ port=’5432’ dbname=’postgres’ user=’postgres’ password=’1111’” -sql “select fid,objectid,bsm,dlmc,qsdwmc,zldwmc,f_xzqhmc,f_year,f_month,partitionno,se_anno_cad_data,tbdlmj,ysdm,dlbm,qsxz,qsdwdm,zldwdm,xzgdlx,zldj,f_xzqhdm,f_diccode,f_fileid,tbbh,shape_length,shape_area,st_transForm(geom,4490) as geom from like_fff_xztc where ST_Intersects(geom,st_transform(st_geomFromText(‘MULTIPOLYGON(((112.973570327973 30.634569982416,112.973501343979 30.6344597898424,112.973423613701 30.6344687491655)))’,4490),4490))” -dialect PostgreSQL –config SHAPE_ENCODING ‘UTF-8’ ogr2ogr shp导入shp 命令 ogr2ogr -f “PostgreSQL” PG:”host=’192.168.7.11’ port=’5432’ dbname=’test’ user=’postgres’ password=’1111’” C:\\Users\\Administrator\\Desktop\\ceshi\\old\\Water_region.shp -progress -nlt PROMOTE_TO_MULTI -lco GEOMETRY_NAME=geom -lco FID=fid -overwrite -nln ‘like_test_update’ -unsetFieldWidth –config SHAPE_ENCODING ‘utf-8’ ogr2ogr mdb导入shp命令 ogr2ogr -f “PostgreSQL” PG:”host=’192.168.7.11’ port=’5432’ dbname=’test_new_app’ user=’postgres’ password=’1111’” /app/laohekou.mdb -progress -nlt PROMOTE_TO_MULTI -lco GEOMETRY_NAME=geom -lco FID=fid -overwrite -nln ‘test_mdb_001’ ogr2ogr shp转入postgis ogr2ogr -f “PostgreSQL” PG:”host=xxx user=xxx password=xxx dbname=cxxxhina” -nlt GEOMETRY 省级行政区.shp 3、 官方gdal的dockerfile简介gdal/docker at master · OSGeo/gdal Alpine basedAlpine version: 3.15 for 3.5 3.16 for GDAL 3.6dev Small: osgeo/gdal:alpine-small-latest Image size: ~ 59 MB Raster drivers: ultrasmall + built-in + SQLite-based ones + network-based ones Vector drivers: ultrasmall + built-in + most XML-based ones + network-based ones + PostgreSQL Using internal libtiff and libgeotiff External libraries enabled: ultrasmall + libexpat, libpq, libssl No GDAL Python Base PROJ grid package (http://download.osgeo.org/proj/proj-datumgrid-1.8.zip) Overall licensing terms of the GDAL build: permissive (MIT, BSD style, Apache, etc..) See alpine-small/Dockerfile Normal: osgeo/gdal:alpine-normal-latest Image size: ~ 277 MB Raster drivers: small + netCDF, HDF5, BAG Vector drivers: small + Spatialite, XLS Using internal libtiff and libgeotiff External libraries enabled: small + libgeos, libhdf5, libhdf5, libkea, libnetcdf, libfreexl, libspatialite, libxml2, libpoppler, openexr, libheif, libdeflate, libparquet GDAL Python Base PROJ grid package (http://download.osgeo.org/proj/proj-datumgrid-1.8.zip) Overall licensing terms of the GDAL build: copy-left (GPL) + LGPL + permissive See alpine-normal/Dockerfile Ubuntu basedUbuntu version: 20.04 for GDAL 3.4 and 3.5 22.04 for GDAL 3.6dev Small: osgeo/gdal:ubuntu-small-latest Image size: ~ 385 MB Raster drivers: all built-in + JPEG + PNG + JP2OpenJPEG + WEBP +SQLite-based ones + network-based ones Vector drivers: all built-in + XML based ones + SQLite-based ones + network-based ones + PostgreSQL Using internal libtiff and libgeotiff External libraries enabled: libsqlite3, libproj, libcurl, libjpeg, libpng, libwebp, libzstd, libexpat, libxerces-c, libpq, libssl, libgeos, libspatialite GDAL Python (Python 3.8 for Ubuntu 20.04, Python 3.10 for Ubuntu 22.04) Base PROJ grid package (http://download.osgeo.org/proj/proj-datumgrid-1.8.zip) Overall licensing terms of the GDAL build: LGPL + permissive (MIT, BSD style, Apache, etc..) See ubuntu-small/Dockerfile Full: osgeo/gdal:ubuntu-full-latest (aliased to osgeo/gdal) Image size: ~ 1.48 GB Raster drivers: all based on almost all possible free and open-source dependencies Vector drivers: all based on almost all possible free and open-source dependencies Using internal libtiff and libgeotiff External libraries enabled: small + libnetcdf, libhdf4, libhdf5, libtiledb, libkea, mongocxx 3.4, libspatialite, unixodbc, libxml2, libcfitsio, libmysqlclient, libkml, libpoppler, pdfium, openexr, libheif, libdeflate, libparquet GDAL Python (Python 3.8 for Ubuntu 20.04, Python 3.10 for Ubuntu 22.04) All PROJ grid packages (equivalent of latest of proj-data-X.zip from http://download.osgeo.org/proj/ at time of generation, &gt; 500 MB) Overall licensing terms of the GDAL build: copy-left (GPL) + LGPL + permissive 点击进入后可以看到介绍，目前分别在两个linux发行版本上进行镜像文件创建，分别是Ubuntu，和alpine alpine是精简的服务目前不包括第三方的拓展目前在linux上不支持pgeo（改数据源可以读取mdb和其他格式数据的支持） ubuntu是完整版是提供全量的服务，支持pgeo 介绍 linux下面读取微软的软件word，Excel，PPT是有平台和驱动，但是像access这种个人数据库需要ODBC这个开源平台来读取这类数据，gdal在linux平台通过ODBC接口来读取access个人数据库，同时对数据解析，我们需要利用这个解析工具来读取数据被我们java读取，或者通过ogr2ogr命令来转换数据 ODBC 是linux平台的数据的解析 ogrinfo 能识别 odbcinst -j 表示odbc正确安装 方法一osgeo/gdal:ubuntu-full基础上安装(unixODBC,mdbtools)1234567891011121314151617181920212223242526272829303132333435FROM osgeo/gdal:ubuntu-full-3.0.4RUN cp /etc/apt/sources.list /etc/apt/sources.list.bak# RUN echo &#x27;deb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse \\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse\\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse \\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse \\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse \\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse \\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse \\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse \\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse \\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse\\n&#x27; &gt; /etc/apt/sources.listRUN apt-get update &amp;&amp; cat /etc/apt/sources.list &amp;&amp; apt-get -y --fix-missing --no-install-recommends install gcc make g++ bison flex gawk unixodbc-dev libgdal-javaCOPY ./unixODBC-2.3.11 /usr/unixODBC-2.3.11COPY ./mdbtools-1.0.0 /usr/mdbtools-1.0.0RUN chmod -R 777 /usr/unixODBC-2.3.11/ &amp;&amp; chmod -R 777 /usr/mdbtools-1.0.0RUN cd /usr/unixODBC-2.3.11/ &amp;&amp; ./configure --prefix=/usr/unixODBC-2.3.11 --includedir=/usr/include --libdir=/usr/lib --bindir=/usr/bin --sysconfdir=/etcRUN cd /usr/unixODBC-2.3.11/ &amp;&amp; makeRUN cd /usr/unixODBC-2.3.11/ &amp;&amp; make installRUN cd /usr/mdbtools-1.0.0 &amp;&amp; ./configure --with-unixodbc=/usr/unixODBC-2.3.11RUN cd /usr/mdbtools-1.0.0 &amp;&amp; makeRUN cd /usr/mdbtools-1.0.0 &amp;&amp; make install# RUN echo &quot;export LD_LIBRARY_PATH_LIKE=/usr/local/lib&quot; &gt;&gt; /etc/profile &amp;&amp; source /etc/profileCOPY ./odbcinst.ini /etcENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/libRUN cp /usr/share/java/gdal.jar /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/# 指定时区ENV TZ=Asia/Shanghai# 设定工作目录WORKDIR /appADD config /app/configADD ./shp2pg.py /app/py/shp2pg.py # 声明端口EXPOSE 11002ADD uts-stg.jar /app/uts-stg.jarCMD [&quot;java&quot;, &quot;-Djava.security.egd=file:/dev/./urandom&quot;,&quot;-Djava.library.path=/usr/lib/jni&quot;, &quot;-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005 &quot;, &quot;-jar&quot;, &quot;/app/uts-stg.jar&quot;]","categories":[],"tags":[]},{"title":"gdal使用和环境初始化在Windows和linux下的介绍","slug":"gdal使用和环境初始化在Windows和linux下的介绍","date":"2022-05-14T05:06:11.000Z","updated":"2024-03-14T14:55:29.477Z","comments":true,"path":"54871149.html","link":"","permalink":"https://imlike.cc/54871149.html","excerpt":"","text":"gdal是用c语言编写的服务能解析地理信息服务中的各种解析服务 在Windows中可以使用别人编译打包好的服务，里面以后各种依赖的DLL和jar包，可以提供个java调用，同时也支持其他语言。 我下载的版本release-1911-x64-gdal-3-0-4-mapserver-7-4-3.zip 1.配置gdal的环境 GDAL Java bindings GDAL 2.0.0 in Eclipse - Geographic Information Systems Stack Exchange 否则会包下列错误: UnsatisfiedLinkError: E:\\jdk\\jdk-12.0.2\\bin\\gdalalljni.dll: Can’t find dependent libraries 如果是默认安装的路径在C:\\Program Files (x86)\\GDAL\\ 下需要修改 创建路径 变量的名称 路径 CREATE GDAL_DATA C:\\gdal\\bin\\gdal-data 必须 CREATE GDAL_DRIVER_PATH C:\\gdal\\bin\\gdal\\plugins 必须 APPEND PATH C:\\gdal\\bin 必须 APPEND PATH C:\\gdal\\bin\\gdal\\apps 可选 同时将压缩包内 配置完成是命令行输入 gdalinfo –version看是否能正常调用 如果出现ogr_MSSQLSpatial.dl加载不了的情况不用理会,在github上貌似Windows系统的集成问题如果觉得看着难受可以 直接重命名 ogr_MSSQLSpatial.dll.bak 参考文章: Gdal环境配置（Windows系统）_gdal配置环境变量_魔希达的博客-CSDN博客","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"https://imlike.cc/tags/linux/"},{"name":"gdal","slug":"gdal","permalink":"https://imlike.cc/tags/gdal/"},{"name":"windows","slug":"windows","permalink":"https://imlike.cc/tags/windows/"}]},{"title":"springsecurity入门介绍","slug":"springsecurity入门介绍","date":"2022-05-01T06:53:48.000Z","updated":"2024-03-14T14:55:29.481Z","comments":true,"path":"329899c5.html","link":"","permalink":"https://imlike.cc/329899c5.html","excerpt":"","text":"Spring Security系列之一 简单介绍和实战 - 掘金 20210902#_【项目实践】一文带你搞定Spring Security + JWT - 1024搜-程序员专属的搜索引擎 Spring Session 中文文档 参考手册 中文版 springsecurity和shiro都是权限和认证的框架，目前公司有基于springsecurity开发集成包，目前使用结合使用过程梳理一下springsecurity的特性和注意事项和集成的要点 目前引入的springsecurity的 springsecurity -outh 2.2.2.6 和需要和session打交道的session-core和session-data-redis2.4.1 利用session-redis来存储session Web系统中登录认证（Authentication）的核心就是凭证机制，无论是Session还是JWT，都是在用户成功登录时返回给用户一个凭证，后续用户访问接口需携带凭证来标明自己的身份。后端会对需要进行认证的接口进行安全判断，若凭证没问题则代表已登录就放行接口，若凭证有问题则直接拒绝请求。这个安全判断都是放在过滤器里统一处理的： 一般需要实现 需要实现UserDetails1AuthUser extends User UserDetails, CredentialsContainer 实现loadUserByUsernameorg.springframework.security.core.userdetails.UserDetailsService.loadUserByUsername PasswordEncoder 密码的认证12345@Beanpublic PasswordEncoder passwordEncoder() &#123; &#x2F;&#x2F; 这里我们使用bcrypt加密算法，安全性比较高 return new BCryptPasswordEncoder();&#125; 继承WebSecurityConfigurerAdapterorg.springframework.security.config.annotation.web.configuration.WebSecurityConfigurerAdapter 由 authenticationManager() 的默认实现用来尝试获取 AuthenticationManager。如果被覆盖，应该使用 AuthenticationManagerBuilder 来指定 AuthenticationManager。authenticationManagerBean() 方法可用于将生成的 AuthenticationManager 公开为 Bean。 userDetailsServiceBean() 可用于公开使用 AuthenticationManagerBuilder 作为 Bean 创建的最后填充的 UserDetailsService。 UserDetailsService 也将自动填充到 HttpSecurity.getSharedObject(Class) 以与其他 SecurityContextConfigurer （即 RememberMeConfigurer ）一起使用例如，以下配置可用于在内存中注册身份验证，以公开内存中的 UserDetailsService：UserDetailsService: @Override protected void configure(AuthenticationManagerBuilder auth) { auth // enable in memory based authentication with a user named // “user” and “admin” .inMemoryAuthentication().withUser(“user”).password(“password”).roles(“USER”).and() .withUser(“admin”).password(“password”).roles(“USER”, “ADMIN”); } // Expose the UserDetailsService as a Bean @Bean @Override public UserDetailsService userDetailsServiceBean() throws Exception &#123; return super.userDetailsServiceBean(); &#125; 目前使用的方式设置认证的方式和控制那些需要被权限管理 继承SecurityMetadataSourceorg.springframework.security.access.SecurityMetadataSource 1实现动态的权限验证，当然要先要获取对应的资源，然后再将他们对应哪些角色可以访问的关系表示出来","categories":[],"tags":[]},{"title":"spring知识回顾","slug":"spring知识回顾","date":"2022-05-01T03:53:25.000Z","updated":"2024-03-14T14:55:29.481Z","comments":true,"path":"d8ed5f9d.html","link":"","permalink":"https://imlike.cc/d8ed5f9d.html","excerpt":"","text":"spring的利用的面向切面编程（AOP），动态代理（反射）和filter，intercept， ApplicationContent 123QualifierRouceseAutowired 拦截器利用了反射对请求进行自定义的操作可以访问action上下文和栈的对象，保证数据12345678910111213141516171819202122@Componentpublic class MyInterceptor implements HandlerInterceptor &#123; @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; System.out.println(&quot;Interceptor 前置&quot;); return true; &#125; @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception &#123; System.out.println(&quot;Interceptor 处理中&quot;); &#125; @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception &#123; System.out.println(&quot;Interceptor 后置&quot;); &#125;&#125; 过滤器利用原始servlet来进行操作主要可用于鉴权和12345678910111213141516171819202122@Componentpublic class MyFilter implements Filter &#123; @Override public void init(FilterConfig filterConfig) throws ServletException &#123; System.out.println(&quot;Filter 前置&quot;); &#125; @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException &#123; System.out.println(&quot;Filter 处理中&quot;); filterChain.doFilter(servletRequest, servletResponse); &#125; @Override public void destroy() &#123; System.out.println(&quot;Filter 后置&quot;); &#125;&#125; AOP123456789101112131415161718192021222324252627282930313233343536@Aspect@Componentpublic class Aspect &#123; @Pointcut(&quot;@annotation(com.HasAuth)&quot;) public void pointCut() &#123; &#125; @Around(&quot;pointCut()&quot;) public Object around(ProceedingJoinPoint point) throws Throwable &#123; Object[] args = point.getArgs(); /* if (StringUtil.IsEmpty(args[0])) &#123; args[0] = &quot;default&quot;; &#125;*/ return point.proceed(args); &#125; //标注该方法体为后置通知，当目标方法执行成功后执行该方法体 @AfterReturning(&quot;within(com.abchina.irms..*) &amp;&amp; @annotation(rl)&quot;) public void addLogSuccess(JoinPoint jp, rmpfLog rl)&#123; Object[] parames = jp.getArgs();//获取目标方法体参数 String params = parseParames(parames); //解析目标方法体的参数 String className = jp.getTarget().getClass().toString();//获取目标类名 className = className.substring(className.indexOf(&quot;com&quot;)); String signature = jp.getSignature().toString();//获取目标方法签名 String methodName = signature.substring(signature.lastIndexOf(&quot;.&quot;)+1, signature.indexOf(&quot;(&quot;)); String modelName = getModelName(className); //根据类名获取所属的模块 ... &#125; //标注该方法体为异常通知，当目标方法出现异常时，执行该方法体 @AfterThrowing(pointcut=&quot;within(com.abchina.irms..*) &amp;&amp; @annotation(rl)&quot;, throwing=&quot;ex&quot;) public void addLog(JoinPoint jp, rmpfLog rl, BusinessException ex)&#123; ... &#125; &#125; 不加载特定的bean12345@Configuration @EnableSpringConfigured@ComponentScan(basePackages = &#123;&quot;com.example&quot;&#125;, excludeFilters=&#123;@ComponentScan.Filter(type = FilterType.REGEX, pattern = &quot;org.xxx.yyy.*&quot;), @ComponentScan.Filter(type=FilterType.ASSIGNABLE_TYPE, value=Foo.class)&#125;)public class MySpringConfiguration &#123;&#125; stream 自定义function","categories":[],"tags":[]},{"title":"这些科技将在2022年改变你的生活内容（华尔街日报转载）","slug":"这些科技将在2022年改变你的生活内容（华尔街日报转载）","date":"2022-04-16T13:45:36.000Z","updated":"2024-03-14T14:55:29.486Z","comments":true,"path":"a683129.html","link":"","permalink":"https://imlike.cc/a683129.html","excerpt":"","text":"Facebook公司更名？全球芯片荒？数字艺术品卖出天价？没有哪个水晶球準确预测了2021年的科技业景象。这也正是这项一年一度的活动有意思和令人兴奋的地方之一——我们团队每年都会集思广益，对接下来一年科技圈会发生些什么进行预测。可就算我们对当前趋势作了最为详尽的分析，以及请来行业专家和占卜工具加持，也依然无法精确预测未来。 有一点可以确定的是，2022年将充满新技术带来的惊喜（参见：混合现实和家用机器人），人们将继续讨论科技对世界带来的影响（参见：社交媒体和可持续性），同时还会有许多意想不到的事情发生，因为最近时常会有一些令你大呼意外的时刻。 以下是我们对2022年科技行业的预测： 电动车价格更亲民人们购买电动车时选择为数不多的局面成为过去。到2022年底时，可供美国消费者挑选的电动车车型将超过100款。而且，许多即将面世的车型要比一两年前的那些价格更亲民。 电动车在美国乘用车保有量中的佔比仍不到3%，研究显示，若想让更多人选择电动车，价格是一大关键因素。（另一个关键因素是基础设施。美国目前的公共快速充电桩数量远远达不到专家估计的需求量。） 为企业提供咨询服务的Guidehouse公司首席研究分析师斯科特·谢帕德（Scott Shepard）指出，从全球范围看，2015年至2020年间，尽管电动车的平均续航里程增加了45%，但平均定价下降了30%。他预计2022年这一趋势将得以延续，届时至少还有四款价格在4万美元以下的电动车登陆美国。 福特F-150「闪电」电动皮卡（Ford F-150 Lightning，制造商建议零售价39,974美元起）的表现或许可以给我们一点启发。该公司在接到近20万辆F-150「闪电」的订单后，不得不关闭了预购服务；福特最新的业绩报告显示，有75%的买家是头一次购买福特品牌的汽车。 与此同时，日产汽车（Nissan）将早年投放市场的聆风（Leaf）电动车的起售价从32,620美元大幅下调至28,375美元。聆风降价后（以及购车时享受7,500美元的稅收抵免后）在同档次的汽车中，价格会低于2022年款本田思域（Honda Civic）燃油掀背车。除此之外，还有许多电动汽车车型的起售价都在4万美元以下，如雪佛兰（Chevrolet）、现代（Hyundai）、起亚（Kia）、马自达（Mazda）、Mini和大众（Volkswagen）的一些车型，它们有的已经上市，有的不久便会投放市场。 可持续性继续受关注微软（Microsoft）利用回收的海洋塑胶垃圾做了一款鼠标。苹果（Apple）的最新款iPhone和iPad都含有回收材料。亚马逊（Amazon）的新款Echo智能喇叭的显示屏同样用到了回收塑胶，而且在待机状态下十分节能。2021年每一场重要的科技主题演讲中，都会包含应对气候变化的环节，2022年这一现象只会有增无减。 可以说，科技业留下的碳足迹比航空业更大，眼下科技公司正在努力改变这一现状，实现可持续发展。无论是从手机、平板还是电脑的设计中，都可以看到它们这方面的努力。 可持续性还意味着，你的同一部电子设备可以用得更久。加州大学洛杉矶分校（University of California, Los Angeles）管理学教授马加利·德尔马斯（Magali Delmas）说，「真正的问题在于科技业研发的产品使用壽命太短，而且很难维修，接着就被扔进垃圾填埋场。」 电子设备制造商也在尝试解决这一问题。苹果将在2022年初推出一项自助维修计划，按照该计划，用户和独立技术人员可以利用正版苹果零部件自主维修设备，这也是倡导「维修权」的人士一直以来都在游说的一项举措。戴尔公司（Dell）近日展示了概念性笔记本电脑Luna，它使用的螺丝更少，而且没有风扇，从而最大限度地提高了可维修性。 社交媒体加大力度保护未成年人2021年我们意识到，社交媒体类应用程式——尤其是Instagram和TikTok——在阻止年轻用户观看不良内容以及防止他们沉迷方面做得不够。2022年我们会看到，美国国会议员和社交媒体公司将加大对儿童的保护力度。 针对目前主要由算法驱动的内容展示方式，Instagram已承诺在2022年初推出替代方案。按照现有方式，用户基本无法控制自己可以看到哪些内容。Instagram负责人亚当·莫塞里（Adam Mosseri）在12月向美国参议院一个下属委员会表示，该公司2022年将推出更多针对青少年的家长控制功能，包括从3月开始，允许家长设定时间限制。 TikTok则表示，它已开始调整算法，这样人们看到的内容中就不会充斥著大量有关紊乱饮食、抑郁症和其他涉及不良话题的影片。该公司发言人说，它将继续依据用户年龄对功能作出限制，同时也会为家长提供工具。 然而，由于不相信这些企业会自己做出必要的改变，议员们跨党派合作，著手制定新的法律。马萨诸塞州民主党参议员埃德·马基（Ed Markey）和路易斯安那州共和党参议员比尔·卡西迪（Bill Cassidy）提出了新版本的《儿童和青少年网络私隐保护法》（Children and Teens’ Online Privacy Protection Act, 简称：Coppa 2.0），提议将目前的私隐保护对象扩大到13至15岁的用户，同时禁止针对儿童的个性化广告，等等。康涅狄格州民主党参议员理查德·布卢门塔尔（Richard Blumenthal）和田纳西州共和党参议员玛莎·布莱克本（Marsha Blackburn）也在推动立法，要求提高社交媒体算法的透明度。 芯片！芯片！从2020年初开始，全球随处可见的疫情「封锁令」使得人们对许多搭载微芯片的东西有了更大的需求——与外界联系需要的智能手机、居家办公和学习需要的笔记本电脑和台式电脑，以及代替公共交通工具的私家车。 受此影响，2020年及2021年整个上半年，人们对芯片的需求达到了前所未有的程度。有迹象显示，芯片需求已开始回落。作为全球最大的智能手机市场，中国2021年第三季度智能手机销量较上年同期下滑9%。（不过苹果表示iPhone的需求依然强劲，预计公司2021年利润将创下纪录新高。） 没有人确切知道芯片供应何时才能完全满足需求，一些分析师说，这可能要等到2023年，届时从亚利桑那州到北京，将有更多的芯片制造产能投产。但鉴於人们对电子设备的需求将终于得以满足，加之过去18个月内芯片制造商一直开足马力生产，供不应求局面结束的日子或许指日可待。 二手芯片制造设备交易公司SDI Fabsurplus的斯蒂芬·豪伊（Stephen Howe）说，回首2000年时，芯片厂经营者曾预计，芯片需求将一直旺盛，高增长态势也将持续。「可你瞧，到了2000年6月，整个市场经历了断崖式下跌，我敢肯定，这一幕还会重演。」他说。 机器人在哪儿，哪儿就是家能照顾孩子、能洗碗，还能疏通马桶的全能型家用机器人在2022年还不会出现，但比立式智能喇叭、扫地机器人再能干一点的家用机器人会在今年走进人们的视野——而且它或许还会提供陪伴功能。 亚马逊基於智能喇叭Alexa打造的家用机器人Astro可以利用传感器在家中自由移动。它不仅具备了Alexa的基本功能（如播放音乐、回答问题等），还带有监控功能，当你不在家时，可以通过自带摄像头替你「看家」。如果一位年长亲人的家中有Astro，你还可以通过「Alexa Together」功能与亲人远程互动。 亚马逊负责设备与服务业务的副总裁大卫·林普（David Limp）说，Astro代表着向「环境智能」（AMI）的转变 ，有了这种技术，我们的电脑将退至幕后，让我们「在现实世界中互动，而不是把头埋在手机里。」 亚马逊从2021年12月开始，向一小批受邀测试者递送了这款售价1,000美元的机器人。林普说，提出测试申请的人数比他当初的预期高出了许多倍，目前公司正在努力增加2022年的库存。 此外，老年伴侣机器人ElliQ等类似设备正在65岁及以上的独居老人中接受测试。厂商计划2022年扩展ElliQ的功能，增加管家服务，让用户可以通过ElliQ订购食品杂货等。 「混合现实」成为现实我们知道，过去大约五年来，有些人一直在说虚拟现实（VR）将改变我们的生活。但我们向你保证，2022年VR技术将取得重大进展，增强现实（AR）也会带来更好的体验。（记住，VR是把你带入一个虚拟世界，AR是在现实世界中加入数位化元素，二者合称混合现实，即Mixed Reality，但我们不会将其简称为MR。） Meta（即过去的Facebook）计划发布一款头戴式设备，其功能比目前的Quest 2更先进，价格也更贵。设备中的新传感器和改进后的光学元件会让虚拟世界中的你与在现实中有更高的相似度（项目代号「坎布里亚计划」（Project Cambria））。你的一颦一笑也会体现在虚拟场景中。另外，这款头显能够显示你所处的真实空间，但会加入一些数位化功能——比如在真实的桌子上方投射出一张巨大的虚拟萤幕。这些都是Meta构建元宇宙目标中的一砖一瓦——在这个虚拟世界中，我们可以工作、购物、閒逛，还可以做很多事情。 在这场「元宇宙」竞赛中，不会只有Meta一名选手。有报导称，苹果可能会在2022年晚些时候推出一款混合现实头戴式设备。据天风国际证券（TF International Securities）研究苹果公司的分析师郭明錤说，这款头显将搭载苹果自己的芯片（就像苹果MacBook里的芯片），而且同时具有VR和AR功能。苹果发言人拒绝置评。 不拘於手腕之上的健康传感器近十年来，健康追踪器一直都是戴在我们的手腕上，如今它们开始挣脱这种束缚。智能戒指Oura虽然没有显示器，但它配备了迷你传感器，可以监测心率、血氧水平和皮肤温度。Whoop卖的是文胸、紧身裤等服装，但追踪器被巧妙地置于这些服饰中。Google（Google）的Nest Hub智能屏不带摄像头，它利用雷达来监测使用者的睡眠情况。除此之外，Sleep Number、Eight Sleep等公司的智能床垫也可以记录睡眠状态。 Oura首席产品官克里斯·贝赫勒（Chris Becherer）说，「这不涉及什么新技术，也不是什么新型传感器。」他还说，硬件制造商正在寻找更多方法来简化追踪方式。 下一个重要的健康设备可能是你的耳塞。据《华尔街日报》（Wall Street Journal）报导，苹果正在研究利用AirPods来读取体温和监测身体姿势的可行性。据熟悉这些计划的人告诉我们的同事，这类耳塞会从耳朵内部测量佩戴者的核心体温，并依靠运动传感器来监测用户是否驼背。《华尔街日报》报导，苹果还在研发有助於检测抑郁症的iPhone功能。 更亮的显示屏如果你发现，你的手机或是智能手表的显示屏比笔记本电脑、电视或是汽车仪表盘的显示屏更亮，对比度也要更高，那么你就已经见证了显示屏技术近年来最大的进化式飞跃之一。 自iPhone X之后，苹果的高端iPhone一直都在使用有机发光二极管（OLED）萤幕，这种技术与液晶显示屏（LCD）的区別在于其中的每个像素可以独立发光，而无需另外的光源。使用OLED的好处是，不仅显示屏可以做得更薄（甚至可以折叠），而且对比度和能效也更高。 三星（Samsung）在手机上应用OLED的时间更早——这家韩国电子产品巨头是OLED技术的先驱之一。以往你可能见过一些OLED电视，但它们的价格异常昂贵。现在，这项技术的制造成本正变得越来越低。2021年，配备OLED显示屏的笔记本电脑开始出现，2022年将有更多类似机型面世。对平板电脑和电视来说也是如此。 微发光二极管（MicroLED）是另一种先进的显示屏技术，有朝一日或许会走进寻常百姓家，成为更多人负担得起的「OLED继任者」。三星已利用这些微小的自发光像素研制出了影片墙，但成本却是天文数字。在2022年1月举行的国际消费电子展CES上，智能眼镜生产商Vuzix计划展示带有微型MicroLED立体显示屏的智能眼镜， 可适用于工作场合的AR场景。 密码登录渐行渐远密码有时会成为「雷区」。它们会被人猜中，或是被盗。於是为了防止骇客入侵，我们用上了密码管理软体，利用它们为每一种服务创造独一无二的随机密码。此时此刻，我们当中有许多人甚至都不知道自己的密码！谢天谢地，不需要以密码登录的服务正变得越来越多。 例如，你可以通过往电子邮箱里发送一个特殊链接来登录《华尔街日报》的网站（WSJ.com）。登录工作协同平台Slack和支付平台PayPal也可以采取同样的方式。支付系统Shopify和订座软体Resy会向你的手机发送验证码。微软最近允许用户跳过密码，转而通过应用程式Microsoft Authenticator生成的验证码进行身份验证，这串代码类似通过手机或电子邮箱发送的安全密钥或是验证码。 致力於保护用户身份的科技公司Transmit Security通过一项研究发现，如果一项服务可以用生物特征识別方式来验证身份，那么消费者订购的概率会增加44%；而如果有密码之外的其他登录方式，消费者订购的概率会增加35%。「2022年不会标志著密码时代的终结，但在密码走向消亡的过程中，我们会看到一些具有转折意义的事件。」行业组织FIDO Alliance执行董事安德鲁·希基亚（Andrew Shikiar）说，该组织提倡通过安全密钥、人脸识別、指纹或语音密码来验证身份。 网络提速再见，3G。感谢你为老版本的Kindle电子阅读器以及奶奶的翻盖手机提供了联网功能。2022年，美国运营商将淘汰老旧的第三代蜂窝网络，为超高速的第五代移动通信（5G）腾出更多空间。没错，这意味着那些3G设备很快就会变成一堆废铁。（美国电话电报公司AT&amp;T的3G服务结束日期为2022年2月，T-Mobile和Verizon分別为7月和12月。） 这些电信公司计划在未来一年扩大网络。T-Mobile说，到2022年年底时，它的服务将再覆盖5,000万美国人。享受5G服务的不仅只有手机。移动运营商正在固话运营商无法提供宽带的地方，利用5G网络提供家用互联网服务。2021年9月，T-Mobile将业务扩展到佛罗里达州和北卡罗来纳州的一些城市，Verizon则将加州佛里蒙特（Fremont）和纽约州尼亚加拉瀑布城（Niagara Falls）纳入了自己的5G家用网络。 不管是家用网络还是移动端网络，我们的网速都很有可能变得更快，尽管这当中也会有一些「减速带」。美国总统拜登（Biden）的基建法案已於2021年11月获得通过，虽然其中包括了用于扩大农村地区宽带接入的650亿美元资金，但各州若想拿到这笔钱，还需经过漫长的申请过程。此外，AT&amp;T和Verizon近日在铺设5G网络的过程中也遇到了阻碍，因为美国航空管理局（Federal Aviation Administration，简称FAA）担心驾驶舱安全系统可能受到干扰。 加密货币不再小众当人们谈论加密货币和非同质化代币（NFT）时，你表面上点头附和，其实心里在想，关于去中心化网络和区块链的种种讨论都会是过眼云烟。抱歉，你想错了。事实上在2022年，你已在使用的应用程式、服务和游戏中会出现购买、出售和转账数字货币及代币的工具，如此一来，你或许已经置身其间了。 目前，PayPal的应用程式Venmo和支付公司Block（前身为Square）旗下Cash App都提供了便捷的加密货币购买及转账方式。Cash App最近还新增了一项简易功能，可以通过借记卡向任意美国用户提供比特币。越来越多的购物场所也将开始接受你最喜爱的加密货币。眼下，特斯拉（Tesla）的某些特定商品就可以接受狗狗币支付。 你会加入像DeSo这样由区块链技术支撑的去中心化社交媒体网站吗？也许不会，但现有的社交媒体应用程式可能会开始拥抱这个世界。Instagram负责人莫塞里说，Instagram正在「积极探索NFT，以及我们如何才能让更广泛的受众更容易接触到它们。」 无人机送货今年你可能会头一次收到由无人机投送的包裹，不过这取决於你所在的区域。 以色列初创公司Flytrex在北卡罗来纳州运营有三个配送站，它刚刚获得FAA批准，可以在方圆1英里（约1.6公里）左右的范围内利用无人机送货。该公司说，获得此项授权后，它可以为包括沃尔玛（Walmart）在内的多家零售商，面向1万多户家庭提供送货服务。Google母公司Alphabet Inc.旗下Wing Aviation去年10月表示，它正在对无人机送货进行测试，预计2022年将在人口稠密的城市地区——达拉斯-沃斯堡（Dallas-Ft. Worth）——首次推出商用无人机送货服务。 与此同时，未来一年中，综合医疗服务提供商与产品生产商嘉德诺（Cardinal Health）将通过一种特殊的无人机——Zipline制造的小型固定翼飞机——向北卡罗来纳州坎纳波利斯（Kannapolis）一处配送中心方圆10英里（约16公里）内的药店提供补给。Zipline说，它还在沃尔玛总部所在地阿肯色州本顿维尔（Bentonville）为该公司提供送货上门服务。 根据FAA的规定，所有这些无人机将继续由人类操控。 随着FAA发放的许可越来越多，加之亚马逊、联合包裹速递服务公司（UPS）及其他十余家公司的无人机也都跃跃欲试，2022年可能会有更多企业竞相跟进。 英文原文：Tech That Will Change Your Life in 2022 原文链接：https://theinitium.com/article/20220116-wsj-tech-that-will-change-your-life-in-2022/ 端传媒：https://theinitium.com/misc/about/","categories":[],"tags":[{"name":"repost","slug":"repost","permalink":"https://imlike.cc/tags/repost/"}]},{"title":"pgsql调优转载","slug":"pgsql调优转载","date":"2022-04-13T07:11:42.000Z","updated":"2024-03-14T14:55:29.480Z","comments":true,"path":"3990369431.html","link":"","permalink":"https://imlike.cc/3990369431.html","excerpt":"","text":"2022-4-13 更新—————- select * from pg_tables where tablename like ‘pg_temp%’; 不会有对应的表 select * from pg_namespace where nspname like ‘pg%temp%’; 但是会床创建临时的scheme 持续更新 —————————– 官方文档 根据进程可以查询命令执行的时间和效率 查询耗时 123SELECT procpid, START, now() - START AS lap, current_query FROM ( SELECT backendid, pg_stat_get_backend_pid (S.backendid) AS procpid,pg_stat_get_backend_activity_start (S.backendid) AS START,pg_stat_get_backend_activity (S.backendid) AS current_query FROM (SELECTpg_stat_get_backend_idset () AS backendid) AS S) AS S WHERE current_query &lt;&gt; &#39;&lt;IDLE&gt;&#39; and procpid&#x3D;58762 ORDER BY lap DESC; 查询连接 查询进程 1SELECT * FROM pg_stat_activity where pid&#x3D;&#x3D;?? and query &#x3D;?? and pg_stat_statements 123修改postgresql.conf的shared_preload_libraries增加pg_stat_statements来载入③执行SQL启动插件：create extension pg_stat_statements; 重置计数器（可选操作）。 1select pg_stat_reset(); 1select pg_stat_statements_reset(); 使用命令查看最耗时的SQL 1select * from pg_stat_statements order by total_time desc limit 5; 查询读取Buffer次数最多的SQL，buffer次数多，可能是因为没有索引，也同时导致了cpu高。 1select * from pg_stat_statements order by shared_blks_hit+shared_blks_read desc limit 5; 获取后面的 1234567891011121314151617181920212223242526272829select to_char(trigger_start_time,&#39;YYYY-MM-DD 00:00:00&#39;) as timePoint,sum(trigger_count) as totalCount from network_attack where trigger_start_time &gt;&#x3D; to_timestamp($1,&#39;yyyy-MM-dd hh24:mi:ss&#39;) and trigger_start_time &lt; to_timestamp($2,&#39;yyyy-MM-dd hh24:mi:ss&#39;) group by timePoint order by timePoint descrelid oid 一个表的 OIDschemaname name 这个表所在的模式的名称relname name 这个表的名称seq_scan bigint 在这个表上发起的顺序扫描的次数seq_tup_read bigint 被顺序扫描取得的活着的行的数量idx_scan bigint 在这个表上发起的索引扫描的次数idx_tup_fetch bigint 被索引扫描取得的活着的行的数量n_tup_ins bigint 被插入的行数n_tup_upd bigint 被更新的行数（包括 HOT 更新的行）n_tup_del bigint 被删除的行数n_tup_hot_upd bigint 被更新的 HOT 行数（即不要求独立索引更新的行更新）n_live_tup bigint 活着的行的估计数量n_dead_tup bigint 死亡行的估计数量n_mod_since_analyze bigint 从这个表最后一次被分析后备修改的行的估计数量last_vacuum timestamp with time zone 上次这个表被手动清理的时间（不统计VACUUM FULL）last_autovacuum timestamp with time zone 上次这个表被自动清理守护进程清理的时间last_analyze timestamp with time zone 上次这个表被手动分析的时间last_autoanalyze timestamp with time zone 上次这个表被自动清理守护进程分析的时间vacuum_count bigint 这个表已被手工清理的次数（不统计VACUUM FULL）autovacuum_count bigint 这个表已被自动清理守护进程清理的次数analyze_count bigint 这个表已被手工分析的次数autoanalyze_count bigint 这个表已被自动清理守护进程分析的次数 seq_tup_read 远大于 idx_tup_fetch 说明索引建的有问题 n_tup_upd 说明有频繁更新的动作","categories":[],"tags":[]},{"title":"maven 从pom.xml 下载jar包","slug":"maven 从pom.xml 下载jar包","date":"2022-04-12T05:12:35.000Z","updated":"2024-03-14T14:55:29.479Z","comments":true,"path":"1868978831.html","link":"","permalink":"https://imlike.cc/1868978831.html","excerpt":"","text":"打包会有问题 root lib-A 当前是需要直接打root的jar包，但是lib包下面有直接引用的本地jar并不在maven仓库当中使用如下 格式如下： 1234567&lt;dependency&gt; &lt;groupId&gt;com.shentong&lt;/groupId&gt; &lt;artifactId&gt;pinyinAnalyzer&lt;/artifactId&gt; &lt;version&gt;4.3.1&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;$&#123;pom.basedir&#125;/src/main/resources/lib/pinyinAnalyzer4.3.1.jar&lt;/systemPath&gt; &lt;/dependency&gt; 打包中如果有测试代码编译不通过的可用参考添加 1234567&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;testFailureIgnore&gt;true&lt;/testFailureIgnore&gt; &lt;/configuration&gt;&lt;/plugin&gt; 我后面发现打的jar包lib文件夹下面的确是有lib-A项目，但是啊就是跑的后面一旦涉及引用lib-A下面的本地第三方jar就说类找不到网上有人给出如下解决方案 1234567&lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;includeSystemScope&gt;true&lt;/includeSystemScope&gt; &lt;/configuration&gt; &lt;/plugin&gt; 我分别在lib-A和root项目下添加了，神奇的事发生了，打包的后服务可以运行，但是lib-A下面的controller全都没有，对应的请求都是404，人都快裂开了 只能排除法先注释root下发现不行，后面注释lib-A下居然成功了，真TMD邪门 补充maven install 包含外部jar的打包情况——- 参考链接 java - Maven add jars through systemPath/system but not added to war or anywhere else - Stack Overflow Maven package 时出现should not point at files within the project directory_隐0士的博客-CSDN博客 在maven的pom文件中引用本地的jar 12345678 &lt;dependency&gt; &lt;groupId&gt;com.belerweb&lt;/groupId&gt; &lt;artifactId&gt;pinyin4j&lt;/artifactId&gt; &lt;version&gt;2.5.1&lt;/version&gt;&lt;!-- &lt;scope&gt;system&lt;/scope&gt;--&gt;&lt;!-- &lt;systemPath&gt;$&#123;project.basedir&#125;/src/main/resources/lib/pinyin4j-2.5.0.jar&lt;/systemPath&gt;--&gt; &lt;systemPath&gt;$&#123;pom.basedir&#125;/src/main/resources/lib/pinyin4j-2.5.1.jar&lt;/systemPath&gt; &lt;/dependency&gt; 但是打包时会报错 should not point at files within the project directory dependencies.dependency.systemPath 在stackoverflow上有两种方式 第一种是pom中添加jar所在文件夹的仓库 1234567&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;localrepository&lt;/id&gt; &lt;url&gt;file://$&#123;project.basedir&#125;/src/main/resources/lib&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt; 但是在打包的时候还是会出问题 第二种使用plugin来实现 123456789101112131415161718192021222324&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-install-plugin&lt;/artifactId&gt; &lt;version&gt;2.5.2&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;compile&lt;/phase&gt; &lt;!-- &lt;goals&gt; &lt;goal&gt;copy-dependencies&lt;/goal&gt; &lt;/goals&gt;--&gt; &lt;configuration&gt; &lt;groupId&gt;com.belerweb&lt;/groupId&gt; &lt;artifactId&gt;pinyin4j&lt;/artifactId&gt; &lt;version&gt;2.5.0&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;file&gt;$&#123;pom.basedir&#125;/src/main/resources/lib/pinyinAnalyzer4.3.1.jar&lt;/file&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; tips 下面的两种尝试过失败也列出来 使用configuration复制来自于网络和正确的配置有出入 12345678910111213141516&lt;execution&gt; &lt;id&gt;pinyin4j&lt;/id&gt; &lt;phase&gt;compile&lt;/phase&gt; &lt;configuration&gt; &lt;file&gt;$&#123;pom.basedir&#125;/src/main/resources/lib/pinyin4j-2.5.0.jar&lt;/file&gt; &lt;repositoryLayout&gt;default&lt;/repositoryLayout&gt; &lt;groupId&gt;com.belerweb&lt;/groupId&gt; &lt;artifactId&gt;pinyin4j&lt;/artifactId&gt; &lt;version&gt;2.5.0&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;generatePom&gt;true&lt;/generatePom&gt; &lt;/configuration&gt; &lt;goals&gt; &lt;goal&gt;pinyin4j&lt;/goal&gt; &lt;/goals&gt;&lt;/execution&gt; 使用clean 12345678910111213141516&lt;execution&gt; &lt;id&gt;pinyinAnalyzer&lt;/id&gt; &lt;phase&gt;clean&lt;/phase&gt; &lt;configuration&gt; &lt;file&gt;$&#123;pom.basedir&#125;/src/main/resources/lib/pinyinAnalyzer4.3.1.jar&lt;/file&gt; &lt;repositoryLayout&gt;default&lt;/repositoryLayout&gt; &lt;groupId&gt;com.shentong&lt;/groupId&gt; &lt;artifactId&gt;pinyinAnalyzer&lt;/artifactId&gt; &lt;version&gt;4.3.1&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;generatePom&gt;true&lt;/generatePom&gt; &lt;/configuration&gt; &lt;goals&gt; &lt;goal&gt;install-file&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; 使用phase&gt;init&lt;/phase 12345678910111213141516&lt;execution&gt; &lt;id&gt;pinyinAnalyzer&lt;/id&gt; &lt;phase&gt;init&lt;/phase&gt; &lt;configuration&gt; &lt;file&gt;$&#123;pom.basedir&#125;/src/main/resources/lib/pinyinAnalyzer4.3.1.jar&lt;/file&gt; &lt;repositoryLayout&gt;default&lt;/repositoryLayout&gt; &lt;groupId&gt;com.shentong&lt;/groupId&gt; &lt;artifactId&gt;pinyinAnalyzer&lt;/artifactId&gt; &lt;version&gt;4.3.1&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;generatePom&gt;true&lt;/generatePom&gt; hconfiguration&gt; &lt;goals&gt; &lt;goal&gt;install-file&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; 之前一直用IntelliJ IDEA 的直接下载jar包,但是这次一直下载不下来指定版本jar包,看本地的代码库localRepository本地没有,看本地部署的maven中心,所以使用了参考地址 创建pom.xml 文件 123456789101112131415161718&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;Spider&lt;/groupId&gt; &lt;artifactId&gt;Spider&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;dependencies&gt; &lt;!-- https://mvnrepository.com/artifact/junit/junit --&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 使用mvn命令 下载后在本地仓库看到, 然后在IntelliJ IDEA刷新 1mvn dependency:copy-dependencies","categories":[],"tags":[{"name":"maven","slug":"maven","permalink":"https://imlike.cc/tags/maven/"},{"name":"Java","slug":"Java","permalink":"https://imlike.cc/tags/Java/"}]},{"title":"cloudflare的DNS解析和page服务以及转发到指定ip地址","slug":"cloudflare的DNS解析和page服务以及转发到指定ip地址","date":"2022-04-09T14:16:10.000Z","updated":"2024-03-14T14:55:29.476Z","comments":true,"path":"a4faf5d1.html","link":"","permalink":"https://imlike.cc/a4faf5d1.html","excerpt":"","text":"Origin CA certificates · Cloudflare SSL docs","categories":[],"tags":[]},{"title":"安卓推送消息","slug":"安卓推送消息","date":"2022-04-05T08:45:32.000Z","updated":"2024-03-14T14:55:29.484Z","comments":true,"path":"514a98ba.html","link":"","permalink":"https://imlike.cc/514a98ba.html","excerpt":"","text":"目前想给自己的手机客户端推送消息和提醒，之前一直用的是 WxPusher微信推送服务, Server酱·Turbo版，通过关注指定的微信公众号来实现微信提醒，原理图如下 但是微信始终有缺陷，我一直在考虑有没有一直利用现有的安卓和苹果推送的壳APP，然后调用接口推送消息到客户端。 pushdeer.com 设置设备 设置key 开启APP的通知权限 输入推送消息","categories":[],"tags":[]},{"title":"nginx详细的配置和用途","slug":"nginx详细的配置和用途","date":"2022-03-23T18:51:11.000Z","updated":"2024-03-14T14:55:29.480Z","comments":true,"path":"2474801124.html","link":"","permalink":"https://imlike.cc/2474801124.html","excerpt":"","text":"2022-03-23补充————后置代理和从定向 location /api/test.html { return 301 http://8.136.132.68:8081/404.html;return 301 $scheme://$host:$server_port/404.html; &#125; location /stg/swagger-ui.html { proxy_pass http://$host:$server_port/404.html; } 2022-1-20——————— 在 Nginx 中配置二级域名 - Mincong Huang 目前ss证书都是Let’s Encrypt 和cloudflare , 12345678910111213141516171819202122232425262728293031323334353637 # 配置证书地址 ssl_certificate &#x2F;etc&#x2F;nginx&#x2F;imlike.cc.pem; ssl_certificate_key &#x2F;etc&#x2F;nginx&#x2F;imlike.cc.key; server&#123; listen 443 ssl http2; listen [::]:443 ssl http2; server_name _; location &#x2F; &#123; # 一些基本反向代理配置以及获取请求真是IP地址 proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-Ip $remote_addr; proxy_set_header X-Forwarded-For $remote_addr; proxy_pass http:&#x2F;&#x2F;8.136.132.68:5212&#x2F;; &#125; # access_log &#x2F;home&#x2F;wwwlogs&#x2F;git.aboutme.ink.log; &#125; # 监听80端口,提供本地静态服务 server &#123; listen 80; server_name localhost; #charset koi8-r; #access_log logs&#x2F;host.access.log main; location &#x2F; &#123; root &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html; index index.html index.htm; &#125;&#125;# 监听80的配置 跳转到对应HTTPSserver &#123; listen 80; server_name _; return 301 https:&#x2F;&#x2F;$host$request_uri;&#125; 如果出现下图的问题,需要看看后置代理的服务是否有问题和nginx日志 curl -iL file.imlike.cc 2021-12-31补充 12345678gzip on; #是否开启gzip模块 on表示开启 off表示关闭 gzip_static on;gzip_buffers 4 16k; #设置压缩所需要的缓冲区大小gzip_comp_level 6; #压缩级别1-9，数字越大压缩的越好，也越占用CPU时间gzip_min_length 1k; #设置允许压缩的最小字节gzip_http_version 1.0; #设置压缩http协议的版本,默认是1.1gzip_types text&#x2F;plain application&#x2F;x-javascript text&#x2F;css text&#x2F;javascript application&#x2F;x-httpd-php image&#x2F;jpeg image&#x2F;gif image&#x2F;png application&#x2F;octet-stream application&#x2F;javascript; #设置压缩的文件类型gzip_vary on; #加上http头信息Vary: Accept-Encoding给后端代理服务器识别是否启用 gzip 压缩 2021-12-17补充使用nginx来代理并替换内容 12345678910111213141516171819202122location &#x2F;uu&#x2F; &#123;# 添加gzip的支持 否则geoserver返回的内容不能被替换 proxy_set_header Accept-Encoding &quot;&quot;; # 你想要代理的地址 proxy_pass http:&#x2F;&#x2F;192.168.11.20:8080&#x2F;geoserver&#x2F;; # sub_filter_types text&#x2F;html; # 参数分别为 on(一次),off(全部替换) sub_filter_once off; # 需要替换的内容,就是网页的内容 sub_filter &#39;http:&#x2F;&#x2F;127.0.0.1&#x2F;geoserver&#x2F;&#39; &#39;http:&#x2F;&#x2F;127.0.0.1:8088&#x2F;uu&#x2F;&#39;; proxy_redirect default; # 返回重定向的 # proxy_set_header Host $host; # proxy_set_header Host $host:$server_port; # proxy_set_header X-Real-IP $remote_addr; # proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125; tips nginx -V 查看是否支持 http_sub_module 否则不会替换 sub_filter 只支持替换,不支持正则表达式,复杂的需要使用第三方的库 {IP}:{PORT}/uu/ 才能被识别 但是geoserver通常访问是 xxx/geoserver/web 所以代理后是 xxx/uu/web/ 否则跳转不过去 2021-08-20 补充nginx使用和配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051重定向后找不到 host&#x2F;uu&#x2F;geoserver -&gt; host&#x2F;uu&#x2F;geoserver&#x2F;web&#x2F; -&gt; host&#x2F;uu&#x2F;geoserver&#x2F;index.html&#x2F; 是相对路径 没有 &#x2F; 绝对路径配置文件 location &#x2F;uts&#x2F; &#123; proxy_pass http:&#x2F;&#x2F;192.168.11.20:90&#x2F;uu&#x2F;; proxy_redirect default; proxy_set_header Host $host; &#125; location &#x2F;uu&#x2F; &#123; sub_filter_once off; # sub_filter_types text&#x2F;html; sub_filter &quot;http:&#x2F;&#x2F;192.168.11.20:8080&#x2F;geoserver&#x2F;&quot; &quot;http:&#x2F;&#x2F;127.0.0.1&#x2F;uu&#x2F;&quot;; proxy_pass http:&#x2F;&#x2F;192.168.11.20:8080&#x2F;geoserver&#x2F;; proxy_redirect default; # 返回重定向的 # proxy_set_header Host $host; # proxy_set_header Host $proxy_host; # proxy_set_header X-Real-IP $remote_addr; # proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125; location &#x2F;uts &#123; proxy_host 192.168.11.20:8080&#125; http:&#x2F;&#x2F;127.0.0.1&#x2F;uts&#x2F; &#x2F;uts&#x2F; http:&#x2F;&#x2F;127.0.0.1&#x2F;uts&#x2F;configDetail &#x2F;uts&#x2F;configDetaillocation &#x2F;uts&#x2F; &#123; proxy_host 192.168.11.20:8080&#x2F;&#125; http:&#x2F;&#x2F;127.0.0.1&#x2F;uts&#x2F; &#x2F; http:&#x2F;&#x2F;127.0.0.1&#x2F;uts&#x2F;configDetail configDetaillocation &#x2F;uts&#x2F; &#123; proxy_host 192.168.11.20:8080&#x2F;ss&#x2F;&#125; http:&#x2F;&#x2F;127.0.0.1&#x2F;uts&#x2F; &#x2F; http:&#x2F;&#x2F;127.0.0.1&#x2F;uts&#x2F;configDetail configDetaillocation &#x2F;uts&#x2F; &#123; proxy_host 192.168.11.20:8080&#125; http:&#x2F;&#x2F;127.0.0.1&#x2F;uts&#x2F; &#x2F;uts&#x2F; http:&#x2F;&#x2F;127.0.0.1&#x2F;uts&#x2F;configDetail &#x2F;uts&#x2F;configDetaillocation &#x2F;uts &#123; proxy_host 192.168.11.20:8080&#x2F;&#125; http:&#x2F;&#x2F;127.0.0.1&#x2F;uts&#x2F; &#x2F;&#x2F; http:&#x2F;&#x2F;127.0.0.1&#x2F;uts&#x2F;configDetail &#x2F;&#x2F;configDetail 2021-7-29补充 root alias区别 同样配置nginx目录 alias的路径只用于 url 不会根据这个去, 其配置的文件夹来找寻具体目录地址,同时如果在alias的根路径下还会显示 路径下所有文件.相当于 alias设置虚拟目录 但是root不是它会匹配,同样的如果root配置的location对应 文件夹不存在那就会包404 借鉴下面博文https://www.cnblogs.com/kevingrace/p/6187482.html 1）alias指定的目录是准确的，即location匹配访问的path目录下的文件直接是在alias目录下查找的；2）root指定的目录是location匹配访问的path目录的上一级目录,这个path目录一定要是真实存在root指定目录下的；3）使用alias标签的目录块中不能使用rewrite的break（具体原因不明）；另外，alias指定的目录后面必须要加上”/“符号！！4）alias虚拟目录配置中，location匹配的path目录如果后面不带”/“，那么访问的url地址中这个path目录后面加不加”/“不影响访问，访问时它会自动加上”/“； 但是如果location匹配的path目录后面加上”/“，那么访问的url地址中这个path目录必须要加上”/“，访问时它不会自动加上”/“。如果不加上”/“，访问就会失败！5）root目录配置中，location匹配的path目录后面带不带”/“，都不会影响访问。 举例说明（比如nginx配置的域名是www.wangshibo.com）：示例一location /huan/ { alias /home/www/huan/;} 在上面alias虚拟目录配置下，访问http://www.wangshibo.com/huan/a.html实际指定的是/home/www/huan/a.html。注意：alias指定的目录后面必须要加上”/“，即/home/www/huan/不能改成/home/www/huan 上面的配置也可以改成root目录配置，如下，这样nginx就会去/home/www/huan下寻找http://www.wangshibo.com/huan的访问资源，两者配置后的访问效果是一样的！location /huan/ { root /home/www/;} http://blackwolfsec.cc/2018/05/23/Nginx_alias_misconfig_path_traversle/ 借鉴下 补充域名相关的知识 1www.xxx.com&#96; ,&#96;www.zzz.xxx.com&#96;,&#96;www.xxx.com&#x2F;getName www: World Wide Web 万维网 xxx: 注册的名称 zzz: 子域名(Second-level domain；英文缩写：SLD) com:顶级域名(Top-level domains)是org, URL: http://www.example.net/index.html,www.xxx.com/getName 对于第三个需要配置 server-&gt;多个location配置 对于第二中需要配置多个server,或者server-&gt;server_name 转载地址nginx 常用配置记录-知乎专栏 server模块server模块是http的子模块，它用来定义一个虚拟主机 例子： 12345678910111213server &#123; listen 80; server_name localhost www.example.com; root &#x2F;Users&#x2F;yangyi&#x2F;www;# 全局定义，表示在该server下web的根目录 client_max_body_size 100M; keepalive_timeout 20; index index.php index.html index.htm; charset utf-8; access_log logs&#x2F;host.access.log main; #用来指定此虚拟主机的访问日志存放路径，输出格式为main。 error_log logs&#x2F;host.error.log error; #错误日志存放路径，输出格式为error。 error_page 404 &#x2F;404.html; #状态码为404时的时候的网页地址,还可定义500,502之类的 ....&#125; 以上一些配置为在该server下具有全局性，例如 root，可在location中重新定义root 关于server_name 用来指定IP地址或者域名，多个域名之间用空格分开 当我们想定义多个server监听同一个端口但访问的host不一样时，server_name就派上用场了。nginx会根据HTTP请求的header Host选择nginx配置文件里符合条件的server_name的server配置 匹配顺序如下 完全匹配的server_name; 后缀匹配: *.example.com; 前缀匹配: www.example.*; 正则匹配: ~\\w+.com; listen指令里配置了default或default_server的server;如｀listen 80 default｀ 第一个匹配上listen的server。 所以当我们监听的的端口只有一个server配置的时候，server_name 可以不填 关于root和alias的区别两者作用差不多，区别在于最终映射的地址不同，例： 1234567location &#x2F;request_path&#x2F;image&#x2F; &#123; # 如果现在访问 &#x2F;request_path&#x2F;image&#x2F;a&#x2F;b.jpg,root映 # 射的地址为&#x2F;local_path&#x2F;image&#x2F;request_path&#x2F;image&#x2F;a&#x2F;b.jpg, # 而alias为&#x2F;local_path&#x2F;image&#x2F;a&#x2F;b.jpg root &#x2F;local_path&#x2F;image&#x2F;; alias &#x2F;local_path&#x2F;image&#x2F;; &#125; 关于error_page语法：error_page code … [=[response]] uri使用字段: http, server, location, if in location 产生的效果为内部跳转(internal redirect)，即用户页面地址不变，但内容实际上为设置的uri对应的内容我们也可以自定义设置返回的状态码，如：error_page 502 503 =200 /50x.html;这样用户访问产生502 、503的时候给用户的返回状态是200，内容是50x.html。还有一种写法是 = 后面不带状态码，是针对相应的内容不是一个静态的页面，最终返回的状态码取决于对应url服务返回的状态码。 location模块 最主要和最复杂的配置，通过定位和解析url，判断该选择什么配置，支持正则和条件判断； 简单例子，匹配所有请求 1234location &#x2F; &#123; root &#x2F;home&#x2F;www&#x2F;html; index index.php index.html index.htm;&#125; 一个正则匹配的例子： 1234# 匹配.php结尾的请求location ~ \\.php$ &#123; ....&#125; 一个反向代理的例子： 12345678910# 匹配到&#x2F;api开头的路由时候，将请求转发到http:&#x2F;&#x2F;192.168.0.1，但是通常不是直接填写地址，而是设置一个&#96;upstream&#96;配置，后面会提到location &#x2F;api &#123; proxy_pass http:&#x2F;&#x2F;192.168.0.1; #请求转向地址192.168.0.1 #不修改被代理服务器返回的响应头中的location头 proxy_redirect off; #使用nginx反向代理后，如果要使服务获取真实的用户信息，用以下的设置 proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;&#125; vue-router、react-router等路由框架要开启history模式可以选择的nginx配置的例子 1234location &#x2F; &#123; alias static&#x2F;; try_files $uri $uri&#x2F; &#x2F;index.html;&#125; location的匹配规则和顺序 第一种是 = 类型，表示精确匹配，优先级最高，一旦匹配到忽略之后的正则匹配 ^~ 类型，表示前缀匹配，是字符串开头匹配而非正则匹配，当匹配到该规则时，停止往下面的搜索，所以如果存在两个^~ 匹配的时候要注意有顺序之分。优先级比正则高。 ~ 和 ~*,正则匹配，两者区别是后者不区分大小写。有顺序之分，匹配到第一个正则停止搜索。 /uri,普通字符串匹配，无顺序之分，会选择匹配长度最长的配置。 / 通用匹配，匹配所有请求 还有一种特殊匹配类型 @url，只用于nginx内部跳转，例: 1234567location &#x2F; &#123; root &#x2F;var&#x2F;www&#x2F;html; error_page 404 @40x;&#125;location @40x &#123; root &#x2F;var&#x2F;www&#x2F;errors&#x2F;40x.html;&#125; upstream模块 upstream后端服务器提供简单的负载均衡（轮循调度和客户端 IP）。 例子： 1234567891011upstream backend &#123; server backend1.example.com weight&#x3D;5; server backend2.example.com:8080; server unix:&#x2F;tmp&#x2F;backend3;&#125; server &#123; location &#x2F; &#123; proxy_pass http:&#x2F;&#x2F;backend; &#125;&#125; 有常用两种负载均衡支持调度算法，分别是 weight 和 ip_hash 。weight 模式下可以为每个 server 设置weight值，weight值越大，分配到的访问机率越高，ip_hash 为同一个ip的分配同一个后端服务器，这样我们不用解决session共享问题。 include include指令 使nginx配置更加灵活，将部分配置直接拆分出来,分成不同的配置文件 例子： 1http &#123; include mime.types; include vhost&#x2F;*.conf; # 虚拟主机配置&#125; 关于 mime.types：mime type 和 文件扩展名的对应关系一般放在 mime.types 里，然后 用 include mime.types;mime.types作用：通过文件的扩展名设置了Content-Type，Nginx如果没找到对应文件的扩展名的话，就使用默认的Type，默认Type通用 default_type 定义，比如 default_type application/octet-stream ;完整的 mime.types 配置：https://github.com/h5bp/server-configs-nginx/blob/master/mime.types 关于 vhost/*.conf :一个虚拟主机对一个文件配置，放到vhost文件夹下面，然后通过include指令包含进来，这样更便于维护和管理 gzip gzip 压缩，用来对静态资源进行压缩，需要客户端同时支持才有效。 配置： 1234567891011121314151617http &#123; #开启gzip压缩 gzip on; #IE6的某些版本对gzip的压缩支持很不好,故关闭 gzip_disable &quot;MSIE [1-6]\\.(?!.*SV1)&quot;; #HTTP1.0以上的版本都启动gzip gzip_http_version 1.0； #指定哪些类型的相应才启用gzip压缩，多个用空格分隔 gzip_types application&#x2F;javascript application&#x2F;json text&#x2F;css text&#x2F;plain; # 压缩等级，可选1-9，值越大压缩时间越长压缩率越高， # 通常选5，能压缩到原来的1&#x2F;4 gzip_comp_level 5;&#125; rewrite模块（ngx_http_rewrite_mode）rewrite模块配合很多模块一起使用，包含几个指令： break if return rewrite rewrite_log set uninitialized_variable_warn 这里简单介绍下break、if、return，重点介绍 rewrite break语法: break;使用字段: server, location, if此指令的意思是停止执行当前虚拟主机的后续rewrite指令集 例子 12345# 如果访问的文件名不存在，反向代理到localhost 。这里的break也是停止rewrite检查if (!-f $request_filename) &#123; break; proxy_pass http:&#x2F;&#x2F;127.0.0.1;&#125; if语法: if (condition) { … }使用字段: server, locationif 判断一个条件，如果条件成立，则后面的大括号内的语句将执行，相关配置从上级继承。 条件(conditon)可以是如下任何操作： 当表达式只是一个变量时，如果值为空或任何以0开头的字符串都会当做false； 使用“=”和“!=”比较一个变量和字符串； 使用“”做正则表达式匹配，“*”做不区分大小写的正则匹配，“!~”做区分大小写的正则不匹配； 使用“-f”和“!-f” 检查一个文件是否存在； 使用“-d”和“!-d”检查一个目录是否存在； 使用“-e”和“!-e”检查一个文件、目录、符号链接是否存在； 使用“-x”和“ !-x”检查一个文件是否可执行； 1234# 如果提交方法为POST，则返回状态405（Method not allowed）if ($request_method &#x3D; POST) &#123; return 405;&#125; return语法: return code [text];return code URL;return URL;使用字段: server, location, if停止处理并为客户端返回状态码，非标准的444状态码将关闭连接，不发送任何响应头。可以使用的状态码有：204，400，402-406，408，410, 411, 413, 416与500-504。如果状态码附带文字段落，该文本将被放置在响应主体。相反，如果状态码后面是一个URL，该URL将成为location头部值。没有状态码的URL将被视为一个302状态码，这种情况下URL需要以“http://”, “https://”, 或者 “$scheme”开头。 rewrite语法: rewrite regex replacement [flag];使用字段: server, location, if使用nginx提供的全局变量或自己设置的变量，然后结合正则表达式和标志位实现url重写以及重定向。多条 rewrite 顺序靠前且匹配的优先执行。可以通过设置 flag 停止继续处理。如果replacement 以“http://”, “https://”, 或者 “$scheme”开头，那么将立即停止处理并临时重定向给客户端。 flag可以是如下参数： last，完成该rewrite规则的执行后，停止处理后续rewrite指令集；然后查找匹配改变后URI的新location； break，完成该rewrite规则的执行后，停止处理后续rewrite指令集，并不再重新查找；但是当前location内剩余非rewrite语句和location外的的非rewrite语句可以执行； redirect，返回302临时重定向，地址栏会显示跳转后的地址； permanent，返回301永久重定向，地址栏会显示跳转后的地址；即表示如果客户端不清理浏览器缓存，那么返回的结果将永久保存在客户端浏览器中了。 因为301和302不能简单的只返回状态码，还必须有重定向的URL，这就是return指令无法返回301,302的原因了。两种重定向的方式对客户端来说是一样的，就是重新加载另外一个url。我们在实际设置中推荐只使用永久重定向，因为临时重定向可能造成在url为A页面，但显示的是B页面的内容，造成网址劫持。 比较难以理解的是 last 和 break，让我们先了解 nginx 运行的11个阶段，注意到第四阶段： 12345678910111213141516171819typedef enum &#123; NGX_HTTP_POST_READ_PHASE &#x3D; 0, &#x2F;&#x2F;读取请求头 NGX_HTTP_SERVER_REWRITE_PHASE, &#x2F;&#x2F;执行rewrite NGX_HTTP_FIND_CONFIG_PHASE, &#x2F;&#x2F;根据uri替换location NGX_HTTP_REWRITE_PHASE, &#x2F;&#x2F;根据替换结果继续执行rewrite NGX_HTTP_POST_REWRITE_PHASE, &#x2F;&#x2F;执行rewrite后处理 NGX_HTTP_PREACCESS_PHASE, &#x2F;&#x2F;认证预处理 请求限制，连接限制 NGX_HTTP_ACCESS_PHASE, &#x2F;&#x2F;认证处理 NGX_HTTP_POST_ACCESS_PHASE, &#x2F;&#x2F;认证后处理， 认证不通过， 丢包 NGX_HTTP_TRY_FILES_PHASE, &#x2F;&#x2F;尝试try标签 NGX_HTTP_CONTENT_PHASE, &#x2F;&#x2F;内容处理 NGX_HTTP_LOG_PHASE &#x2F;&#x2F;日志处理 &#125; ngx_http_phases; 所以我们再来理解last与break的区别：last： 停止当前这个请求，并根据rewrite匹配的规则重新发起一个请求。新请求又从第一阶段开始执行…break：相对last，break并不会重新发起一个请求，只是跳过当前的rewrite阶段，并执行本请求后续的执行阶段… 通过实例会更加清晰的理解 12345678910111213141516171819server &#123; listen 80 default_server; server_name dcshi.com; root www; location &#x2F;break&#x2F; &#123; rewrite ^&#x2F;break&#x2F;(.*) &#x2F;test&#x2F;$1 break; echo &quot;break page&quot;; &#125; location &#x2F;last&#x2F; &#123; rewrite ^&#x2F;last&#x2F;(.*) &#x2F;test&#x2F;$1 last; echo &quot;last page&quot;; &#125; location &#x2F;test&#x2F; &#123; echo &quot;test page&quot;; &#125;&#125; 请求:http://dcshi.com/break/***输出: break page分析：正如上面讨论所说，break是跳过当前请求的rewrite阶段，并继续执行本请求的其他阶段，很明显，对于/foo 对应的content阶段的输出为 echo “break page”; (content阶段，可以简单理解为产生数据输出的阶段，如返回静态页面内容也是在content阶段；echo指令也是运行在content阶段，一般情况下content阶段只能对应一个输出指令，如同一个location配置两个echo，最终只会有一个echo指令被执行)；当然如果你把/break/里的echo 指令注释，然后再次访问/break/xx会报404，这也跟我们预期一样：虽然/break/xx被重定向到/test/xx,但是break指令不会重新开启一个新的请求继续匹配，所以nginx是不会匹配到下面的/test/这个location；在echo指令被注释的情况下，/break/ 这location里只能执行nginx默认的content指令，即尝试找/test/xx这个html页面并输出起内容，事实上，这个页面不存在，所以会报404的错误。 请求: http://dcshi.com/last/***输出: test page分析: last与break最大的不同是，last会重新发起一个新请求，并重新匹配location，所以对于/last,重新匹配请求以后会匹配到/test/,所以最终对应的content阶段的输出是test page; 解释完了 last 和 break ，我们再写一个实例用来说明 rewrite 可以做什么事情。通过rewrite将不带www的请求统一转向为www： 12345678910server &#123; # ........ server_name www.abc.com ;&#x2F;&#x2F;只留一个 # ......｝server &#123; server_name abc.com; rewrite ^(.*)$ http:&#x2F;&#x2F;www.abc.com$1 permanent; &#125; 或者是： 1234567891011server &#123; listen 80; server_name abc.com www.abc.com; if ( $host !&#x3D; &#39;www.abc.com&#39; ) &#123; rewrite ^&#x2F;(.*) http:&#x2F;&#x2F;www.abc.com&#x2F;$1 permanent; &#125; location &#x2F; &#123; root &#x2F;data&#x2F;www&#x2F;www; index index.html index.htm; &#125;&#125; 参考： 在线文档-nginx-zh WEB请求处理二：Nginx请求反向代理 http://blog.sina.com.cn/s/blog_4f9fc6e10102ux0w.html","categories":[],"tags":[{"name":"nginx","slug":"nginx","permalink":"https://imlike.cc/tags/nginx/"}]},{"title":"分布式文件服务介绍","slug":"distriubte-file-description","date":"2022-03-10T07:33:12.000Z","updated":"2024-03-14T14:55:29.476Z","comments":true,"path":"ce67ced3.html","link":"","permalink":"https://imlike.cc/ce67ced3.html","excerpt":"","text":"对象、文件和块存储：有什么区别？ | IBM 什么是对象存储？ - 对象 vs. 文件 vs. 块 | NetApp 文件存储、块存储还是对象存储？ 对象存储 没有结构的文件可以是图片，日志，视频，音频。所有的对象处于同一层级，没有文件存储那个月的文件夹或者子目录，此外，对象存储不会将所有数据一起存储在一个文件中。对象还包含元数据，这是有关有助于处理和可用性的文件的信息。用户可以使用对象存储设置固定键元数据的值，或者他们可以为与对象关联的自定义元数据创建键和值。 文件存储 role,group,user,promession 文件存储就是原始的，将数据一起保存单个文件中，文件拓展名内心有创建文件和文件类型确定，文件可以存储在NAS(网络附加存储)设备上 块存储 将数据拆分为固定的数据块，使用唯一标识符单独存储，这些块可以存储在不同的环境中，当用户检索一个块时存储系统将这些块重新组合成一个单元，块存储是硬盘驱动器 对象存储在维基百科上的部分介绍 云存储[编辑]主条目：云存储 市场上可用的绝大多数云存储都是对象存储架构。一些值得注意的例子是2006 年 3 月首次亮相的Amazon Web Services S3、Microsoft Azure Blob Storage、Rackspace Files（其代码于 2010 年捐赠给 Openstack 项目并以OpenStack Swift的形式发布）和2010 年 5 月发布的Google Cloud Storage。 基于对象的文件系统[编辑]一些分布式文件系统使用基于对象的架构，其中文件元数据存储在元数据服务器中，文件数据存储在对象存储服务器中。文件系统客户端软件与不同的服务器交互，并将它们抽象为用户和应用程序呈现完整的文件系统。 对象存储系统[编辑]对象存储的一些早期版本被用于归档，因为实现是针对数据服务进行优化的，比如不变性，而不是性能。EMC Centera和 Hitachi HCP（以前称为 HCAP）是两种常用的归档对象存储产品。另一个例子是 Quantum Lattus 对象存储平台。 更多通用的对象存储系统在 2008 年左右进入市场。在雅虎邮箱等 Web 应用程序中“专属”存储系统的惊人增长以及云存储的早期成功的诱惑下，对象存储系统承诺了云的规模和功能存储，能够在企业内或有抱负的云存储服务提供商处部署系统。 混合存储[编辑]一些对象存储系统支持统一文件和对象 (UFO) 存储，允许一些客户端将对象存储在存储系统上，同时其他客户端将文件存储在同一存储系统上。虽然由于与混合旋转磁盘和闪存存储混淆，“混合存储”不是这个概念被广泛接受的术语，但在某些对象存储产品中可以使用 [16]对同一组数据的互操作接口。 “俘虏”对象存储[编辑]当对象存储产品没有商用或用例非常具体时，一些大型互联网公司开发了自己的软件。Facebook 著名地发明了他们自己的对象存储软件，代号为 Haystack，以有效地满足他们特殊的大规模照片管理需求。[17] 基于对象的存储设备[编辑]协议和设备层的对象存储是 20 年前提出的[*模糊] ，并在近 10 年前被批准用于SCSI命令集[*模糊]，称为“基于对象的存储设备命令”（OSD），[18]然而，它有直到希捷 Kinetic 开放式存储平台开发后才投入生产。[19] [20]对象存储设备 的SCSI命令集是由 SNIA 的一个工作组为国际信息技术标准委员会(INCITS) 的 T10 委员会开发的。[21] T10 负责所有 SCSI 标准。 如何选择最佳 Kubernetes 存储？ Kubernetes — Storage Overview — PV, PVC and Storage Classo | Medium目前流行开源分布式存储系统的优劣对比分析 - 墨天轮 分布式服务的简单理解 数据库存储选型经验总结 主流分布式文件系统选型，写得太好了！ - 知乎 分布式文件系统架构对比_架构_Davies_InfoQ精选文章 对象存储、文件存储、块存储的区别和联系 | Zollty’s Blog 目前文件服务器分为原生云大数据之前和之后，原生云主要是应对当前大量非结构数据存储的需求，同时支持的对象存储（oss,amazon s3），文件存储实现((posix),块存储() 原生云之前的fastdfs, 目前排除hdfs原因如下 1 HDFS的块状存在机制，其实是不太适合小文件的，但文件服务器和网盘的场景，很大程度上会出现大量的小文件，那么HDFS在IO这块并不是优势，这样效率会很低，并且会消耗大量的主节点内存，致命伤。 2 HDFS并不支持随机写，这对于文件存储模型来说也是不适合的。 3 HDFS的优势并不在高吞吐，所以如果是频繁读写的文件存储场景，对于他来说也是不适合的。 其实Hadoop的HDFS之所以出名，并不是单独作为文件系统而出名的，而是在于Hadoop的整套解决方案。","categories":[],"tags":[]},{"title":"minio的入门介绍","slug":"minio-description","date":"2022-03-09T06:57:24.000Z","updated":"2024-03-14T14:55:29.479Z","comments":true,"path":"68368fb0.html","link":"","permalink":"https://imlike.cc/68368fb0.html","excerpt":"","text":"minio中的概念 ·Object：存储到Minio的基本对象，如文件、字节流，Anything… ·Bucket：用来存储Object的逻辑空间。每个Bucket之间的数据是相互隔离的。对于客户端而言，就相当于一个存放文件的顶层文件夹。 ·Drive：即存储数据的磁盘，在MinlO启动时，以参数的方式传入。Minio中所有的对象数据都会存储在Drive里。 ·Set：即一组Drive的集合，分布式部署根据集群规模自动划分一个或多个Set，每个Set中的Drive 分布在不同位置。一个对象存储在一个Set上。（For example：（1．．．64｝is divided into 4 sets each of size 1 ​ 一个对象存储在一个Set上I一个集群划分为多个Set ​ 一个Set包含的Drive数量是固定的，默认由系统根据集群规模自动计算得出一个SET中的Drive尽可能分布在不同的节点上 纠删码EC（Erasure Code）MinlO 使用纠删码机制来保证高可靠性，使用highwayhash来处理数据损坏（Bit Rot Protection）。关于纠删码，简单来说就是可以通过数学计算，把天失的数据进行还原，它可以将n份原始数据，增加m份数据，并能通过n＋m份中的任意n份数据，还原为原始数据。即如果有任意小于等于m份的数据失效，仍然能通过剩下的数据还原出来。 分布式存储可靠性常用方法分布式存储，很关键的点在于数据的可靠性，即保证数据的完整，不丢失，不损坏。只有在可靠性实现的前提下，才有了追求一致性、高可用、高性能的基础。而对于在存储领域，一般对于保证数据可靠性的方法主要有两类，一类是冗余法，一类是校验法。 冗余冗余法最简单直接，即对存储的数据进行副本备份，当数据出现丢失，损坏，即可使用备份内容进行恢复，而副本备份的多少，决定了数据可靠性的高低。这其中会有成本的考量，副本数据越多，数据越可靠，但需要的设备就越多，成本就越高。可靠性是允许丢失其中一份数据。当前已有很多分布式系统是采用此种方式实现，如Hadoop的文件系统（3个副本），Redis的集群，MySQL的主备模式等。 校验校验法即通过校验码的数学计算的方式，对出现丢失、损坏的数据进行校验、还原。注意，这里有两个作用，一个校验，通过对数据进行校验和（checksum）进行计算，可以检查数据是否完整，有无损坏或更改，在数据传输和保存时经常用到，如TCP协议；二是恢复还原，通过对数据结合校验码，通过数学计算，还原丢失或损坏的数据，可以在保证数据可靠的前提下，降低冗余，如单机硬盘存储中的RAID技术，纠删码（Erasure Code）技术等。MinlO采用的就是纠删码技术。 分布式Minio优势数据保护分布式Minio采用纠删码来防范多个节点宕机和位哀减bit rot。 分布式Minio至少需要4个硬盘，使用分布式Minio自动引入了纠删码功能。高可用 单机Minio服务存在单点故障，相反，如果是一个有N块硬盘的分布式Minio，只要有N／2硬盘在线，你的数据就是安全的。不过你需要至少有N／2＋1个硬盘来创建新的对象。 例如，一个16节点的Minio集群，每个节点16块硬盘，就算8台服務器宕机，这个集群仍然是可读的，不过你需要9台服務器才能写数据。一致性 Minio在分布式和单机模式下，所有读写操作都严格遵守read—after—write一致性模型。 漫谈 MinIO 集群扩容方法_节点 MinIO集群采用去中心化无共享架构，各节点间为对等关系，连接至任一节点均可实现对集群的访问，并通过DNS轮询等方式实现节点间的负载均衡。这种节点间保持对等关系的设计并非最常见的分布式集群架构。当前大多数的分布式存储集群，其节点往往可划分为多类角色，例如负责连接并处理外部应用请求的访问节点、负责存储元数据的管理节点、实际的数据存储节点等。MinIO则与之不同，MinIO集群中的所有节点都同时承担了多种角色，集元数据存储、数据存储、应用访问等功能于一体，真正实现了去中心化和所有节点的完全对等。其优势在于有效地减少了集群内的复杂调度过程以及因中心节点带来的故障风险和性能瓶颈。 这种设计拓展节点的时候容易出现问题","categories":[],"tags":[]},{"title":"aliyun的服务警告","slug":"aliyun-service-alarm","date":"2022-03-08T02:16:22.000Z","updated":"2024-03-14T14:55:29.475Z","comments":true,"path":"7d4de1d4.html","link":"","permalink":"https://imlike.cc/7d4de1d4.html","excerpt":"","text":"","categories":[],"tags":[]},{"title":"使用curl命令来下载页面中连接","slug":"curl-download-page-url-file","date":"2022-03-07T14:07:55.000Z","updated":"2024-03-14T14:55:29.476Z","comments":true,"path":"d40194d4.html","link":"","permalink":"https://imlike.cc/d40194d4.html","excerpt":"","text":"html - How to download HTTP directory with all files and sub-directories as they appear on the online files/folders list? - Stack Overflow wget -r -np -nH –cut-dirs=3 -R index.html http://hostname/aaa/bbb/ccc/ddd/","categories":[],"tags":[]},{"title":"java线程","slug":"java线程","date":"2022-03-06T11:58:16.000Z","updated":"2024-03-14T14:55:29.478Z","comments":true,"path":"8d977941.html","link":"","permalink":"https://imlike.cc/8d977941.html","excerpt":"","text":"需要先梳理java线程和系统的线程 通常系统是 线程 下面还有进程 thread-process 在java中有自己定义线程需要和系统的线程区别 阻塞/非阻塞：是指一个业务需要a,b，c三个结果，阻塞是等待结果，非阻塞是去完成别的 串行/并行：这个通常是指业务逻辑的是逐个完成，还是一起完成 多线程：100件事的并行完成，通常还包含这线程池 对于多线程执行就存在资源挤占和数据处理问题通常是有两种方式 锁在特定操作通常是修改数据时加锁，让其他线程逐个执行，具有排他性 通过刷新内存，保证多线程中某个线程修改，其他线程能拿到最新的数据 要是我定义的话 线程可能会是 start run waiting waiting_timeout end 加锁的话需要参照物，java实现的就是在object 同时object对象实例有 wait 这是为啥？？ 为什么方法和属性不放在thread类里面，后者已经有很多本地的方法，包括Thread.sleepThread.yield interrupt join sleep wait notify notyfyall 方法 功能 备注 static Thread currentThread() 返回当前线程，即当前代码的执行线程（对象） 同一段代码对Thread.currentThread()的调用，其返回值可能对应着不同的线程（对象） void run() 用于实现线程的任务处理逻辑 该方法是由Java虚拟机直接调用的，一般情况下应用程序不应该调用该方法 void start() 启动相应线程 该方法的返回并不代表相应的线程已经被启动。一个Thread实例的start方法只能够被调用一次，多次调用会导致异常的抛出 void join() 等待相应线程运行结束 若线程A调用线程B的join方法，那么线程A的运行会被暂停，直到线程B运行结束 static void yield() 使当前线程主动放弃其对处理器的占用，这可能导致当前线程被暂停 这个方法是不可靠的。该方法被调用时当前线程可能仍然继续运行（视系统当前的运行状况而定） static void sleep(long millis) 使当前线程休眠（暂停运行）指定的时间 JAVA中的线程世界 - 掘金","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://imlike.cc/tags/java/"}]},{"title":"spotify_windows_no_ad","slug":"spotify-windows-no-ad","date":"2022-02-26T08:46:43.000Z","updated":"2024-03-14T14:55:29.481Z","comments":true,"path":"f7af877b.html","link":"","permalink":"https://imlike.cc/f7af877b.html","excerpt":"","text":"下载最新的屏蔽广告的dll，项目地址https://github.com/mrpond/BlockTheSpot重新安装spotify 浏览到您的 Spotify 安装文件夹%APPDATA%\\Spotify 重命名chrome_elf.dll为chrome_elf_bak.dll chrome_elf.zip从发行版下载 解压chrome_elf.dll并config.ini 注意对应的版本 这边提供下载地址：https://www.aliyundrive.com/s/qCvBVumnsJt 1234如果重新安装主要需要同时删除 C:\\Users\\&#123;USER)_NAME&#125;\\AppData\\Roaming\\Spotify 参考链接","categories":[],"tags":[{"name":"spotify","slug":"spotify","permalink":"https://imlike.cc/tags/spotify/"},{"name":"no_ad","slug":"no-ad","permalink":"https://imlike.cc/tags/no-ad/"}]},{"title":"Linux下使用docker搭建ue流的环境","slug":"Linux下使用docker搭建ue流的环境","date":"2022-02-22T08:04:03.000Z","updated":"2024-03-14T14:55:29.473Z","comments":true,"path":"ae4f29ca.html","link":"","permalink":"https://imlike.cc/ae4f29ca.html","excerpt":"","text":"Linux 下使用docker搭建ue流的环境 本项目依赖的软件均依赖与 https://hub.fastgit.org/adamrehn 这个开发者的官方地址:https://docs.adamrehn.com/ 以及官方docker仓库地址: https://hub.docker.com/u/adamrehn 虚幻容器:https://unrealcontainers.com 目前就是需要前置环境包括 docker ,python3 ,cuda 需要用 pip3 来安装开发者写的 ue4-docker 命令来实现 构建Linux,Windows的容器给 docker来使用 同时还需要 其他工具提供给Linux环境 Docker版本之一的64位版本（CentOS7+、Debian7.7+、Fedora26+、Ubuntu14.04+） 最低8GB的RAM 用于构建容器映像的至少300GB可用磁盘空间 Host OS Linux containers Windows containers NVIDIA Container Toolkit Optimality Linux Yes No Yes Optimal for Linux containers Windows Server Yes Yes No Optimal for Windows containers when using process isolation mode Windows 10 Yes Yes No Sub-optimal for both Linux and Windows containers macOS Yes No No Sub-optimal for Linux containers 直接调用gpu界面不使用图形界面来实现 先在ubuntu上安装docker环境12345678910111213sudo apt-get remove docker docker-engine docker.io containerd runc//更新apt软件包索引并安装软件包以允许apt通过HTTPS使用存储库：sudo apt-get update//添加Docker的官方GPG密钥:curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -//验证您现在是否拥有带有指纹的密钥:sudo apt-key fingerprint 0EBFCD88//设置镜像仓库：（使用阿里云镜像地址）sudo add-apt-repository &quot;deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable&quot;安装最新版的dockersudo apt-get install docker-ce docker-ce-cli containerd.io 或者 12curl https:&#x2F;&#x2F;get.docker.com | sh \\ &amp;&amp; sudo systemctl --now enable docker python环境12345//安装前看一下是否有软件install python3 python3-devel python3-pip配置Linux的防火墙ue4-docker setup 安装NVIDIA-toolkithttps://nvidia.github.io 访问会有问题需要加速 12345distribution&#x3D;$(. &#x2F;etc&#x2F;os-release;echo $ID$VERSION_ID) \\ &amp;&amp; curl -s -L https:&#x2F;&#x2F;nvidia.github.io&#x2F;nvidia-docker&#x2F;gpgkey | sudo apt-key add - \\ &amp;&amp; curl -s -L https:&#x2F;&#x2F;nvidia.github.io&#x2F;nvidia-docker&#x2F;$distribution&#x2F;nvidia-docker.list | sudo tee &#x2F;etc&#x2F;apt&#x2F;sources.list.d&#x2F;nvidia-docker.list sudo apt-get update 12345678&#x2F;&#x2F;安装依赖软件sudo apt-get install -y nvidia-docker2&#x2F;&#x2F;重启docker 服务sudo systemctl restart docker&#x2F;&#x2F;根据 cuda 版本来选择执行下面命令即可docker run --rm --gpus all nvidia&#x2F;cuda:11.0-base nvidia-smidocker run --rm --gpus all nvidia&#x2F;cuda:10.0-base nvidia-smi docker pull adamrehn/ue4-build-prerequisites:cudagl10.0 docker-ue4 来构建 adamrehn/ue4-full:4.23.1-pixelstreaming 来提供给使用 该过程很慢,可能会失败,需要注意 ue4-docker build custom:4.23.1-pixelstreaming -repo=https://github.com/adamrehn/UnrealEngine.git -branch=4.23.1-pixelstreaming –cuda=10.1 ue4-docker build custom:4.25-pixelstreaming -repo=https://github.com/ImmortalEmperor/UnrealEngine.git -branch=4.25-pixelstreaming –cuda=10.1 –no-engine 使用ue4-example-dockerfiles来搭建 ue4视频流的demo克隆 https://hub.fastgit.org/adamrehn/ue4-example-dockerfiles.git地址来 构建adamrehn/ue4-full:4.23.1-pixelstreaming https://github.com/EpicGames/UnrealEngine.git server 是需要的 webrtc 也是需要的 1ue4-docker build docker run –gpus=all -e DISPLAY –rm adamrehn/ue4-runtime:18.04-cudagl10.1-hostaudio","categories":[],"tags":[{"name":"docker","slug":"docker","permalink":"https://imlike.cc/tags/docker/"},{"name":"linux","slug":"linux","permalink":"https://imlike.cc/tags/linux/"},{"name":"视频流","slug":"视频流","permalink":"https://imlike.cc/tags/%E8%A7%86%E9%A2%91%E6%B5%81/"}]},{"title":"kubernetes部署ceph","slug":"kubernetes部署ceph集成kuboard","date":"2022-02-16T08:39:35.000Z","updated":"2024-03-14T14:55:29.479Z","comments":true,"path":"b592fe66.html","link":"","permalink":"https://imlike.cc/b592fe66.html","excerpt":"","text":"k8s集成ceph当前推荐的集群安装方式有 Deploying a new Ceph Cluster with cephadm (opens new window)和 Rook 一、使用cephadm来管理ceph集群首先 这种方式依赖的环境太多 Python 3 Systemd Podman or Docker for running containers Time synchronization (such as chrony or NTP) LVM2 for provisioning storage devices 并且我是用sealons来初始化kubernetes环境中已经有python2.6，安装python上后维护会很复杂，安装步骤繁琐（节点支持用dnf apt 和下载二进制文件来实现安装）但是还是繁琐 二、使用rook来管理集群目前使用后者rook来实现部署，这个项目也是原生云基金会来推动的，加快kubernetes-storage的安装部署工作 123Rook 将分布式存储系统转变为自我管理、自我扩展、自我修复的存储服务。它可以自动执行存储管理员的任务：部署、引导、配置、供应、扩展、升级、迁移、灾难恢复、监控和资源管理。Rook 使用 Kubernetes 平台的强大功能通过 Kubernetes Operator 为每个存储提供商提供服务。 1.8/quickstart 初始化rook后 Kubernetes存储系统-云原生存储Rook部署-阿里云开发者社区 在 2.1、部署中的问题直接创建会由于k8s角色和权限问题导致报错，报错的内容如下 system:serviceaccount:rook-ceph:rook-ceph-system” cannot get resource “pods” t同时在网上看到的在rook的要求也看到 Pod Security Policies需要 需要配置cluster-admin 12345678910kubectl create clusterrolebinding add-on-cluster-admin \\ --clusterrole&#x3D;cluster-admin \\ --serviceaccount&#x3D;kube-system:master kubectl create rolebinding default-view \\ --clusterrole&#x3D;view \\ --serviceaccount&#x3D;rook-ceph:rook-ceph-system \\ --namespace&#x3D;rook-ceph 是在界面配置对应的角色才可以正常访问rook-ceph下的服务。 三、kubectl来初始化参考链接使用 CephFS 作为存储类 - Rook | Kuboard 核心部分就是 12345kubectl create -f https:&#x2F;&#x2F;kuboard.cn&#x2F;statics&#x2F;learning&#x2F;ceph&#x2F;rook-1.5.4&#x2F;crds.yamlkubectl create -f https:&#x2F;&#x2F;kuboard.cn&#x2F;statics&#x2F;learning&#x2F;ceph&#x2F;rook-1.5.4&#x2F;common.yamlkubectl create -f https:&#x2F;&#x2F;kuboard.cn&#x2F;statics&#x2F;learning&#x2F;ceph&#x2F;rook-1.5.4&#x2F;operator.yamlkubectl create -f https:&#x2F;&#x2F;kuboard.cn&#x2F;statics&#x2F;learning&#x2F;ceph&#x2F;rook-1.5.4&#x2F;cluster.yaml 部署服务同时新建rook-ceph的 namespace 1234567891011121314151617181920cat &gt; myfs.yaml &lt;&lt;EOFapiVersion: ceph.rook.io/v1kind: CephFilesystemmetadata: name: myfs namespace: rook-cephspec: metadataPool: replicated: size: 3 dataPools: - replicated: size: 3 preservePoolsOnDelete: true metadataServer: activeCount: 1 activeStandby: trueEOFkubectl create -f myfs.yaml 创建对应的 File System Name ，提供给选择 使用kubectl的yml文件初始化后的服务列表 创建对应的存储类","categories":[],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://imlike.cc/tags/kubernetes/"},{"name":"ceph","slug":"ceph","permalink":"https://imlike.cc/tags/ceph/"},{"name":"kuboard","slug":"kuboard","permalink":"https://imlike.cc/tags/kuboard/"}]},{"title":"nvidia-jetson-nano-use","slug":"nvidia-jetson-nano-use","date":"2022-02-07T07:03:25.000Z","updated":"2024-03-14T14:55:29.480Z","comments":true,"path":"dce64a3e.html","link":"","permalink":"https://imlike.cc/dce64a3e.html","excerpt":"","text":"参考地址System settings not opening on Ubuntu 16.04 LTS 123sudo apt-get remove unity-control-centersudo apt autoremovesudo apt-get install unity-control-center Jetson nano平台深度学习环境配置 | 米奇妙妙屋 Jetson nano配置VNC实现过程图解_其它相关_脚本之家","categories":[],"tags":[]},{"title":"k8s_init_and_deploy_sample_service","slug":"k8s-init-and-deploy-sample-service","date":"2022-01-27T03:07:50.000Z","updated":"2024-03-14T14:55:29.479Z","comments":true,"path":"16f52beb.html","link":"","permalink":"https://imlike.cc/16f52beb.html","excerpt":"","text":"初始化k8s我了解有下列一键部署k8s,第一个是网友推荐，第二个是kuboard上面推荐 fanux/sealos: 一条命令离线安装高可用 Kubernetes🔥 ⎈ 🐳不依赖haproxy keepalived eip-work/kuboard-spray: 使用图形化的界面离线安装、维护高可用的 K8S 集群基于kubespray 目前使用 sealos来一键初始化kubernetes环境 一、准备 hostname不能重名 服务过程中会自动关闭防火墙 需要保证节点服务的可访问 默认使用containerd而不是docker为容器平台 12345678910111213# 下载并安装sealos, sealos是个golang的二进制工具，直接下载拷贝到bin目录即可, release页面也可下载wget -c https:&#x2F;&#x2F;sealyun.oss-cn-beijing.aliyuncs.com&#x2F;latest&#x2F;sealos &amp;&amp; \\ chmod +x sealos &amp;&amp; mv sealos &#x2F;usr&#x2F;bin# 下载离线资源包wget -c https:&#x2F;&#x2F;sealyun.oss-cn-beijing.aliyuncs.com&#x2F;05a3db657821277f5f3b92d834bbaf98-v1.22.0&#x2F;kube1.22.0.tar.gz# 安装一个三master的kubernetes集群sealos init --passwd &#39;123456&#39; \\ --master 192.168.0.2 --master 192.168.0.3 --master 192.168.0.4 \\ --node 192.168.0.5 \\ --pkg-url &#x2F;root&#x2F;kube1.22.0.tar.gz \\ --version v1.22.0 1.1 常见问题1.1.1初始化失败现象如下： generator kubeconfig failed couldn’t create a kubeconfig container runtime is not running: output 解决方法： 不用安装docker和其他服务，服务器只需要wget 服务 1.1.2kubectl get pod The connection to the server localhost:8080 was refused - did you specify the right host or port? 解决方法： 将主节点的 admin.conf 拷到对应节点后配置环境变量 1234567scp /etc/kubernetes/admin.conf 192.168.31.133:/etc/kubernetesecho &quot;export KUBECONFIG=/etc/kubernetes/admin.conf&quot; &gt;&gt; ~/.bash_profilesource ~/.bash_profilekubectl get pods k8s集群部分常见问题处理 - 云+社区 - 腾讯云 二、kuboard安装在 K8S 中安装 Kuboard v3 | Kuboard 我选择使用 hostPath 提供持久化而不是 StorageClass 提供持久化（还没有创建） 界面如下 三、使用kubectl来创建pod并部署命令来创建，部署 注意kubectl 需要指定namespace的值，否则默认是default kubectl get pod No resources found in default namespace. 3.1. 创建nginx镜像kubectl run nginx –image=nginx –port=80 NAME READY STATUS RESTARTS AGE nginx 1/1 Running 0 83s 3.2. 部署nginxkubectl create deployment nginx –image=nginx deployment.apps/nginx created 否则 运行下面命令会出现没有部署服务 kubectl get deployment No resources found in default namespace. 3.3. nginx端口暴露能被外部访问 kubectl expose deployment nginx –port=80 –type=LoadBalancer 3.4. 查看pod的信息kubectl describe service nginx Name: nginxNamespace: defaultLabels: app=nginxAnnotations: Selector: app=nginxType: LoadBalancerIP Family Policy: SingleStackIP Families: IPv4IP: 10.109.244.210IPs: 10.109.244.210Port: 80/TCPTargetPort: 80/TCPNodePort: 30682/TCPEndpoints: 100.118.167.130:80Session Affinity: NoneExternal Traffic Policy: ClusterEvents: kubectl get pods –all-namespaces 配置文件来部署12345678910111213141516171819202122apiVersion: apps/v1kind: Deploymentmetadata: name: hello-node namespace: default labels: app: hello-nodespec: replicas: 3 selector: matchLabels: app: hello-node template: metadata: labels: app: hello-node spec: containers: - name: hello-node-container image: gcr.io/$DEVSHELL_PROJECT_ID/hello-node:1.0 ports: - containerPort: 8080 kubectl apply -f nginx-deployment.yaml 3.5常见问题3.5.1 k8s某个节点挂掉如果遇到k8s某个节点挂掉，但是服务器还是可以连接的情况，说明是服务器节点通信问题，首先参考下面的连接查看日志和对应端口是否是正常的 服务器重启后kubernetes无法启动的原因 - 数据库敲门人 The connection to the server :6443 was refused - did you specify the right host or port? - General Discussions - Discuss Kubernetes 查看日志的命令如下 journalctl -u kubelet.service journalctl -xefu kubelet 查看节点命令如下kubectl get node –watch 重启kubelet的服务systemctl restart kubelet 查看端口的连接情况netstat -nap|grep 6443The connection to the server apiserver.cluster.local:6443 was refused - did you specify the right host or port? 使用的是sealons重新添加该集群建议可以使用 sealos delete 删除掉线的节点 然后再使用 sealos join 命令能把节点重新加入到集群中 3.5.2 容器运行时报错Network plugin returns error日志如下 cat /var/log/pods/kube-system_kube-apiserver-master2_d653fc9e62a54eaa54def1ae5c0dde56/kube-apiserver container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized 首先查看kube-system中NetworkPlugin是否存在 kubectl get pod -n kube-system k8s中存在网络插件一般包括**Choose network plugin (cilium, calico, contiv, weave or flannel)**，我看着初始化的是calico 同时查看对应calico的日志 kubectl logs calico-node-rkqkq –namespace=kube-system kubectl get pods -o wide -A 查看对应的配置信息 kubectl describe pod calico-node-rkqkq –namespace=kube-system 配置文件存放地址 /root/kube/conf/calico.yaml 下面推荐的解决方案： k8s join集群时报错Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: - 丛丛丛丛 - 博客园 kubeadm - Calico: Kubernetes pods can’t ping each other use Cluster IP - Stack Overflowkubectl delete-f /etc/kubernetes/addons/calico.yaml kubenates启动calico-node时，有节点总是报错CrashLoopBackOff_dkgee-CSDN博客 四、部署minio和创建volumeclass根据官方文档可以找到 Deploy MinIO Operator on Kubernetes — MinIO Hybrid Cloud Documentation operator/README.md at master · minio/operator 需要 Installing · Krew或者minio/helm/minio at master )包管理软件 4.1使用krew初始化minio我选择krew包管理，需要使用到git命令 然后运行下面这段内容 123456789( set -x; cd &quot;$(mktemp -d)&quot; &amp;&amp; OS=&quot;$(uname | tr &#x27;[:upper:]&#x27; &#x27;[:lower:]&#x27;)&quot; &amp;&amp; ARCH=&quot;$(uname -m | sed -e &#x27;s/x86_64/amd64/&#x27; -e &#x27;s/\\(arm\\)\\(64\\)\\?.*/\\1\\2/&#x27; -e &#x27;s/aarch64$/arm64/&#x27;)&quot; &amp;&amp; KREW=&quot;krew-$&#123;OS&#125;_$&#123;ARCH&#125;&quot; &amp;&amp; curl -fsSLO &quot;https://github.com/kubernetes-sigs/krew/releases/latest/download/$&#123;KREW&#125;.tar.gz&quot; &amp;&amp; tar zxvf &quot;$&#123;KREW&#125;.tar.gz&quot; &amp;&amp; ./&quot;$&#123;KREW&#125;&quot; install krew) 运行过程中出了问题 变量${KREW}=krew-linux_amd64 我是下载 https://github.com/kubernetes-sigs/krew/releases/latest/download/krew-linux_amd64.tar.gz、 export PATH=”$HOME/.krew}/bin:$PATH” 目前使用kuboard web界面管理k8s集群 存储类StorageClass | Kuboard但是目前只支持界面配置 cephfs，nfs Storage Classes | Kubernetes 官方支持的volumeclass也没有minio需要手写 yml的配置文件","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"https://imlike.cc/tags/linux/"},{"name":"container","slug":"container","permalink":"https://imlike.cc/tags/container/"},{"name":"k8s","slug":"k8s","permalink":"https://imlike.cc/tags/k8s/"}]},{"title":"分布式文件存储","slug":"distributed-file-storage","date":"2022-01-19T08:30:54.000Z","updated":"2024-03-14T14:55:29.476Z","comments":true,"path":"1128ed08.html","link":"","permalink":"https://imlike.cc/1128ed08.html","excerpt":"","text":"文件存储选型专业名称对象存储、文件存储、块存储的区别和联系 | Zollty’s Blog 文件系统了解及对比选型 | 笔记 目前主要存储的种类对象存储，文件存储，块存储。需要使用 对象存储：是目前流行的存储服务也是云服务厂商提供的类似于Amazon 的 s3(Simple Storage Service ) 和阿里云对象存储OSS（Object Storage Service） 文件存储：狭义理解 Linux 实现POSIX接口，能挂载在Linux上，FTP、NFS服务器SAMBA 块存储： 磁盘阵列，硬盘 ，可能会使用RAID NAS这种来实现数据的冗余备份 存储类型 介绍 对象存储 类似于s3,主要是存储静态的数据,不受扩展不受元数据约束限制的块设备 文件存储 通常是实现了POSIX接口,能挂载librados，libcephfs 块存储 磁盘阵列，硬盘 通常情况下云服务厂商会有数据容灾（ 同城人员存储，跨区域复制，合规保留策略，版本控制功能），数据加密，权限和访问（用户级别资源隔离，防盗链，访问控制ACL） 分布式 –&gt;容错冗余备份 –&gt;方便拓展 –&gt;稳定性-&gt;速度（磁盘io，网络io，软件优化，服务器CPU等硬件） 是否要兼容目前k8s环境 提供restful接口 目前文件主要为零散文件和打包后的大文件 稳定性和调试是否方便 技术选型 系统 文件元数据 文件内容读取 POSIX REST API 针对大量小文件进行了优化 SeaweedFS 查找卷 ID，可缓存 O(1) 磁盘寻道 是的 是的 SeaweedFS Filer 线性可扩展，可定制 O(1) 磁盘寻道 FUSE 是的 是的 GlusterFS 散列 FUSE，NFS Ceph 哈希+规则 FUSE 是的 MooseFS 在记忆中 FUSE 不 MinIO 每个文件的单独元文件 是的 不 minio: 掀桌子的 MinIO - 纯真年代","categories":[],"tags":[]},{"title":"docker容器简化运行jdk镜像","slug":"docker-container-small-ubuntu-install-jdk","date":"2022-01-17T01:20:12.000Z","updated":"2024-03-14T14:55:29.476Z","comments":true,"path":"dda6ae1a.html","link":"","permalink":"https://imlike.cc/dda6ae1a.html","excerpt":"","text":"容器操作公司线上运行的容器是jre,没有调试和debug的工具,同时使用起来很不方便 查看系统版本运行 cat /etc/issue 看到是debian 7的系统后 在/usr/lib/jvm 目录先看到的openjdk11 参考该链接的内容How To Install OpenJDK Java 11 On Debian Real Quick The openjdk-11-jre package contains just the Java Runtime Environment. If you want to develop Java programs then please install the openjdk-11-jdk package.To Install OpenJDK 11 Debian Java Development Kit (JDK) - Enter the below command on Debian command line. 需要还安装jdk 在线安装openjdk11将 /etc/apt/source.list 备份后修改为debian 7 阿里的源 12345678deb http://mirrors.aliyun.com/debian/ stretch main non-free contribdeb-src http://mirrors.aliyun.com/debian/ stretch main non-free contribdeb http://mirrors.aliyun.com/debian-security stretch/updates maindeb-src http://mirrors.aliyun.com/debian-security stretch/updates maindeb http://mirrors.aliyun.com/debian/ stretch-updates main non-free contribdeb-src http://mirrors.aliyun.com/debian/ stretch-updates main non-free contrib#deb http://mirrors.aliyun.com/debian/ stretch-backports main non-free contrib#deb-src http://mirrors.aliyun.com/debian/ stretch-backports main non-free contrib 运行 apt install openjdk-11-jdk 一直卡在安装 openjdk-11-jre-headless_11.0.13+8-1 起初因为是源的问题,切换后还是很慢,后面考虑应该连接的是官方的地址,于是尝试从网站下载对应的openjdk-11-jre-headless_11.0.13+8-1_amd64.deb 包使用 dpkg -i安装 . 后面才知道 离线安装使用dpkg -i *.deb 的时候出现依赖没有安装 使用apt-get -f -y install 解决依赖问题后再执行dpkg安装deb包 jdk-11.0.2_linux-x64_bin.deb 大概150mb左右 封装为脚本https://code.aliyun.com/whotolike/deploycode/raw/master/init_debian_container.sh","categories":[],"tags":[{"name":"ubuntu","slug":"ubuntu","permalink":"https://imlike.cc/tags/ubuntu/"},{"name":"docker","slug":"docker","permalink":"https://imlike.cc/tags/docker/"},{"name":"container","slug":"container","permalink":"https://imlike.cc/tags/container/"},{"name":"jdk","slug":"jdk","permalink":"https://imlike.cc/tags/jdk/"}]},{"title":"打包linux的bin二进制包","slug":"pack-linux-binary","date":"2022-01-07T05:29:02.000Z","updated":"2024-03-14T14:55:29.480Z","comments":true,"path":"2aff3a72.html","link":"","permalink":"https://imlike.cc/2aff3a72.html","excerpt":"","text":"二进制的文件需要将压缩包的数据和shell脚本打包的为一个整体 为避免多个sh文件使用 cat &lt;&lt; EOF 来生成上面的sh文件,这样只需要下面这个sh文件就可以将指定的文件夹压缩,同时添加 执行命令那部分内容 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# !/bin/bashecho &#x27;压缩文件。。。&#x27;INSTALL_DIR=$(cd `dirname $0`; pwd)tar -zcvf $INSTALL_DIR/XXXX.tar.gz $INSTALL_DIR/install/*cat &lt;&lt; EOF &gt;&gt;$INSTALL_DIR/install.sh#!/bin/bash#Test shell self-extractingTmpDir=/tmp/XXXTARGET_DIR=./XXXif [ ! -d \\$&#123;TmpDir&#125; ]; then mkdir \\$TmpDir echo \\$TmpDir 路径下不存在,正在创建 else echo \\$TmpDir 文件将会被覆盖 rm -rf \\$TmpDir/*fiARCHIVE=\\$(awk &#x27;/^__ARCHIVE_BELOW__/ &#123;print NR + 1; exit 0; &#125;&#x27; &quot;\\$0&quot;)tail -n+\\$ARCHIVE &quot;\\$0&quot; | tar -xzvm -C \\$TmpDir &gt; /dev/null 2&gt;&amp;1 3&gt;&amp;1if [ ! -d \\$&#123;TARGET_DIR&#125; ]; then echo \\$TARGET_DIR 路径下不存在现在创建 mkdir \\$&#123;TARGET_DIR&#125; else echo \\$TARGET_DIR 路径下的文件将会被覆盖 rm -rf \\$&#123;TARGET_DIR&#125;/*ficp -R \\$TmpDir/* \\$&#123;TARGET_DIR&#125;echo &#x27;-------------------------------------------------------------------------------------------------&#x27;echo &#x27;********************************准备初始化 服务****************************************&#x27;echo &#x27;-------------------------------------------------------------------------------------------------&#x27;/bin/sh \\$TARGET_DIR/install.shexit 0#This line must be the last line of the file__ARCHIVE_BELOW__EOFcat $INSTALL_DIR/install.sh $INSTALL_DIR/XXX.tar.gz &gt; $INSTALL_DIR/XXX_install.runrm -f XXX.tar.gzrm -f install.sh","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"https://imlike.cc/tags/linux/"},{"name":"binary","slug":"binary","permalink":"https://imlike.cc/tags/binary/"}]},{"title":"nginx的深度调优","slug":"nginx-deep-optimize","date":"2022-01-06T08:10:59.000Z","updated":"2024-03-14T14:55:29.480Z","comments":true,"path":"132c3fd6.html","link":"","permalink":"https://imlike.cc/132c3fd6.html","excerpt":"","text":"参考文章 Nginx全能解析及性能调优 - 掘金 Nginx调优 - 知乎 设置静态资源的过期时间 反向代理设置缓存 proxy_cache_path /data/nginx/cache levels=1:2 keys_zone=one:200m inactive=60m loader_threshold=300 loader_files=300 max_size=2000m; 关闭access_log123456# svg, fontslocation ~ &#x2F;last&#x2F;\\.(?:jpg|png|gif|cmpt|b3dm)$ &#123; add_header Access-Control-Allow-Origin &quot;*&quot;; expires 7d; access_log off;&#125; 设置worker_processes grep processor /proc/cpuinfo | wc –l 获取CPU核心数为8 可以获取CPU相关的信息 123456worker_processes auto;worker_rlimit_nofile 65535;#pid logs&#x2F;nginx.pid;events &#123; worker_connections 65535;&#125; 通常需要的设置的worker_processes 为核心数除以2的n次方.推荐设置为auto 设置gzip压缩 这个需要确认这个请求是支持gzip压缩的可以参考 tips 12345678gzip on; #是否开启gzip模块 on表示开启 off表示关闭gzip_static on;gzip_buffers 4 16k; #设置压缩所需要的缓冲区大小gzip_comp_level 6; #压缩级别1-9，数字越大压缩的越好，也越占用CPU时间gzip_min_length 1k; #设置允许压缩的最小字节gzip_http_version 1.0; #设置压缩http协议的版本,默认是1.1 如果nginx前面还有反向代理 这要设置为1.0gzip_types text&#x2F;plain application&#x2F;x-javascript text&#x2F;css text&#x2F;javascript application&#x2F;x-httpd-php image&#x2F;jpeg image&#x2F;gif image&#x2F;png application&#x2F;octet-stream application&#x2F;javascript; #设置压缩的文件类型gzip_vary on; #加上http头信息Vary: Accept-Encoding给后端代理服务器识别是否启用 gzip 压缩 Linux环境下设置io多路复用(epoll)模型1234567worker_processes auto;worker_rlimit_nofile 65535;events &#123; use epoll; # 这个设置是针对2.6内核 cat &#x2F;proc&#x2F;version可读取 multi_accept on; worker_connections 65535;&#125;","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"https://imlike.cc/tags/linux/"},{"name":"nginx","slug":"nginx","permalink":"https://imlike.cc/tags/nginx/"}]},{"title":"树莓派使用cloudreve搭建nas和离线下载","slug":"树莓派使用cloudreve搭建nas和离线下载","date":"2022-01-03T17:11:52.000Z","updated":"2024-03-14T14:55:29.484Z","comments":true,"path":"1661309199.html","link":"","permalink":"https://imlike.cc/1661309199.html","excerpt":"","text":"2022-1-20 docker 部署cloudreve 12345678910111213141516171819202122232425# 下载docker pull xavierniu&#x2F;cloudreve# 创建mkdir -p ~&#x2F;cloudreve&#x2F;uploads \\ &amp;&amp; mkdir -p ~&#x2F;cloudreve&#x2F;avatar \\ &amp;&amp; touch ~&#x2F;cloudreve&#x2F;conf.ini \\ &amp;&amp; touch ~&#x2F;cloudreve&#x2F;cloudreve.db# 启动docker run -d \\ --name cloudreve \\ -e PUID&#x3D;$UID \\ -e PGID&#x3D;$GID \\ -e TZ&#x3D;&quot;Asia&#x2F;Shanghai&quot; \\ -p 5212:5212 \\ --restart&#x3D;unless-stopped \\ -v &#x2F;cloudreve&#x2F;uploads:&#x2F;cloudreve&#x2F;uploads \\ -v ~&#x2F;cloudreve&#x2F;conf.ini:&#x2F;cloudreve&#x2F;conf.ini \\ -v ~&#x2F;cloudreve&#x2F;cloudreve.db:&#x2F;cloudreve&#x2F;cloudreve.db \\ -v ~&#x2F;cloudreve&#x2F;avatar:&#x2F;cloudreve&#x2F;avatar \\ xavierniu&#x2F;cloudreve docker logs -f cloudreve 树莓派搭建满速家庭云网盘（Lv.1） 干货Get!]Android搭建Cloudreve私人云盘 允许SHH使用root登陆sudo passwd 来重置root用户的密码 vim /etc/ssh/sshd_config 注释掉 PermitRootLogin without-password 使用 PermitRootLogin yes 连接树莓派将文件传过去现在下载 https://github.com/cloudreve/Cloudreve/releases 选择cloudreve_3.0.0-rc1_linux_arm.tar.gz sudo mkdir /home/cloud 使用ftp的客户端连接注意使用 root 将下载cloudreve_3.0.0-rc1_linux_arm.tar.gz 放到 /home/cloud下 sudo tar -xzvf cloudreve_3.0.0-rc1_linux_arm.tar.gz 解压后也是个单文件cloudreve sudo ./cloudreve 后面终端会打印出来用户admin@cloudreve.org 和密码 初始化cloudreve_3.0.0-rc1_linux_arm和设置登陆后下面是初始化的页面 tips 只有这个入口可以设置用户(包括管理员)使用的存储的地址,可以本地-&gt;这个本地可以选择硬盘挂载路径,也可以用onedrive 设置cloudreve后台启动vim /usr/lib/systemd/system/cloudreve.service 粘贴以下内容然后文内的 PATH_TO_CLOUDREVE 更换为程序所在目录 12345678910111213141516 [Unit] Description&#x3D;Cloudreve Documentation&#x3D; &#x3D;[查看链接](https:&#x2F;&#x2F;docs.cloudreve.org&#x2F;) After&#x3D;network.target Wants&#x3D;network.target[Service] WorkingDirectory&#x3D;&#x2F;PATH_TO_CLOUDREVE ExecStart&#x3D;&#x2F;PATH_TO_CLOUDREVE&#x2F;cloudreve Restart&#x3D;on-abnormal RestartSec&#x3D;5s KillMode&#x3D;mixed StandardOutput&#x3D;null StandardError&#x3D;syslog[Install] WantedBy&#x3D;multi-user.target 然后Ctrl+o保存Ctrl+x退出 然后配置进程更新配置systemctl daemon-reload启动服务systemctl start cloudreve设置开机启动systemctl enable cloudreve 然后我们就可以通过以下指令来管理服务了# 启动服务systemctl start cloudreve # 停止服务systemctl stop cloudreve # 重启服务systemctl restart cloudreve # 查看状态systemctl status cloudreve 软连接到挂载的硬盘ln -s /home/device /home/cloud/uploads/1/disk fdisk -l 查看分区和挂载状况 123456sudo su 建一个目录作为U盘的挂接点。比如说要把U盘挂载到 &#x2F;mnt&#x2F;usb，那么需要采用下列命令新建 &#x2F;mnt&#x2F;usb。 mkdir &#x2F;mnt&#x2F;usb 然后就可以采用mount命令把U盘挂载在&#x2F;mnt&#x2F;usb。 mount &#x2F;dev&#x2F;sdb1 &#x2F;mnt&#x2F;usb 输入命令 cd &#x2F;mnt&#x2F;usb进入目录&#x2F;mnt&#x2F;usb，然后输入 ls命令就可以查看U盘里的内容了。mount &#x2F;dev&#x2F;sda1 &#x2F;home&#x2F;cloud&#x2F;uploads&#x2F;1&#x2F;diskmount &#x2F;dev&#x2F;sda1 &#x2F;home&#x2F;device","categories":[],"tags":[{"name":"树莓派","slug":"树莓派","permalink":"https://imlike.cc/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/"},{"name":"cloud","slug":"cloud","permalink":"https://imlike.cc/tags/cloud/"},{"name":"nas","slug":"nas","permalink":"https://imlike.cc/tags/nas/"}]},{"title":"VMware虚拟机添加磁盘格式化ext后挂载","slug":"vmware-extent-disk","date":"2021-12-23T09:17:39.000Z","updated":"2024-03-14T14:55:29.482Z","comments":true,"path":"284f509c.html","link":"","permalink":"https://imlike.cc/284f509c.html","excerpt":"","text":"https://blog.csdn.net/weixin_39787057/article/details/110810477 vmware扩容虚拟机后怎么扩容到根目录,而不是去新建一个挂载点 背景：学习oracle时，安装的centos系统硬盘只有20G，容量不够用，所以通过VMWare给系统扩容。但是查询后，硬盘容量并未增加。 $ 输入lsblk 查看当前系统的分区情况 解决办法：增加磁盘操作后，Linux并不认为你已经把磁盘空间扩大，所以需要对新增加的容量进行分区以及合并。 fdisk -l #先看一下Disk /dev/sda: 42.9 GB 开始新增分区，先输入fdisk /dev/sda，再输入p 新增分区输入： n 回车（默认为主分区primary） 分区号，起始扇区，结束扇区都默认（回车） 设置分区格式输入： t 分区号默认（回车） Hex code 为 8e （8e代表Linux LVM分区类型） w （写入分区表） 确认一下，发现dev/sda3分区创建好了，分区格式为linux LVM类型 重启一下，以便重读分区表 开始格式化分区 1mkfs.ext3 &#x2F;dev&#x2F;sda3 开始合并 12345678910 pvcreate /dev/sda3 vgextend centos /dev/sda3 # 输入y确认 #查看一下当前的Volume卷详情 vgdisplay -v 查看新分区sda3详情，容量为5119 将系统盘/dev/mapper/centos-root与sda3的5119空余容量合并 lvextend -l+5119 /dev/mapper/centos-root quit 最后将文件系统进行扩容，让系统识别，输入命令： 1xfs_growfs &#x2F;dev&#x2F;mapper&#x2F;centos-root lsblk 成功扩容！！","categories":[],"tags":[]},{"title":"docker运行hbase","slug":"docker-deploy-hbase","date":"2021-12-14T07:22:14.000Z","updated":"2024-03-14T14:55:29.476Z","comments":true,"path":"412589e1.html","link":"","permalink":"https://imlike.cc/412589e1.html","excerpt":"","text":"docker部署hbase系统为 centos7 docker 安装的版本为 20.10.9 一,docker安装卸载系统仓库自带的docker 12345678sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine 有三种方式来安装docker 更新官方仓库后yum安装 手动下载文件安装 脚本文件安装 目前准备使用第三种方式来安装 1curl -fsSL https:&#x2F;&#x2F;get.docker.com -o get-docker.sh &amp;&amp; sh get-docker.sh 二,docker-compose来实现单节点容器的编排1234curl -L https:&#x2F;&#x2F;get.daocloud.io&#x2F;docker&#x2F;compose&#x2F;releases&#x2F;download&#x2F;1.25.1&#x2F;docker-compose-&#96;uname -s&#96;-&#96;uname -m&#96; -o &#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-composechmod +x &#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-compose /usr/local/bin/docker-compose -f docker-compose-distributed-local.yml up -d 123456789101112131415resourcemanager 8088 hbase-master 16000hbase-regionserver 16020zookeeper 2888 2181 3888historyserver 8188namenode 50070datanode 50075nodemanager 8042 三,使用docker stack来实现1.创建swarm的网络保证各个节点都能平等的被编排docker swarm init 初始化一个swarm 的网络 docker swarm join-token worker 显示 加入的token docker swarm join-token manager 显示加入的token 目前使用下面加入manager,因为worker节点不能显示集群的细节 在其他服务器上输入 下面类似的内容加入节点 docker swarm join –token SWMTKN-1-0w6kjxx0j4epupf6r844dxpzoyxikrxqqgrqg319vqfg0y1gw7-31q237m76pyii2g0pw1c8nmtu 192.168.31.136:2377 2.docker node ls 查看目前的网络 docker network create –driver overlay –scope swarm hbase docker stack deploy –compose-file docker-compose-v3.yml hdp docker stack ls docker stack down hadoop docker network create –driver overlay –attachable –subnet 10.11.0.0/24 sg-hadoop Creating service hdp_nodemanagerCreating service hdp_hbase-master /opt/hbase-1.2.6/bin/hbase-daemon.sh restart regionserver echo -e “hbase-region2\\nhbase-region” &gt; /etc/hbase/regionservers docker service update –with-registry-auth 需要追加 –with-registry-auth docker exec -t 70f261259630 netstat -an | grep LISTEN | grep 50070 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173version: &#x27;3&#x27;services: namenode: image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8 networks: - hbase volumes: - namenode:/hadoop/dfs/name environment: - CLUSTER_NAME=test env_file: - ./hadoop.env deploy: mode: global # replicas: 1 restart_policy: condition: on-failure # placement: # constraints: # - node.hostname == akswnc4.aksw.uni-leipzig.de labels: traefik.docker.network: hbase traefik.port: 9000 ports: - 9000:9000 # - 50070:50070 datanode: image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8 networks: - hbase volumes: - datanode:/hadoop/dfs/data env_file: - ./hadoop.env environment: SERVICE_PRECONDITION: &quot;namenode:9000&quot; deploy: mode: global restart_policy: condition: on-failure labels: traefik.docker.network: hbase traefik.port: 9867 ports: - 9867:9867 resourcemanager: image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8 networks: - hbase environment: SERVICE_PRECONDITION: &quot;namenode:9000 datanode:9867&quot; env_file: - ./hadoop.env deploy: mode: global # replicas: 1 restart_policy: condition: on-failure # placement: # constraints: # - node.hostname == akswnc4.aksw.uni-leipzig.de labels: traefik.docker.network: hbase traefik.port: 8088 healthcheck: disable: true ports: - 8088:8088 nodemanager: image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8 networks: - hbase environment: SERVICE_PRECONDITION: &quot;namenode:9000 datanode:9867 resourcemanager:8088&quot; env_file: - ./hadoop.env deploy: mode: global restart_policy: condition: on-failure labels: traefik.docker.network: hbase traefik.port: 8042 ports: - 8042:8042 historyserver: image: bde2020/hadoop-historyserver:2.0.0-hadoop3.2.1-java8 networks: - hbase volumes: - hadoop_historyserver:/hadoop/yarn/timeline environment: SERVICE_PRECONDITION: &quot;namenode:9000 datanode:9867 resourcemanager:8088&quot; env_file: - ./hadoop.env deploy: mode: global # replicas: 1 # placement: # constraints: # - node.hostname == akswnc4.aksw.uni-leipzig.de labels: traefik.docker.network: hbase traefik.port: 8188 ports: - 8188:8188 zookeeper: image: zookeeper:3.4.10 networks: - hbase environment: ZOO_MY_ID: 1 ZOO_SERVERS: server.1=0.0.0.0:2888:3888 ports: - 2181:2181 hbase-master: image: bde2020/hbase-master:1.0.0-hbase1.2.6 hostname: hbase-master networks: - hbase env_file: - ./hbase-distributed-local.env environment: SERVICE_PRECONDITION: &quot;namenode:9000 datanode:9867 zookeeper:2181&quot; ports: - 16000:16000 - 16010:16010 hbase-region: image: bde2020/hbase-regionserver:1.0.0-hbase1.2.6 hostname: hbase-regionserver networks: - hbase env_file: - ./hbase-distributed-local.env environment: HBASE_CONF_hbase_regionserver_hostname: hbase-region SERVICE_PRECONDITION: &quot;namenode:9000 datanode:9867 zookeepe:2181 hbase-master:16010&quot; ports: - 16020:16020 - 16030:16030 visualizer: image: dockersamples/visualizer:stable ports: - &quot;8080:8080&quot; stop_grace_period: 1m30s deploy: mode: global volumes: - &quot;/var/run/docker.sock:/var/run/docker.sock&quot; deploy: placement: constraints: [node.role == manager]volumes: datanode: namenode: hadoop_historyserver:networks: hbase: external: name: hbase 将自动创建service 3.使用docker stack ls来查看创建stack 4.使用docker service ls来查看service列表 5.使用yml中visualizer组件来显示部署的情况 组件 节点 默认端口 配置 用途说明 HDFS DateNode 50010 dfs.datanode.address datanode服务端口，用于数据传输 HDFS DateNode 50075 dfs.datanode.http.address http服务的端口 HDFS DateNode 50475 dfs.datanode.https.address http服务的端口 HDFS DateNode 50020 dfs.datanode.ipc.address ipc服务的端口 HDFS NameNode 50070 dfs.namenode.http-address http服务的端口 HDFS NameNode 50470 dfs.namenode.https-address https服务的端口 HDFS NameNode 8020 fs.defaultFS 接收Client连接的RPC端口，用于获取文件系统metadata信息。 HDFS journalnode 8485 dfs.journalnode.rpc-address RPC服务 HDFS journalnode 8480 dfs.journalnode.http-address HTTP服务 HDFS ZKFC 8019 dfs.ha.zkfc.port ZooKeeper FailoverController，用于NN HA YARN ResourceManage 8032 yarn.resourcemanager.address RM的applications manager(ASM)端口 YARN ResourceManage 8030 yarn.resourcemanager.scheduler.address scheduler组件的IPC端口 YARN ResourceManage 8031 yarn.resourcemanager.resource-tracker.address IPC YARN ResourceManage 8033 yarn.resourcemanager.admin.address IPC YARN ResourceManage 8088 yarn.resourcemanager.webapp.address http服务端口 YARN NodeManager 8040 yarn.nodemanager.localizer.address localizer IPC YARN NodeManager 8042 yarn.nodemanager.webapp.address http服务端口 YARN NodeManager 8041 yarn.nodemanager.address NM中container manager的端口 YARN JobHistory Server 10020 mapreduce.jobhistory.address IPC YARN JobHistory Server 19888 mapreduce.jobhistory.webapp.address http服务端口 HBase Master 60000 hbase.master.port IPC (主节点（HBase 主节点和任何备份的 HBase 主节点） HBase Master 60010 hbase.master.info.port http服务端口 (主节点（HBase 主节点和备份 HBase 主节点如果有)) HBase RegionServer 60020 hbase.regionserver.port IPC( 所有从节点) HBase RegionServer 60030 hbase.regionserver.info.port http服务端口 HBase HQuorumPeer 2181 hbase.zookeeper.property.clientPort HBase-managed ZK mode，使用独立的ZooKeeper集群则不会启用该端口。 HBase HQuorumPeer 2888 hbase.zookeeper.peerport HBase-managed ZK mode，使用独立的ZooKeeper集群则不会启用该端口。 HBase HQuorumPeer 3888 hbase.zookeeper.leaderport HBase-managed ZK mode，使用独立的ZooKeeper集群则不会启用该端口。 Hive Metastore 9085 /etc/default/hive-metastore中export PORT=来更新默认端口 Hive HiveServer 10000 /etc/hive/conf/hive-env.sh中export HIVE_SERVER2_THRIFT_PORT=来更新默认端口 ZooKeeper Server 2181 /etc/zookeeper/conf/zoo.cfg中clientPort= 来自 ZooKeeper 的 config 的属性zoo.cfg。客户端将连接的端口。 ZooKeeper Server 2888 /etc/zookeeper/conf/zoo.cfg中server.x=[hostname]:nnnnn[:nnnnn]，标蓝部分 follower用来连接到leader，只在leader上监听该端口 ZooKeeper Server 3888 /etc/zookeeper/conf/zoo.cfg中server.x=[hostname]:nnnnn[:nnnnn]，标蓝部分 ZooKeeper 对等点用于相互通信的端口。有关更多信息，请参见此处 十,遇到的坑1.在hadoop配置的需要主要配置url类似于 host:port 这种时 host 不能报考_ 等字符,否则会报 contain a valid host:port authority 包含 _ 2.使用docker stack 初始化的环境所有的服务会有个stack名称的前缀,且不能修改 在集群中加入容器中后可以通过 服务名称就是 STACK_NAME 来调用也可以通过在yml中定义的services下面的节点名称来调用这个需要注意 初始化后出现所有的service会加前缀导致域名不能和访问 3.哈哈哈 1File &#x2F;hbase&#x2F;.tmp&#x2F;hbase.version could only be written to 0 of the 1 minReplication nodes. There are 1 datanode(s) running and 1 node(s) are excluded in this operation 4.配置出错导致服务绑定对应的ip 1java.io.IOException: Problem binding to hbase-region&#x2F;10.0.4.107:16040 : Cannot assign requested address. To switch ports use the &#39;hbase.regionserver.port&#39; configuration property.","categories":[],"tags":[]},{"title":"clickhouse的简单教程","slug":"clickhouse-tutorial","date":"2021-12-10T03:15:32.000Z","updated":"2024-03-14T14:55:29.476Z","comments":true,"path":"3b2506a.html","link":"","permalink":"https://imlike.cc/3b2506a.html","excerpt":"","text":"clickhouse部署和查询小结/etc/clickhouse-server/users.xml docker version ：1.13.1 简单修改 config.xml 中0.0.0.0 保证外部能访问 同时修改users.xml 的为 123456 如果使用md5 加密 着需要修改标签为 目前保存配置文件的地方有两个 /var/lib/clickhouse/preprocessed_configs ， /etc/clickhouse-server/ 后面的目录下的文件会复制到 前者，如果如果直接修改前面目录内的文件是不会成功的。 起初是直接修改 不行1docker run -d --name some-clickhouse-server --privileged=true --ulimit nofile=262144:262144 --volume=/root/clickhouse_data:/var/lib/clickhouse -p 8123:8123 yandex/clickhouse-server 修改 /root/clickhouse_data/preprocessed_configs/users.xml 后无论使用 docker restart 还是进容器里面重启后都是都没有修改。 然后是挂载目录还不行1docker run --rm --name clickhouse-server --privileged&#x3D;true --net&#x3D;host --ulimit nofile&#x3D;262144:262144 --volume&#x3D;&#x2F;root&#x2F;clickhouse_data:&#x2F;var&#x2F;lib&#x2F;clickhouse --volume&#x3D;&#x2F;root&#x2F;clickhouse_config:&#x2F;etc&#x2F;clickhouse-server yandex&#x2F;clickhouse-server 报错没有修改的权限，这部分由于涉及到具体的操作就放弃了 Configuration file ‘/etc/clickhouse-server/config.xml’ isn’t readable by user with id ‘101’ 最后在本地rpm包部署成功1234567sudo yum install yum-utilssudo rpm --import https:&#x2F;&#x2F;repo.clickhouse.com&#x2F;CLICKHOUSE-KEY.GPGsudo yum-config-manager --add-repo https:&#x2F;&#x2F;repo.clickhouse.com&#x2F;rpm&#x2F;stable&#x2F;x86_64sudo yum install clickhouse-server clickhouse-client如果没有 yum-config-manager 需要安装 yum-utils final 使用dockerfile实现 Processing configuration file ‘/etc/clickhouse-server/config.xml’.Merging configuration file ‘/etc/clickhouse-server/config.d/docker_related_config.xml’.Logging trace to /var/log/clickhouse-server/clickhouse-server.logLogging errors to /var/log/clickhouse-server/clickhouse-server.err.logProcessing configuration file ‘/etc/clickhouse-server/config.xml’.Merging configuration file ‘/etc/clickhouse-server/config.d/docker_related_config.xml’.Saved preprocessed configuration to ‘/var/lib/clickhouse/preprocessed_configs/config.xml’.Processing configuration file ‘/etc/clickhouse-server/users.xml’.Saved preprocessed configuration to ‘/var/lib/clickhouse/preprocessed_configs/users.xml’. 由于不能直接挂载 所有使用Dockerfile 来打包 内容为 12FROM yandex/clickhouse-serverCOPY users.xml /etc/clickhouse-server/users.xml 然后 **docker build -t clickhouse:lastet . ** ，打包完成后运行 使用clickhouse入门我们使用的是最简单的clickhouse ，引擎使用 MergeTree 使用 官方提供的数据来实现 https://clickhouse.com/docs/en/getting-started/example-datasets/cell-towers/ 123456-- auto-generated definitioncreate table kafka_iot_dev_data_state_queue( json String) engine &#x3D; Kafka SETTINGS kafka_broker_list &#x3D; &#39;&#123;IP&#125;&#39;, kafka_topic_list &#x3D; &#39;&#123;TOPIC&#125;&#39;, kafka_group_name &#x3D; &#39;&#123;GROUP_NAME&#125;&#39;, kafka_format &#x3D; &#39;JSONAsString&#39;; 123456789101112131415161718192021CREATE MATERIALIZED VIEW zhyl.TABLE_NAME1 TO zhyl.TABLE_NAME2( &#96;rdid&#96; String, &#96;prdid&#96; String, &#96;kdcode&#96; String, &#96;devid&#96; String, &#96;rdstate&#96; String, &#96;rdtime&#96; DateTime, &#96;content&#96; Nullable (String), &#96;dev&#96; Nullable (String))ASSELECT JSONExtractString(json, &#39;rdid&#39;) AS rdid, JSONExtractString(json, &#39;prdid&#39;) AS prdid, JSONExtractString(json, &#39;kdcode&#39;) AS kdcode, JSONExtractString(json, &#39;devid&#39;) AS devid, JSONExtractString(json, &#39;rdstate&#39;) AS rdstate, JSONExtractString(json, &#39;rdtime&#39;) AS rdtime, JSONExtractString(json, &#39;content&#39;) AS content, JSONExtractString(json, &#39;dev&#39;) AS devFROM zhyl.kafka_iot_dev_data_rd_queue; 1234567891011121314151617create table TABLE_NAME1( rdid String, prdid String, kdcode String, devid String, rdstate String, rdtime DateTime(&#39;Asia&#x2F;Shanghai&#39;), content String, dev String) engine &#x3D; MergeTree PARTITION BY (toYYYYMM(rdtime)) ORDER BY rdid SETTINGS index_granularity &#x3D; 8192;select rdtime from kafka_data_11 WHERE rdtime &lt; &#39;2021-11-30 02:53:49&#39; and rdtime &gt; &#39;2021-11-27 02:53:49&#39; AND kdcode &#x3D; &#39;hmb&#39; 目前接触的函数包括 visitParamExtractString 解析json数据后获取key对应的value visitParamExtractRaw 解析json数据后防护原始值 补充 使用tabix来查询和管理clickhouse","categories":[],"tags":[{"name":"docker","slug":"docker","permalink":"https://imlike.cc/tags/docker/"},{"name":"clickhouse","slug":"clickhouse","permalink":"https://imlike.cc/tags/clickhouse/"}]},{"title":"hadoop-hbase的概览","slug":"hadoop-hbase-summary","date":"2021-11-03T08:27:34.000Z","updated":"2024-03-14T14:55:29.477Z","comments":true,"path":"4d0383ea.html","link":"","permalink":"https://imlike.cc/4d0383ea.html","excerpt":"","text":"每个服务的组成包括: hdfs: namenode datanode yarn history nodemanager hbase (实际写入读取都走的hdfs) hmaster : Master主要负责管理RegionServer集群，如负载均衡及资源分配等，它本身也可以以集群方式运行，但同一时刻只有一个master处于激活状态。当工作中的master宕掉后，zookeeper会切换到其它备选的master上。 hregionserver:负责具体数据块的读写操作。 http://c.biancheng.net/view/6500.html hbase服务 常见端口hadoop 3 core-default.xml hdfs-default.xml hdfs-rbf-default.xml yarn-default.xml mapred-default.xml PORT CONFIG NAME CONFIG VALUE 0 dfs.balancer.address 0.0.0.0:0 9866 dfs.datanode.address 0.0.0.0:9866 9864 dfs.datanode.http.address 0.0.0.0:9864 9865 dfs.datanode.https.address 0.0.0.0:9865 9867 dfs.datanode.ipc.address 0.0.0.0:9867 8111 dfs.federation.router.admin-address 0.0.0.0:8111 50071 dfs.federation.router.http-address 0.0.0.0:50071 50072 dfs.federation.router.https-address 0.0.0.0:50072 8888 dfs.federation.router.rpc-address 0.0.0.0:8888 8480 dfs.journalnode.http-address 0.0.0.0:8480 8481 dfs.journalnode.https-address 0.0.0.0:8481 8485 dfs.journalnode.rpc-address 0.0.0.0:8485 0 dfs.mover.address 0.0.0.0:0 50100 dfs.namenode.backup.address 0.0.0.0:50100 50105 dfs.namenode.backup.http-address 0.0.0.0:50105 9870 dfs.namenode.http-address 0.0.0.0:9870 9871 dfs.namenode.https-address 0.0.0.0:9871 9868 dfs.namenode.secondary.http-address 0.0.0.0:9868 9869 dfs.namenode.secondary.https-address 0.0.0.0:9869 50200 dfs.provided.aliasmap.inmemory.dnrpc-address 0.0.0.0:50200 2181 hadoop.registry.zk.quorum localhost:2181 10020 mapreduce.jobhistory.address 0.0.0.0:10020 10033 mapreduce.jobhistory.admin.address 0.0.0.0:10033 19888 mapreduce.jobhistory.webapp.address 0.0.0.0:19888 19890 mapreduce.jobhistory.webapp.https.address 0.0.0.0:19890 0 yarn.nodemanager.address $&#123;yarn.nodemanager.hostname&#125;:0 8049 yarn.nodemanager.amrmproxy.address 0.0.0.0:8049 8048 yarn.nodemanager.collector-service.address $&#123;yarn.nodemanager.hostname&#125;:8048 8040 yarn.nodemanager.localizer.address $&#123;yarn.nodemanager.hostname&#125;:8040 8042 yarn.nodemanager.webapp.address $&#123;yarn.nodemanager.hostname&#125;:8042 8044 yarn.nodemanager.webapp.https.address 0.0.0.0:8044 8032 yarn.resourcemanager.address $&#123;yarn.resourcemanager.hostname&#125;:8032 8033 yarn.resourcemanager.admin.address $&#123;yarn.resourcemanager.hostname&#125;:8033 8031 yarn.resourcemanager.resource-tracker.address $&#123;yarn.resourcemanager.hostname&#125;:8031 8030 yarn.resourcemanager.scheduler.address $&#123;yarn.resourcemanager.hostname&#125;:8030 8088 yarn.resourcemanager.webapp.address $&#123;yarn.resourcemanager.hostname&#125;:8088 8090 yarn.resourcemanager.webapp.https.address $&#123;yarn.resourcemanager.hostname&#125;:8090 8089 yarn.router.webapp.address 0.0.0.0:8089 8091 yarn.router.webapp.https.address 0.0.0.0:8091 8047 yarn.sharedcache.admin.address 0.0.0.0:8047 8045 yarn.sharedcache.client-server.address 0.0.0.0:8045 8046 yarn.sharedcache.uploader.server.address 0.0.0.0:8046 8788 yarn.sharedcache.webapp.address 0.0.0.0:8788 10200 yarn.timeline-service.address $&#123;yarn.timeline-service.hostname&#125;:10200 8188 yarn.timeline-service.webapp.address $&#123;yarn.timeline-service.hostname&#125;:8188 8190 yarn.timeline-service.webapp.https.address $&#123;yarn.timeline-service.hostname&#125;:8190","categories":[],"tags":[]},{"title":"RESTful 资源命名最佳实践","slug":"restful-resource-naming-practise","date":"2021-10-18T02:46:56.000Z","updated":"2024-03-14T14:55:29.481Z","comments":true,"path":"26c9c081.html","link":"","permalink":"https://imlike.cc/26c9c081.html","excerpt":"","text":"本文转载自 RESTful 资源命名最佳实践 RESTful 资源命名最佳实践 2019-09-20 2021-05-07 7 6k 5 分钟 在 Rest 中，数据的呈现方式叫做资源 (Resource)。拥有强大而一致的 REST 资源命名策略，是最好的设计决策。 一个资源可以是单个的也可以是一个集合。比如 customers 是一个集合，而 customer 是单个资源。我们可以定义 customers 这个集合的资源的 URI 是 /customers, 而单个 customer 资源的 URI 是 /customers/&#123;customerId&#125;。 资源也可以包含子集合的资源。比如，使用 /customers/&#123;customerId&#125;/accounts 来表示某个 customer 下的 account 集合资源。同样的，对于 account 集合资源下的单个 account 我们可以定义成这样:/customers/&#123;customerId&#125;/accounts/&#123;accountId&#125; REST API 使用统一资源标识符（URI）来寻址资源。REST API 设计者应该创建 URI，将 REST API 的资源模型传达给潜在的客户端开发人员。当资源命名良好时，API 直观且易于使用。如果做得不好，那么相同的 API 会感觉难以使用和理解。 使用名词来表示资源RESTful URI 应该引用作为事物（名词）的资源而不是引用动作（动词），因为名词具有动词不具有的属性 - 类似于具有属性的资源。资源的一些示例是： 系统的用户 用户账户 (银行的场景): 网络设备 它们的资源 URI 可以被设计成下面这样: 1234http:&#x2F;&#x2F;api.example.com&#x2F;device-management&#x2F;managed-devices http:&#x2F;&#x2F;api.example.com&#x2F;device-management&#x2F;managed-devices&#x2F;&#123;device-id&#125; http:&#x2F;&#x2F;api.example.com&#x2F;user-management&#x2F;users&#x2F;http:&#x2F;&#x2F;api.example.com&#x2F;user-management&#x2F;users&#x2F;&#123;id&#125; 为了更好的说明我们把资源原型分为四个种类 (document,collection,store 以及 controller), 你应该总是把资源放到其中一个原型中，并且遵守它的统一命名。 document文档资源是一种类似于对象实例或数据库记录的单一概念 (比如 mysql 中的一行记录，Mongodb 中的 document), 在 REST 中，你可以将其视为资源集合中的单个资源。文档的状态表示通常包括具有值的字段和指向其他相关资源的链接。 使用单数名称表示文档资源原型 123http:&#x2F;&#x2F;api.example.com&#x2F;device-management&#x2F;managed-devices&#x2F;&#123;device-id&#125;http:&#x2F;&#x2F;api.example.com&#x2F;user-management&#x2F;users&#x2F;&#123;id&#125;http:&#x2F;&#x2F;api.example.com&#x2F;user-management&#x2F;users&#x2F;admin collection集合资源是服务端管理的资源目录。客户可以建议将新资源添加到集合中。但是，要由集合选择是否创建新资源。集合资源选择它想要包含的内容，并决定每个包含的资源的 URI。 使用复数名称表示集合资源原型 123http:&#x2F;&#x2F;api.example.com&#x2F;device-management&#x2F;managed-deviceshttp:&#x2F;&#x2F;api.example.com&#x2F;user-management&#x2F;usershttp:&#x2F;&#x2F;api.example.com&#x2F;user-management&#x2F;users&#x2F;&#123;id&#125;&#x2F;accounts storestore 是客户端管理的资源库 ,store 资源允许 API 客户端放入资源，取出资源，并决定何时删除它们。store 永远不会生成新的 URI。相反，每个存储的资源都有一个客户端在最初放入存储时选择的 URI。 使用复数名称表示 store 资源原型 12http:&#x2F;&#x2F;api.example.com&#x2F;cart-management&#x2F;users&#x2F;&#123;id&#125;&#x2F;cartshttp:&#x2F;&#x2F;api.example.com&#x2F;song-management&#x2F;users&#x2F;&#123;id&#125;&#x2F;playlists controllercontroller 资源有点像程序的概念，controller 资源就像可执行函数，带有参数和返回值；输入和输出。 使用动词表示 controller 原型 12345&#x2F;&#x2F; 查看用户的信用卡http:&#x2F;&#x2F;api.example.com&#x2F;cart-management&#x2F;users&#x2F;&#123;id&#125;&#x2F;cart&#x2F;checkout&#x2F;&#x2F; 播放整个播放列表http:&#x2F;&#x2F;api.example.com&#x2F;song-management&#x2F;users&#x2F;&#123;id&#125;&#x2F;playlist&#x2F;play 这里的 controller 为什么要用动词呢？其实大家可以想象下 Spring 中 Controller 做了什么事情，它调用了 service 组合成各个业务逻辑，将数据组合起来之后进行返回. 一致性使用一致的资源命名约定和 URI 格式，以最小化和最大可读性和可维护性。你可以实现以下设计提示以实现一致性： 使用正斜杠（/）表示层次关系正斜杠（/）字符用于 URI 的路径部分，以指示资源之间的层次关系 12345http:&#x2F;&#x2F;api.example.com&#x2F;device-managementhttp:&#x2F;&#x2F;api.example.com&#x2F;device-management&#x2F;managed-deviceshttp:&#x2F;&#x2F;api.example.com&#x2F;device-management&#x2F;managed-devices&#x2F;&#123;id&#125;http:&#x2F;&#x2F;api.example.com&#x2F;device-management&#x2F;managed-devices&#x2F;&#123;id&#125;&#x2F;scriptshttp:&#x2F;&#x2F;api.example.com&#x2F;device-management&#x2F;managed-devices&#x2F;&#123;id&#125;&#x2F;scripts&#x2F;&#123;id&#125; 不要在 URI 中使用尾部正斜杠（/）作为 URI 路径中的最后一个字符，正斜杠（/）不会添加语义值，并可能导致混淆。最好完全放弃它们 1234http:&#x2F;&#x2F;api.example.com&#x2F;device-management&#x2F;managed-devices&#x2F;&#x2F;*这个版本更好*&#x2F;http:&#x2F;&#x2F;api.example.com&#x2F;device-management&#x2F;managed-devices 使用连字符（ - ）来提高 URI 的可读性要使你的 URI 易于扫描和解释，请使用连字符（ - ）字符来提高长路径段中名称的可读性。 12345&#x2F;&#x2F; 更好可读性http:&#x2F;&#x2F;api.example.com&#x2F;inventory-management&#x2F;managed-entities&#x2F;&#123;id&#125;&#x2F;install-script-location&#x2F;&#x2F; 可读性不够高http:&#x2F;&#x2F;api.example.com&#x2F;inventory-management&#x2F;managedEntities&#x2F;&#123;id&#125;&#x2F;installScriptLocation 不用使用下滑线 _可以使用下划线代替连字符作为分隔符 - 但是根据应用程序的字体，下划线 _ 字符可能会在某些浏览器或屏幕中被部分遮挡或完全隐藏。为避免这种混淆，请使用连字符 - 而不是下划线 _。 12345&#x2F;&#x2F; 更具可读性http:&#x2F;&#x2F;api.example.com&#x2F;inventory-management&#x2F;managed-entities&#x2F;&#123;id&#125;&#x2F;install-script-location&#x2F;&#x2F; 更容易出错http:&#x2F;&#x2F;api.example.com&#x2F;inventory_management&#x2F;managed_entities&#x2F;&#123;id&#125;&#x2F;install_script_location 在 URI 中使用小写字母方便时，URI 路径中应始终首选小写字母。 RFC 3986 将 URI 定义为区分大小写，但协议和 host 除外 123456&#x2F;&#x2F; 1http:&#x2F;&#x2F;api.example.org&#x2F;my-folder&#x2F;my-doc&#x2F;&#x2F; 2HTTP:&#x2F;&#x2F;API.EXAMPLE.ORG&#x2F;my-folder&#x2F;my-doc&#x2F;&#x2F; 3http:&#x2F;&#x2F;api.example.org&#x2F;My-Folder&#x2F;my-doc 在上面的例子中，1 和 2 是相同的，但 3 不是，因为它使用大写字母的 My-Folder。 不要使用文件扩展名文件扩展名看起来很糟糕，不会增加任何优势。删除它们也会减少 URI 的长度。没理由保留它们。除了上述原因，如果你想使用文件扩展突出显示 API 的媒体类型，那么你应该依赖于通过 Content-Type 标头传达的媒体类型来确定如何处理正文的内容。 12345&#x2F;&#x2F; 不要这样用http:&#x2F;&#x2F;api.example.com&#x2F;device-management&#x2F;managed-devices.xml&#x2F;&#x2F; 正确的URIhttp:&#x2F;&#x2F;api.example.com&#x2F;device-management&#x2F;managed-devices 切勿在 URI 中使用 CRUD 函数名称URI 不应用于指示执行 CRUD 功能。URI 应该用于唯一标识资源，而不是对它们的任何操作。应使用 HTTP 请求方法来指示执行哪个 CRUD 功能。 12345678910111213&#x2F;&#x2F; 获取所有设备HTTP GET http:&#x2F;&#x2F;api.example.com&#x2F;device-management&#x2F;managed-devices&#x2F;&#x2F; 创建新设备HTTP POST http:&#x2F;&#x2F;api.example.com&#x2F;device-management&#x2F;managed-devices&#x2F;&#x2F; 根据给定id获取设备HTTP GET http:&#x2F;&#x2F;api.example.com&#x2F;device-management&#x2F;managed-devices&#x2F;&#123;id&#125;&#x2F;&#x2F; 根据给定id更新设备HTTP PUT http:&#x2F;&#x2F;api.example.com&#x2F;device-management&#x2F;managed-devices&#x2F;&#123;id&#125;&#x2F;&#x2F; 根据给定id删除设备HTTP DELETE http:&#x2F;&#x2F;api.example.com&#x2F;device-management&#x2F;managed-devices&#x2F;&#123;id&#125; 使用查询组件过滤 URI 集合很多时候，你会遇到需要根据某些特定资源属性对需要排序，过滤或限制的资源集合的要求。为此，不要创建新的 API - 而是在资源集合 API 中启用排序，过滤和分页功能，并将输入参数作为查询参数传递 1234http:&#x2F;&#x2F;api.example.com&#x2F;device-management&#x2F;managed-deviceshttp:&#x2F;&#x2F;api.example.com&#x2F;device-management&#x2F;managed-devices?region&#x3D;USAhttp:&#x2F;&#x2F;api.example.com&#x2F;device-management&#x2F;managed-devices?region&#x3D;USA&amp;brand&#x3D;XYZhttp:&#x2F;&#x2F;api.example.com&#x2F;device-management&#x2F;managed-devices?region&#x3D;USA&amp;brand&#x3D;XYZ&amp;sort&#x3D;installation-date","categories":[],"tags":[]},{"title":"why-mongodb-use-b-tree","slug":"why-mongodb-use-b-tree","date":"2021-10-18T02:35:49.000Z","updated":"2024-03-14T14:55:29.483Z","comments":true,"path":"bf7d2fd3.html","link":"","permalink":"https://imlike.cc/bf7d2fd3.html","excerpt":"","text":"本文转载自 why-mongodb-use-b-tree 除了 B+ 树，你可能还听说过 B 树、 B- 树，实际上， B- 树就是 B 树，英文翻译都是 B-Tree ，这里的 “-” 并不是相对 B+ 树中的 “+” ，而只是一个连接符。而 B 树实际上是低级版的 B+ 树，或者说 B+ 树是 B 树的改进版。 B+ treeB+ tree 实际上是一颗 m 叉平衡查找树 (不是二叉树) 平衡查找树定义：树中任意一个节点的左右子树的高度相差不能大于 1 B+ tree 123456789101112131415161718192021222324252627282930313233343536373839404142434445&#x2F;*** 这是B+树非叶子节点的定义。** 假设keywords&#x3D;[3, 5, 8, 10]* 4个键值将数据分为5个区间：(-INF,3), [3,5), [5,8), [8,10), [10,INF)* 5个区间分别对应：children[0]...children[4]** m值是事先计算得到的，计算的依据是让所有信息的大小正好等于页的大小：* PAGE_SIZE &#x3D; (m-1)*4[keywordss大小]+m*8[children大小]*&#x2F;public class BPlusTreeNode &#123; &#x2F;&#x2F; 5叉树 public static int m &#x3D; 5; &#x2F;&#x2F; 键值，用来划分数据区间 public int[] keywords &#x3D; new int[m-1]; &#x2F;&#x2F; 保存子节点指针 public BPlusTreeNode[] children &#x3D; new BPlusTreeNode[m];&#125;&#x2F;*** 这是B+树中叶子节点的定义。** B+树中的叶子节点跟内部结点是不一样的,* 叶子节点存储的是值，而非区间。* 这个定义里，每个叶子节点存储3个数据行的键值及地址信息。** k值是事先计算得到的，计算的依据是让所有信息的大小正好等于页的大小：* PAGE_SIZE &#x3D; k*4[keyw..大小]+k*8[dataAd..大小]+8[prev大小]+8[next大小]*&#x2F;public class BPlusTreeLeafNode &#123; public static int k &#x3D; 3; &#x2F;&#x2F; 数据的键值 public int[] keywords &#x3D; new int[k]; &#x2F;&#x2F; 数据地址 public long[] dataAddress &#x3D; new long[k]; &#x2F;&#x2F; 这个结点在链表中的前驱结点 public BPlusTreeLeafNode prev; &#x2F;&#x2F; 这个结点在链表中的后继结点 public BPlusTreeLeafNode next;&#125; 在 B+ 树中，树中的节点并不存储数据本身，而是只是作为索引。除此之外，所有记录的节点按大小顺序存储在同一层的叶节点中，并且每个叶节点通过指针连接。 总结下，B + 树有以下特点 B + 树的每个节点可以包含更多节点，其原因有两个，其一是降低树的高度 (索引不会全部存储在内存中，内存中可能撑不住，所以一般都是将索引树存储在磁盘中，只是将根节点放到内存中，这样对每个节点的访问，实际上就是访问磁盘，树的高度就等于每次查询数据时磁盘 IO 操作的次数), 另一种是将数据范围更改为多个间隔。间隔越大，数据检索越快 (可以想象跳表) 每个节点不在是存储一个 key, 而是存储多个 key 叶节点来存储数据，而其他节点用于索引 叶子节点通过两个指针相互链接，顺序查询性能更高。 这样设计还有以下优点： B + 树的非叶子节点仅存储键，占用很小的空间，因此节点的每一层可以索引的数据范围要宽得多。换句话说，可以为每个 IO 操作搜索更多数据 叶子节点成对连接，符合磁盘的预读特性。例如，叶节点存储 50 和 55，它们具有指向叶节点 60 和 62 的指针。当我们从磁盘读取对应于 50 和 55 的数据时，由于磁盘的预读特性，我们将顺便提一下 60 和 62。读出相应的数据。这次是顺序读取，而不是磁盘搜索，加快了速度。 支持范围查询，局部范围查询非常高效，每个节点都可以索引更大，更准确的范围，这意味着 B + 树单磁盘 IO 信息大于 B 树，并且 I / O 效率更高 由于数据存储在叶节点层中，并且有指向其他叶节点的指针，因此范围查询仅需要遍历叶节点层，而无需遍历整个树。 由于磁盘访问速度和内存之间存在差距，为了提高效率，应将磁盘 I / O 最小化。磁盘通常不是严格按需读取的，而是每次都被预读。磁盘读取所需的数据后，它将向后读取内存中的一定长度的数据。这样做的理论基础是计算机科学中众所周知的本地原理： 关于 MySQL 数据索引是如何实现的，可以参考这篇文章：https://time.geekbang.org/column/article/77830 B-TreeB-Tree 实际上也是一颗 m 叉平衡查找树 B-Tree 所有的 key 值分布在整个树中 所有的 key 值出现在一个节点中 搜索可以在非叶子节点处结束 在完整的关键字搜索过程中，性能接近二分搜索。 B 树和 B + 树之间的区别 B + 树中的非叶子节点不存储数据，并且存储在叶节点中的所有数据使得查询时间复杂度固定为 log n。 B 树查询时间的复杂度不是固定的，它与键在树中的位置有关，最好是 O（1）。 由于 B + 树的叶子节点是通过双向链表链接的，所以支持范围查询，且效率比 B 树高 B 树每个节点的键和数据是一起的 为什么 MongoDB 使用 B-Tree,Mysql 使用 B+Tree ?B + 树中的非叶子节点不存储数据，并且存储在叶节点中的所有数据使得查询时间复杂度固定为 log n。B 树查询时间复杂度不是固定的，它与键在树中的位置有关，最好是 O (1)。 我们已经说过，尽可能少的磁盘 IO 是提高性能的有效方法。MongoDB 是一个聚合数据库，而 B 树恰好是键域和数据域的集群。 至于为什么 MongoDB 使用 B 树而不是 B + 树，可以从其设计的角度考虑它。MongoDB 不是传统的关系数据库，而是以 BSON 格式 (可以认为是 JSON) 存储的 nosql。目的是高性能，高可用性和易于扩展。 Mysql 是关系型数据库，最常用的是数据遍历操作 (join)，而 MongoDB 它的数据更多的是聚合过的数据，不像 Mysql 那样表之间的关系那么强烈，因此 MongoDB 更多的是单个查询。 由于 Mysql 使用 B + 树，数据在叶节点上，叶子节点之间又通过双向链表连接，更加有利于数据遍历，而 MongoDB 使用 B 树，所有节点都有一个数据字段。只要找到指定的索引，就可以对其进行访问。毫无疑问，单个查询 MongoDB 平均查询速度比 Mysql 快。 Hash 索引简而言之，哈希索引使用某种哈希算法将键值转换为新的哈希值。不需要像 B + 树那样从根节点到叶节点逐步搜索。只需要一种哈希算法，就可以立即找到对应的位置，速度非常快。(此处可以想想 Java 中的 HashMap)。 B + 树索引和 Hash 索引的区别 如果是等价查询，则哈希索引显然具有绝对优势，因为只需一种算法即可找到相应的键值；当然，前提是键值是唯一的，如果存在 hash 冲突就必须链表遍历了。 哈希索引不支持范围查询 (不过改造之后可以，Java 中的 LinkedHashMap 通过链表保存了节点的插入顺序，那么也可以使用链表将数据的大小顺序保存起来) 这样做虽然支持了范围查询但是时间复杂度是 O (n), 效率比跳表和 B+Tree 差 哈希索引无法使用索引排序以及模糊匹配 哈希索引也不支持多列联合索引的最左边匹配规则。 键值大量冲突的情况下，Hash 索引效率极低","categories":[],"tags":[]},{"title":"think-about-cluster","slug":"think-about-cluster","date":"2021-10-15T02:33:48.000Z","updated":"2024-03-14T14:55:29.481Z","comments":true,"path":"2525ce5c.html","link":"","permalink":"https://imlike.cc/2525ce5c.html","excerpt":"","text":"分布式集群 分布式: 即拆分原有的功能,避免冗余和服务的耦合 集群: 提供多台服务避免单点故障 使用分布式集群好处很多 避免某个服务出问题服务不受影响,同时提供多台性能没那么强的数据也成本更低,数据的分块和副本也能最大程度减少数据的丢失,单机io瓶颈 坏处就是: 开发成本 超时等待,不同节点数据的同步,分布式事务,grpc接口的各种可能情况 通常会有3个以上的节点,数据过来会被分块和创建副本,同时会有选取机制或者是专门的服务负责协调数据的同步和客户端访问具体的数据节点 下面举出比较有特点的分布式服务 elasticsearch kafka hbase zookeeper 节点 datandoe coordinate数据 doc数据集 index数据分块 shard 节点 broke数据 message数据集 topic数据集分块 partition replica例外消费者comsumer生产者 producter消费者组 comsumer-group元数据保存在zookeeper上 节点 datanode数据 节点 node选举半数以上有个master,存储元数据leaderfollower index：对外暴露的搜索单元，由一系列的shard组成。shard：es实际执行的单元，开始针对索引的写入和查询，都会拆分为segment：es里面完整的索引集合，由document和索引文件document：对应一天天数据translog: 事务日志 table：由多行数据组成region：由很多store组成，有多少个列族就有多少个storeStore: 由Memstore和StoreFile组成，一个列簇对应一个Store，包含在内存中的Memstore以及持久化到hdfs里的多个HFile，StoreFile的底层就是HFile，它是HFile的一个轻量级封装row, column, cell: 一个row由rowKey和多个column及其对应值组成，一张表的所有row都按rowkey的字典序由小到大排序，column由columnFamily和Qualifier组成，cell是row, column, timestamp, type, value等组成。HLog: 类似于ES的Translog，主要也是用于数据的恢复。 merge 索引 merge 分块 rebalance 消费者rebalance topic 的推送 为什么hbase会有列族 可以没有严格限定列的格式就是因为 hbase的 下面是补充 分布式系统常见异常问题 通信异常：网络不可用（消息延迟或者丢失），会导致分布式系统内部无法顺利进行一次网络通信进行沟通协调，所以可能造成多节点数据丢失和 状态不一致，还有可能造成数据乱序。解决方案：重试机制 网络分区：网络不连通，但各个子网络的内部网络是正常的，从而导致整个系统的网络环境被切分成了若干个孤立的区域，分布式系统就会出现局 部小集群造成数据不一致。解决方案：把数据状态不是最新的给下线掉 节点故障/机器宕机：服务器节点出现的宕机或”僵死”现象，这是常态，而不是异常。解决方案：数据副本协议，异步复制 分布式三态：即成功、失败和超时，分布式环境中的网络通信请求发送和结果响应都有可能丢失，所以请求发起方无法确定消息是否处理成功。分 布式系统的可用性：在用户能忍受的时间范围内，一定给出响应！解决方案：超时处理 存储数据丢失：对于有状态节点来说，数据丢失意味着状态丢失，通常只能从其他节点读取、恢复存储的状态。解决方案：副本协议 2.3 分布式系统性能指标 性能：下面三个性能指标（吞吐，延迟，并发）往往会相互制约，追求高吞吐的系统，往往很难做到低延迟；系统平均响应时间较长时，也很难提高 QPS。系统的吞吐能力，指系统在某一时间可以处理的数据/请求总量，通常可以用系统每秒处理的总数据/总请求量来衡量； 系统的响应延迟，指系统完成某一功能需要使用的时间； 系统的并发能力，指系统可以同时完成某一功能的能力，通常也用QPS(query per second)来衡量。 可用性：系统的可用性（availability）指系统在面对各种异常时可以正确提供服务的能力。系统的可用性可以用系统停服务的时间与正常服务的时间 的比例来衡量，也可以用某功能的失败次数与成功次数的比例来衡量。可用性是分布式的重要指标，衡量了系统的鲁棒性，是系统容错能力的体现。（5 个 9 的可靠性：一年只有 5 分钟的宕机时间！6 个 9 的可靠性，也就是 31 秒，99.9999%） 可扩展性：系统的可扩展性（scalability）指分布式系统通过扩展集群机器规模提高系统性能（吞吐、延迟、并发）、存储容量、计算能力的特性。好 的分布式系统总在追求 “线性扩展性”，也就是使得系统的某一指标可以随着集群中的机器数量线性增长。最期望的情况：动态热部署 一致性：分布式系统为了提高可用性，总是不可避免的使用副本的机制，从而引发副本一致性的问题。越是强的一致性的模型，对于用户使用来说使 用起来越简单。 ZooKeeper架构设计分析/","categories":[],"tags":[]},{"title":"think-about-data-structure","slug":"think-about-data-structure","date":"2021-10-15T02:17:39.000Z","updated":"2024-03-14T14:55:29.482Z","comments":true,"path":"b760fb1a.html","link":"","permalink":"https://imlike.cc/b760fb1a.html","excerpt":"","text":"为什么有数据结构的问题就是数据本来是无序和杂乱的怎么在保存的时候方便后续更加方便查询和分析如果是像记事本和word这种的确是可以使用单考虑到大量的数据就要考虑效率和成本 最后就发现了很多方法 内存保存 基于机械硬盘保存附以数据结构 混合内存和硬盘保存包括数据的生命周期来实现 内存保存redis,Cassandra,Neo4j 等将数据保存在内存中实现快速存储,同时还提供数据的持久化,和过期时间,同时包括其他的有效特性 磁盘保存 二分查找: 将文件数据有序保存，使用二分查找来完成特定key的查找。 哈希：用哈希将数据分割为不同的bucket B+树：使用B+树 或者 ISAM 等方法，可以减少外部文件的读取 外部文件： 将数据保存为日志，并创建一个hash或者查找树映射相应的文件。 基于硬盘的保存就要优先考虑磁盘原理,磁盘的探测器读取磁道的数据,所以数据是顺序读写的,且会预读取数据到内存,目前很多开发的分布式服务都基于此设计为(LSM Log Structured-Merge Tree) 包括kafka, hbase 等尽量避免顺序读写 b+数这类设计目标提高数据的查询的效率,在数据保存的时候将数据建议归类以树状的结构排列,同时在包括叶子节点和根节点来实现快速找对数据节点 通常数据库还有一个方式 WAL(writte ahaead log) 即写数据之前会先累积数据后在保存在 现在常见的数据保存结构有哪些 树状结构 b数,b+树,b-树 线性 lsm结构 数据保存读写会累积不是覆盖这种 数据库索引常见,b-tree,hash,gist,gin,sp-gist,brin 方便查询 方便读写 B-Tree 索引是 MySQL 数据库中使用最为频繁的索引类型，除了 Archive 存储引擎之外的其他所有的存储引擎都支持 B-Tree 索引。不仅仅在 MySQL 中是如此，实际上在其他的很多数据库管理系统中B-Tree 索引也同样是作为最主要的索引类型，这主要是因为B-Tree 索引的存储结构在数据库的数据检索中有非常优异的表现。 一般来说， MySQL 中的 B-Tree 索引的物理文件大多都是以 Balance Tree 的结构来存储的，也就是所有实际需要的数据都存放于 Tree 的 Leaf Node ，而且到任何一个 Leaf Node 的最短路径的长度都是完全相同的，所以我们大家都称之为 B-Tree 索引当然，可能各种数据库（或 MySQL 的各种存储引擎）在存放自己的 B-Tree 索引的时候会对存储结构稍作改造。如 Innodb 存储引擎的 B-Tree 索引实际使用的存储结构实际上是 B+Tree ，也就是在 B-Tree 数据结构的基础上做了很小的改造，在每一个Leaf Node (叶子节点)上面出了存放索引键的相关信息之外，还存储了指向与该 Leaf Node 相邻的后一个 LeafNode 的指针信息，这主要是为了加快检索多个相邻 Leaf Node 的效率考虑。 B+树是一个平衡的多叉树，从根节点到每个叶子节点的高度差值不超过1，而且同层级的节点间有指针相互链接。 在B+树上的常规检索，从根节点到叶子节点的搜索效率基本相当，不会出现大幅波动，而且基于索引的顺序扫描时，也可以利用双向指针快速左右移动，效率非常高。 因此，B+树索引被广泛应用于数据库、文件系统等场景。顺便说一下，xfs文件系统比ext3/ext4效率高很多的原因之一就是，它的文件及目录索引结构全部采用B+树索引，而ext3/ext4的文件目录结构则采用Linked list, hashed B-tree、Extents/Bitmap等索引数据结构，因此在高I/O压力下，其IOPS能力不如xfs。 所以使用 混合内存和硬盘保存包括数据的生命周期来实现代表程序为 elasticsearch 数据过来会被建立索引保存在内存中后面会被保存在磁盘,数据的生命周期为hot warm cold delete 阶段 / action 优先级设置 取消跟随 滚动索引 分片分配 只读 强制段合并 收缩索引 冻结索引 删除 hot √ √ √ × × × × × × warm √ √ × √ √ √ √ × × cold √ √ × √ × × × √ × delete × × × × × × × × √","categories":[],"tags":[]},{"title":"k8s-deploy-geoserver-activemq","slug":"k8s-deploy-geoserver-activemq","date":"2021-10-15T01:40:51.000Z","updated":"2024-03-14T14:55:29.479Z","comments":true,"path":"c1c66eff.html","link":"","permalink":"https://imlike.cc/c1c66eff.html","excerpt":"","text":"目前公司需要搭建geoserver集群集成jms和jdbcstore的插件,实现数据保存在数据库,同时通过jms来同步集群的缓存数据 部署activeMQ基本信息 #### 包括注解包括你需要备注的信息标签: 用于部署pod 服务,用于设置部署在指定标签节点 在高级设置里面可以设置副本数即类似于克隆了多个镜像服务,他们的配置都是一样的分层级类似于归类的作业,突出显示和归纳 配置容器 这部分主要是要注意要注意如果服务是需要暴露端口的必须要配置,否则服务启动会有问题,暴露的端口不是直接给外部访问,是节点的容器组IP的暴露出来的可以用于容器内部的通信容器的镜像通常就是 dockerhub官方提供的,亦或者是内网的镜像仓库harbor部署的,命令参数目前使用的少,看介绍可以替代dockerfile中 cmd部分执行的命令环境变量即添加容器内部的环境变量,包括很多敏感的secret和key ,需要运行时使用便可以放在上面资源请求/限制: 即对容器调用host的资源进行限制,不挤占其他服务的资源​ 挂载存储这里主要是注意一点 挂载数据卷到容器中,倘若容器中的配置文件在挂载卷不存在,但是配置挂载会替换容器中的文件导致pod启动失败​ 还有配置路径也是需要注意上面的路径的容器内的绝对路径下面的子路径是相对路径但是相对的是 挂载的卷的的根节点 类似于 /var/lib/kubelet/pods/8c7ba28d-59bd-4ad3-a592-4212ad540939/volumes/kubernetes.io~glusterfs/pvc-02a2b3a6-bd84-4687-8c37-67eb0b4a1ed1如果配置的是文件那么下面挂载卷内的文件会替换上面容器绝对路径的文件 服务/应用路由 需要主要的要主要在服务定义 如果是暴露在集群内部那么选择clusterip只会容器上可以使用 通过集群的内部 IP 暴露服务，选择该值时服务只能够在集群内部访问。 这也是默认的 ServiceType。如果选择nodePort那么则是需要在集群对外暴露端口通常都是五位数的端口,通过集群每个节点的端口监听和dns解析,headless是没有任何配置 ** Service 并不会分配 Cluster IP，kube-proxy 不会处理它们， 而且平台也不会为它们进行负载均衡和路由。 DNS 如何实现自动配置，依赖于 Service 是否定义了选择算符** tips如果保存后pod启动有问题报 不挂载卷,可以直接启动,但是挂载就一直报Back-off restarting failed container 这种一般都是容器启动的时有问题导致的运行失败,需要看日志,体现出来就是容器启动后没有一种运行的进程导致 被回收 优化目标 使用jdbcstore插件后大比例不 不显示空白 结果jdbcstore和jms插件的冲突 高版本(19)geoserver集群的部署 使用jdbcstore插件后大比例不 不显示空白现象在使用jdbcstore插件来实现数据的数据库保存,但是在预览图层时会出现放大到一定程度层级后空白 的情况 解决方法在查询源码后知道新版的geoserver优化的SQL查询逻辑调用了 老版的geoserver实际执行的SQL 12345-- 对比可以知道 在勾选Support on the fly geometry simplification 后 没有使用ST_Simplify 这个函数在SELECT &quot;fid&quot;,encode(ST_AsBinary(ST_Simplify(ST_Force2D(&quot;geom&quot;), 3.8116173982620243, true)),&#x27;base64&#x27;) as &quot;geom&quot; FROM &quot;public&quot;.&quot;qj_temp_gzw&quot; WHERE &quot;geom&quot; &amp;&amp; ST_GeomFromText(&#x27;POLYGON ((38371404.953002505 3413497.2549508526, 38371404.953002505 3417181.212940211, 38374478.06952626 3417181.212940211, 38374478.06952626 3413497.2549508526, 38371404.953002505 3413497.2549508526))&#x27;, 4526) 勾选SELECT &quot;fid&quot;,encode(ST_AsBinary(ST_Force2D(&quot;geom&quot;)),&#x27;base64&#x27;) as &quot;geom&quot; FROM &quot;public&quot;.&quot;qj_temp_gzw&quot; WHERE &quot;geom&quot; &amp;&amp; ST_GeomFromText(&#x27;POLYGON ((38371404.953002505 3413497.2549508526, 38371404.953002505 3417181.212940211, 38374478.06952626 3417181.212940211, 38374478.06952626 3413497.2549508526, 38371404.953002505 3413497.2549508526))&#x27;, 4526) 不勾选 新版的geoserver实际执行的SQL 12345-- 对比可以确定 新版本勾选 Support on the fly geometry simplification后 区别在于选用ST_AsTWKB作为简化内容导致的数据缺失SELECT &quot;fid&quot;,encode(ST_AsTWKB(ST_Simplify(ST_Force2D(&quot;geom&quot;), 3.8116173923015597, true),0), &#x27;base64&#x27;) as &quot;geom&quot; FROM &quot;public&quot;.&quot;qj_temp_gzw&quot; WHERE &quot;geom&quot; &amp;&amp; ST_GeomFromText(&#x27;POLYGON ((38371404.953002505 3413497.2549508526, 38371404.953002505 3417181.212940211, 38374478.06952626 3417181.212940211, 38374478.06952626 3413497.2549508526, 38371404.953002505 3413497.2549508526))&#x27;, 4526)SELECT &quot;fid&quot;,encode(ST_AsBinary(ST_Force2D(&quot;geom&quot;)), &#x27;base64&#x27;) as &quot;geom&quot; FROM &quot;public&quot;.&quot;qj_temp_gzw&quot; WHERE &quot;geom&quot; &amp;&amp; ST_GeomFromText(&#x27;POLYGON ((38371404.953002505 3413497.2549508526, 38371404.953002505 3417181.212940211, 38374478.06952626 3417181.212940211, 38374478.06952626 3413497.2549508526, 38371404.953002505 3413497.2549508526))&#x27;, 4526) debug方法调用 org.geoserver.wms.GetMap#executeInternal org.geotools.renderer.lite.StreamingRenderer#drawPlain org.geotools.data.store.ContentFeatureSource#getReader(org.geotools.data.Query) org.geoserver.wms.GetMapOutputFormat#produceMap org.geoserver.wms.map.RenderedImageMapOutputFormat#produceMap(org.geoserver.wms.WMSMapContent, boolean) renderer.paint( org.geotools.data.postgis.PostGISDialect#encodeGeometryColumnSimplified 最后修改gt-jdbc-postgis jar下面的 org.geotools.data.postgis.PostGISDialect#isAggregatedSortSupported 然后重新打包替换 1234567891011static final Version V_1_5_0 &#x3D; new Version(&quot;1.5.0&quot;);static final Version V_2_0_0 &#x3D; new Version(&quot;2.0.0&quot;);static final Version V_2_1_0 &#x3D; new Version(&quot;2.1.0&quot;);static final Version V_2_2_0 &#x3D; new Version(&quot;2.30.0&quot;); &#x2F;&#x2F;修改这个字段static final Version PGSQL_V_9_0 &#x3D; new Version(&quot;9.0&quot;);static final Version PGSQL_V_9_1 &#x3D; new Version(&quot;9.1&quot;); 保证无论碰到什么版本的geoserver使用老的方法 解决jdbcstore和jms插件的冲突现象启动失败会报MultipleBeansException org.geoserver.config.GeoServerLoader 解决方法查询知道在 jms-cluster 下面有个 org.geoserver.config.ReadOnlyGeoServerLoader 继承了org.geoserver.config.DefaultGeoServerLoader 同时gs-jdbcconfig 下面 org.geoserver.jdbcconfig.JDBCGeoServerLoader也继承DefaultGeoServerLoader 导致冲突在org.geoserver.config.GeoServerLoaderProxy#lookupGeoServerLoader 会判断GeoServerLoader的类,如果是两个就报错 目前是默认使用JDBCGeoServerLoader 代码如下 12345678910111213141516protected GeoServerLoader lookupGeoServerLoader(ApplicationContext appContext) &#123; GeoServerLoader loader = null; try &#123; loader = (GeoServerLoader) GeoServerExtensions.bean( Class.forName(&quot;org.geoserver.jdbcconfig.JDBCGeoServerLoader&quot;), appContext); &#125; catch (ClassNotFoundException e) &#123; &#125; if (loader == null) &#123; loader = new DefaultGeoServerLoader(resourceLoader); &#125; return loader; &#125; 高版本(19)geoserver集群的部署现象集群挂载在同一个盘上面,修改一个jmx的配置,其他节点也跟着修改 解决方法针对不同节点创建各自的配置文件,在挂载的时候配置映射文件","categories":[],"tags":[]},{"title":"geoserver-high-version-some-scale-lose-layer","slug":"geoserver-high-version-some-scale-lose-layer","date":"2021-09-14T12:59:54.000Z","updated":"2024-03-14T14:55:29.477Z","comments":true,"path":"8070f974.html","link":"","permalink":"https://imlike.cc/8070f974.html","excerpt":"","text":"启动geoserver报错文件找不到现象123456789101112131415java.io.FileNotFoundException: src&#x2F;main&#x2F;webapp at org.eclipse.jetty.webapp.WebInfConfiguration.unpack(WebInfConfiguration.java:671) at org.eclipse.jetty.webapp.WebInfConfiguration.preConfigure(WebInfConfiguration.java:152) at org.eclipse.jetty.webapp.WebAppContext.preConfigure(WebAppContext.java:506) at org.eclipse.jetty.webapp.WebAppContext.doStart(WebAppContext.java:544) at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68) at org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:138) at org.eclipse.jetty.server.Server.start(Server.java:416) at org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:108) at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:113) at org.eclipse.jetty.server.Server.doStart(Server.java:383) at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68) at org.geoserver.web.Start.main(Start.java:125)九月 15, 2021 11:21:37 上午 org.geoserver.web.Start main严重: GeoServer startup complete in 0.186s 解决方法org.geoserver.web.Start#main ​ wah.setWar(“src/web/app/src/main/webapp”); geoserver 调用wms服务时访问的数据 org.geoserver.wms.GetMap#executeInternal org.geotools.renderer.lite.StreamingRenderer#drawPlain org.geotools.data.store.ContentFeatureSource#getReader(org.geotools.data.Query) org.geoserver.wms.GetMapOutputFormat#produceMap org.geoserver.wms.map.RenderedImageMapOutputFormat#produceMap(org.geoserver.wms.WMSMapContent, boolean) renderer.paint( org.geotools.data.postgis.PostGISDialect#encodeGeometryColumnSimplified geoserver 在一定版本下出现数据缺失,本地发布的文件没有这种情况,目前只出现在用postgis发布的数据shp图层数据 参考了这个github-issueshttps://github.com/AtlasOfLivingAustralia/spatial-hub/issues/66 之前版本查询SQL 1SELECT &quot;ogc_fid&quot;,encode(ST_AsBinary(CASE WHEN ST_HasArc(&quot;geom&quot;) THEN &quot;geom&quot; ELSE ST_Simplify(ST_Force2D(&quot;geom&quot;), 1.71661376953125E-5, true) END),&#x27;base64&#x27;) as &quot;geom&quot; FROM &quot;public&quot;.&quot;cg4201061__hhbjx&quot; WHERE &quot;geom&quot; &amp;&amp; ST_GeomFromText(&#x27;POLYGON ((111.91068649291992 32.39775896072388, 111.91068649291992 32.41295099258423, 111.92733764648438 32.41295099258423, 111.92733764648438 32.39775896072388, 111.91068649291992 32.39775896072388))&#x27;, 4490) 现在geoserver 25.2 1234567SELECT &quot;fid&quot;, encode( ST_AsTWKB ( ST_Simplify ( ST_Force2D ( &quot;geom&quot; ), 3.8116173923015597, TRUE ), 0 ), &#x27;base64&#x27; ) AS &quot;geom&quot; FROM &quot;public&quot;.&quot;qj_temp_gzw&quot; WHERE &quot;geom&quot; &amp;&amp; ST_GeomFromText ( &#x27;POLYGON ((38371118.635024056 3413740.625232533, 38371118.635024056 3417424.5832218914, 38374191.75154781 3417424.5832218914, 38374191.75154781 3413740.625232533, 38371118.635024056 3413740.625232533))&#x27;, 4526 ) 在老的版本中 加入了判断是否 encode(ST_AsBinary(CASE WHEN ST_HasArc(“geom”) THEN “geom” ELSE ST_Simplify(ST_Force2D(“geom”), 1.71661376953125E-5, true) END),’base64’) as “geom” 是否是如果一个geometry对象或geometry collection包含一个CircularString对象，则返回TRUE这样如果包含圆的geometry会被简化 而 ST_Simplify() 函数则是通过数据简化 简化太多的圆变成三角形，中间变成八边形 老版本geoserver(2.15)执行SQL的区别 1234567-- 对比可以知道 在勾选Support on the fly geometry simplification 后 没有使用ST_Simplify 这个函数在SELECT &quot;fid&quot;,encode(ST_AsBinary(ST_Simplify(ST_Force2D(&quot;geom&quot;), 3.8116173982620243, true)),&#x27;base64&#x27;) as &quot;geom&quot; FROM &quot;public&quot;.&quot;qj_temp_gzw&quot; WHERE &quot;geom&quot; &amp;&amp; ST_GeomFromText(&#x27;POLYGON ((38371404.953002505 3413497.2549508526, 38371404.953002505 3417181.212940211, 38374478.06952626 3417181.212940211, 38374478.06952626 3413497.2549508526, 38371404.953002505 3413497.2549508526))&#x27;, 4526) 勾选SELECT &quot;fid&quot;,encode(ST_AsBinary(ST_Force2D(&quot;geom&quot;)),&#x27;base64&#x27;) as &quot;geom&quot; FROM &quot;public&quot;.&quot;qj_temp_gzw&quot; WHERE &quot;geom&quot; &amp;&amp; ST_GeomFromText(&#x27;POLYGON ((38371404.953002505 3413497.2549508526, 38371404.953002505 3417181.212940211, 38374478.06952626 3417181.212940211, 38374478.06952626 3413497.2549508526, 38371404.953002505 3413497.2549508526))&#x27;, 4526) 不勾选 新版本geoserver(2.19) 执行SQL的区别 123456789-- 对比可以确定 新版本勾选 Support on the fly geometry simplification后 区别在于选用ST_AsTWKB作为简化内容导致的数据缺失SELECT &quot;fid&quot;,encode(ST_AsTWKB(ST_Simplify(ST_Force2D(&quot;geom&quot;), 3.8116173923015597, true),0), &#x27;base64&#x27;) as &quot;geom&quot; FROM &quot;public&quot;.&quot;qj_temp_gzw&quot; WHERE &quot;geom&quot; &amp;&amp; ST_GeomFromText(&#x27;POLYGON ((38371404.953002505 3413497.2549508526, 38371404.953002505 3417181.212940211, 38374478.06952626 3417181.212940211, 38374478.06952626 3413497.2549508526, 38371404.953002505 3413497.2549508526))&#x27;, 4526)SELECT &quot;fid&quot;,encode(ST_AsBinary(ST_Force2D(&quot;geom&quot;)), &#x27;base64&#x27;) as &quot;geom&quot; FROM &quot;public&quot;.&quot;qj_temp_gzw&quot; WHERE &quot;geom&quot; &amp;&amp; ST_GeomFromText(&#x27;POLYGON ((38371404.953002505 3413497.2549508526, 38371404.953002505 3417181.212940211, 38374478.06952626 3417181.212940211, 38374478.06952626 3413497.2549508526, 38371404.953002505 3413497.2549508526))&#x27;, 4526)","categories":[],"tags":[]},{"title":"ue4-linux-running-webrtc","slug":"ue4-linux-running-webrtc","date":"2021-09-10T02:54:48.000Z","updated":"2024-03-14T14:55:29.482Z","comments":true,"path":"71a7527e.html","link":"","permalink":"https://imlike.cc/71a7527e.html","excerpt":"","text":"— 2021-8-10 补充—–chrome://webrtc-internals/可以查看当前浏览器的webrtc连接情况,方便查看debug 前置环境Pixel Streaming in Linux containers Linux上运行UE4像素流PixelStreaming nvidia-container-toolkit docker(docker) nvidia驱动 nvidia-smi adamrehn这个作者主要讲 ue4适配在linux中,其实源码的ue4 在编译后是可以在linux上跑的 主要通过 ue4-docker(python) 来打包 ue4的编译环境 docker build 来打包环境 和编译好的二进制文件 合并为一个镜像 就是pixel-stream-linux 以下面为dockerfile为例子 adamrehn/ue4-full:4.25-pixelstreaming 是 ue4-docker 生成的打包的环境 adamrehn/ue4-runtime:18.04-cudagl10.2 是在docker hub中存在的 运行环境 12345678910111213141516171819202122232425#在包含引擎工具和像素流的虚幻引擎容器映像中执行构建FROM adamrehn&#x2F;ue4-full:4.25-pixelstreaming AS builder#克隆或复制你的虚幻项目的源代码在这里，例如:RUN git clone --progress --depth 1 https:&#x2F;&#x2F;github.com&#x2F;user&#x2F;project.git &#x2F;tmp&#x2F;project# or:COPY --chown&#x3D;ue4:ue4 . &#x2F;tmp&#x2F;project#创建并打包我们的虚幻项目#(为了简洁起见，我们使用ue4cli，但我们也可以直接调用RunUAT)WORKDIR &#x2F;tmp&#x2F;projectRUN ue4 package复制打包文件到容器图像，其中包括CUDA，但不包括任何虚幻引擎组件FROM adamrehn&#x2F;ue4-runtime:18.04-cudagl10.2COPY --from&#x3D;builder --chown&#x3D;ue4:ue4 &#x2F;tmp&#x2F;project&#x2F;dist&#x2F;LinuxNoEditor &#x2F;home&#x2F;ue4&#x2F;project#启用NVIDIA驱动程序所需的NVENC视频编码APIENV NVIDIA_DRIVER_CAPABILITIES $&#123;NVIDIA_DRIVER_CAPABILITIES&#125;,video#创建一个指向libvideo -encode.so路径的符号链接。1将被挂载，因为UE4似乎忽略了LD_LIBRARY_PATH#(将“ProjectName”替换为你实际的虚幻项目的名称)RUN ln -s &#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libnvidia-encode.so &#x2F;home&#x2F;ue4&#x2F;project&#x2F;ProjectName&#x2F;Binaries&#x2F;Linux&#x2F;libnvidia-encode.so 安装成功的标准 运行 docker run –rm –gpus all nvidia/cuda:11.0-base nvidia-smi 有结果就可以 目前有两种方式来运行实例程序 打包dockerfile来运行 在上面的docker引用 使用命令 docker run –rm -ti –network=host –gpus=all adamrehn/pixel-streaming-example-project:4.25 -PixelStreamingIP=192.168.11.20 -PixelStreamingPort=8888 -RenderOffScreen -ForceRes -ResX=1920 -ResY=1080 以上是 ue4后台 如果不需要外放访问,不需要考虑上面 STUN,SDUN这些都是 nat外网穿透的服务 后面只要启动SignallingWebServer 也是就是ue安装目录 \\Epic Games\\UE_4.26\\Engine\\Source\\Programs\\PixelStreaming\\WebServers\\SignallingWebServer 启动中效果有问题详细解释我在github 上提了issue https://github.com/adamrehn/pixel-streaming-linux/issues/53 在修改cirrus.js后就可以解决 // let playerId = msg.playerId; 解析为数字 let playerId = parseInt(msg.playerId); delete msg.playerId; // no need to send it to the player let player = players.get(playerId); webrtc关联的内容补充如下参考这篇博文https://www.cnblogs.com/chyingp/p/sdp-in-webrtc.html 对于 webrtc spd内容的解读后对照着看了 SignallingWebServer/run.bat 服务的 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175v&#x3D;0o&#x3D;- 4274722770078417046 2 IN IP4 127.0.0.1s&#x3D;-t&#x3D;0 0a&#x3D;group:BUNDLE 0 1 2a&#x3D;msid-semantic: WMSm&#x3D;audio 9 UDP&#x2F;TLS&#x2F;RTP&#x2F;SAVPF 111 103 104 9 0 8 106 105 13 110 112 113 126c&#x3D;IN IP4 0.0.0.0a&#x3D;rtcp:9 IN IP4 0.0.0.0a&#x3D;ice-ufrag:R2GRa&#x3D;ice-pwd:jo3amy6z8ly9guu6xeyC0rwfa&#x3D;ice-options:tricklea&#x3D;fingerprint:sha-256 C0:7D:CF:1E:FC:D1:DD:CD:83:38:C7:5D:8F:0D:0E:0F:2A:F6:B0:97:49:FF:98:4A:10:07:5C:94:80:81:B1:2Aa&#x3D;setup:actpassa&#x3D;mid:0a&#x3D;extmap:1 urn:ietf:params:rtp-hdrext:ssrc-audio-levela&#x3D;extmap:2 http:&#x2F;&#x2F;www.webrtc.org&#x2F;experiments&#x2F;rtp-hdrext&#x2F;abs-send-timea&#x3D;extmap:3 http:&#x2F;&#x2F;www.ietf.org&#x2F;id&#x2F;draft-holmer-rmcat-transport-wide-cc-extensions-01a&#x3D;extmap:4 urn:ietf:params:rtp-hdrext:sdes:mida&#x3D;extmap:5 urn:ietf:params:rtp-hdrext:sdes:rtp-stream-ida&#x3D;extmap:6 urn:ietf:params:rtp-hdrext:sdes:repaired-rtp-stream-ida&#x3D;recvonlya&#x3D;rtcp-muxa&#x3D;rtpmap:111 opus&#x2F;48000&#x2F;2a&#x3D;rtcp-fb:111 transport-cca&#x3D;fmtp:111 minptime&#x3D;10;useinbandfec&#x3D;1a&#x3D;rtpmap:103 ISAC&#x2F;16000a&#x3D;rtpmap:104 ISAC&#x2F;32000a&#x3D;rtpmap:9 G722&#x2F;8000a&#x3D;rtpmap:0 PCMU&#x2F;8000a&#x3D;rtpmap:8 PCMA&#x2F;8000a&#x3D;rtpmap:106 CN&#x2F;32000a&#x3D;rtpmap:105 CN&#x2F;16000a&#x3D;rtpmap:13 CN&#x2F;8000a&#x3D;rtpmap:110 telephone-event&#x2F;48000a&#x3D;rtpmap:112 telephone-event&#x2F;32000a&#x3D;rtpmap:113 telephone-event&#x2F;16000a&#x3D;rtpmap:126 telephone-event&#x2F;8000m&#x3D;video 9 UDP&#x2F;TLS&#x2F;RTP&#x2F;SAVPF 96 97 98 99 100 101 122 102 121 127 120 125 107 108 109 35 36 124 119 123 118 114 115 116 37c&#x3D;IN IP4 0.0.0.0a&#x3D;rtcp:9 IN IP4 0.0.0.0a&#x3D;ice-ufrag:R2GRa&#x3D;ice-pwd:jo3amy6z8ly9guu6xeyC0rwfa&#x3D;ice-options:tricklea&#x3D;fingerprint:sha-256 C0:7D:CF:1E:FC:D1:DD:CD:83:38:C7:5D:8F:0D:0E:0F:2A:F6:B0:97:49:FF:98:4A:10:07:5C:94:80:81:B1:2Aa&#x3D;setup:actpassa&#x3D;mid:1a&#x3D;extmap:14 urn:ietf:params:rtp-hdrext:toffseta&#x3D;extmap:2 http:&#x2F;&#x2F;www.webrtc.org&#x2F;experiments&#x2F;rtp-hdrext&#x2F;abs-send-timea&#x3D;extmap:13 urn:3gpp:video-orientationa&#x3D;extmap:3 http:&#x2F;&#x2F;www.ietf.org&#x2F;id&#x2F;draft-holmer-rmcat-transport-wide-cc-extensions-01a&#x3D;extmap:12 http:&#x2F;&#x2F;www.webrtc.org&#x2F;experiments&#x2F;rtp-hdrext&#x2F;playout-delaya&#x3D;extmap:11 http:&#x2F;&#x2F;www.webrtc.org&#x2F;experiments&#x2F;rtp-hdrext&#x2F;video-content-typea&#x3D;extmap:7 http:&#x2F;&#x2F;www.webrtc.org&#x2F;experiments&#x2F;rtp-hdrext&#x2F;video-timinga&#x3D;extmap:8 http:&#x2F;&#x2F;www.webrtc.org&#x2F;experiments&#x2F;rtp-hdrext&#x2F;color-spacea&#x3D;extmap:4 urn:ietf:params:rtp-hdrext:sdes:mida&#x3D;extmap:5 urn:ietf:params:rtp-hdrext:sdes:rtp-stream-ida&#x3D;extmap:6 urn:ietf:params:rtp-hdrext:sdes:repaired-rtp-stream-ida&#x3D;recvonlya&#x3D;rtcp-muxa&#x3D;rtcp-rsizea&#x3D;rtpmap:96 VP8&#x2F;90000a&#x3D;rtcp-fb:96 goog-remba&#x3D;rtcp-fb:96 transport-cca&#x3D;rtcp-fb:96 ccm fira&#x3D;rtcp-fb:96 nacka&#x3D;rtcp-fb:96 nack plia&#x3D;rtpmap:97 rtx&#x2F;90000a&#x3D;fmtp:97 apt&#x3D;96a&#x3D;rtpmap:98 VP9&#x2F;90000a&#x3D;rtcp-fb:98 goog-remba&#x3D;rtcp-fb:98 transport-cca&#x3D;rtcp-fb:98 ccm fira&#x3D;rtcp-fb:98 nacka&#x3D;rtcp-fb:98 nack plia&#x3D;fmtp:98 profile-id&#x3D;0a&#x3D;rtpmap:99 rtx&#x2F;90000a&#x3D;fmtp:99 apt&#x3D;98a&#x3D;rtpmap:100 VP9&#x2F;90000a&#x3D;rtcp-fb:100 goog-remba&#x3D;rtcp-fb:100 transport-cca&#x3D;rtcp-fb:100 ccm fira&#x3D;rtcp-fb:100 nacka&#x3D;rtcp-fb:100 nack plia&#x3D;fmtp:100 profile-id&#x3D;2a&#x3D;rtpmap:101 rtx&#x2F;90000a&#x3D;fmtp:101 apt&#x3D;100a&#x3D;rtpmap:122 VP9&#x2F;90000a&#x3D;rtcp-fb:122 goog-remba&#x3D;rtcp-fb:122 transport-cca&#x3D;rtcp-fb:122 ccm fira&#x3D;rtcp-fb:122 nacka&#x3D;rtcp-fb:122 nack plia&#x3D;fmtp:122 profile-id&#x3D;1a&#x3D;rtpmap:102 H264&#x2F;90000a&#x3D;rtcp-fb:102 goog-remba&#x3D;rtcp-fb:102 transport-cca&#x3D;rtcp-fb:102 ccm fira&#x3D;rtcp-fb:102 nacka&#x3D;rtcp-fb:102 nack plia&#x3D;fmtp:102 level-asymmetry-allowed&#x3D;1;packetization-mode&#x3D;1;profile-level-id&#x3D;42001f;x-google-start-bitrate&#x3D;10000;x-google-max-bitrate&#x3D;20000a&#x3D;rtpmap:121 rtx&#x2F;90000a&#x3D;fmtp:121 apt&#x3D;102a&#x3D;rtpmap:127 H264&#x2F;90000a&#x3D;rtcp-fb:127 goog-remba&#x3D;rtcp-fb:127 transport-cca&#x3D;rtcp-fb:127 ccm fira&#x3D;rtcp-fb:127 nacka&#x3D;rtcp-fb:127 nack plia&#x3D;fmtp:127 level-asymmetry-allowed&#x3D;1;packetization-mode&#x3D;0;profile-level-id&#x3D;42001f;x-google-start-bitrate&#x3D;10000;x-google-max-bitrate&#x3D;20000a&#x3D;rtpmap:120 rtx&#x2F;90000a&#x3D;fmtp:120 apt&#x3D;127a&#x3D;rtpmap:125 H264&#x2F;90000a&#x3D;rtcp-fb:125 goog-remba&#x3D;rtcp-fb:125 transport-cca&#x3D;rtcp-fb:125 ccm fira&#x3D;rtcp-fb:125 nacka&#x3D;rtcp-fb:125 nack plia&#x3D;fmtp:125 level-asymmetry-allowed&#x3D;1;packetization-mode&#x3D;1;profile-level-id&#x3D;42e01f;x-google-start-bitrate&#x3D;10000;x-google-max-bitrate&#x3D;20000a&#x3D;rtpmap:107 rtx&#x2F;90000a&#x3D;fmtp:107 apt&#x3D;125a&#x3D;rtpmap:108 H264&#x2F;90000a&#x3D;rtcp-fb:108 goog-remba&#x3D;rtcp-fb:108 transport-cca&#x3D;rtcp-fb:108 ccm fira&#x3D;rtcp-fb:108 nacka&#x3D;rtcp-fb:108 nack plia&#x3D;fmtp:108 level-asymmetry-allowed&#x3D;1;packetization-mode&#x3D;0;profile-level-id&#x3D;42e01f;x-google-start-bitrate&#x3D;10000;x-google-max-bitrate&#x3D;20000a&#x3D;rtpmap:109 rtx&#x2F;90000a&#x3D;fmtp:109 apt&#x3D;108a&#x3D;rtpmap:35 AV1X&#x2F;90000a&#x3D;rtcp-fb:35 goog-remba&#x3D;rtcp-fb:35 transport-cca&#x3D;rtcp-fb:35 ccm fira&#x3D;rtcp-fb:35 nacka&#x3D;rtcp-fb:35 nack plia&#x3D;rtpmap:36 rtx&#x2F;90000a&#x3D;fmtp:36 apt&#x3D;35a&#x3D;rtpmap:124 H264&#x2F;90000a&#x3D;rtcp-fb:124 goog-remba&#x3D;rtcp-fb:124 transport-cca&#x3D;rtcp-fb:124 ccm fira&#x3D;rtcp-fb:124 nacka&#x3D;rtcp-fb:124 nack plia&#x3D;fmtp:124 level-asymmetry-allowed&#x3D;1;packetization-mode&#x3D;1;profile-level-id&#x3D;4d001f;x-google-start-bitrate&#x3D;10000;x-google-max-bitrate&#x3D;20000a&#x3D;rtpmap:119 rtx&#x2F;90000a&#x3D;fmtp:119 apt&#x3D;124a&#x3D;rtpmap:123 H264&#x2F;90000a&#x3D;rtcp-fb:123 goog-remba&#x3D;rtcp-fb:123 transport-cca&#x3D;rtcp-fb:123 ccm fira&#x3D;rtcp-fb:123 nacka&#x3D;rtcp-fb:123 nack plia&#x3D;fmtp:123 level-asymmetry-allowed&#x3D;1;packetization-mode&#x3D;1;profile-level-id&#x3D;64001f;x-google-start-bitrate&#x3D;10000;x-google-max-bitrate&#x3D;20000a&#x3D;rtpmap:118 rtx&#x2F;90000a&#x3D;fmtp:118 apt&#x3D;123a&#x3D;rtpmap:114 red&#x2F;90000a&#x3D;rtpmap:115 rtx&#x2F;90000a&#x3D;fmtp:115 apt&#x3D;114a&#x3D;rtpmap:116 ulpfec&#x2F;90000a&#x3D;rtpmap:37 flexfec-03&#x2F;90000a&#x3D;rtcp-fb:37 goog-remba&#x3D;rtcp-fb:37 transport-cca&#x3D;fmtp:37 repair-window&#x3D;10000000m&#x3D;application 9 UDP&#x2F;DTLS&#x2F;SCTP webrtc-datachannelc&#x3D;IN IP4 0.0.0.0a&#x3D;ice-ufrag:R2GRa&#x3D;ice-pwd:jo3amy6z8ly9guu6xeyC0rwfa&#x3D;ice-options:tricklea&#x3D;fingerprint:sha-256 C0:7D:CF:1E:FC:D1:DD:CD:83:38:C7:5D:8F:0D:0E:0F:2A:F6:B0:97:49:FF:98:4A:10:07:5C:94:80:81:B1:2Aa&#x3D;setup:actpassa&#x3D;mid:2a&#x3D;sctp-port:5000a&#x3D;max-message-size:262144 下面是其他日志 目前前端只需要简单的js来实现 123456789&#123; &quot;type&quot;: &quot;iceCandidate&quot;, &quot;candidate&quot;: &#123; &quot;candidate&quot;: &quot;candidate:2859681617 1 udp 2113937151 4ffffdaa-0101-4122-a4fb-b6c4c308d3e5.local 54016 typ host generation 0 ufrag R2GR network-cost 999&quot;, &quot;sdpMid&quot;: &quot;0&quot;, &quot;sdpMLineIndex&quot;: 0&#125;&#125;","categories":[],"tags":[]},{"title":"gis-coordinate","slug":"gis-coordinate","date":"2021-09-09T08:28:12.000Z","updated":"2024-03-14T14:55:29.477Z","comments":true,"path":"8707cf2d.html","link":"","permalink":"https://imlike.cc/8707cf2d.html","excerpt":"","text":"https://blog.geohey.com/-ge-xiao-you-xi-rang-ni-che-di-nong-dong-mo-qia-tuo-tou-ying/常见的坐标系和格式","categories":[],"tags":[]},{"title":"启动的时候复制jar包里面的文件出来，给jvm调用","slug":"copy-running-jarfile-inside","date":"2021-08-23T09:05:54.000Z","updated":"2024-03-14T14:55:29.476Z","comments":true,"path":"12016e7b.html","link":"","permalink":"https://imlike.cc/12016e7b.html","excerpt":"","text":"成功方式利用jarFile来实现 文件读取 参考链接 https://www.codenong.com/1463192/ 123456789101112131415String destDirPath &#x3D; getCurrentDirPath(); &#x2F;&#x2F; 读取jar文件 URL resource &#x3D; this.getClass().getResource(&quot;&#x2F;PSTools&quot;); JarURLConnection conn &#x3D; (JarURLConnection) resource.openConnection(); JarFile jarFile &#x3D; conn.getJarFile(); final Enumeration&lt;JarEntry&gt; entries &#x3D; jarFile.entries(); while (entries.hasMoreElements()) &#123; final JarEntry entry &#x3D; entries.nextElement(); if (entry.getName().contains(&quot;.exe&quot;)) &#123; System.out.println(&quot;File :&quot; + entry.getName()); JarEntry fileEntry &#x3D; jarFile.getJarEntry(entry.getName()); InputStream input &#x3D; jarFile.getInputStream(fileEntry); FileUtils.copyInputStreamToFile(input,new File(destDirPath,entry.getName())); &#125; &#125; 错误1主要是 希望列出jar内的文件路径来实现读取 jar包内指定路径下的所有文件并复制出来 12345File[] files = new File(currentDirPath).listFiles(); String path = this.getClass().getProtectionDomain().getCodeSource().getLocation().getPath(); for (File file : files) &#123; FileUtils.copyDirectory(file,new File(path)); &#125; 错误2这个是这参考这位博主的文章 https://yebukong.com/article/1102629952022507521.html 没有生效 读取读取不到 jar包内 classpath 的文件 12345&#x2F;* ResourcePatternResolver resolver &#x3D; new PathMatchingResourcePatternResolver(); Resource[] resources &#x3D; resolver.getResources(&quot;static&#x2F;blog&#x2F;**&quot;); for (Resource resource : resources) &#123; FreeMarkerUtil.copyResourceToFile(resource, aPath,&quot;&#x2F;static&#x2F;blog&#x2F;&quot;); &#125;*&#x2F; 错误3目前只能读到文件 1String path &#x3D; new File(getClass().getResource(&quot;&quot;).getPath()).getParentFile().getParent()); 错误4目前读取不到jar 下面的文件夹和具体文件 12URL[] urls &#x3D; &#123;this.getClass().getResource(&quot;&#x2F;PSTools&quot;) &#125;; URLClassLoader cl &#x3D; URLClassLoader.newInstance(urls);","categories":[],"tags":[]},{"title":"sso-auth2-jwt","slug":"sso-auth2-jwt","date":"2021-08-12T02:40:15.000Z","updated":"2024-03-14T14:55:29.481Z","comments":true,"path":"8ad614a9.html","link":"","permalink":"https://imlike.cc/8ad614a9.html","excerpt":"","text":"https://www.zoucz.com/blog/2020/01/06/83f05da0-307b-11ea-90b5-eb40e9720ed0/ https://www.cnblogs.com/cjsblog/p/10548022.html 项目的实例地址参考地址 关于单点登录(sso)的介绍单点登录是一处登录,处处皆可以访问,但是要注意登录只是一部分还应该考虑权限,通常是4A (account , audio, authtation,) 账号,审计,权限,授权 目前通常的协议是 实现逻辑 LDPA(协议),windowOA CAS JWT OAuth2 上述方法不是互斥是 介绍实现单点登录的技术类似的认证的协议或者实现包括 ldpa,windowsoa ,jwt(使用它来生成口令) 中心认证服务cas是利用认证服务器来专门做认证然后403来实现跳转 OAuth2 是利用应用注册+登录应用跳转+认证成功 通常大型互联网公司会提供自己的oauth的服务 类似于QQ,微博,GitHub,google 授权给第三方去登录 引入jar包来实现 每个服务中的接口来统一但是这有一个问题就是只限于java的服务 基于上述技术显示的方案和例子 cas实现的方案技术 https://www.apereo.org/projects/cas oauth的实现","categories":[],"tags":[]},{"title":"ue4_linux_runing","slug":"ue4-linux-runing","date":"2021-07-19T03:53:44.000Z","updated":"2024-03-14T14:55:29.482Z","comments":true,"path":"bec1def7.html","link":"","permalink":"https://imlike.cc/bec1def7.html","excerpt":"","text":"linux环境运行虚幻运行和二进制包后续补充 2021-12-15 不知道服务器谁重新安装了驱动,导致在nvidia-smi 显示的驱动和版本对不上 编译虚幻引擎参考链接 https://docs.unrealengine.com/4.26/zh-CN/SharingAndReleasing/Linux/BeginnerLinuxDeveloper/SettingUpAnUnrealWorkflow/ 拉取虚幻引擎的代码这里需要在epic账号上关联github账号就可以看到prive权限 https://github.com/EpicGames/UnrealEngine.git 然后拉取代码 这个过程需要持续40分钟 进入到目录中安装，编译，打包环境 1234567891011//设置生成项目文件所需的文件。./Setup.sh//从终端运行 GenerateProjectFiles.sh 以生成项目文件./GenerateProjectFiles.sh要构建项目，从终端运行 make。make //运行打包好的程序./Engine/Binaries/Linux/UE4Editor 运行后环境一直报错 这个时候需要的步骤的 网上很多都是用下面这个命令 sudo add-apt-repository ppa:graphics-drivers/ppa &amp;&amp; sudo apt update sudo apt install nvidia-graphics-drivers-396 nvidia-settings vulkan vulkan-utils hwinfo –gfxcard 查看显卡的信息 lspci | grep VGA 查询vga显示 参考链接1 参考链接2 ubuntu-drivers devices apt-get install nvidia-driver-460 但会有问题 第一个就是 nvidia-graphics-drivers-396这个驱动找不到 实际上是需要执行 需要等待nvidia-settings 不可点击 驱动安装成功后","categories":[],"tags":[]},{"title":"googlevoice保号，订阅广告短信和公共电话","slug":"google-voice-keep","date":"2021-07-13T03:21:17.000Z","updated":"2024-03-14T14:55:29.477Z","comments":true,"path":"18d2c52b.html","link":"","permalink":"https://imlike.cc/18d2c52b.html","excerpt":"","text":"转载 https://www.80tm.com/2020/04/05/google-voice%E4%BF%9D%E5%8F%B7%E6%96%B9%E6%B3%95/ 1、订阅促销短信。 发送join 到 527365 ，然后要回复CC先发 code 到 25666 然后回复 PROMO 也可以查询美国沃尔玛和好市多的促销短信订阅方式。 2.、打公共电话 有点儿公共资源。。。建议打个机器人试试就好了。 美国亚马逊免费客服热线：+1（888）280-4331 美国微软激活Windows免费电话：+1（888）571-2048 同时可查询其它美国大型公司的客服电话，注意IPhone是人工客服。 3、自定义电话 用IFTTT的定时服务定期拨打电话给你即可，这里推荐两个service Keep Google Voice Active 一个月拨打一次，可自定义时间和日期（搜索Keep Gooogle Voice Active） Alarm Clock Phone Call 强迫症福利，可自定义拨打频率、时间 4、30美金转成永久这里看https://www.googlevoice.cn/buy-permanent-use/","categories":[],"tags":[]},{"title":"常见的postgresql的函数","slug":"pgsql-json-function","date":"2021-07-08T07:41:36.000Z","updated":"2024-03-14T14:55:29.480Z","comments":true,"path":"fd4a70cc.html","link":"","permalink":"https://imlike.cc/fd4a70cc.html","excerpt":"","text":"http://www.postgres.cn/docs/10/functions-json.html 命令 12345678910111213141516171819202122232425262728293031323334353637383940// 通过 数据函数来操作 字符串属性SELECT jsonb_set(&#x27;&#123;&quot;rtime&quot;:&quot;2021-6-15 08:59:44&quot;,&quot;objid&quot;:&quot;001&quot;,&quot;addition&quot;:&#123;&quot;x&quot;:114.5379008,&quot;y&quot;:30.4835323,&quot;z&quot;:0&#125;&#125;&#x27;,&#x27;&#123;addition,z&#125;&#x27;,&#x27;12&#x27;) UPDATE snspdw_position SET content= (jsonb_set(content::jsonb,&#x27;&#123;directorytypecode&#125;&#x27;,&#x27;&quot;947998009457774691&quot;&#x27;::jsonb))//updata 字符串数据UPDATE biz_object set confjson = (jsonb_set(confjson::jsonb,&#x27;&#123;directorytypecode&#125;&#x27;,&#x27;&quot;947998009457774691&quot;&#x27;::jsonb)),directorytypecode = &#x27;947998009457774691&#x27; where modelcode = &#x27;829609839767532682&#x27; and objectcode = &#x27;948118654552248419&#x27;; // 根据 官网提供的实例测试SELECT jsonb_set(&#x27;&#123;&quot;a&quot;:[null,&#123;&quot;b&quot;:[1,2]&#125;]&#125;&#x27;, &#x27;&#123;a,1,b,1000&#125;&#x27;, jsonb &#x27;3&#x27;, true)select version()SELECT jsonb_set(&#x27;&#123;&quot;rtime&quot;:&quot;2021-6-15 08:59:44&quot;,&quot;objid&quot;:&quot;001&quot;,&quot;addition&quot;:&#123;&quot;x&quot;:114.5379008,&quot;y&quot;:30.4835323,&quot;z&quot;:0&#125;&#125;&#x27;, &#x27;$.payFundAcc.addition.z&#x27;, &#x27;3&#x27;) &#123;&quot;tel&quot;: 13249872314, &quot;name&quot;: &quot;Mike&quot;, &quot;address&quot;: &quot;Guangzhou&quot;&#125; json_set(data,&quot;&#123;address&#125;&quot;,&quot;Guangzhou&quot;) SELECT jsonb_set(&#x27;&#123;&quot;tel&quot;: 13249872314, &quot;name&quot;: &quot;Mike&quot;, &quot;address&quot;: &quot;Guangzhou&quot;&#125;&#x27;,&#x27;&#123;address&#125;&#x27;,&#x27;&quot;hahah&quot;&#x27;) SELECT jsonb_set(&#x27;&#123;&quot;rtime&quot;:&quot;2021-6-15 08:59:44&quot;,&quot;objid&quot;:&quot;001&quot;,&quot;addition&quot;:&#123;&quot;x&quot;:114.5379008,&quot;y&quot;:30.4835323,&quot;z&quot;:0&#125;&#125;&#x27;,&#x27;&#123;addition,z&#125;&#x27;,&#x27;12&#x27;) SELECT &#x27;&#123;&quot;tel&quot;: 13249872314, &quot;name&quot;: &quot;Mike&quot;, &quot;address&quot;: &quot;Guangzhou&quot;&#125;&#x27;::jsonb","categories":[],"tags":[]},{"title":"ue4安卓打包","slug":"ue4-pack-android","date":"2021-07-07T08:15:57.000Z","updated":"2024-03-14T14:55:29.482Z","comments":true,"path":"54f728db.html","link":"","permalink":"https://imlike.cc/54f728db.html","excerpt":"","text":"ue4安卓预制环境硬件要求 支持的最低GPU自4.25版起，以下GPU满足ES3.1的上述要求： Adreno 4xx Mali T6xx和更高版本 Mali G71和更高版本 PowerVR Rogue G6100 下载安装安装开发SDK https://developer.nvidia.com/codeworks-android-archive 选择 CodeWorks-1R7-windows_b166.exe 版本来实现 创建好的服务 配置安卓服务 设置层级为安卓 安装安卓sdk ERROR: Android toolchain NDK r15c not supported; please use NDK r21 to NDK r23 (NDK r21b recommended) UATHelper: Packaging (Android (ETC2)): &gt; Task :app:compileDebugJavaWithJavac FAILEDUATHelper: Packaging (Android (ETC2)): The following annotation processors are not incremental: compiler-1.1.1.jar (android.arch.lifecycle:compiler:1.1.1).UATHelper: Packaging (Android (ETC2)): Make sure all annotation processors are incremental to improve your build speed.UATHelper: Packaging (Android (ETC2)): Z:\\app\\src\\main\\java\\com\\epicgames\\ue4\\GameActivity.java:3531: ����: �Ҳ�������UATHelper: Packaging (Android (ETC2)): powerManager.addThermalStatusListener(getMainExecutor(), new PowerManager.OnThermalStatusChangedListener() {UATHelper: Packaging (Android (ETC2)): 32 actionable tasks: 3 executed, 29 up-to-dateUATHelper: Packaging (Android (ETC2)): ^UATHelper: Packaging (Android (ETC2)): ����: �� OnThermalStatusChangedListenerUATHelper: Packaging (Android (ETC2)): λ��: �� PowerManagerUATHelper: Packaging (Android (ETC2)): ע: ĳЩ�����ļ�ʹ�û򸲸����ѹ�ʱ�� API��UATHelper: Packaging (Android (ETC2)): ע: �й���ϸ��Ϣ, ��ʹ�� -Xlint:deprecation ���±��롣UATHelper: Packaging (Android (ETC2)): 1 ������UATHelper: Packaging (Android (ETC2)): FAILURE: Build failed with an exception.UATHelper: Packaging (Android (ETC2)): * What went wrong:UATHelper: Packaging (Android (ETC2)): Execution failed for task ‘:app:compileDebugJavaWithJavac’.UATHelper: Packaging (Android (ETC2)): &gt; Compilation failed; see the compiler error output for details.UATHelper: Packaging (Android (ETC2)): * Try:UATHelper: Packaging (Android (ETC2)): Run with –stacktrace option to get the stack trace. Run with –info or –debug option to get more log output. Run with –scan to get full insights.UATHelper: Packaging (Android (ETC2)): * Get more help at https://help.gradle.orgUATHelper: Packaging (Android (ETC2)): BUILD FAILED in 8s 需要安装Android_studio同时安装sdk","categories":[],"tags":[]},{"title":"sublime_shortcut","slug":"sublime-shortcut","date":"2021-07-04T01:24:36.000Z","updated":"2024-03-14T14:55:29.481Z","comments":true,"path":"29702c78.html","link":"","permalink":"https://imlike.cc/29702c78.html","excerpt":"","text":"link: https://blog.csdn.net/lol_IP/article/details/82416020第一步：选中全部内容 ctrl+A第二步：进入待操作状态 ctrl+shift+L第三步：通过←和→控制光标的位置 第四步：在光标处添加内容 注：也可以只对多行进行操作，对多行进行操作只需在第一步选择相应的行就可以了","categories":[],"tags":[]},{"title":"python下载离线的网页以 gitbook上的某个页面","slug":"python-download-offline-html","date":"2021-06-23T14:29:34.000Z","updated":"2024-03-14T14:55:29.480Z","comments":true,"path":"1a58ae58.html","link":"","permalink":"https://imlike.cc/1a58ae58.html","excerpt":"","text":"先上代码 本代码有参考 https://blog.csdn.net/gorquanwu/article/details/81739589 这篇文章去实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134# author: leek# date：2021-6-23from urllib import requestfrom bs4 import BeautifulSoup as bsimport timeimport osimport re&#x27;&#x27;&#x27; 用来爬取网站网页 gitbook 页面 离线下载到本地 实现功能：url深度抓取，保存每个页面的css、html、js等文件&#x27;&#x27;&#x27;# 深度爬取当前页面子网站子网站def get_urls(url, baseurl, urls): with request.urlopen(url) as f: data = f.read().decode(&#x27;utf-8&#x27;) # link = bs(data).find_all(&#x27;a&#x27;) link = bs(data).find(&quot;nav&quot;).find_all(&#x27;a&#x27;) for i in link: suffix = i.get(&#x27;href&#x27;) # 设置排除写入的子连接 if suffix == &#x27;#&#x27; or suffix == &#x27;#carousel-example-generic&#x27; or &#x27;javascript:void(0)&#x27; in suffix: continue else: # 构建urls childurl = baseurl +&quot;/&quot;+ suffix if childurl not in urls: urls.append(childurl)# 获取每个页面代码以及获取页面上的css，js，img路径def get_source(url, path): try: with request.urlopen(url) as f: html_source = f.read().decode() # 添加时间截以区分文件夹名字 timeStr = str(int(time.time())) pattertitile = &#x27;&lt;title&gt;(.*?)&lt;/title&gt;&#x27; patternimg = &#x27;&lt;img src=&quot;(.*?)&quot;&#x27; titleStr = re.compile(pattertitile, re.S).findall(html_source)[0] if &#x27;|&#x27; in titleStr: title = (titleStr.split(&quot;|&quot;)[1]).split(&#x27; &#x27;)[1] + timeStr else: title = titleStr + timeStr path11 = path + &#x27;/&#x27; + title arrayurl= url.split(&#x27;/&#x27;) htmlFile = path for x in range(3, len(arrayurl)): htmlFile +=(&quot;/&quot;+arrayurl[x]) # 获取css，js，img地址 imgHref = re.compile(patternimg, re.S).findall(html_source) # 创建文件路径下的父节点 os.makedirs(os.path.abspath(htmlFile + os.path.sep + &quot;..&quot;), exist_ok=True) # 读取并保存html with open(htmlFile, &#x27;w&#x27;, encoding=&#x27;UTF-8&#x27;) as f: f.write(html_source) print(htmlFile+ &quot;文件保存成功&quot;) time.sleep(1) except: print(url + &quot;保存html文件时报错&quot;)# 保存js文件def save_css_js(path): url = &quot;http://sdk.g-bim.cn&quot; filename = path with request.urlopen(url) as total_html: html_source = total_html.read().decode() jsHref = re.compile(&#x27;&lt;script src=&quot;(.*?)&quot;&#x27;, re.S).findall(html_source) cssHref = re.compile( &#x27;&lt;link rel=&quot;stylesheet&quot; href=&quot;(.*?)&quot;&#x27;, re.S).findall(html_source) for j in jsHref : try: with request.urlopen(url+&quot;/&quot;+j) as ww: js_source = ww.read().decode() # filename =(path+&#x27;\\\\&#x27;+ j).replace(&#x27;/&#x27;,&#x27;\\\\&#x27;) os.getcwd() filename =path+j os.makedirs(os.path.abspath(filename+os.path.sep+&quot;..&quot;), exist_ok=True) with open(filename, &#x27;w&#x27;, encoding=&#x27;UTF-8&#x27;) as f: f.write(js_source) print(j.split(&#x27;/&#x27;)[-1] + &quot; js文件保存成功&quot;) time.sleep(1) except: print(&quot;该&quot; + j.split(&#x27;/&#x27;)[-1] + &quot; js文件无法下载&quot;) for k in cssHref: try: with request.urlopen(url+&quot;/&quot;+k) as vv: filename = path+ k js_source = vv.read().decode() os.makedirs(os.path.abspath(filename + os.path.sep + &quot;..&quot;), exist_ok=True) with open(filename, &#x27;w&#x27;, encoding=&#x27;UTF-8&#x27;) as f: f.write(js_source) print(k.split(&#x27;/&#x27;)[-1] + &quot; js文件保存成功&quot;) time.sleep(1) except: print(&quot;该&quot; + k.split(&#x27;/&#x27;)[-1] + &quot; js文件无法下载&quot;)# 保存img文件def save_img(href, path): for i in range(0, len(href)): url = &quot;http://sdk.g-bim.cn&quot; + href[i] filename = path + &#x27;\\\\&#x27; + href[i].split(&#x27;/&#x27;)[-1] try: with request.urlopen(url) as w: img_source = w.read() with open(filename, &#x27;wb&#x27;) as f: f.write(img_source) print(href[i].split(&#x27;/&#x27;)[-1] + &quot; 图像文件保存成功&quot;) time.sleep(1) except: print(&quot;该&quot; + href[i].split(&#x27;/&#x27;)[-1] + &quot; 图像无法下载&quot;) continueif __name__ == &#x27;__main__&#x27;: # 抓取网址 url = &#x27;http://sdk.g-bim.cn&#x27; # 相对路径地址 baseurl = &#x27;http://sdk.g-bim.cn&#x27; # 文件保存位置 basedir = r&#x27;C:\\Users\\Administrator\\Desktop\\HTML_bak&#x27; urls = [] # 获取所有地址 get_urls(url, baseurl, urls) # save_css_js(r&#x27;../html_bak/&#x27;) # 获取代码 for u in urls: get_source(u,r&#x27;../html_bak&#x27;) 期间有些问题 读取文件是出现UnicodeDecodeErrorUnicodeDecodeError: ‘gbk’ codec can’t decode byte 0x89 in position 14: illegal open(‘order.log’,’r’, encoding=’UTF-8’) 尽量指定u8目前HTML的编码一般都是它避免乱码 在python环境下window和Linux分隔符的区别123456path&#x3D;r&#39;..&#x2F;html_bak&#x2F;&#39; with open(path, &#39;w&#39;, encoding&#x3D;&#39;UTF-8&#39;) as f: \\\\windows平台但是 加上r后 自动加 不用管全部用&#x2F;即可 在打开文件时会自动管理 https:&#x2F;&#x2F;blog.csdn.net&#x2F;qq_29831163&#x2F;article&#x2F;details&#x2F;106263729 读取指定文件路径下父文件夹如果不存在着直接新建1os.makedirs(os.path.abspath(filename+os.path.sep+&quot;..&quot;), exist_ok&#x3D;True)","categories":[],"tags":[]},{"title":"geomesa-input-error","slug":"geomesa-input-error","date":"2021-06-22T07:44:29.000Z","updated":"2024-03-14T14:55:29.477Z","comments":true,"path":"null.html","link":"","permalink":"https://imlike.cc/null.html","excerpt":"","text":"导入失败 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172ERROR java.lang.NoSuchMethodError: org.locationtech.jts.geom.Polygon.getExteriorRing()Lorg&#x2F;locationtech&#x2F;jts&#x2F;geom&#x2F;LineString;java.util.concurrent.ExecutionException: java.lang.NoSuchMethodError: org.locationtech.jts.geom.Polygon.getExteriorRing()Lorg&#x2F;locationtech&#x2F;jts&#x2F;geom&#x2F;LineString; at java.util.concurrent.FutureTask.report(FutureTask.java:122) at java.util.concurrent.FutureTask.get(FutureTask.java:192) at org.locationtech.geomesa.tools.ingest.LocalConverterIngest$$anonfun$runIngest$1.apply(LocalConverterIngest.scala:183) at org.locationtech.geomesa.tools.ingest.LocalConverterIngest$$anonfun$runIngest$1.apply(LocalConverterIngest.scala:183) at scala.collection.immutable.List.foreach(List.scala:381) at org.locationtech.geomesa.tools.ingest.LocalConverterIngest.runIngest(LocalConverterIngest.scala:183) at org.locationtech.geomesa.tools.ingest.AbstractConverterIngest.run(AbstractConverterIngest.scala:41) at org.locationtech.geomesa.tools.ingest.IngestCommand$$anonfun$execute$2.apply(IngestCommand.scala:106) at org.locationtech.geomesa.tools.ingest.IngestCommand$$anonfun$execute$2.apply(IngestCommand.scala:105) at scala.Option.foreach(Option.scala:257) at org.locationtech.geomesa.tools.ingest.IngestCommand$class.execute(IngestCommand.scala:105) at org.locationtech.geomesa.hbase.tools.HBaseRunner$$anon$2.execute(HBaseRunner.scala:32) at org.locationtech.geomesa.tools.Runner$class.main(Runner.scala:30) at org.locationtech.geomesa.hbase.tools.HBaseRunner$.main(HBaseRunner.scala:17) at org.locationtech.geomesa.hbase.tools.HBaseRunner.main(HBaseRunner.scala)Caused by: java.lang.NoSuchMethodError: org.locationtech.jts.geom.Polygon.getExteriorRing()Lorg&#x2F;locationtech&#x2F;jts&#x2F;geom&#x2F;LineString; at org.locationtech.geomesa.features.serialization.WkbSerialization$class.writePolygon(WkbSerialization.scala:121) at org.locationtech.geomesa.features.serialization.WkbSerialization$class.serializeWkb(WkbSerialization.scala:45) at org.locationtech.geomesa.features.kryo.serialization.KryoGeometrySerialization$.serializeWkb(KryoGeometrySerialization.scala:14) at org.locationtech.geomesa.features.serialization.WkbSerialization$class.writeGeometryCollection(WkbSerialization.scala:176) at org.locationtech.geomesa.features.serialization.WkbSerialization$class.serializeWkb(WkbSerialization.scala:48) at org.locationtech.geomesa.features.kryo.serialization.KryoGeometrySerialization$.serializeWkb(KryoGeometrySerialization.scala:14) at org.locationtech.geomesa.features.kryo.impl.KryoFeatureSerialization$KryoGeometryWkbWriter$.apply(KryoFeatureSerialization.scala:228) at org.locationtech.geomesa.features.kryo.impl.KryoFeatureSerialization$class.writeFeature(KryoFeatureSerialization.scala:70) at org.locationtech.geomesa.features.kryo.impl.KryoFeatureSerialization$class.serialize(KryoFeatureSerialization.scala:42) at org.locationtech.geomesa.features.kryo.KryoFeatureSerializer$MutableActiveSerializer.serialize(KryoFeatureSerializer.scala:75) at org.locationtech.geomesa.index.api.WritableFeature$FeatureLevelWritableFeature$$anonfun$values$1$$anonfun$apply$1.apply(WritableFeature.scala:154) at org.locationtech.geomesa.index.api.WritableFeature$FeatureLevelWritableFeature$$anonfun$values$1$$anonfun$apply$1.apply(WritableFeature.scala:154) at org.locationtech.geomesa.index.api.package$KeyValue.value$lzycompute(package.scala:184) at org.locationtech.geomesa.index.api.package$KeyValue.value(package.scala:184) at org.locationtech.geomesa.hbase.data.HBaseIndexAdapter$HBaseIndexWriter$$anonfun$write$1.apply(HBaseIndexAdapter.scala:614) at org.locationtech.geomesa.hbase.data.HBaseIndexAdapter$HBaseIndexWriter$$anonfun$write$1.apply(HBaseIndexAdapter.scala:612) at scala.collection.Iterator$class.foreach(Iterator.scala:742) at scala.collection.AbstractIterator.foreach(Iterator.scala:1194) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at org.locationtech.geomesa.hbase.data.HBaseIndexAdapter$HBaseIndexWriter.write(HBaseIndexAdapter.scala:612) at org.locationtech.geomesa.index.api.IndexAdapter$BaseIndexWriter.write(IndexAdapter.scala:149) at org.locationtech.geomesa.index.geotools.GeoMesaFeatureWriter$class.writeFeature(GeoMesaFeatureWriter.scala:52) at org.locationtech.geomesa.index.geotools.GeoMesaFeatureWriter$TableFeatureWriter.writeFeature(GeoMesaFeatureWriter.scala:141) at org.locationtech.geomesa.index.geotools.GeoMesaFeatureWriter$GeoMesaAppendFeatureWriter$class.write(GeoMesaFeatureWriter.scala:227) at org.locationtech.geomesa.index.geotools.GeoMesaFeatureWriter$$anon$3.write(GeoMesaFeatureWriter.scala:108) at org.locationtech.geomesa.utils.geotools.FeatureUtils$.write(FeatureUtils.scala:141) at org.locationtech.geomesa.tools.ingest.LocalConverterIngest$LocalIngestWorker$1$$anonfun$run$1$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$4$$anonfun$apply$5$$anonfun$apply$6.apply(LocalConverterIngest.scala:117) at org.locationtech.geomesa.tools.ingest.LocalConverterIngest$LocalIngestWorker$1$$anonfun$run$1$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$4$$anonfun$apply$5$$anonfun$apply$6.apply(LocalConverterIngest.scala:115) at scala.collection.Iterator$class.foreach(Iterator.scala:742) at org.locationtech.geomesa.utils.collection.CloseableIterator$FlatMapCloseableIterator.foreach(CloseableIterator.scala:133) at org.locationtech.geomesa.tools.ingest.LocalConverterIngest$LocalIngestWorker$1$$anonfun$run$1$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$4$$anonfun$apply$5.apply(LocalConverterIngest.scala:115) at org.locationtech.geomesa.tools.ingest.LocalConverterIngest$LocalIngestWorker$1$$anonfun$run$1$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$4$$anonfun$apply$5.apply(LocalConverterIngest.scala:109) at org.locationtech.geomesa.utils.io.CloseablePool$CommonsPoolPool.borrow(CloseablePool.scala:68) at org.locationtech.geomesa.tools.ingest.LocalConverterIngest$LocalIngestWorker$1$$anonfun$run$1$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$4.apply(LocalConverterIngest.scala:109) at org.locationtech.geomesa.tools.ingest.LocalConverterIngest$LocalIngestWorker$1$$anonfun$run$1$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$4.apply(LocalConverterIngest.scala:108) at org.locationtech.geomesa.utils.io.package$WithClose$.apply(package.scala:64) at org.locationtech.geomesa.tools.ingest.LocalConverterIngest$LocalIngestWorker$1$$anonfun$run$1$$anonfun$apply$1$$anonfun$apply$2.apply(LocalConverterIngest.scala:108) at org.locationtech.geomesa.tools.ingest.LocalConverterIngest$LocalIngestWorker$1$$anonfun$run$1$$anonfun$apply$1$$anonfun$apply$2.apply(LocalConverterIngest.scala:106) at scala.collection.Iterator$class.foreach(Iterator.scala:742) at org.locationtech.geomesa.utils.collection.CloseableIterator$CloseableSingleIterator.foreach(CloseableIterator.scala:86) at org.locationtech.geomesa.tools.ingest.LocalConverterIngest$LocalIngestWorker$1$$anonfun$run$1$$anonfun$apply$1.apply(LocalConverterIngest.scala:106) at org.locationtech.geomesa.tools.ingest.LocalConverterIngest$LocalIngestWorker$1$$anonfun$run$1$$anonfun$apply$1.apply(LocalConverterIngest.scala:105) at org.locationtech.geomesa.utils.io.package$WithClose$.apply(package.scala:64) at org.locationtech.geomesa.tools.ingest.LocalConverterIngest$LocalIngestWorker$1$$anonfun$run$1.apply(LocalConverterIngest.scala:105) at org.locationtech.geomesa.tools.ingest.LocalConverterIngest$LocalIngestWorker$1$$anonfun$run$1.apply(LocalConverterIngest.scala:102) at org.locationtech.geomesa.utils.io.CloseablePool$CommonsPoolPool.borrow(CloseablePool.scala:68) at org.locationtech.geomesa.tools.ingest.LocalConverterIngest$LocalIngestWorker$1.run(LocalConverterIngest.scala:102) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) 这个是由于 lib下面的几个包出现问题包括 jts ,geotool的包有问题 这边操作 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556[root@node-kubeadm-250 geomesa-hbase_2.11-3.1.0]# bin&#x2F;geomesa-hbase ingest --catalog china --feature-name china_1 --input-format shp &quot;&#x2F;usr&#x2F;local&#x2F;geomesa&#x2F;china.shp&quot;INFO No converter defined - will attempt to detect schema from input filesINFO Inferred converter:&#123; &quot;fields&quot; : [ &#123; &quot;name&quot; : &quot;the_geom&quot;, &quot;transform&quot; : &quot;$1&quot; &#125;, &#123; &quot;name&quot; : &quot;adcode&quot;, &quot;transform&quot; : &quot;$2&quot; &#125;, &#123; &quot;name&quot; : &quot;name&quot;, &quot;transform&quot; : &quot;$3&quot; &#125;, &#123; &quot;name&quot; : &quot;childrenNu&quot;, &quot;transform&quot; : &quot;$4&quot; &#125;, &#123; &quot;name&quot; : &quot;level&quot;, &quot;transform&quot; : &quot;$5&quot; &#125;, &#123; &quot;name&quot; : &quot;subFeature&quot;, &quot;transform&quot; : &quot;$6&quot; &#125;, &#123; &quot;name&quot; : &quot;adchar&quot;, &quot;transform&quot; : &quot;$7&quot; &#125; ], &quot;id-field&quot; : &quot;$0&quot;, &quot;options&quot; : &#123; &quot;encoding&quot; : &quot;UTF-8&quot;, &quot;error-mode&quot; : &quot;skip-bad-records&quot;, &quot;parse-mode&quot; : &quot;incremental&quot;, &quot;validators&quot; : [ &quot;index&quot; ] &#125;, &quot;type&quot; : &quot;shp&quot;&#125;Use inferred converter (y&#x2F;n)? yPersist this converter for future use (y&#x2F;n)? yINFO Added import in reference.conf and saved inferred converter to &#x2F;usr&#x2F;local&#x2F;geomesa&#x2F;geomesa-hbase_2.11-3.1.0&#x2F;conf&#x2F;china_1_4.confINFO In future commands, the converter may be invoked with &#39;--converter china_1&#39;INFO Schema &#39;china_1&#39; existsINFO Running ingestion in local modeINFO Ingesting 1 file with 1 thread[&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] 100% complete 35 ingested 0 failed in 00:00:01INFO Local ingestion complete in 00:00:02INFO Ingested 35 features with no failures for file: &#x2F;usr&#x2F;local&#x2F;geomesa&#x2F;china.shp 解决方法 在geotool中依赖jts这个类但是 gt-main-23.3.jar 里面使用的是jts-17.jar gt-main-23.0.jar 里面使用的是jts-16.jar 所以需要降级 使用gt-main-23.0.jar 否则会报错","categories":[],"tags":[]},{"title":"netty的简单上手使用","slug":"netty-use","date":"2021-06-14T03:16:40.000Z","updated":"2024-03-14T14:55:29.480Z","comments":true,"path":"6e6bd3bb.html","link":"","permalink":"https://imlike.cc/6e6bd3bb.html","excerpt":"","text":"使用netty来简单写个demo，熟悉语法和常见的命令 123456&lt;dependency&gt; &lt;groupId&gt;io.netty&lt;&#x2F;groupId&gt; &lt;artifactId&gt;netty-all&lt;&#x2F;artifactId&gt; &lt;version&gt;4.1.49.Final&lt;&#x2F;version&gt;&lt;&#x2F;dependency&gt; client: 12345678910111213141516171819202122232425262728293031323334353637383940public class ITDragonAIOClient implements Runnable&#123; private static Integer PORT = 8888; private static String IP_ADDRESS = &quot;127.0.0.1&quot;; private AsynchronousSocketChannel asynSocketChannel ; public ITDragonAIOClient() throws Exception &#123; asynSocketChannel = AsynchronousSocketChannel.open(); // 打开通道 &#125; public void connect()&#123; asynSocketChannel.connect(new InetSocketAddress(IP_ADDRESS, PORT)); // 创建连接 和NIO一样 &#125; public void write(String request)&#123; try &#123; asynSocketChannel.write(ByteBuffer.wrap(request.getBytes())).get(); ByteBuffer byteBuffer = ByteBuffer.allocate(1024); asynSocketChannel.read(byteBuffer).get(); byteBuffer.flip(); byte[] respByte = new byte[byteBuffer.remaining()]; byteBuffer.get(respByte); // 将缓冲区的数据放入到 byte数组中 System.out.println(new String(respByte,&quot;utf-8&quot;).trim()); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; @Override public void run() &#123; while(true)&#123; &#125; &#125; public static void main(String[] args) throws Exception &#123; for (int i = 0; i &lt; 10; i++) &#123; ITDragonAIOClient myClient = new ITDragonAIOClient(); myClient.connect(); new Thread(myClient, &quot;myClient&quot;).start(); String []operators = &#123;&quot;+&quot;,&quot;-&quot;,&quot;*&quot;,&quot;/&quot;&#125;; Random random = new Random(System.currentTimeMillis()); String expression = random.nextInt(10)+operators[random.nextInt(4)]+(random.nextInt(10)+1); myClient.write(expression); &#125; &#125; &#125; server: 123456789101112131415161718192021222324252627282930313233/** * AIO, 也叫 NIO2.0 是一种异步非阻塞的通信方式 * AIO 引入了异步通道的概念 AsynchronousServerSocketChannel和AsynchronousSocketChannel 其read和write方法返回值类型是Future对象。 */public class ITDragonAIOServer &#123; private ExecutorService executorService; // 线程池 private AsynchronousChannelGroup threadGroup; // 通道组 public AsynchronousServerSocketChannel asynServerSocketChannel; // 服务器通道 public void start(Integer port)&#123; try &#123; // 1.创建一个缓存池 executorService = Executors.newCachedThreadPool(); // 2.创建通道组 threadGroup = AsynchronousChannelGroup.withCachedThreadPool(executorService, 1); // 3.创建服务器通道 asynServerSocketChannel = AsynchronousServerSocketChannel.open(threadGroup); // 4.进行绑定 asynServerSocketChannel.bind(new InetSocketAddress(port)); System.out.println(&quot;server start , port : &quot; + port); // 5.等待客户端请求 asynServerSocketChannel.accept(this, new ITDragonAIOServerHandler()); // 一直阻塞 不让服务器停止，真实环境是在tomcat下运行，所以不需要这行代码 Thread.sleep(Integer.MAX_VALUE); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; public static void main(String[] args) &#123; ITDragonAIOServer server = new ITDragonAIOServer(); server.start(8888); &#125; &#125; serverhandler: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class ITDragonAIOServerHandler implements CompletionHandler&lt;AsynchronousSocketChannel, ITDragonAIOServer&gt; &#123; private final Integer BUFFER_SIZE = 1024; @Override public void completed(AsynchronousSocketChannel asynSocketChannel, ITDragonAIOServer attachment) &#123; // 保证多个客户端都可以阻塞 attachment.asynServerSocketChannel.accept(attachment, this); read(asynSocketChannel); &#125; //读取数据 private void read(final AsynchronousSocketChannel asynSocketChannel) &#123; ByteBuffer byteBuffer = ByteBuffer.allocate(BUFFER_SIZE); asynSocketChannel.read(byteBuffer, byteBuffer, new CompletionHandler&lt;Integer, ByteBuffer&gt;() &#123; @Override public void completed(Integer resultSize, ByteBuffer attachment) &#123; //进行读取之后,重置标识位 attachment.flip(); //获取读取的数据 String resultData = new String(attachment.array()).trim(); System.out.println(&quot;Server -&gt; &quot; + &quot;收到客户端的数据信息为:&quot; + resultData); // String response = resultData + &quot; = &quot; + CalculatorUtil.cal(resultData); String response = resultData + &quot; = &quot; ; write(asynSocketChannel, response); &#125; @Override public void failed(Throwable exc, ByteBuffer attachment) &#123; exc.printStackTrace(); &#125; &#125;); &#125; // 写入数据 private void write(AsynchronousSocketChannel asynSocketChannel, String response) &#123; try &#123; // 把数据写入到缓冲区中 ByteBuffer buf = ByteBuffer.allocate(BUFFER_SIZE); buf.put(response.getBytes()); buf.flip(); // 在从缓冲区写入到通道中 asynSocketChannel.write(buf).get(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (ExecutionException e) &#123; e.printStackTrace(); &#125; &#125; @Override public void failed(Throwable exc, ITDragonAIOServer attachment) &#123; exc.printStackTrace(); &#125; &#125;","categories":[],"tags":[]},{"title":"使用青龙并添加jd的脚本","slug":"jd-script-use","date":"2021-06-11T01:17:51.000Z","updated":"2024-03-14T14:55:29.478Z","comments":true,"path":"8cefd92d.html","link":"","permalink":"https://imlike.cc/8cefd92d.html","excerpt":"","text":"找到一个jd_script 主要是京东的脚本可以操作，在加入 fetchMetadata: sill mapToRegistry uri https://registry.npm.taobao.org/got JS_TOOL执行 jd panelon后执行如果报错 缺 /root/jd/config 那就新建 /root/jd/config 文件夹 然后将 /root/JS_TOOL/sample中的文件复制到/root/jd/config 最后来就可以新建 tips: 期间 npm install 会出现 fetchMetadata: verb afterAdd /root/.npm/ee-first/1.1.1/package/package.json 这个是下载的过程比较耗时可能10分钟。 预先要求的条件 docker的环境 npm config set registry http://registry.npmjs.org/ --global npm cache clear --force 1bash jd.sh panelon","categories":[],"tags":[]},{"title":"hadoop_windows","slug":"hadoop-windows","date":"2021-05-26T06:53:02.000Z","updated":"2024-03-14T14:55:29.477Z","comments":true,"path":"77e0f92d.html","link":"","permalink":"https://imlike.cc/77e0f92d.html","excerpt":"","text":"Unable to find native drivers in HADOOP_HOME. Please, refer to Hadoop Wiki for more details. link 配置环境变量 包括java和hadoop的注意一点尽量不要有空格否则在java -versionhadoop version 的时候回报错 java_home incorrect 我这边没看到10 的版本就先使用了最近的","categories":[],"tags":[]},{"title":"idea2020.1.3","slug":"idea2020-1-3","date":"2021-04-15T13:36:38.000Z","updated":"2024-03-14T14:55:29.478Z","comments":true,"path":"47eff7d.html","link":"","permalink":"https://imlike.cc/47eff7d.html","excerpt":"","text":"最新版的idea激活已经变了该为到期后接着试用,这个方法对于我这种激活到底的人非常不友好还是提供下载的链接 介绍适用于idea2020.2.1.3及以下本部分参考这篇文章 https://www.jiweichengzhu.com/article/7f4f3c9d095743908db6ec2648a59b3e 链接: https://pan.baidu.com/s/1nC060_Rv-_7tzRc1Be-dmg 提取码: htpi jetbrains-agent这个要注意版本我用的最近的 ChangeLogs.txt是 v3.2.1 build date: 2020-04-11 * 什么都没做。 v3.2.0 build date: 2020-04-10 修复激活弹Activation code界面问题。 现在离线模式已经不需要了。 lcoalidea初始化后保存的文件,如果要重置的话直接删除这个文件即可 roaming 下面有配置文件可以注意以下 还有就是要注意jetbrains-agent.jar 版本一定要对应上 否则会出现 key is invalid,是同样的key 大概就是这样 ide-eval-resetter-2.1.6来激活下面只提供下载链接不提供具体步骤 链接: https://pan.baidu.com/s/1VWogd64sbzyagSx_vvKYyg 提取码: zjga 复制这段内容后打开百度网盘手机App，操作更方便哦","categories":[],"tags":[]},{"title":"Linux常用命令参考手册","slug":"Linux常用命令参考手册-转载","date":"2021-03-25T14:06:10.000Z","updated":"2024-03-14T14:55:29.474Z","comments":true,"path":"c66a3331.html","link":"","permalink":"https://imlike.cc/c66a3331.html","excerpt":"","text":"转载地址 : https://github.com/xjh22222228/linux-manual/stargazers Linux 常用命令参考手册 日常运维的最佳拍档 x 99 目录 文件操作 head tail ls pwd wc find mkdir mktemp chattr more paste stat grep touch cd rm rmdir cp cat mv locate open source tree ln file 系统管理 top nohup watch ping which shutdown reboot ps uptime crontab uname ifconfig whereis kill killall chmod lsof netstat chown systemctl service free jobs type printenv set export unset alias time clear 用户管理 useradd userdel passwd chpasswd chsh users who w last su whoami 压缩、解压 zip unzip gzip bzip2 tar 加解密 md5sum base64 网络 wget curl scp 磁盘 df du 包管理 yum apt-get 其他 目录名称含义 重定向输入和输出 管道 echo date man sleep history xargs cal expr bc timeout exit vim basename read tee 目录名称含义 目录名 描述 / 虚拟目录的根目录，通常不会在这里存储文件 /bin 二进制目录，存放许多用户级的GNU工具 /boot 启动目录，存放启动文件 /dev 设备目录，Linux在这里创建设备节点 /etc 系统配置文件目录 /home 主目录，Linux在这里创建用户目录 /lib 库目录，存放系统和应用程序的库文件 /media 媒体目录，可移动媒体设备的常用挂载点 /mnt 挂载目录，另一个可移动媒体设备的常用挂载点 /opt 可选目录，常用于存放第三方软件包和数据文件 /proc 进程目录，存放现有硬件及当期进程的相关信息 /root ROOT用户的主目录 /sbin 系统二进制目录，存放许多GNU管理员级工具 /run 运行目录，存放系统运作时的运行时数据 /srv 服务目录，存放本地服务的相关文件 /sys 系统目录，存放系统硬件信息的相关文件 /tmp 临时目录，可以在该目录中创建和删除临时工作文件 /usr 用户二进制目录，大量用户级的GNU工具和数据文件都存储在这里 /var 可变目录，用以存放经常变化的文件，比如日志文件 重定向输入和输出输出重定向将命令输出的内容发送到一个文件中叫做 输出重定向。 使用 &gt; 大于号。 下面展示了几个例子 12345678# 1、echo &quot;Hello World&quot; &gt; log.txt# 2、ps -ef &gt; ps.txt# 3、history &gt; a.txt 有时不想覆盖文件而是追加内容，比如日志，可以使用 &gt;&gt; 2个大于号。 1echo &quot;H&quot; &gt;&gt; log.txt 输入重定向和输出重定向正好相反，将文件的内容定向到命令。 12# 统计input.txt文本行数wc -l &lt; input.txt # 等价于 wc -l input.txt 还有一种叫内联重定向，比较少见，但也挺有用。 使用2个 &lt;&lt; 小于号。然后跟着一个开头标记和结尾标记。 12345# 统计行数，输出2wc -l &lt;&lt; EOF第一行第二行EOF 开头标记必须和结尾标记一致，标记名称可以是任何字符串。 下面这个也是可以的。 12345# 输出2wc -l &lt;&lt; Hello第一行第二行Hello 管道将一个命令的输出作为另一个命令的输入称为管道。 管道用 | 符号。 12345# 将 ls 输出内容作为 wc 输入ls | wc# 执行一个脚本，这没有什么意义，只是一个例子echo &quot;./bash.sh&quot; | bash head显示文件的头部内容，如果不指定参数默认显示10行 12345678# 显示前10行内容head README.md# 或者显示多个文件head README.md package.json# -n 指定显示行数head -n 100 README.md tail显示文件的末尾部分 1234567891011121314# 默认显示末尾10行tail README.md# -n 指定显示末尾20行tail -n 20 README.md# 实时监听README.md文件变化tail -f README.md# 根据文件名进行追踪, 如果删除后创建相同的文件名会继续追踪tail -F README.md# 显示文件的最后10个字符tail -c README.md top实时查看系统执行中的程序, top 命令跟 ps 命令相似，但它是实时的。 默认情况下 top 命令启动时会按照 %CPU 值对进程排序。 名称 描述 PID 进程的ID USER 进程的优先级 PR 进程的优先级 NI 进程的谦让度值 VIRT 进程占用的虚拟内存总量 RES 进程占用的物理内存总量 SHR 进程和其他进程共享的内存总量 S 进程的状态（D=可中断的休眠状态，R在运行状态，S休眠状态，T跟踪状态或停止状态，Z=僵化状态） %CPU 进程使用的CPU时间比例 %MEM 进程使用的内存占可用内存的比例 TIME+ 自进程启动到目前为止的CPU时间总量 COMMAND 进程所对应的命令行名称，也就是启动的程序名 12345678910111213# 实时监听进程变化# PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND # 1 root 20 0 125124 3612 2428 S 0.0 0.2 0:04.88 systemdtop# 显示2条top -n 2# 显示指定的进程信息top -pid &lt;pid&gt;# 查看进程所有的线程top -H -p &lt;pid&gt; ls显示当前目录下的文件和目录，输出的列表是按字母排序 (某些发行版可能不一样)。 1234567891011121314151617181920212223# 显示当前目录列表ls# 显示目录列表的详细信息ls -l# 显示指定目录ls ./src# 显示目录列表详细信息和大小ls -lh# 列出所有文件包括隐藏文件ls -a# -F 可以显示类型，用以区分是文件还是目录ls -F # 后缀为 ”/“ 代表是目录，”*“ 为可执行文件，没有则为文件# 过滤文件列表, * 代表0个或多个字符， ? 代表一个字符ls javasc*# -i 查看inode编号, 每一个文件或目录都有一个唯一的编号，这个数字由内核分配给文件系统中的每一个对象ls -i pwd显示当前工作目录 1pwd wc统计文件的行数、字数、字节数, 常见用于统计代码行数 1234567891011# 统计字节数wc -c README.md# 统计行数wc -l README.md# 统计字数wc -w README.md# 统计字符数wc -m README.md whoami显示自身的用户名称, 此命令等价于 id -un 1% whoami # xiejiahe alias设置命令别名，用于简化较长的命令 12345678# 列出所有已设置的别名alias# 删除所有别名unalias -a# 设置别名alias ll=&#x27;ls -l&#x27; wget用于从网络下载文件到本地 123456789101112# 下载某个文件wget https://www.xiejiahe.com/robots.txt# 指定下载后文件名wget -O ro.txt https://www.xiejiahe.com/robots.txt# 断开续传，一般用于大文件，防止重新下载wget -c https://www.xiejiahe.com/robots.txt# 使用后台下载, 对于大文件非常有用wget -b https://www.xiejiahe.com/robots.txttail -f wget-log # 查看后台下载进度 df查看已挂载的磁盘使用情况。 描述 名称 描述 Filesystem 设备的设备位置文件 Size 能容纳多少个1024字节大小的块 Used 已用了多少个1024字节大小的块 Avail 还有多少个1024字节大小的块可用 Use% 已用空间所占的比例 Mounted on 设备挂载到了哪个挂载点上 12345678910# 显示每个有数据的已挂载文件系统# Filesystem 1K-blocks Used Available Use% Mounted on# /dev/vda1 41151808 1853208 37185168 5% /df# 格式化大小，以kb以上进行显示df -h# 查看全部文件系统信息df -a du显示文件或目录所占用的磁盘空间 1234567891011121314151617# 默认显示当前目录的所有文件、目录、和子目录的磁盘使用情况du# 查看指定文件所占用磁盘空间du README.md# 查看指定目录所占用磁盘空间, 输出的最后一行是累计总大小du src# -h 以K，M，G为单位，提高信息的可读性。du -h src # 20K src# -s 只显示总大小，列出最后累计的值du -s src# 显示当前所有已列出文件总大小du -c find指定某个目录下查找文件 12345678910111213141516171819202122232425# 在当前目录递归搜索文件名为 README.md 文件find . -name README.md# 通过通配符进行查找, 必须用引号括着, 这里查找所有后缀为 .md 文件find . -name &quot;*.md&quot;find . -iname &quot;*.md&quot; # 忽略文件大小写# 排除文件，只要加 ! , 排除掉所有 .md 后缀的文件find . ! -name &quot;*.md&quot;# 根据类型进行过滤搜索# f 普通文件, l 符号连接# d 目录, c 字符设备# b 块设备, s 套接字, p Fifofind . -type f# 限定目录递归深度find . -maxdepth 3 # 最大为3个目录find . -mindepth 3 # 最小为3个目录# 查找文件大小大于 25k 文件 find /root -size +25k# 查找10天前文件 -mtime 修改时间、 -ctime 创建时间、 -atime 访问时间find /root -mtime +10 mkdir创建目录 12345678# 在当前目录下创建 temp 目录mkdir temp# 创建多层目录mkdir -p temp/temp2/temp3# 基于权限创建mkdir -m 777 temp mktemp创建临时目录或文件，Linux使用 /tmp 目录来存放不需要永久保留的文件，大多数Linux发行版配置了系统在启动时自动删除 /tmp 目录的所有文件。 默认情况下， mktemp 会在本地目录中创建一个文件，只要指定一个文件名模板就行，模板可以包含任意文本文件名，在文件名末尾加上6个X就行了。 12345678# 创建本地临时文件, 会在当前目录下创建一个叫 log.XXXXXX, XXXXXX是一个随机字符码，保证文件名在目录中是唯一的。mktemp log.XXXXXX # log.J3awfb# -t, 在 /tmp 目录创建临时文件, 返回绝对路径地址mktemp -t log.XXXXXX # /tmp/log.G5g9dX# -d 创建临时目录, 这样就能用该目录进行任何需要的操作了，比如创建其他的临时文件mktemp -d dir.XXXXXX touch创建一个空文件, 如果文件存在只会修改文件的创建时间 1touch README.md ssh远程连接服务器工具 12345# 简单的连接, 省略了端口号,默认为22ssh root@192.168.0.0# 指定端口号连接ssh -p 23 root@192.168.0.0 nohup程序以挂起方式运行, 不会影响终端交互。 因为程序会以后台的方式运行，所以标准输出不会显示在屏幕上, 默认情况下会在当前目录生成一个叫 nohup.out 文件，里面包含了标准输出内容。 123456# 例如运行一个 node.js 程序nohup node main.jsnohup node main.js &amp; # 可以在最后加 &amp; ，表示后台模式，让出CLI以供其他使用。# 在当前目录会出现 nohup.out 文件，里面包含了 Hello Worldnohuo echo &quot;Hello World&quot; cd进入指定目录 1234567891011121314151617181920# 进入当前 src 目录cd src# 回到上一次目录cd -# 返回上一级目录cd ..cd ../../.. # 返回多级# 进入家目录cd ~cd # 或者不带任何参数# 将上一个命令的参数作为cd参数使用cd !$# 模糊匹配目录，有时目录名很长一个一个敲效率就很低# * 代表0个或多个字符， ? 代表一个字符cd javasc* echo输出字符串或者变量 注: 一般情况下字符串不必加双引号, 如果包含转义字符就必须要加 12345678910111213# 在终端输出 Hello Worldecho &quot;Hello World&quot;echo Hello World # 也可以不加双引号echo &quot;Hello\\nWorld&quot; # 必须加双引号, 否则无法转义# 打印系统环境变量，如果变量不存在输出为空echo $PATH# &gt; 输出重定向，将内容输出到文件中echo Hello World &gt; 1.txt# -n 不换行, 默认情况下echo 是占一整行echo -n Hello; echo World time测试某条命令执行所需花费时间 123# time 后面跟着要测试的命令# 输出: 0.02s user 0.01s system 0% cpu 6.233 totaltime curl https://github.com/xjh22222228/linux-manual clear用于清除当前终端所有信息，本质上只是向后翻了一页，往上滚动还能看到之前的操作信息 注：笔者用得比较多的是 command + K 可以完全清除终端所有操作信息。 1clear rm删除指定目录或文件 注: 使用此命令需要非常小心, 一但删除无法恢复 123456789# 删除当前 1.txt 文件rm 1.txt# -i 删除前询问是否真的要删除，因为一旦删除无法恢复rm -i README.md# 这条命令比较常用, 强制删除目录或文件# -r 如果是目录递归删除, -f 强制删除 不发出任何警告rm -rf ./src rmdir删除指定空目录 注：rmdir 实际上用得并不多，因为不是很灵活，基本上使用 rm 代替 12345678# 删除当前 temp 空目录, 如果不是空目录会发出警告rmdir temp# -p 参数可以删除多层空目录, 发现temp3是空目录删除掉，然后接着往父级找如果还是空目录继续删除...rmdir -p temp1/temp2/temp3# -i 删除前询问确认删除rmdir -i temp watch通常用于监听1个命令的运行结果、定时执行命令 12345# 每5秒执行一次 tail 命令, 如果不指定-n 默认为2秒watch -n 5 &quot;tail README.md&quot;# -d 高亮显示变化内容watch -n 5 -d &quot;tail README.md&quot; ping测试目标地址是否可连接、延迟度 12345678# 测试 github.com 连通性, 按 ctrl + C 停止 ping github.com# ping 5次后断开ping -c 5 xiejiahe.com# 每5秒ping 一次ping -i 5 xiejiahe.com cp拷贝文件或目录 1234567891011# 将当前 README.md 文件拷贝到上一层cp ./README.md ../README.md# -a 将原文件属性一同拷贝cp -a ./README.md ../README.md# -r 拷贝目录cp -r home ../home# -i 如果目标文件存在会询问用户是否需要覆盖cp -i README.md README.md which查找某个命令存储在哪个位置, 输出绝对路径, which会在环境变量 $PATH 设置的目录里去查找。 注: 可以通过 echo $PATH 查看设置的目录. 1234567which top # /usr/bin/top# 查找pwd发现会找不到，因为 pwd 是bash的内置命令which pwd# 打印多个命令which ls vi cat查看指定整个文件内容 123456789# 查看 README.md 文件所有内容cat README.mdcat README.md README2.md # 或者一次性显示多个文件# -n 每一行显示行号cat -n README.md# -b 只给有内容的行显示行号cat -b README.md mvmv 有2个用途： 将文件或目录移动到另一个位置 将文件或目录重命名 注：实际上 mv 是用来移动文件或目录，只不过有类似重命名的功能而已。 12345678# 将 README.md 重命名为 README-2.md, 如果 README-2.md 存在会直接覆盖。mv README.md README-2.md# 将 README.md 移动到上一层目录mv README.md ../README.md# -i 交互式操作，如果目标文件存在则进行询问是否覆盖mv -i README.md ../README.md cal显示当前日历 123456789101112cal# 输出 June 2020 Su Mo Tu We Th Fr Sa 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30# 显示临近3个月, 只能是3个月cal -3 expr执行数学运算，expr 命令比较鸡助，通常在 shell 脚本当中看到。但在shell脚本也不建议用。 expr 后面每个表达式都要有一个空格，否则是不合法。 注：expr 只支持整数运算，这是一个限制。 123456# 3expr 1 + 2expr 1+2 # 这样是不行的# 在浮点数计算时会丢失小数， 这里等于 2expr 5 / 2 bcbash计算器，用来执行数学运算， 与 expr 不同，因为 expr 命令不支持浮点数运算，所以可以用 bc命令替代。 bash计算器实际上是一种编程语言，它允许在命令行中输入浮点表达式，然后解释并计算该表达式。最后返回结果。 bc 大多数情况下是在 shell 脚本中使用。 123456# 敲 bc 然后回车进入交互式， 输入 quit 退出bcscale=2 # 保留几位小数，默认是05 / 2# 输出 2.50 在 shell 脚本使用 1234567891011121314151617#!/bin/bash# 配合管道n=$(echo &quot;scale=2; 5 / 2&quot; | bc)echo $n# 第二种, 内联重定向，解决第一种表达式过长问题n1=$(bc &lt;&lt; EOFscale=25 / 2EOF)echo $n1 timeout在指定时长范围内执行命令，并在规定时间结束后停止进程。 意思是在规定时间内必须完成，否则停止进程。 12345# 模拟超过3秒, 因为sleep阻塞5秒所以在3秒内无法完成，则停止进程time 3 sleep 5# 比如打包, 1分钟内要打包完成，否则停止进程time 60 npm run build exit退出当前登录Shell, 可以使用快捷键退出 Ctrl + D。 等价命令 logout 1exit vimvi 编辑器是Unix系统最初的编辑器，在GNU项目将vi编辑器移植到开源世界时，他们决定对其做一些改进，开发人员也就将它重命名为 vi improved。 vim(vi improved) 是 vi 的升级版，所以只需要知道 vim 即可， 是一个非强大的文本编辑器，学习成本不低，需要长期使用才能记牢每一个指令操作。 这是一份速查表，使用的时候注意区分大小写。 vim 的快捷键指令非常多，只列出一些实用性便于记忆。 打开文件 12345678910111213# 最简单的打开文件方式, 如果文件不存在会开辟一段新的缓冲区域来编辑。vim README.md# 打开文件并定位到最后一行vim + README.md# 打开文件并定位到某一行, +号后面指定行数vim +100 README.md# 打开多个文件# :bn 切换下一个文件# :bp 切换上一个文件vim 1.txt 2.txt 光标定位插入 快捷键 描述 i 在当前光标位置插入 I 在当前光标行第一个字符插入 a 在当前光标后一个字符插入 A 光标到当前行最后一个字符插入 H 光标到第一行第一个字符 (是以终端大小来计算，不是原文本的第一行) M 光标到中间第一行 (是以终端大小来计算) L 光标到最后行第一个字符 (是以终端大小来计算) E 将光标定位到右边的空格 o 在当前光标下一行插入 O 在当前光标上一行插入 撤销 | 快捷键 | 描述 | | ———- |—————– | | u | 撤销上一次编辑内容 | | U | 撤销当前光标整行内容 | | Ctrl + r | 还原初始文件状态 | 删除 | 快捷键 | 描述 | | ———- |—————– | | dd | 删除当前行 | | dj | 删除上一行 | | dk | 删除下一行 | | :1,$d | 删除所有行 | 拷贝/粘贴 | 快捷键 | 描述 | | ———- |—————– | | yy | 拷贝当前行 | | p | 在当前光标粘贴上一次拷贝的内容 | 搜索 快捷键 描述 :/text 从上往下查找 text，按 n 向下搜索, 按 N 想前搜索 :?text 从下往上查找 text，按 n 向下搜索, 按 N 想前搜索 替换 快捷键 描述 :s/old/new 找到old第一次出现的地方并用new来替换 :s/old/new/g 找到所有old出现的地方并用new来替换(当前屏幕) :%s/old/new/g 替换整个文件中的所有old :%s/old/new/gc 替换整个文件中的所有old，但在每次出现时提示 :n,ms/old/new/g 替换行号n和m之间的所有old 退出 先按 ESC 键然后再操作后面的快捷键。 快捷键 描述 :w 写入但不退出 :q! 退出但不保存 :wq 保存写入内容并退出 设置 快捷键 描述 :set ignorecase 忽略大小写查找 :set noignorecase 不忽略大小写查找 :set hlsearch 高亮搜索结果 :set nohlsearch 关闭高亮搜索显示 :set hlsearch 高亮搜索结果 :set number 显示行号 :set nonumber 不显示行号 basename打印目录或者文件的基本名称。 12345# 输出：index.htmlbasename /www/index.html# 输出 wwwbasename /www/ readread 命令从标准输入（键盘）或另一个文件描述符中接收输入。 通常用在Shell脚本, 在收到输入后，read命令会将数据存放进一个变量。 1234567891011# 最简单用法, data 是自定义变量名，用户输入内容并回车后结束read data # echo $data 会打印用户输入的内容# -p 指定提示符read -p 确认要删除吗？ data# -t 指定超时（秒）read -t 5 -p 确认要删除吗？ data# —s 隐藏用户输入，比如密码，实际上是将文本颜色设置成背景颜色一样read -s -p &quot;请输入您的密码：&quot; data teetee命令相当于管道的一个T型接头，它将从STDIN标准输入过来的数据同时发往两处，一处是STDOUT，另一处是tee命令指定的文件名。 tee 命令通常用于 shell 脚本当中。 12345# date内容打印到屏幕上并且重定向输出到 date.txt 文件中date | tee date.txt# -a 以追加方式，默认情况下会覆盖输出文件内容date | tee -a date.txt tee 命令只是一个语法糖，如果不用 tee 可以这样做： 123456# 1、将date结果保存到 var 变量中var=$(date)# 2、将结果打印到屏幕上 STDOUTecho $var# 3、将结果重定向到文件echo $var &gt; date.txt last显示用户最近登录信息 1234last # root pts/0 183.58.247.64 Sun Jan 5 13:57 - 14:28 (00:30)# 指定显示条目数last -n 1 shutdown将系统关机或重启操作。 123456789101112# 立即重启系统shutdown -r now# 关闭系统并切断电源shutdown -h now # 立即关机, 实际上调用 init 0# 把前一个关机或重启取消掉shutdown -c# 设定一个时间关机, &quot;&amp;&quot; 符号表示后台模式，让出CLIshutdown -h 05:33 &amp;shutdown +5 &quot;5分钟后关机&quot; # 5分钟后关机，同时送出警告信息给登入用户： reboot有点类似 shutdown 命令， 用于重新启动系统。 1234567891011# 重启系统reboot# -f 强制重启reboot -f# 用于模拟重新启动系统，不会真实重启，数据会写入 /var/log/wtmp reboot -w# 在重新启动之前关闭所有网络界面reboot -i uname打印系统信息 1234567891011121314151617# 不带任何参数打印当前操作系统内核名称uname # Linux , 等价于 uname -s# 打印系统所有信息uname -a# -r 打印系统版本 , 如果次版本号都是偶数，说明是一个稳定版uname -r # 3.10.0-514.26.2.el7.x86_64# 打印网络节点主机名称uname -n # Yin.local# 打印处理器名称uname -p # i386# 打印主机的硬件架构名称uname -m # x86_64 ifconfig配置或显示系统网卡的网络参数 12345# 显示所有网络参数信息ifconfig# 配置网卡IP地址ifconfig eth0 192.168.1.111 who显示当前所有用户登录信息 1234567891011121314# 显示当前登录系统的用户whoxiejiahe console Jun 15 21:38xiejiahe ttys001 Jun 15 21:44xiejiahe ttys002 Jun 15 21:44xiejiahe ttys003 Jun 15 21:44xiejiahe ttys004 Jun 15 21:44xiejiahe ttys005 Jun 15 21:44# 显示登录账号名和总人数who -q# 显示上次系统启动时间who -b # reboot ~ Jun 15 21:38 whereis用来定位指令的二进制程序、源代码文件和man手册页等相关文件的路径。 注意：whereis 是从数据库里查找的，因此特别快，默认情况下一星期更新一次数据，所以有时会查找删除的数据或者刚建立的数据无法找到问题。 12345678# 查找 nginxwhereis nginx # nginx: /usr/sbin/nginx /usr/lib64/nginx /etc/nginx /usr/share/nginx /usr/share/man/man8/nginx.8.gz /usr/share/man/man3/nginx.3pm.gz# -b 指定只查找二进制where -b nginx # nginx: /usr/sbin/nginx /usr/lib64/nginx /etc/nginx /usr/share/nginx# -m 指定查找说明文件 manwhereis -m nginx # nginx: /usr/share/man/man8/nginx.8.gz /usr/share/man/man3/nginx.3pm.gz zip归档数据，将目录或文件归档为 .zip 格式， zip 不是Linux中的标准归档工具。 1234567891011# 归档文件zip README.zip README.md# 归档目录需要 -r 递归处理zip -r temp.zip temp# -S 包含系统隐藏文件zip -r -S temp.zip temp# 指定归档效率 1-9zip -r -9 temp.zip temp unzip提取 zip 归档的文件或目录 12345678# 将 demo.zip 提取到当前目录下unzip demo.zip# 列表 demo.zip 文件内容，但不提取unzip -v demo.zip# -d 指定将文件提取到 src 目录下unzip demo.zip -d src locate搜索文件，与 find 命令很像，但更快，因为是从数据库里查找, 通常每天会进行数据更新。 12345# 搜索 README.md 相关文件locate README.md# 忽略大小写locate -i README.md kill结束程序，kill 命令只支持进程id杀死，不支持进程名称。 进程信号 信号 名称 描述 1 HUP 挂起 2 INT 中断 3 QUIT 结束运行 9 KILL 无条件终止 11 SEGV 段错误 15 TERM 尽可能终止 17 STOP 无条件停止运行，但不终止 18 TSTP 停止或暂停，但继续在后台运行 19 CONT 在STOP或TSTP之后恢复执行 注：程序进程 id 可通过 top 等命令查看。 123456789101112# 杀死 pid 为88 进程，不带参数默认等价 kill -15kill 88# 无条件终止进程，以下是等价，可以用进程名称信号kill -KILL 88kill -9 88# 显示信号kill -l# 杀死指定用户的所有进程kill -u nginx killall杀死进程，可以杀死多个进程，比 kill 要强大, 支持通过进程名称杀死, 还支持通配符。 12# 杀死以tcp进程名称开头的所有进程killall tcp* chmod修改文件或目录权限 chmod [参数选项] [mode, 八进制或符号表示] files… u 符号代表当前用户。 g 符号代表和当前用户在同一个组的用户，以下简称组用户。 o 符号代表其他用户。 a 符号代表所有用户。 r 符号代表读权限以及八进制数4。 w 符号代表写权限以及八进制数2。 x 符号代表执行权限以及八进制数1。 X 符号代表如果目标文件是可执行文件或目录，可给其设置可执行权限。 s 符号代表设置权限suid和sgid，使用权限组合u+s设定文件的用户的ID位，g+s设置组用户ID位。 t 符号代表只有目录或文件的所有者才可以删除目录下的文件。 + 符号代表添加目标用户相应的权限。 - 符号代表删除目标用户相应的权限。 = 符号代表添加目标用户相应的权限，删除未提到的权限。 文件权限码 权限 二进制值 八进制值 描述 — 000 0 没有任何权限 –x 001 1 只有执行权限 -w- 010 2 只有写入权限 -wx 011 3 有写入和执行权限 r– 100 4 只有读取权限 r-x 101 5 有读取和执行权限 rw- 110 6 有读取和写入权限 rwx 111 7 有全部权限 12345678910# README.md 文件设为所有用户可读取chmod a+r README.md# -R 递归目录下所有文件chmod a+r src/# 也可以用八进制符号表示# 3个数字分别为 x,y,z 表示User、Group、及Other的权限。# r=4, w=2, x=1chmod 777 README.md # 等价于 chmod a=rwx README.md lsof列出当前系统打开文件的工具 12345678## 打印所有打开文件的的列表lsof# 查看指定端口被占用情况lsof -i:8080# -p 列出指定进程号所打开的文件lsof -p 6112 ps查看当前系统进程状态。 ps 命令非常复杂，且参数极多，由于 ps 历史问题，参数风格支持了三种 UNIX/BSD/``GNU` 这里不详细的介绍，感兴趣可以自行了解。 Unix 风格参数 - 前面加单破折线 BSD 风格的参数 - 前面不加破折线 GNU 风格长参数 - 前面加双破折线 Unix 风格参数 参数 描述 -A 显示所有进程 -N 显示与指定参数不符的所有进程 -a 显示除控制进程和无终端进程外的所有进程 -d 显示除控制进程外的所有进程 -e 显示所有进程 -C cmdlist 显示包含在cmdlist列表中的进程 -G grplist 显示组ID在grplist列表中的进程 -U userlist 显示属主的用户ID在userlist列表中的进程 -g grplist 显示会话或组ID在grplist列表中的进程 -p pidlist 显示PID在pidlist列表中的进程 -s sesslist 显示会话ID在sesslist列表中的进程 -t ttylist 显示终端ID在ttylist列表中的进程 -u userlist 显示有效用户ID在userlist列表中的进程 -F 显示更多额外输出（相对-f参数而言） -O format 显示默认的输出列以及format列表指定的特定列 -M 显示进程的安全信息 -c 显示进程的额外条调度器信息 -l 显示长列表 -o format 仅显示由format指定的列表 -y 不要显示进程标记（process tag, 表明进程状态的标记） -Z 显示安全标签（security context）信息 -H 用层级格式来显示进程（树状，用来显示父进程） -n namelist 定义了WCHAN列显示的值 -w 采用宽输出模式，不限宽显示 -L 显示进程中的线程 -V 显示PS命令的版本号 1234567891011121314# 显示所有进程信息ps -A# 显示指定用户进程信息ps -u root# 显示所有进程信息包括命令行ps -ef # -e 等价于 -A , 即等价于 ps -Af# 这是 BSD 风格参数，列出所有正在内存中的进程ps aux# 配合 grep 查询指定进程ps -ef | grep nginx openopen 命令可在 linux / mac 具有可视化界面下进行文本编辑、打开应用程序等功能。 1234567891011121314# 在mac下用Finder打开当前目录open .# 用默认应用程序打开文件open README.md# 用默认编辑器打开文件open -e README.md# 如果是一个URL用默认浏览器打开页面open https://github.com/xjh22222228/linux-manual.git# 指定某个应用程序打开某个文件, 如果不指定文件默认直接打开程序open -a /Applications/Google\\ Chrome.app README.md curlcurl 是一个非常强大的网络传输工具, 利用URL规则在命令行下工作的文件传输工具。 123456789101112131415161718192021222324252627282930313233343536373839# 查看HTTP响应curl https://github.com/xjh22222228/linux-manual# -s 不输出错误和进度信息, 只显示正常结果curl -s https://github.com/xjh22222228/linux-manual# -o 指定文件名下载到本地，等价于 wgetcurl https://github.com/xjh22222228/linux-manual -o 1.txt # 1.txt# -O 下载文件到本地, 并将URL最后部分当做文件名curl https://github.com/xjh22222228/linux-manual -O # linux-manualcurl https://github.com/xjh22222228/linux-manual -O --progress # 显示下载进度条# -L HTTP请求跟随服务器重定向curl -L https://github.com/xjh22222228/linux-manual# -I 或 -head 显示HTTP响应报文curl https://github.com/xjh22222228/linux-manual -I# -H 设置请求头curl -H &#x27;Content-Type: application/json&#x27; -H &#x27;Content-Type: application/json&#x27; https://github.com/xjh22222228/linux-manual# 通过POST请求发送JSON数据, -X 指明HTTP请求方法, -d 实体内容curl -H &quot;Content-type: application/json&quot; -X POST -d &#x27;&#123;&quot;age&quot;:&quot;18&quot;&#125;&#x27; https://github.com/xjh22222228/linux-manual# 发送时携带 cookiecurl https://github.com/xjh22222228/linux-manual --cookie &quot;age=18;name=xjh&quot;# -v 查看整个传输过程curl https://github.com/xjh22222228/linux-manual -v# -F(--form) 利用POST上传文件, file 是字段名, =@ 必须存在curl https://example.com/upload -F &quot;file=@/home/demo.png&quot;# -u 指定提供用户名密码进行授权，通常Ftp等服务curl -u admin:123123 ftp://demo/README.md# 下载并执行脚本curl -fsSL https://deno.land/x/install/install.sh | sh date显示或设置系统时间日期 格式化符号 %% 百分号 %a 当地缩写的工作日名称（例如，Sun） %A 当地完整的工作日名称（例如，Sunday） %b 当地缩写的月份名称（例如，Jan） %B 当地完整的月份名称（例如，January） %c 当地的日期和时间（例如，Thu Mar 3 23:05:25 2005） %C 世纪，和%Y类似，但是省略后两位（例如，20） %d 一月中的一天（例如，01） %D 日期，等价于%m/%d/%y %e 一月中的一天，格式使用空格填充，等价于%_d %F 完整的日期；等价于%+4Y-%m-%d %g ISO标准计数周的年份的最后两位数字 %G ISO标准计数周的年份，通常只对%V有用 %h 等价于%b %H 小时，范围（00..23） %I 小时，范围（00..23） %j 一年中的一天，范围（001..366） %k 小时，使用空格填充，范围（0..23），等价于%_H %l 小时，使用空格填充，范围（1..12），等价于%_I %m 月，范围（01..12） %M 分钟，范围（00..59） %n 换行符 %N 纳秒，范围（000000000..000000000） %p 用于表示当地的AM或PM，如果未知则为空白 %P 类似于%p，但用小写表示 %q 季度，范围（1..4） %r 当地以12小时表示的时钟时间（例如，11:11:04 PM） %R 24小时每分钟；等价于%H:%M %s 自协调世界时1970年01月01日00时00分以来的秒数 %S 秒数，范围（00..60） %t 水平制表符 %T 时间；等价于%H:%M:%S %u 一周中的一天（1..7），1代表星期一 %U 一年中的第几周，周日作为一周的起始（00..53） %V ISO标准计数周，该方法将周一作为一周的起始（01..53） %w 一周中的一天（0..6），0代表星期天 %W 一年中的第几周，周一作为一周的起始（00..53） %x 当地的日期表示（例如，12/31/99） %X 当地的时间表示（例如，23:13:48） %y 年份后两位数字，范围（00..99） %Y 年份 %z +hhmm格式的数值化时区格式（例如，-0400） %:z +hh:mm格式的数值化时区格式（例如，-04:00） %::z +hh:mm:ss格式的数值化时区格式（例如，-04:00:00） %:::z 数值化时区格式，相比上一个格式增加’:’以显示必要的精度（例如，-04，+05:30） %Z 时区缩写（如EDT） - (连字符) 不要填充相应的字段。 _ (下划线) 使用空格填充相应的字段。 0 (数字0) 使用数字0填充相应的字段。 + 用数字0填充，未来年份大于4位数字则在前面加上’+’号。 ^ 允许的情况下使用大写。 # 允许的情况下将默认的大写转换为小写，默认的小写转换为大写。 123456789# 显示当前时间date# 格式化当前时间date +&quot;%Y-%m-%d %H:%M.%S&quot; # 2020-07-01 00:00.00# 设置系统时间date -s # 设置当前时间, 须rootdate -s &quot;2020-07-01 00:00:00&quot; # 设置全部时间 netstat查看网络系统状态信息 参数说明： 参数 描述 a或–all 显示所有连线中的Socket A&lt;网络类型&gt;或–&lt;网络类型&gt; 列出该网络类型连线中的相关地址 c或–continuous 持续列出网络状态 C或–cache 显示路由器配置的快取信息 e或–extend 显示网络其他相关信息 F或–fib 显示FIB g或–groups 显示多重广播功能群组组员名单 h或–help 在线帮助 i或–interfaces 显示网络界面信息表单 l或–listening 显示监控中的服务器的Socket M或–masquerade 显示伪装的网络连线 n或–numeric 直接使用IP地址，而不通过域名服务器 N或–netlink或–symbolic 显示网络硬件外围设备的符号连接名称 o或–timers 显示计时器 p或–programs 显示正在使用Socket的程序识别码和程序名称 r或–route 显示Routing Table s或–statistics 显示网络工作信息统计表 t或–tcp 显示TCP传输协议的连线状况 u或–udp 显示UDP传输协议的连线状况 v或–verbose 显示指令执行过程 V或–version 显示版本信息 w或–raw 显示RAW传输协议的连线状况 x或–unix 此参数的效果和指定”-A unix”参数相同 -ip或–inet 此参数的效果和指定”-A inet”参数相同 1234567891011121314# 列出所有占用端口netstat -ntlp# 显示所有网络状况netstat -a# 显示所有tcp网络状况netstat -at# 显示所有udp网络状况netstat -au# 配合grep命令查看某个端口被占用情况netstat -ap | grep 8080 w查看当前登入系统的用户信息, 有哪些用户正在登陆, 以及他们正在执行的程序。 此命令与 who 相似，默认情况下比 who 命令输出内容更详细。 12345w# 输出 22:44:33 up 748 days, 14:16, 1 user, load average: 0.04, 0.03, 0.05USER TTY FROM LOGIN@ IDLE JCPU PCPU WHATroot pts/0 183.11.111.11 22:40 1.00s 0.03s 0.00s w chown用来变更文件或目录的拥有者或所属群组 12345678# 将 README.md 文件拥有者设为 byrootchown byroot README.md# 使用-R递归处理文件chown -R byroot src/# 改变所属群组, 拥有者设为 byroot 群组设为 byrootgroupchown byroot:byrootgroup README.md uptime查看系统负载信息， 此命令非常简单，没有太多的参数。 12345# 21:51:53 当前时间# up 750 days, 13:24 当前系统运行的天数，小时，分钟 （从上次开机起计算）# 1 user 当前系统登录用户数# load average: 0.08, 0.07, 0.06 一分钟、5分钟、15分钟平均负载, 这3个值不能大于CPU个数，如果大于了说明系统负载高，性能低。uptime # 21:51:53 up 750 days, 13:24, 1 user, load average: 0.08, 0.07, 0.06 chattr用于修改文件属性 参数: a：让文件或目录仅供附加用途 b：不更新文件或目录的最后存取时间 c：将文件或目录压缩后存放 d：将文件或目录排除在倾倒操作之外 i：不得任意更动文件或目录 s：保密性删除文件或目录 S：即时更新文件或目录 u：预防意外删除 -R：递归处理，将指令目录下的所有文件及子目录一并处理 -v&lt;版本编号&gt;：设置文件或目录版本 -V：显示指令执行过程 +&lt;属性&gt;：开启文件或目录的该项属性 -&lt;属性&gt;：关闭文件或目录的该项属性 =&lt;属性&gt;：指定文件或目录的该项属性 12345# 锁定该文件, 防止文件被修改或删除chattr +i README.md # chattr -i README.md 解锁# 可以使用 lsattr 查看赋予的属性lsattr README.md gzipGNU 压缩/解压工具，用 Lempel-Ziv编码，格式为 .gz, 压缩后原文件将被删除 注意：gzip 不能用于压缩整个目录, 只能用于压缩文件, 如果需要压缩整个目录可以考虑使用 zip 命令。 123456789101112131415# 压缩 README.md 文件, 压缩完成后 README.md 文件会被删除gzip README.md # README.md.gz# 递归压缩目录下的所有文件gzip -r ./logs# 加 -v 显示压缩执行过程gzip -rv ./logs# 压缩 .tar 后缀文件gzip -r src.tar # 压缩后为 src.tar.gz# -d 解压之前gzip压缩后的文件gzip -d README.mdgzip -dr ./logs # 或者递归解压目录下的所有文件 bzip2采用 Burrows-Wheeler块排序文本压缩算法和霍夫曼编码，将文件压缩成 .bz2 格式，也可用于解压 .bz2 123456789# 压缩 README.md 文件bzip2 README.md # 不保留源文件 README.md.bz2bzip2 -k README.md # -k 保留源文件# 解压bzip2 -d README.md.bz2 # 源文件将被删除bzip2 -dk README.md.bz2 # -k 保留源文件bzip -dt README.md.bz2 # -t --test 测试解压, 实际不解压，模拟整个解压过程 tar归档数据，是Linux中标准归档工具。 参数： -A - 将一个已有tar归档文件追加到另一个已有tar归档文件 -c 创建一个新的tar归档文件 -d 检查归档文件和文件系统的不同之处 -r 追加文件到已有tar归档文件结尾 -t 列出已有tar归档文件的内容 -u 将比tar归档文件中已有的同名文件新的文件追加到该tar归档文件中 -x 从已有的tar归档文件中提取文件 -f 输出结果到文件或设备file 12345# -c 创建一个归档文件tar -cvf demo.tar src/# -x 提取归档文件内容tar -xvf demo.tar more分页查看文件内容, 每次查看一屏, 每屏能显示多少内容取决于终端大小。 与 cat 命令不同，cat 只能一次显示全部内容，如果内容太多部分会被截取掉。 快捷键： 空格或PageUp - 查看下一屏内容 B或PageDown - 查看上一屏内容 回车 - 查看下一行内容 Q - 退出 1234567more README.md# 从第10行开始显示more +10 README.md# 显示查看进度more -d README.md # --More--(17%)[Press space to continue, &#x27;q&#x27; to quit.] crontab周期性执行任务, 通常用于定时备份。 * * * * * 分别含义： 1234567891011121314151617181920212223* * * * *┬ ┬ ┬ ┬ ┬│ │ │ │ ││ │ │ │ ││ │ │ │ └───── 一周中的某一天 (0 - 7) 0或7代表是星期日│ │ │ └────────── 月份 (1 - 12)│ │ └─────────────── 一个月的某一天 (1 - 31)│ └──────────────────── 小时 (0 - 23)└───────────────────────── 分钟 (0 - 59)# 列出该用户设置crontab -l# 编辑该用户设置crontab -e# 删除该用户设置crontab -r* * * * * 命令# 每天18点18分执行 echo `date` &gt; README.md18 18 * * * echo `date` &gt; README.md# 每一分钟执行* * * * */1 echo `date` &gt; README.md man查看指令帮助手册 man 信息说明 名称 描述 NAME 显示命令名和一段简短的描述 SYNOPSIS 命令的语法 CONFI GURATION 命令配置信息 DESCRIPTION 命令的一般性描述 OPTIONS 命令选项描述 EXIT STATUS 命令的退出状态指示 RETURN VALUE 命令的返回值 ERRORS 命令的错误消息 ENVIRONMENT 描述所使用的环境变量 FILES 命令用到的文件 VERSIONS 命令的版本信息 CONFORMING TO 命令所遵从的标准 NOTES 其他有帮助的资料 BUGS 提供提交BUG的途径 EXAMPLE 展示命令的用法 AUTHORS 命令开发人员的信息 COPYRIGHT 命令源代码的版权状况 SEE ALSO 与该命令类型的其他命令 1234567891011# 查看 ls 指令帮助手册man ls# 可以通过数字来阅读某一部分内容, 比如阅读第一部分man 1 ls# -a 在所有手册中查找man -a ls# -k, 搜索关键字, 如果忘记了完整的命令可以通过关键字搜索出来，比如 nginxman -k ngi sleep将目前动作延迟一段时间, 通常用于脚本当中 时间参数, 这是可选的，默认s： s 秒 m 分钟 h 小时 d 天 12# 5秒后输出 Hellosleep 5s; echo Hello 下面是一段 Shell 脚本， 延迟10秒后再去请求 12345#!/bin/bashsleep 10scurl https://www.xiejiahe.com/ source在当前Shell环境中从指定文件读取和执行命令， 通常用于重新执行环境。 别名 . 点符号 1source ~/.bash_profile # 等价 . ~/.bash_profile paste合并N个文件的列，相当于追加文件内容。 12345# 1.txt 和 2.txt 合并输出paste 1.txt 2.txt# 1.txt 2.txt 合并后保存为 3.txtpaste 1.txt 2.txt &gt; 3.txt stat用于显示文件或目录的状态信息 123456789stat logs# File: ‘logs/’# Size: 16384 Blocks: 32 IO Block: 4096 directory# Device: fd01h/64769d Inode: 669067 Links: 5# Access: (0755/drwxr-xr-x) Uid: ( 0/ root) Gid: ( 0/ root)# Access: 2020-07-07 17:24:23.941816812 +0800# Modify: 2020-07-12 11:46:55.567707577 +0800# Change: 2020-07-12 11:46:55.567707577 +0800# Birth: - tree生成目录树结构, 通常用于描述项目结构。 123456789101112131415161718# 递归当前目录下所有文件并生成目录树tree# .# ├── LICENSE# ├── README.md# ├── b.md# └── media# └── poster.jpg# -I 忽略某些目录tree -I &quot;node_modules|.git|.svn&quot;# 只显示目录tree -d# 指定要递归的目录层级tree -L 3 yum基于RPM的软件包管理器, 特点安装快捷，命令简洁好记。 1234567891011121314151617181920212223242526# 安装nginxyum install nginx# 指定 -y 安装时自动全部 yesyum -y install nginx# 查找包yum search nginx# 显示所有已安装的包yum list# 升级包yum -y update nginx# 移除包yum -y remove nginx# 清除缓存yum clean all# 显示安装包信息yum info nginx# 检查可更新的包程序yum check-update history列出当前系统使用过的命令，默认保存1000条, 通常保存在 ~/.bash_history 文件中，注意的是只有在Shell退出时才写入到文件。 1234567891011# 列出当前使用过的命令history# 指定要显示的条数history 50# 清空历史命令history -c# -a 强制写入到 ~/.bash_history 文件中而不用等shell退出才写入history -a md5sum计算和校验文件报文摘要 123456# 计算文件md5mmd5sum README.md # d41d8cd98f00b204e9800998ecf8427e README.md# 校验文件, 查看文件是否被篡改过md5sum README.md &gt; README.md5 # 计算文件md5并保存在 README.md5 , 保存的文件名和后缀可以随意命名md5sum -c README.md5 # -c 从指定的文件读取md5并校验, 会从当前目录寻找 README.md su切换到其他用户。 12345678# 切换到 admin 身份su admin# -c 执行完指令后切换回原身份su -c ls admin# 可以通过以下查找当前系统用户列表cat /etc/passwd xargs给命令传递参数的一个过滤器，也是组合多个命令的一个工具, 将左侧的标准输出放进右侧标准输入。 此命令可以将多次操作简便为一次操作。 12345# 统计代码find -name &quot;*.js&quot; | xargs wc -l # 等价于 wc -l a.js b.js c.js ...# 批量下载文件cat download.txt | xargs wget scp加密的方式在本地主机和远程主机之间复制文件 注：需要有读写权限，否则会无法操作。 1234567891011# 从远程主机下载文件到本地scp root@192.168.0.100:/root/file.zip /home/file.zip# 从远程主机下载目录到本地，需要 -r 递归scp -r root@192.168.0.100:/root/dir /home/dir# 从本地主机上传文件到远程主机scp /home/file.zip root@192.168.0.100:/root/file.zip# # 从本地主机上传目录到远程主机，需要 -r 递归scp -r /home/dir root@192.168.0.100:/root/dir grep强大的文本搜索工具，被称为Linux命令三剑客。 123456789101112131415161718192021# 从 README.md 文件中搜索 linux 关键字grep &quot;linux&quot; README.mdgrep &quot;linux&quot; README.md README2.md # 多个文件搜索# 输出时高亮显示grep &quot;linux&quot; README.md --color# -o 只输出匹配部分grep -o &quot;linux&quot; README.md --color# -n 输出到匹配的行数grep -n &quot;linux&quot; README.md# -c 输出到匹配次数grep -c &quot;linux&quot; README.md# -r 递归目录文件搜索grep -r &quot;linux&quot; ./src# 使用 glob 风格表达式搜索egrep &quot;[0-9]&quot; # 等价于 grep -E &quot;[0-9]&quot; README.md systemctl系统服务管理器指令, 通常用来设置某个服务器默认开机启动或关闭。 命令：systemctl [command] [unit] 1234567891011121314151617181920212223242526# 立即启动服务systemctl start nginx.service# 立即停止服务systemctl stop nginx.service# 重启服务，stop 后 startsystemctl restart nginx.service# 重新载入服务, 一般情况下重新载入新的配置systemctl reload nginx.service# 下次开机时默认启动服务systemctl enable nginx.service# 下次开机时不会启动服务systemctl disable nginx.service# 查看某个服务状态信息systemctl status nginx.service# 当前服务是否正在运行中systemctl is-active nginx.service# 查看服务开机有没有默认启动systemctl is-enable nginx.service base64base64 编码/解码文件或标准输入输出 1234567891011# 编码字符串printf &quot;hello world&quot;|base64 # aGVsbG8gd29ybGQ=# 解码字符串printf aGVsbG8gd29ybGQ=|base64 -d # hello world# 编码文件, 将结果保存在 decode.txtbase64 README.md &gt; decode.txt# 从标准输入中读取已经进行base64编码的内容进行解码base64 -d decode.txt ln将某一个文件在另外一个位置建立并产生同步的链接。 当不同的2个目录需要同时引用某一个文件时此命令就派上用场了。 软链接也可以叫符号链接： 软链接，以路径的形式存在。类似于Windows操作系统中的快捷方式 软链接可以 跨文件系统 ，硬链接不可以 软链接可以对一个不存在的文件名进行链接 软链接可以对目录进行链接 硬链接： 硬链接，以文件副本的形式存在。但不占用实际空间, 从根本上而言就是同一个文件。 不允许给目录创建硬链接 硬链接只有在同一个文件系统中才能创建 12345678# 默认创建硬链接，修改 README.md 内容， a.md 也会同步修改, 修改a.md README.md 也会同步修改ln README.md a.md# -s 创建软链接ln -s README.md a.md # 如果删除了 README.md a.md 将失效# -f 强制执行ln -f README.md ./src/a.md service管理操作系统服务的命令, 是Redhat Linux兼容的发行版中用来控制系统服务的实用工具，它以启动、停止、重新启动和关闭系统服务，还可以显示所有系统服务的当前状态。 1234567891011121314# 启动 docker 服务service docker start# 查看 docker 状态service docker status# 停止 docker 服务service docker stop# 重新启动 docker 服务service docker restart# 查看所有服务状态service --status-all free显示内存使用情况 选项 b 字节单位显示 k KB单位显示 m MB单位显示 g GB单位显示 s&lt;秒&gt; 每S秒监控内存使用情况 解释： total 内存总数 used 已使用内存 free 空闲内存 shared 当前已废弃内存 buff/cache 缓存内存数 1204660 可用内存数 1234567891011free# 输出以下, 默认以字节为单位 total used free shared buff/cache availableMem: 1882192 485312 448424 704 948456 1204660Swap: 0 0 0# MB单位显示free - m# 10秒执行一次查询free -s 10 apt-getapt-get命令 是Debian Linux发行版中的APT软件包管理工具。所有基于Debian的发行都使用这个包管理系统。 1234567891011121314# 安装一个docker软件apt-get install docker# 卸载软件，保留配置文件apt-get remove docker# 卸载软件并删除配置文件apt-get –purge remove docker# 更新所有已安装的软件包apt-get upgrade# 删除软件备份，主要用来释放空间apt-get clean file查看文件类型, 比如文件、目录、二进制、符号链接等 12345# 输出 README.md: ASCII textfile README.md# index.html: HTML document, UTF-8 Unicode text, with very long lines, with no line terminatorsfile index.html jobs显示当前运行在后台模式中的所有用户的进程（作业） 1234# 先来启一个后台进程, 比如启一个sleep命令进程， &amp; 符号表示后台运行sleep 3 &amp;# 查看后台进程jobs # 输出：[1]+ Running sleep 3 &amp; typetype 命令有2个作用： 用来查找命令的位置，类似 which 命令 检测某个命令是内建命令还是外部命令 普及：内建命令和外部命令的区别：内建命令不会衍生出子进程，而外部命令会衍生出一个子进程然后执行命令, 所以内建命令执行效率要高。 12345# cd is a shell builtin 表示这是shell内建命令type cd# ps is hashed (/usr/bin/ps) 表示这是一个外部命令type ps printenv列出全局环境变量, 有个 env 命令很像，但 printenv 可以打印变量的值。 普及：所有系统环境变量都是大写字母，用于区分普通用户的环境变量。 12345# 列出所有全局环境变量printenv# 也可以显示指定全局环境变量的值, 等价于 echo $HOMEprintenv HOME # /root set列出所有全局变量、局部变量和普通用户定义的变量，按照字母顺序对结果进行排序。 注意：所有系统全局变量都是大写，用户定义的环境变量全部采用小写，这是标准规范。 123456set# OPTIND=1# OSTYPE=linux-gnu# PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin# PIPESTATUS=([0]=&quot;0&quot;)# ... export导出环境变量, 可以把一个局部变量导出成全局环境变量 注意：export 只有在当前Shell有效，退出后将失效 1234# 先声明一个局部环境变量my_var=&#x27;Hello&#x27;# 然后将其导出全局环境变量export my_var unset删除环境变量 注意：unset 只在当前shell删除环境变量，假如环境变量设置在 ~/.bash_profile 等文件中用户重新启动依然生效。如果是在子进程删除全局环境变量只在子进程有效，不会影响父进程。 12# 删除 HOME 环境变量，前面不需要带 $ 符号unset HOME useradd给系统添加新用户 注： 在创建新用户时如果不指定具体的值，就会使用系统那些默认值。 在创建新用户时如果未指定密码，需要使用 passwd 命令进行更改。 useradd 命令行参数 参数 描述 -c comment 给新用户添加备注 -d home_dir 为主目录指定一个名字（如果不想用登录名作为主目录名的话） -e expire_date 用YYYY-MM-DD格式指定一个账户过期的日期 -f inactive_days 指定这个账户密码过期后多少天这个账户被禁用；0表示密码已过期就立即禁用，1表示禁用这个功能 -g initial_group 指定用户登录组的GID或组名 -G group 指定用户除登录组之外所属的一个或多个附加组 -k 必须和-m一起使用，将/etc/skel目录的内容复制到用户的HOME目录 -m 创建用户的HOME目录 -M 不创建用户的HOME目录（当默认设置里要求创建时才使用这个选项） -n 创建一个与用户登录名同名的新租 -r 创建系统账户 -p passwd 为用户账户指定默认密码 (需要使用openssl把明文进行加密后设置，否则无效) -s shell 指定默认的登录shell -u uid 为账户指定唯一的UID 更改默认值参数 参数 描述 -b default_home 更改默认的创建用户HOME目录的位置 -e expiration_date 更改默认的心账户的过期日期 -f inactive 更改默认的心用户从密码过期到账户被禁用的天数 -g group 更改默认的组名称或GID -s shell 更改默认的登录shell 123456789101112131415161718# -D, 查看默认值useradd -D# 输出：GROUP=100 # 新用户会被添加到GID为100的公共组HOME=/home # 新用户的HOME目录将位于 /home/loginnameINACTIVE=-1 # 新用户账户密码在过期后不会被禁用EXPIRE= # 新用户账户未被设置过期日期SHELL=/bin/bash # 新用户账户将bash shell作为默认shellSKEL=/etc/skel # 系统会将/etc/skel目录下的内容复制到用户的HOME目录下CREATE_MAIL_SPOOL=yes # 系统为该用户账户在mail目录下创建一个用于接收邮件的文件# 创建一个 test 用户, -m 创建 /home/test 目录useradd -m test# 创建一个用户并设置密码useradd -m test # 不指定-p，因为需要加密那样很麻烦passwd test # 通过passwd修改指定用户密码 添加新用户后可以执行 cat /etc/passwd 查看用户列表。 userdel删除用户 12345# 删除用户，默认会从 /etc/passwd 文件中删除用户信息，而不会删除系统中属于该账户的任何文件userdel 用户名# -r 用来删除用户目录， 之前创建的 /home/用户名 就不存在了, 使用-r参数需要小心，要检查是否有重要文件。userdel -r 用户名 passwd修改用户密码, 只有 root 用户才有权限修改别人的密码。 使用 passwd 一般用于修改单个用户密码，如果想批量修改那么需要 chpasswd 命令。 12345# 如果不指定用户名，修改的是自己当前用户密码， 回车后输入新密码passwd# 修改指定用户密码，比如test用户passwd test chpasswd类似 passwd 命令也是用于修改用户密码，但它支持批量修改用户。 chpasswd 命令从标准输入自动读取登录名和密码对（由冒号分割）列表，给密码加密。 12345# 利用输入重定向从文本中读取chpasswd &lt; users.txt# 从标准输入读取echo &#x27;test:fff33300..a&#x27; | chpasswd user.txt 内容： 123test:helloworld0123..test1:fff33300..admin:youyouyou00..11 chsh修改默认用户登录 shell 12# 必须使用完整路径，不能使用shell名chsh -s /bin/sh users显示当前登录系统的所有用户的用户列表 1234users# xiejiahe# root# admin 致谢感谢 《Linux命令行与Shell脚本编程大全》 一书，以上部分命令从这本书进行整理出来， 如有错误，欢迎指正，谢谢！ ⬆ 回顶部","categories":[],"tags":[]},{"title":"log-drive一次 java.io.IOException Connection reset by peer 故障排查","slug":"log-drive一次 java.io.IOException Connection reset by peer 故障排查","date":"2021-03-14T00:29:46.000Z","updated":"2024-03-14T14:55:29.479Z","comments":true,"path":"f85e694a.html","link":"","permalink":"https://imlike.cc/f85e694a.html","excerpt":"","text":"转载地址: https://mengkang.net/1118.html 我的弹幕服务，代码换了个环境运行，虽然服务可用，但是发现出现了如下情况，基本1秒输出来一次： 12345678910111213141516171819202122java.io.IOException: Connection reset by peer at sun.nio.ch.FileDispatcherImpl.read0(Native Method) at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223) at sun.nio.ch.IOUtil.read(IOUtil.java:192) at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380) at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:311) at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:853) at io.netty.buffer.WrappedByteBuf.writeBytes(WrappedByteBuf.java:641) at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:240) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:115) at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:514) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:471) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:385) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:351) at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116) at io.netty.util.internal.chmv8.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1412) at io.netty.util.internal.chmv8.ForkJoinTask.doExec(ForkJoinTask.java:280) at io.netty.util.internal.chmv8.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:877) at io.netty.util.internal.chmv8.ForkJoinPool.scan(ForkJoinPool.java:1706) at io.netty.util.internal.chmv8.ForkJoinPool.runWorker(ForkJoinPool.java:1661) at io.netty.util.internal.chmv8.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:126) https://github.com/netty/netty/issues/5936Connection reset by peer is typically a result of your peer sending a TCP RST to you. tcpdump既然是网络故障，tcpdump 是否可用呢？ 12$sudo netstat -lnp|grep 8657tcp 0 0 0.0.0.0:443 0.0.0.0:* LISTEN 8657&#x2F;java 没有列出全部的 socket 连接，从当前进程监听的端口入手试试。ifconfig可以看到这台机器有两个网卡（eth0 内网 ip，eth1 公网 ip）。 tcpdump 下的 tcp/ip 协议解析先看下 tcpdump 输出tcp/ip 数据的大致格式 参考 http://www.tcpdump.org/tcpdump_man.html 中关于 tcp 协议的描述，我整理了下格式 The general format of a TCP protocol line is 1src &gt; dst: Flags [tcpflags], seq data-seqno, ack ackno, win window, urg urgent, options [opts], length len src and dst are the source and destination IP addresses and ports. tcpflags are some combination of S (SYN), F (FIN), P (PUSH), R (RST), U (URG), W (ECN CWR), E (ECN-Echo) or . (ACK), or none if no flags are set. data-seqno describes the portion of sequence space covered by the data in this packet. ackno is sequence number of the next data expected the other direction on this connection. window is the number of bytes of receive buffer space available the other direction on this connection. urg indicates there is urgent data in the packet. opts are TCP options (e.g., mss 1024). len is the length of payload data. 抓包数据通过tcpdump查看下443 端口的网络情况。与上面的格式略有不同。因为我有两个网卡，请求是从 eth0 进，从 eth1 出，我尝试使用tcpdump -i any 发现第三次握手输出的 askno 数据不对。（为什么会这样呢，还不知道，谁知道请告知下） 123456$sudo tcpdump -i any -nn port 44316:18:41.553460 IP xxx.xxx.238.110.5745 &gt; xxx.xxx.198.40.443: S 806033:806033(0) win 14600 &lt;mss 1460,sackOK,timestamp 2230744217 0,nop,wscale 8&gt;16:18:41.553483 IP xxx.xxx.198.40.443 &gt; xxx.xxx.238.110.5745: S 1720728675:1720728675(0) ack 806034 win 14480 &lt;mss 1460,sackOK,timestamp 2425389802 2230744217,nop,wscale 7&gt;16:18:41.553647 IP xxx.xxx.238.110.5745 &gt; xxx.xxx.198.40.443: . ack 1 win 58 &lt;nop,nop,timestamp 2230744217 2425389802&gt;16:18:41.553677 IP xxx.xxx.238.110.5745 &gt; xxx.xxx.198.40.443: R 1:1(0) ack 1 win 58 &lt;nop,nop,timestamp 2230744217 2425389802&gt; 但是如果你要导出*.pcap文件然后用wireshark来查看，还是得将两个网卡的流量合并到一起才行。 两个网卡分开抓 12345678# 合并到一起写$sudo tcpdump -i eth0 -nn port 443$sudo tcpdump -i eth1 -nn port 44316:51:16.073956 IP xxx.xxx.238.2.61835 &gt; xxx.xxx.198.40.443: S 2659415794:2659415794(0) win 14600 &lt;mss 1460,sackOK,timestamp 2328745293 0,nop,wscale 8&gt;16:51:16.073985 IP xxx.xxx.198.40.443 &gt; xxx.xxx.238.2.61835: S 955422999:955422999(0) ack 2659415795 win 14480 &lt;mss 1460,sackOK,timestamp 2427344323 2328745293,nop,wscale 7&gt;16:51:16.074147 IP xxx.xxx.238.2.61835 &gt; xxx.xxx.198.40.443: . ack 955423000 win 58 &lt;nop,nop,timestamp 2328745293 2427344323&gt;16:51:16.074192 IP xxx.xxx.238.2.61835 &gt; xxx.xxx.198.40.443: R 0:0(0) ack 1 win 58 &lt;nop,nop,timestamp 2328745293 2427344323&gt; 上面的握手比较简单对应着 1时间 协议 src &gt; dst tcpflags data-seqno ack ackno win window &lt;opts&gt; 握手逻辑一次连接建立的三次握手的步骤： Caller sends SYN Recipient responds with SYN, ACK Caller sends ACK 数据分析 src 发送的 tcpflags = S，也就是说发送的SYN请求，随机产生一个值 data-seqno = J(2659415794)，并将该数据包发送给 dst dst 发送的 tcpflags = S，实际发送的时候 SYN 和 ACK 标志位都置1了，返回一个 ackno=J + 1，也就是2659415795，然后随机生成一个新的 data-seqno = K(955422999) 发送给 src src 检查收到的 ackno 是不是等于J+1，然后再发送一个 ackno = K+1，也就是955423000，前面的文档中说道tcpflags为.的时候也表示ACK src 最后为什么又给 dst 发送一个 tcpflags = R 的请求呢？ 为什么会出现 RST因为具有周期性，我大概猜到了，是 lvs 对我的后端服务的健康检查导致的，联系了网络运营服务客服人员，我把.pcap给他们 1sudo tcpdump -i any port 443 -c 3 -w log.pcap 他们确认源地址就是LVS的local地址。健康检查的机制就是建立成功后就 RST 掉，就不占用 session 了。这是公司的规范，问题算是定位了。既然公司的规范，那么我们这边需要定制修改下，针对 vip 发送的 RST 不处理。 其他参考 https://github.com/netty/netty/issues/6029https://tools.ietf.org/html/rfc793https://my.oschina.net/costaxu/blog/127394 12 1","categories":[],"tags":[]},{"title":"ambari-server中postgress启动","slug":"ambari-server中postgress启动","date":"2021-03-10T07:59:20.000Z","updated":"2024-03-14T14:55:29.475Z","comments":true,"path":"466081a0.html","link":"","permalink":"https://imlike.cc/466081a0.html","excerpt":"","text":"Unable to start PostgreSQL server. Status stopped. . Exiting bin pg_ctl status systemctl start postgresql.service FATAL: could not write lock file “postmaster.pid”: No space left on device df -h systemctl start postgresql.service","categories":[],"tags":[]},{"title":"","slug":"docker搭建单机kafka","date":"2021-03-07T04:36:44.000Z","updated":"2024-03-14T14:55:29.476Z","comments":true,"path":"4225d45e.html","link":"","permalink":"https://imlike.cc/4225d45e.html","excerpt":"","text":"搭建单机kafka选型 选择docker容器内集成 kafka,zookeeper 的容器 12## 找到的这个 容器设置了 kafka的hostname和 zookeeper的ip , docker run -d -e ADVERTISED_HOSTNAME=$(hostname -f) -e KAFKA_ADVERTISED_HOST_NAME=192.168.248.128 -e ZOOKEEPER_IP=192.168.248.128 -p 9092:9092 -p 2181:2181 --restart=always jrottenberg/kafka-standalone 基于上面的容器是可以运行 获取到了zookeeper的broker 但是kafka是连接不上 docker-composer 来执行 参考第 https://github.com/conduktor/kafka-stack-docker-compose 但是还有有个问题,我搭建的环境是离线的,尽量还是以 docker命令来执行减少安装其他软件 原始的 kafka,zookeeper 分开安装,在设置环境变量(选用的方案) 前置需要安装的软件prettyZoo 查看zookeeper很方便 cmak (kafka manage) kafka监控软件 安装过程https://ioccc.cc/2018/12/12/zookeeper-kafka/ 改部分参考该篇博文 运行zookeeper123docker run -d --name zookeeper --publish 2181:2181 \\ --volume &#x2F;etc&#x2F;localtime:&#x2F;etc&#x2F;localtime \\ zookeeper:latest 运行kafka1234567docker run -d --name kafka --publish 9092:9092 \\ --link zookeeper \\ --env KAFKA_ZOOKEEPER_CONNECT&#x3D;zookeeper:2181 \\ --env KAFKA_ADVERTISED_HOST_NAME&#x3D;HOST_IP \\ --env KAFKA_ADVERTISED_PORT&#x3D;9092 \\ --volume &#x2F;etc&#x2F;localtime:&#x2F;etc&#x2F;localtime \\ wurstmeister&#x2F;kafka:latest 进去容器 123docker exec -it &lt;ID&gt; &#x2F;bash&#x2F;shcd &#x2F;opt&#x2F;kafka**&#x2F;bin 创建topic 名为test kafka-topics.sh –create –zookeeper zookeeper:2181 –replication-factor 1 –partitions 1 –topic test 列出所有的topic kafka-topics.sh –list –zookeeper zookeeper:2181 创建生产者输入测试信息 kafka-console-producer.sh –broker-list localhost:9092 –topic test kafka-console-consumer.sh –bootstrap-server localhost:9092 –topic test –from-beginning tips需要将 KAFKA_ADVERTISED_HOST_NAME=HOST_IP 改为 docker所在的ip 在cmak中是看不到 topic.需要 用3.3.1 的命令去创建","categories":[],"tags":[{"name":"docker","slug":"docker","permalink":"https://imlike.cc/tags/docker/"}]},{"title":"github的action上实例操作","slug":"github action 示例","date":"2021-03-03T12:00:21.000Z","updated":"2024-03-14T14:55:29.477Z","comments":true,"path":"50c36cff.html","link":"","permalink":"https://imlike.cc/50c36cff.html","excerpt":"","text":"https://51.ruyo.net/16876.html -&gt;&gt;&gt;&gt;&gt; https://github.com/bigleek/UnicomTask https://51.ruyo.net/16050.html -&gt;&gt;&gt;&gt;&gt;&gt; https://github.com/bigleek/Cloud189Checkin-Actions github action是github提供的自动化工具能对你参考的代码解析编译构建操作，也可以衍生出其他服务，之前是pro用户才能使用，在GitHub被微软收购后财大气粗的放出的重量级服务，期间很多人滥用action的虚拟机的环境去挖矿 创建了workflow后在仓库会有个**.github/workflows** 的文件夹，里面可以将敏感信息写到下面的环境变量中 设置action的环境变量的位置 笔者目前对github action有两个用途 1、主要是定时任务和提醒 2、主要是不是GitHub仓库的静态资源到腾讯的cloudbase中 现在先讨论第一种的使用方法 首先创建GitHub的任意仓库并 TencentCloudBase/cloudbase-action: 云开发 Tencent CloudBase Github Action 可以将 Github 项目自动部署到云开发环境，基于 CloudBase Framework，支持静态网站、云函数、云托管、小程序等项目 创建对应的用户并设置权限 授予文件类的权限否则用该用户上传文件时GitHub action会报没有权限","categories":[],"tags":[]},{"title":"分布式事务","slug":"分布式事务","date":"2021-03-03T02:05:55.000Z","updated":"2024-03-14T14:55:29.483Z","comments":true,"path":"cdcc4eae.html","link":"","permalink":"https://imlike.cc/cdcc4eae.html","excerpt":"","text":"转载 自 掘金: https://juejin.cn/post/6844903573667446797常用的分布式事务解决方案 众所周知，数据库能实现本地事务，也就是在同一个数据库中，你可以允许一组操作要么全都正确执行，要么全都不执行。这里特别强调了本地事务，也就是目前的数据库只能支持同一个数据库中的事务。但现在的系统往往采用微服务架构，业务系统拥有独立的数据库，因此就出现了跨多个数据库的事务需求，这种事务即为“分布式事务”。那么在目前数据库不支持跨库事务的情况下，我们应该如何实现分布式事务呢？本文首先会为大家梳理分布式事务的基本概念和理论基础，然后介绍几种目前常用的分布式事务解决方案。废话不多说，那就开始吧～ 什么是事务？事务由一组操作构成，我们希望这组操作能够全部正确执行，如果这一组操作中的任意一个步骤发生错误，那么就需要回滚之前已经完成的操作。也就是同一个事务中的所有操作，要么全都正确执行，要么全都不要执行。 事务的四大特性 ACID说到事务，就不得不提一下事务著名的四大特性。 原子性 原子性要求，事务是一个不可分割的执行单元，事务中的所有操作要么全都执行，要么全都不执行。 一致性 一致性要求，事务在开始前和结束后，数据库的完整性约束没有被破坏。 隔离性 事务的执行是相互独立的，它们不会相互干扰，一个事务不会看到另一个正在运行过程中的事务的数据。 持久性 持久性要求，一个事务完成之后，事务的执行结果必须是持久化保存的。即使数据库发生崩溃，在数据库恢复后事务提交的结果仍然不会丢失。 注意：事务只能保证数据库的高可靠性，即数据库本身发生问题后，事务提交后的数据仍然能恢复；而如果不是数据库本身的故障，如硬盘损坏了，那么事务提交的数据可能就丢失了。这属于『高可用性』的范畴。因此，事务只能保证数据库的『高可靠性』，而『高可用性』需要整个系统共同配合实现。 事务的隔离级别这里扩展一下，对事务的隔离性做一个详细的解释。 在事务的四大特性ACID中，要求的隔离性是一种严格意义上的隔离，也就是多个事务是串行执行的，彼此之间不会受到任何干扰。这确实能够完全保证数据的安全性，但在实际业务系统中，这种方式性能不高。因此，数据库定义了四种隔离级别，隔离级别和数据库的性能是呈反比的，隔离级别越低，数据库性能越高，而隔离级别越高，数据库性能越差。 事务并发执行会出现的问题我们先来看一下在不同的隔离级别下，数据库可能会出现的问题： 更新丢失 当有两个并发执行的事务，更新同一行数据，那么有可能一个事务会把另一个事务的更新覆盖掉。 当数据库没有加任何锁操作的情况下会发生。 脏读 一个事务读到另一个尚未提交的事务中的数据。 该数据可能会被回滚从而失效。 如果第一个事务拿着失效的数据去处理那就发生错误了。 不可重复读 不可重复度的含义：一个事务对同一行数据读了两次，却得到了不同的结果。它具体分为如下两种情况： 虚读：在事务1两次读取同一记录的过程中，事务2对该记录进行了修改，从而事务1第二次读到了不一样的记录。 幻读：事务1在两次查询的过程中，事务2对该表进行了插入、删除操作，从而事务1第二次查询的结果发生了变化。 不可重复读 与 脏读 的区别？ 脏读读到的是尚未提交的数据，而不可重复读读到的是已经提交的数据，只不过在两次读的过程中数据被另一个事务改过了。 数据库的四种隔离级别数据库一共有如下四种隔离级别： Read uncommitted 读未提交 在该级别下，一个事务对一行数据修改的过程中，不允许另一个事务对该行数据进行修改，但允许另一个事务对该行数据读。 因此本级别下，不会出现更新丢失，但会出现脏读、不可重复读。 Read committed 读提交 在该级别下，未提交的写事务不允许其他事务访问该行，因此不会出现脏读；但是读取数据的事务允许其他事务的访问该行数据，因此会出现不可重复读的情况。 Repeatable read 重复读 在该级别下，读事务禁止写事务，但允许读事务，因此不会出现同一事务两次读到不同的数据的情况（不可重复读），且写事务禁止其他一切事务。 Serializable 序列化 该级别要求所有事务都必须串行执行，因此能避免一切因并发引起的问题，但效率很低。 隔离级别越高，越能保证数据的完整性和一致性，但是对并发性能的影响也越大。对于多数应用程序，可以优先考虑把数据库系统的隔离级别设为Read Committed。它能够避免脏读取，而且具有较好的并发性能。尽管它会导致不可重复读、幻读和第二类丢失更新这些并发问题，在可能出现这类问题的个别场合，可以由应用程序采用悲观锁或乐观锁来控制。 什么是分布式事务？到此为止，所介绍的事务都是基于单数据库的本地事务，目前的数据库仅支持单库事务，并不支持跨库事务。而随着微服务架构的普及，一个大型业务系统往往由若干个子系统构成，这些子系统又拥有各自独立的数据库。往往一个业务流程需要由多个子系统共同完成，而且这些操作可能需要在一个事务中完成。在微服务系统中，这些业务场景是普遍存在的。此时，我们就需要在数据库之上通过某种手段，实现支持跨数据库的事务支持，这也就是大家常说的“分布式事务”。 这里举一个分布式事务的典型例子——用户下单过程。 当我们的系统采用了微服务架构后，一个电商系统往往被拆分成如下几个子系统：商品系统、订单系统、支付系统、积分系统等。整个下单的过程如下： 用户通过商品系统浏览商品，他看中了某一项商品，便点击下单 此时订单系统会生成一条订单 订单创建成功后，支付系统提供支付功能 当支付完成后，由积分系统为该用户增加积分 上述步骤2、3、4需要在一个事务中完成。对于传统单体应用而言，实现事务非常简单，只需将这三个步骤放在一个方法A中，再用Spring的@Transactional注解标识该方法即可。Spring通过数据库的事务支持，保证这些步骤要么全都执行完成，要么全都不执行。但在这个微服务架构中，这三个步骤涉及三个系统，涉及三个数据库，此时我们必须在数据库和应用系统之间，通过某项黑科技，实现分布式事务的支持。 CAP理论CAP理论说的是：在一个分布式系统中，最多只能满足C、A、P中的两个需求。 CAP的含义： C：Consistency 一致性 同一数据的多个副本是否实时相同。 A：Availability 可用性 可用性：一定时间内 &amp; 系统返回一个明确的结果 则称为该系统可用。 P：Partition tolerance 分区容错性 将同一服务分布在多个系统中，从而保证某一个系统宕机，仍然有其他系统提供相同的服务。 CAP理论告诉我们，在分布式系统中，C、A、P三个条件中我们最多只能选择两个。那么问题来了，究竟选择哪两个条件较为合适呢？ 对于一个业务系统来说，可用性和分区容错性是必须要满足的两个条件，并且这两者是相辅相成的。业务系统之所以使用分布式系统，主要原因有两个： 提升整体性能 当业务量猛增，单个服务器已经无法满足我们的业务需求的时候，就需要使用分布式系统，使用多个节点提供相同的功能，从而整体上提升系统的性能，这就是使用分布式系统的第一个原因。 实现分区容错性 单一节点 或 多个节点处于相同的网络环境下，那么会存在一定的风险，万一该机房断电、该地区发生自然灾害，那么业务系统就全面瘫痪了。为了防止这一问题，采用分布式系统，将多个子系统分布在不同的地域、不同的机房中，从而保证系统高可用性。 这说明分区容错性是分布式系统的根本，如果分区容错性不能满足，那使用分布式系统将失去意义。 此外，可用性对业务系统也尤为重要。在大谈用户体验的今天，如果业务系统时常出现“系统异常”、响应时间过长等情况，这使得用户对系统的好感度大打折扣，在互联网行业竞争激烈的今天，相同领域的竞争者不甚枚举，系统的间歇性不可用会立马导致用户流向竞争对手。因此，我们只能通过牺牲一致性来换取系统的可用性和分区容错性。这也就是下面要介绍的BASE理论。 BASE理论CAP理论告诉我们一个悲惨但不得不接受的事实——我们只能在C、A、P中选择两个条件。而对于业务系统而言，我们往往选择牺牲一致性来换取系统的可用性和分区容错性。不过这里要指出的是，所谓的“牺牲一致性”并不是完全放弃数据一致性，而是牺牲强一致性换取弱一致性。下面来介绍下BASE理论。 BA：Basic Available 基本可用 整个系统在某些不可抗力的情况下，仍然能够保证“可用性”，即一定时间内仍然能够返回一个明确的结果。只不过“基本可用”和“高可用”的区别是： “一定时间”可以适当延长 当举行大促时，响应时间可以适当延长 给部分用户返回一个降级页面 给部分用户直接返回一个降级页面，从而缓解服务器压力。但要注意，返回降级页面仍然是返回明确结果。 S：Soft State：柔性状态 同一数据的不同副本的状态，可以不需要实时一致。 E：Eventual Consisstency：最终一致性 同一数据的不同副本的状态，可以不需要实时一致，但一定要保证经过一定时间后仍然是一致的。 酸碱平衡ACID能够保证事务的强一致性，即数据是实时一致的。这在本地事务中是没有问题的，在分布式事务中，强一致性会极大影响分布式系统的性能，因此分布式系统中遵循BASE理论即可。但分布式系统的不同业务场景对一致性的要求也不同。如交易场景下，就要求强一致性，此时就需要遵循ACID理论，而在注册成功后发送短信验证码等场景下，并不需要实时一致，因此遵循BASE理论即可。因此要根据具体业务场景，在ACID和BASE之间寻求平衡。 分布式事务协议下面介绍几种实现分布式事务的协议。 两阶段提交协议 2PC分布式系统的一个难点是如何保证架构下多个节点在进行事务性操作的时候保持一致性。为实现这个目的，二阶段提交算法的成立基于以下假设： 该分布式系统中，存在一个节点作为协调者(Coordinator)，其他节点作为参与者(Cohorts)。且节点之间可以进行网络通信。 所有节点都采用预写式日志，且日志被写入后即被保持在可靠的存储设备上，即使节点损坏不会导致日志数据的消失。 所有节点不会永久性损坏，即使损坏后仍然可以恢复。 1. 第一阶段（投票阶段）： 协调者节点向所有参与者节点询问是否可以执行提交操作(vote)，并开始等待各参与者节点的响应。 参与者节点执行询问发起为止的所有事务操作，并将Undo信息和Redo信息写入日志。（注意：若成功这里其实每个参与者已经执行了事务操作） 各参与者节点响应协调者节点发起的询问。如果参与者节点的事务操作实际执行成功，则它返回一个”同意”消息；如果参与者节点的事务操作实际执行失败，则它返回一个”中止”消息。 2. 第二阶段（提交执行阶段）： 当协调者节点从所有参与者节点获得的相应消息都为”同意”时： 协调者节点向所有参与者节点发出”正式提交(commit)”的请求。 参与者节点正式完成操作，并释放在整个事务期间内占用的资源。 参与者节点向协调者节点发送”完成”消息。 协调者节点受到所有参与者节点反馈的”完成”消息后，完成事务。 如果任一参与者节点在第一阶段返回的响应消息为”中止”，或者 协调者节点在第一阶段的询问超时之前无法获取所有参与者节点的响应消息时： 协调者节点向所有参与者节点发出”回滚操作(rollback)”的请求。 参与者节点利用之前写入的Undo信息执行回滚，并释放在整个事务期间内占用的资源。 参与者节点向协调者节点发送”回滚完成”消息。 协调者节点受到所有参与者节点反馈的”回滚完成”消息后，取消事务。 不管最后结果如何，第二阶段都会结束当前事务。 二阶段提交看起来确实能够提供原子性的操作，但是不幸的事，二阶段提交还是有几个缺点的： 执行过程中，所有参与节点都是事务阻塞型的。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。 参与者发生故障。协调者需要给每个参与者额外指定超时机制，超时后整个事务失败。（没有多少容错机制） 协调者发生故障。参与者会一直阻塞下去。需要额外的备机进行容错。（这个可以依赖后面要讲的Paxos协议实现HA） 二阶段无法解决的问题：协调者再发出commit消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了。那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道事务是否被已经提交。 为此，Dale Skeen和Michael Stonebraker在“A Formal Model of Crash Recovery in a Distributed System”中提出了三阶段提交协议（3PC）。 三阶段提交协议 3PC与两阶段提交不同的是，三阶段提交有两个改动点。 引入超时机制。同时在协调者和参与者中都引入超时机制。 在第一阶段和第二阶段中插入一个准备阶段。保证了在最后提交阶段之前各参与节点的状态是一致的。 也就是说，除了引入超时机制之外，3PC把2PC的准备阶段再次一分为二，这样三阶段提交就有CanCommit、PreCommit、DoCommit三个阶段。 1. CanCommit阶段 3PC的CanCommit阶段其实和2PC的准备阶段很像。协调者向参与者发送commit请求，参与者如果可以提交就返回Yes响应，否则返回No响应。 事务询问 协调者向参与者发送CanCommit请求。询问是否可以执行事务提交操作。然后开始等待参与者的响应。 响应反馈 参与者接到CanCommit请求之后，正常情况下，如果其自身认为可以顺利执行事务，则返回Yes响应，并进入预备状态。否则反馈No 2. PreCommit阶段 协调者根据参与者的反应情况来决定是否可以记性事务的PreCommit操作。根据响应情况，有以下两种可能。 假如协调者从所有的参与者获得的反馈都是Yes响应，那么就会执行事务的预执行。 发送预提交请求 协调者向参与者发送PreCommit请求，并进入Prepared阶段。 事务预提交 参与者接收到PreCommit请求后，会执行事务操作，并将undo和redo信息记录到事务日志中。 响应反馈 如果参与者成功的执行了事务操作，则返回ACK响应，同时开始等待最终指令。 假如有任何一个参与者向协调者发送了No响应，或者等待超时之后，协调者都没有接到参与者的响应，那么就执行事务的中断。 发送中断请求 协调者向所有参与者发送abort请求。 中断事务 参与者收到来自协调者的abort请求之后（或超时之后，仍未收到协调者的请求），执行事务的中断。 3. doCommit阶段 该阶段进行真正的事务提交，也可以分为以下两种情况。 该阶段进行真正的事务提交，也可以分为以下两种情况。 3.1 执行提交 发送提交请求 协调接收到参与者发送的ACK响应，那么他将从预提交状态进入到提交状态。并向所有参与者发送doCommit请求。 事务提交 参与者接收到doCommit请求之后，执行正式的事务提交。并在完成事务提交之后释放所有事务资源。 响应反馈 事务提交完之后，向协调者发送Ack响应。 完成事务 协调者接收到所有参与者的ack响应之后，完成事务。 3.2 中断事务 协调者没有接收到参与者发送的ACK响应（可能是接受者发送的不是ACK响应，也可能响应超时），那么就会执行中断事务。 发送中断请求 协调者向所有参与者发送abort请求 事务回滚 参与者接收到abort请求之后，利用其在阶段二记录的undo信息来执行事务的回滚操作，并在完成回滚之后释放所有的事务资源。 反馈结果 参与者完成事务回滚之后，向协调者发送ACK消息 中断事务 协调者接收到参与者反馈的ACK消息之后，执行事务的中断。 分布式事务的解决方案分布式事务的解决方案有如下几种： 全局消息 基于可靠消息服务的分布式事务 TCC 最大努力通知 方案1：全局事务（DTP模型）全局事务基于DTP模型实现。DTP是由X/Open组织提出的一种分布式事务模型——X/Open Distributed Transaction Processing Reference Model。它规定了要实现分布式事务，需要三种角色： AP：Application 应用系统 它就是我们开发的业务系统，在我们开发的过程中，可以使用资源管理器提供的事务接口来实现分布式事务。 TM：Transaction Manager 事务管理器 分布式事务的实现由事务管理器来完成，它会提供分布式事务的操作接口供我们的业务系统调用。这些接口称为TX接口。 事务管理器还管理着所有的资源管理器，通过它们提供的XA接口来同一调度这些资源管理器，以实现分布式事务。 DTP只是一套实现分布式事务的规范，并没有定义具体如何实现分布式事务，TM可以采用2PC、3PC、Paxos等协议实现分布式事务。 RM：Resource Manager 资源管理器 能够提供数据服务的对象都可以是资源管理器，比如：数据库、消息中间件、缓存等。大部分场景下，数据库即为分布式事务中的资源管理器。 资源管理器能够提供单数据库的事务能力，它们通过XA接口，将本数据库的提交、回滚等能力提供给事务管理器调用，以帮助事务管理器实现分布式的事务管理。 XA是DTP模型定义的接口，用于向事务管理器提供该资源管理器(该数据库)的提交、回滚等能力。 DTP只是一套实现分布式事务的规范，RM具体的实现是由数据库厂商来完成的。 有没有基于DTP模型的分布式事务中间件？ DTP模型有啥优缺点？ 方案2：基于可靠消息服务的分布式事务这种实现分布式事务的方式需要通过消息中间件来实现。假设有A和B两个系统，分别可以处理任务A和任务B。此时系统A中存在一个业务流程，需要将任务A和任务B在同一个事务中处理。下面来介绍基于消息中间件来实现这种分布式事务。 ![title](data:image/svg+xml;utf8,) 在系统A处理任务A前，首先向消息中间件发送一条消息 消息中间件收到后将该条消息持久化，但并不投递。此时下游系统B仍然不知道该条消息的存在。 消息中间件持久化成功后，便向系统A返回一个确认应答； 系统A收到确认应答后，则可以开始处理任务A； 任务A处理完成后，向消息中间件发送Commit请求。该请求发送完成后，对系统A而言，该事务的处理过程就结束了，此时它可以处理别的任务了。 但commit消息可能会在传输途中丢失，从而消息中间件并不会向系统B投递这条消息，从而系统就会出现不一致性。这个问题由消息中间件的事务回查机制完成，下文会介绍。 消息中间件收到Commit指令后，便向系统B投递该消息，从而触发任务B的执行； 当任务B执行完成后，系统B向消息中间件返回一个确认应答，告诉消息中间件该消息已经成功消费，此时，这个分布式事务完成。 上述过程可以得出如下几个结论： 消息中间件扮演者分布式事务协调者的角色。 系统A完成任务A后，到任务B执行完成之间，会存在一定的时间差。在这个时间差内，整个系统处于数据不一致的状态，但这短暂的不一致性是可以接受的，因为经过短暂的时间后，系统又可以保持数据一致性，满足BASE理论。 上述过程中，如果任务A处理失败，那么需要进入回滚流程，如下图所示： ![title](data:image/svg+xml;utf8,) 若系统A在处理任务A时失败，那么就会向消息中间件发送Rollback请求。和发送Commit请求一样，系统A发完之后便可以认为回滚已经完成，它便可以去做其他的事情。 消息中间件收到回滚请求后，直接将该消息丢弃，而不投递给系统B，从而不会触发系统B的任务B。 此时系统又处于一致性状态，因为任务A和任务B都没有执行。 上面所介绍的Commit和Rollback都属于理想情况，但在实际系统中，Commit和Rollback指令都有可能在传输途中丢失。那么当出现这种情况的时候，消息中间件是如何保证数据一致性呢？——答案就是超时询问机制。 ![title](data:image/svg+xml;utf8,) 系统A除了实现正常的业务流程外，还需提供一个事务询问的接口，供消息中间件调用。当消息中间件收到一条事务型消息后便开始计时，如果到了超时时间也没收到系统A发来的Commit或Rollback指令的话，就会主动调用系统A提供的事务询问接口询问该系统目前的状态。该接口会返回三种结果： 提交 若获得的状态是“提交”，则将该消息投递给系统B。 回滚 若获得的状态是“回滚”，则直接将条消息丢弃。 处理中 若获得的状态是“处理中”，则继续等待。 消息中间件的超时询问机制能够防止上游系统因在传输过程中丢失Commit/Rollback指令而导致的系统不一致情况，而且能降低上游系统的阻塞时间，上游系统只要发出Commit/Rollback指令后便可以处理其他任务，无需等待确认应答。而Commit/Rollback指令丢失的情况通过超时询问机制来弥补，这样大大降低上游系统的阻塞时间，提升系统的并发度。 下面来说一说消息投递过程的可靠性保证。 当上游系统执行完任务并向消息中间件提交了Commit指令后，便可以处理其他任务了，此时它可以认为事务已经完成，接下来消息中间件一定会保证消息被下游系统成功消费掉！那么这是怎么做到的呢？这由消息中间件的投递流程来保证。 消息中间件向下游系统投递完消息后便进入阻塞等待状态，下游系统便立即进行任务的处理，任务处理完成后便向消息中间件返回应答。消息中间件收到确认应答后便认为该事务处理完毕！ 如果消息在投递过程中丢失，或消息的确认应答在返回途中丢失，那么消息中间件在等待确认应答超时之后就会重新投递，直到下游消费者返回消费成功响应为止。当然，一般消息中间件可以设置消息重试的次数和时间间隔，比如：当第一次投递失败后，每隔五分钟重试一次，一共重试3次。如果重试3次之后仍然投递失败，那么这条消息就需要人工干预。 ![title](data:image/svg+xml;utf8,) ![title](data:image/svg+xml;utf8,) 有的同学可能要问：消息投递失败后为什么不回滚消息，而是不断尝试重新投递？ 这就涉及到整套分布式事务系统的实现成本问题。 我们知道，当系统A将向消息中间件发送Commit指令后，它便去做别的事情了。如果此时消息投递失败，需要回滚的话，就需要让系统A事先提供回滚接口，这无疑增加了额外的开发成本，业务系统的复杂度也将提高。对于一个业务系统的设计目标是，在保证性能的前提下，最大限度地降低系统复杂度，从而能够降低系统的运维成本。 不知大家是否发现，上游系统A向消息中间件提交Commit/Rollback消息采用的是异步方式，也就是当上游系统提交完消息后便可以去做别的事情，接下来提交、回滚就完全交给消息中间件来完成，并且完全信任消息中间件，认为它一定能正确地完成事务的提交或回滚。然而，消息中间件向下游系统投递消息的过程是同步的。也就是消息中间件将消息投递给下游系统后，它会阻塞等待，等下游系统成功处理完任务返回确认应答后才取消阻塞等待。为什么这两者在设计上是不一致的呢？ 首先，上游系统和消息中间件之间采用异步通信是为了提高系统并发度。业务系统直接和用户打交道，用户体验尤为重要，因此这种异步通信方式能够极大程度地降低用户等待时间。此外，异步通信相对于同步通信而言，没有了长时间的阻塞等待，因此系统的并发性也大大增加。但异步通信可能会引起Commit/Rollback指令丢失的问题，这就由消息中间件的超时询问机制来弥补。 那么，消息中间件和下游系统之间为什么要采用同步通信呢？ 异步能提升系统性能，但随之会增加系统复杂度；而同步虽然降低系统并发度，但实现成本较低。因此，在对并发度要求不是很高的情况下，或者服务器资源较为充裕的情况下，我们可以选择同步来降低系统的复杂度。 我们知道，消息中间件是一个独立于业务系统的第三方中间件，它不和任何业务系统产生直接的耦合，它也不和用户产生直接的关联，它一般部署在独立的服务器集群上，具有良好的可扩展性，所以不必太过于担心它的性能，如果处理速度无法满足我们的要求，可以增加机器来解决。而且，即使消息中间件处理速度有一定的延迟那也是可以接受的，因为前面所介绍的BASE理论就告诉我们了，我们追求的是最终一致性，而非实时一致性，因此消息中间件产生的时延导致事务短暂的不一致是可以接受的。 方案3：最大努力通知（定期校对）最大努力通知也被称为定期校对，其实在方案二中已经包含，这里再单独介绍，主要是为了知识体系的完整性。这种方案也需要消息中间件的参与，其过程如下： ![title](data:image/svg+xml;utf8,) 上游系统在完成任务后，向消息中间件同步地发送一条消息，确保消息中间件成功持久化这条消息，然后上游系统可以去做别的事情了； 消息中间件收到消息后负责将该消息同步投递给相应的下游系统，并触发下游系统的任务执行； 当下游系统处理成功后，向消息中间件反馈确认应答，消息中间件便可以将该条消息删除，从而该事务完成。 上面是一个理想化的过程，但在实际场景中，往往会出现如下几种意外情况： 消息中间件向下游系统投递消息失败 上游系统向消息中间件发送消息失败 对于第一种情况，消息中间件具有重试机制，我们可以在消息中间件中设置消息的重试次数和重试时间间隔，对于网络不稳定导致的消息投递失败的情况，往往重试几次后消息便可以成功投递，如果超过了重试的上限仍然投递失败，那么消息中间件不再投递该消息，而是记录在失败消息表中，消息中间件需要提供失败消息的查询接口，下游系统会定期查询失败消息，并将其消费，这就是所谓的“定期校对”。 如果重复投递和定期校对都不能解决问题，往往是因为下游系统出现了严重的错误，此时就需要人工干预。 对于第二种情况，需要在上游系统中建立消息重发机制。可以在上游系统建立一张本地消息表，并将 任务处理过程 和 向本地消息表中插入消息 这两个步骤放在一个本地事务中完成。如果向本地消息表插入消息失败，那么就会触发回滚，之前的任务处理结果就会被取消。如果这量步都执行成功，那么该本地事务就完成了。接下来会有一个专门的消息发送者不断地发送本地消息表中的消息，如果发送失败它会返回重试。当然，也要给消息发送者设置重试的上限，一般而言，达到重试上限仍然发送失败，那就意味着消息中间件出现严重的问题，此时也只有人工干预才能解决问题。 对于不支持事务型消息的消息中间件，如果要实现分布式事务的话，就可以采用这种方式。它能够通过重试机制+定期校对实现分布式事务，但相比于第二种方案，它达到数据一致性的周期较长，而且还需要在上游系统中实现消息重试发布机制，以确保消息成功发布给消息中间件，这无疑增加了业务系统的开发成本，使得业务系统不够纯粹，并且这些额外的业务逻辑无疑会占用业务系统的硬件资源，从而影响性能。 因此，尽量选择支持事务型消息的消息中间件来实现分布式事务，如RocketMQ。 方案4：TCC（两阶段型、补偿型）TCC即为Try Confirm Cancel，它属于补偿型分布式事务。顾名思义，TCC实现分布式事务一共有三个步骤： Try：尝试待执行的业务 这个过程并未执行业务，只是完成所有业务的一致性检查，并预留好执行所需的全部资源 Confirm：执行业务 这个过程真正开始执行业务，由于Try阶段已经完成了一致性检查，因此本过程直接执行，而不做任何检查。并且在执行的过程中，会使用到Try阶段预留的业务资源。 Cancel：取消执行的业务 若业务执行失败，则进入Cancel阶段，它会释放所有占用的业务资源，并回滚Confirm阶段执行的操作。 下面以一个转账的例子来解释下TCC实现分布式事务的过程。 假设用户A用他的账户余额给用户B发一个100元的红包，并且余额系统和红包系统是两个独立的系统。 Try 创建一条转账流水，并将流水的状态设为交易中 将用户A的账户中扣除100元（预留业务资源） Try成功之后，便进入Confirm阶段 Try过程发生任何异常，均进入Cancel阶段 Confirm 向B用户的红包账户中增加100元 将流水的状态设为交易已完成 Confirm过程发生任何异常，均进入Cancel阶段 Confirm过程执行成功，则该事务结束 Cancel 将用户A的账户增加100元 将流水的状态设为交易失败 在传统事务机制中，业务逻辑的执行和事务的处理，是在不同的阶段由不同的部件来完成的：业务逻辑部分访问资源实现数据存储，其处理是由业务系统负责；事务处理部分通过协调资源管理器以实现事务管理，其处理由事务管理器来负责。二者没有太多交互的地方，所以，传统事务管理器的事务处理逻辑，仅需要着眼于事务完成（commit/rollback）阶段，而不必关注业务执行阶段。 TCC全局事务必须基于RM本地事务来实现全局事务TCC服务是由Try/Confirm/Cancel业务构成的， 其Try/Confirm/Cancel业务在执行时，会访问资源管理器（Resource Manager，下文简称RM）来存取数据。这些存取操作，必须要参与RM本地事务，以使其更改的数据要么都commit，要么都rollback。 这一点不难理解，考虑一下如下场景： 假设图中的服务B没有基于RM本地事务（以RDBS为例，可通过设置auto-commit为true来模拟），那么一旦[B:Try]操作中途执行失败，TCC事务框架后续决定回滚全局事务时，该[B:Cancel]则需要判断[B:Try]中哪些操作已经写到DB、哪些操作还没有写到DB：假设[B:Try]业务有5个写库操作，[B:Cancel]业务则需要逐个判断这5个操作是否生效，并将生效的操作执行反向操作。 不幸的是，由于[B:Cancel]业务也有n（0&lt;=n&lt;=5）个反向的写库操作，此时一旦[B:Cancel]也中途出错，则后续的[B:Cancel]执行任务更加繁重。因为，相比第一次[B:Cancel]操作，后续的[B:Cancel]操作还需要判断先前的[B:Cancel]操作的n（0&lt;=n&lt;=5）个写库中哪几个已经执行、哪几个还没有执行，这就涉及到了幂等性问题。而对幂等性的保障，又很可能还需要涉及额外的写库操作，该写库操作又会因为没有RM本地事务的支持而存在类似问题。。。可想而知，如果不基于RM本地事务，TCC事务框架是无法有效的管理TCC全局事务的。 反之，基于RM本地事务的TCC事务，这种情况则会很容易处理：[B:Try]操作中途执行失败，TCC事务框架将其参与RM本地事务直接rollback即可。后续TCC事务框架决定回滚全局事务时，在知道“[B:Try]操作涉及的RM本地事务已经rollback”的情况下，根本无需执行[B:Cancel]操作。 换句话说，基于RM本地事务实现TCC事务框架时，一个TCC型服务的cancel业务要么执行，要么不执行，不需要考虑部分执行的情况。 TCC事务框架应该提供Confirm/Cancel服务的幂等性保障一般认为，服务的幂等性，是指针对同一个服务的多次(n&gt;1)请求和对它的单次(n=1)请求，二者具有相同的副作用。 在TCC事务模型中，Confirm/Cancel业务可能会被重复调用，其原因很多。比如，全局事务在提交/回滚时会调用各TCC服务的Confirm/Cancel业务逻辑。执行这些Confirm/Cancel业务时，可能会出现如网络中断的故障而使得全局事务不能完成。因此，故障恢复机制后续仍然会重新提交/回滚这些未完成的全局事务，这样就会再次调用参与该全局事务的各TCC服务的Confirm/Cancel业务逻辑。 既然Confirm/Cancel业务可能会被多次调用，就需要保障其幂等性。 那么，应该由TCC事务框架来提供幂等性保障？还是应该由业务系统自行来保障幂等性呢？ 个人认为，应该是由TCC事务框架来提供幂等性保障。如果仅仅只是极个别服务存在这个问题的话，那么由业务系统来负责也是可以的；然而，这是一类公共问题，毫无疑问，所有TCC服务的Confirm/Cancel业务存在幂等性问题。TCC服务的公共问题应该由TCC事务框架来解决；而且，考虑一下由业务系统来负责幂等性需要考虑的问题，就会发现，这无疑增大了业务系统的复杂度。 参考文献 大规模SOA系统中的分布事务处理_程立 Life beyond Distributed Transactions: an Apostate’s Opinion 关于如何实现一个TCC分布式事务框架的一点思考 How can a requestor ensure a consistent outcome across multiple, independent providers 关于分布式事务、两阶段提交协议、三阶提交协议 Three-phase commit protocol","categories":[],"tags":[]},{"title":"浏览器插件推荐(转载)","slug":"extentions","date":"2021-03-01T13:23:03.000Z","updated":"2024-03-14T14:55:29.477Z","comments":true,"path":"d4fc710d.html","link":"","permalink":"https://imlike.cc/d4fc710d.html","excerpt":"","text":"转载于 https://www.runningcheese.com/extensions(作者 奔跑中的奶酪 ) 扩展之于浏览器，就像 APP 之于智能手机。 浏览器扩展的数量成千上万，但真正好用的并不多，能被用户选择使用的其实也就 50个 左右，而真正安装到浏览器上的也不会超过 20 个。 此外，网上推荐扩展的文章也很多，但大多”没什么卵用“。 这类文章通常会喜欢标题党，动不动就是这个神器，那个黑科技，又或者喜欢推荐 Adblock Plus 这种入门拓展，还有就是错误地把扩展（或拓展）说成是插件，这就像给你推荐电脑的人，他总是把硬盘说成是内存。 下面是我总结的2020 年度最喜欢浏览器扩展，支持 Chrome、Firefox、Edge，预计能帮你解决 90% 的浏览器扩展需求。 我们需要什么样的扩展？作为一个有 6 年开发经验，试用超过 2000 多款扩展的用户，我经常在想，我们到底需要什么样的扩展？下面是我的两个理解： 1、用合适的工具做事由于浏览器应用层面的限制，扩展的功能是带着“镣铐”的，即便是玩出花来，拓展也没有办法穿透浏览器，如果某些功能，用软件来完成更好，别犹豫，直接用软件吧。 比如截图扩展 Nimbus Screen Capture、文字识别 Copyfish、视频下载 Video DownloadHelper，它们都已经是同类扩展中的佼佼者了，但还是没办法像软件那样的强大而全面。 比如截图软件可以用 FastStone Capture，文字识别软件用 天若ORC，视频下载软件用 IDM。（详细回复关键字 C07） 同样，如果某些功能用“油猴脚本”就能完成，那就没必要安装扩展了，用脚本的方法要来得更加轻便。 2、扩展在精不在多，一个扩展做好一件事我们总是喜欢试用各式各样的扩展，但大多数扩展其实都是伪需求，有没有其实区别不大。 真正有需求的是那些基础的、使用率高的、大多数人都适用的扩展，所以在扩展选择上，我们可以换个思路： 从需要出发，倒推浏览器扩展，然后在同类扩展中选择最好的那一款。 下面是我总结的 2020 年度最喜欢浏览器扩展，支持 Chrome、Firefox、Edge 三款浏览器，带 ★ 为推荐安装，如 Chrome 商店无法正常打开，可以到 https://crxdl.com 下载。 1、常用扩展 功能 介绍 安装 脚本管理 Tampermonkey，相比 油猴、暴力猴 更加稳定高效。★《相关文章》 样式管理 xStyle，相比 Stylish、Stylus 更为简洁，管理也更加方便。★《相关文章》 拨号页 InfinityTab，可能是目前最好的新标签页，但自带的图标有返利尾巴。★ 代理工具 Proxy SwitchyOmega ，方便简单，可以轻松管理和切换多个代理设置。★ 广告过滤 Ublock，简单好用，支持网页广告、视频广告的过滤。★《相关文章》 密码管理 BitWarden，免费、简单、多平台通用的密码管理工具。★《相关文章》 鼠标拖拽 GlitterDrag 是 Firefox 首选，Chrome 用 crxMouse，Edge 用 SuperDrag。★ 鼠标手势 用软件更好，Windows 用 Mouseinc，Mac 用 BetterAndBetter。《相关文章》 视频下载 用软件更好，Windows 用 IDM，Mac 用 NDM。★《相关文章》 下载管理 下载管理，接管自带的下载功能，简洁，没有多余功能。★（火狐自带） 二维码 二维码，将文本、链接、图片生成二维码，也可识别二维码图片。★ 用户代理 User-Agent Switcher and Manager，目前 UA 最全的一款用户代理扩展。 浏览器请求 Header Editor，拥有重定向、反盗链、用户代理修改等功能。★《相关文章》 RSS 阅读 用软件更好，Windows 用 irreader，Mac 用 Reeder。★《相关文章》 截图工具 用软件更好，Windows 用 FastStone Capture，Mac 用 iShot。《相关文章》 OCR识别 用软件更好，Windows 用 天若OCR，Mac 用 Text Scaner。《相关文章》 电子书 用软件更好，Windows 用 京东读书，Mac 用 CleanView。《相关文章》 2、标签管理 功能 简介（专题文章） 安装 稍候阅读 In My Pocket，标记没来得急读完的网页，支持多平台同步。★《相关文章》 会话管理 Tab Session Manager，手动或者自动保存浏览会话。★《相关文章》 快捷笔记 Sidebar Note，支持选中文字一键发送至笔记本，支持快捷键。@ 网页剪藏 Evernote Web Clipper，一键标注和保存完整的网页到 Evernote 上。 小号管理 SessionBox，用于同一网站登录多个账户。（火狐自带） 标签页恢复 Undo Close Tab，简单且高效的标签页恢复扩展。★ 标签页查看 Tree Style Tab，树状显示当前打开的网页，Chrome 用 Aerys。★ 标签页位置 Tab Position Options，调整标签页的打开位置。★ 内存控制 Auto Tab Discard，自动冻结不活动的标签页来节省内存占用。★ 新标签页覆盖 New Tab Override，修改打开新标签页的内容。 防止关闭 LiveOn，关闭最后一个标签页时不关闭浏览器。★（火狐自带） 打开多个链接 Open Multiple URLs，但推荐用网站 openmultipleurl.com 来打开。 标签页复制 Copy as Markdown，复制标签页链接为 Markdown 格式，支持快捷键。★ 浏览器调用 Open With，调用其他浏览器或者配置打开当前网页。《相关文章》 链接多选 Snap Links Plus，使用鼠标右键可框选多个链接和复选框。《相关文章》 3、书签管理 名称 简介 安装 书签检测 Bookmarks Organizer，检测失效与重复的书签。《相关文章》 书签图标刷新 Checkmarks，在侧边栏中打开，然后依次打开刷新图标。（火狐特有） 快速添加书签 Bookmark Tab Here，书签右键菜单上添加“**添加到此…**”的选项。（火狐特有） 书签更新 Update Bookmark，直接在书签的右键菜单中修改标题和地址。（火狐特有） 书签搜索 Holmes，让 Chrome 像 Firefox 一样支持输入 * 后搜索书签。（火狐自带） 4、搜索增强 名称 简介（专题文章） 安装 右键搜索 Selection Context Search，功能最全的右键搜索扩展。★《相关文章》 站内搜索 Selection Context Search，在设置里增加“站内搜索”的选项。★ （同上） 以图搜图 Selection Context Search，在设置里增加“以图搜图”选项。★（同上） 粘贴并搜索 Paste n’ Go，搜索框右键里增加“粘贴并搜索”的选项。 右键限制 用脚本更好，解除大部分网站禁止复制、选择、右键菜单的限制。 搜索切换 用脚本更好，在各个搜索引擎之间跳转，并有友好的自定义设置菜单。 多重搜索 用脚本更好，通过“小书签”然后用“关键字”来实现。《相关文章》 5、英语阅读 名称 简介 翻译工具 沙拉查词，多功能强大的聚合翻译扩展。★《相关文章》 网页翻译 翻译网页，Chrome、Edge 自带全文翻译，也可以用“彩云小译”。★ 词汇发现 单词发现者，高亮网页中的英语单词，可自定义单词难度，扩大您的词汇。 语法检测 Grammarly，检测并自动纠正语法错误。 文本校对 LanguageTool，语法、文风和拼字校对工具。 6、阅读辅助 名称 简介（专题文章） 安装 字体渲染 Font Rendering，解决 Chrome、Edge 字体发虚的问题。★（火狐自带） 雅黑字体 替换中文字体为雅黑，同时避免影响英文部分。★（Mac不必安装） 手形工具 ScrollAnywhere，按住鼠标左键，像手机屏幕一样地滑动网页。★ 快速下一页 NextPage，页尾处按空格自动翻页，按 N 键下一页，按 P 键上一页。★ 阅读模式 简悦，比自带模式模式更加优雅的阅读扩展。 黑夜模式 Dark Mode，将网页变成黑夜阅读模式。 亮度调整 用脚本更好，通过“小书签”然后用“关键字”来实现。《相关文章》 侧边栏浏览 Side View，在侧边栏中查看当前网页。（火狐特有） 访问限制 Block Site，限制访问一些网站，还有上网时间，适合工作的时使用。 维基阅读 Wikiwand，让排版老旧的 Wikipedia 有一个全新的界面。 7、图片浏览 名称 简介（专题文章） 安装 图片查看助手 浮图秀，按 Alt 键显示高清大图，可配合脚本 PicViewer 一起使用。★ 图片保存工具 SAVE IN…，一键保存图片到指定位置，图片管理利器。★《相关文章》 图片批量下载 ImageAssistant，嗅探图片、图片筛选、批量下载等功能。 图床工具 微博图床，使用前需要登录微博网页版，还支持批量上传。 8、音乐收听 名称 简介（专题文章） 安装 音乐收听 Listen 1，聚合来自网易云音乐、QQ音乐、虾米等网站的歌曲。 歌词展示 Lyrics Here，在 Youtube，Spotify 等音乐网站上显示当前播放音乐的歌词。 广播收听 Search to Play the Song，可以说是 Listen 1的简洁版，还能听广播。 歌曲识别 AHA Music，识别当前浏览器播放中的歌曲，还支持本地音乐。 9、视频观看 名称 简介（专题文章） 安装 视频放大 用脚本更好，将视频放大填充到网页全屏。 视频弹窗 Popup Tool，视频小窗播放，支持弹幕，Chrome 用 Separate Window。 弹幕过滤器 Pakku，哔哩哔哩弹幕过滤器，屏蔽相似弹幕。 U2B双语字幕 用脚本更好，让 YouTube 显示双语字幕。 U2B频道屏蔽 BlockTube，屏蔽不想看的视频频道，包括首页用和搜索页面。 刷抖音 Web for TikTok，在线刷 TikTok，就像用手机刷一样。 10、小众扩展 名称 简介 安装 Cookie 编辑 EditThisCookie，编辑 Cookie，比如可以设置帐号永不退出。 Aria2 下载 Aria2 manager，用于导出和自动拦截下载到 Aria2 下载器。 网页监测 Distill Web Monitor，用于跟踪监测网页更新。 历史记录分析 History Master，更加友好的可视化历史记录分析工具。 反反广告 Anti Adblock Detector，去除 Adblock 的检测拦截。 跳过验证码 Buster，跳过谷歌的验证码。 谷歌访问助手 iGuge，用于访问 Google 相关的网站，其他的则需要付费购买。 网页全文索引 Falcon，地址栏输入 f 查找历史浏览网页里的内容。 自选基金助手 自选基金助手，小窗口展示“天天基金网”的数据。 11、开发工具 名称 简介 加密文件编辑 mozlz4-edit，修改加密配置文件，比如 search.json.mozlz4。（火狐特有） 快捷键扩展 Vimium，像 VIM 一样通过键盘来操控浏览器，按 F 键显示快捷键。 GitHub 阅读 Octotree，让Github仓库的代码以树状的形式展示。 自动化工具 Wildfire，让工作实现操作自动化，可以用于替代 iMacros。 取色器 Color Picker，取色器和颜色调整，适合网页开发。 扩展源码查看 Extension source viewer，查看扩展的源码，还可以使用 在线版本 屏幕测距 Dimensions，一个简单实用的网页元素测距工具。 字体检测 Font Finder，一键检查当前网页文字所使用的字体。 网站技术分析 Wappalyzer，探测当前网站所使用的系统构架、站点环境、服务器配置等。 网页加载速度 LoadTime，测试网页加载时间。","categories":[],"tags":[]},{"title":"windows 上的工具清单","slug":"windows上的工具清单","date":"2021-02-26T07:47:42.000Z","updated":"2024-03-14T14:55:29.483Z","comments":true,"path":"839087f3.html","link":"","permalink":"https://imlike.cc/839087f3.html","excerpt":"","text":"https://blog.rxliuli.com/p/5b565be2d93c4e03871e42aff60e2a9a/# 全文转载 本文最后更新于：2021年2月22日 上午 但弄雕虫之技，尘芥草莽，于泥泞自勉…… 开发MS VSCode：前端标准 IDE[便携版], [开源], [跨平台], [免费] 具体可以参考：官网, GitHub, GitBook 中文介绍 MS VSCode 是微软出品的一个跨平台，基于 Web 的，免费的开源文本编辑器。目前已经使用了半年多了，对其的了解也更加深入。就目前看来，VSCode 应该是前端生态最好的 IDE 了。作为一个文本编辑器，大多数的文本文件都有语法支持的扩展真的太棒了，这点要强过专门的 IDE。 主要优点： 开箱即用：基本上下载完就能开始使用了，不需要太多自定义的配置 多语言支持：这是很多文本编辑器/IDE 都没有做到的，例如下面吾辈最喜欢的 IDEA 到目前为止官方仍然只支持英文，但吾辈觉得本土化之后才能对萌新更加友好（门槛低，学习曲线平缓） 插件丰富：目前吾辈使用 VSCode 主要用于前端，编辑 Markdown 文章，一些系统/程序的配置文件等，安装了插件之后使用对各种功能的支持还算不错。 缺点： 很明显，MS 的东西最后会不会变成全家桶/流氓谁也不清楚，虽然去年貌似 MS 在 GitHub 上的贡献量超过了 Google，但想想 MS 的开源黑历史。。。而且，作为一个专门的编程编辑器，有许多的细节上还是很粗糙（貌似开源项目的通病），相比于专业的 IDE（例如 WebStorm）还是相差不少。下面举一些例子 本地文件历史: 没有加入到 Git 管理中的文件丢失，血与泪的教训啊 单词检查: 英文单词写错了还不知道，萌萌哒！ 代码跳转功能很弱: 专业 IDE 里几乎所有有关连的东西都是可以跳转的。例如在 CSS 中查看使用 class 的引用列表，在 HTML 中跳转到 CSS 的 class 定义处，HTML 中有代码提示 CSS 类。JS 中查看函数的引用列表，查看函数的定义处，查看函数的实现（老实说 VSCode 基于 ts 实现的代码提示感觉真是在下一盘大棋。。。） Git 支持: VSCode 中 Git 支持还不成熟，并没有像 IDE 中那么方便与强大 HTML 标签修改的时候没有自动修改对应的标签: 纯粹是细节性问题 TODO 支持: 对 TODO 标签没有支持 常用的 Plugin auto-rename-tag: 自动重命名对应标签 code-settings-sync: VSCode 设置同步 dotenv: env 语法支持 es7-react-js-snippets: react es7 的代码片段 gitlens: Git 增强插件 inline-html: js 模板字符串中的 HTML 代码提示 intellij-idea-keybindings: IDEA 快捷键映射 language-stylus: stylus 语言支持 markdown-all-in-one: Markdown 编辑主要插件 markdown-pdf: 将 Markdown 导出 PDF 文档 material-icon-theme: material 风格的图标 path-intellisense: 路径提示 prettier-vscode: 配合 ESLint 做代码格式自动修复 quokka-vscode: 持续监听/运行 js 文件 stylus-supremacy: stylus 格式化 vsc-material-theme: Material 风格的 VSCode vscode-eslint: JavaScript 代码规范检查 vscode-extension-auto-import: 自动 import 函数 vscode-jest-runner: 快速运行/调试 jest 测试 vscode-markdownlint: Markdown 语法检查 vscode-postfix-ts: Markdown 语法检查 vscode-todo-highlight: TODO 注释高亮 vscode-wakatime: 记录与统计代码的时间 VSCode Settings Google Chrome：最好的浏览器[部分开源], [跨平台], [免费] 官网, Google Chromium 各版本下载 非常有名的浏览器，来源于 Google，在 PC 上不管是用于开发还是日常使用都能够满足需求。相比于国内的流氓而言非常干净简洁（至少，看起来而言），相比于 FireFox 来说更快，插件体系也已经足够庞大完整，基本上想要的都有了。 常用的 Plugin： AutoPagerize：自动翻页插件，浏览很多网站时不需要手动点击下一页了，可以自动加载出来下一页的结果。 Checker Plus for Gmail™：对于日常使用 Gmail 的吾辈而言非常有用 crxMouse Chrome™ 手势：鼠标手势插件，可以使用手势更简单地完成一些事情 Dark Reader：为所有网站加上黑色主题，大部分情况下都还不错 Enhanced Github：显示 GitHub Repository 大小，允许单独下载每一个文件 Enhancer for YouTube™：怎么说呢，Youtube 已经很好了，但吾辈还是觉得需要这个插件来优化播放体验 Fatkun 图片批量下载：批量下载网页上的图片，偶尔用一下吧 Free Download Manager：FDM Chrome 集成插件，将 Chrome 下载链接使用 FDM 多线程下载 GitHub Hovercard：GitHub 增强插件，鼠标悬浮在仓库链接上面就可以预览 Image Search Options：使用右键以图搜图 Isometric Contributions：GitHub 美化插件，将 GitHub 贡献以 3D 的效果显示出来 JetBrains IDE Support：使用 Chrome 实时显示 IDEA 的 HTML/CSS/JavaScript 文件，与 IDEA 的插件配合使用 LastPass: Free Password Manager：跨平台的免费密码管理器，有了这个之后再也不用所有网站都使用同一个密码了 Mailto: for Gmail™：对于 mailto 协议的链接以 Gmail 网页版打开 Markdown Here：在线将 Markdown 转换为有格式的文档，例如在一个普通的富文本编辑器（不支持 Markdown）中，可以先用 Markdown 语法写内容，然后转换一下就得到了有样式的内容了。 Neat URL：移除网址中的无用段，例如返利链接后面的参数 Octotree：GitHub 代码树状图插件，方便查看项目文件 OwO：颜文字插件，多亏了这个让吾辈能够愉快的刷推了 Proxy SwitchyOmega：科学上网必需 Stylus：使用自定义网站样式的插件，比 Stylish 的名声好一些 Tabliss：新标签页插件 Tampermonkey：使用自定义网站脚本的插件，可以使用各种 user.js 脚本，相当于小型的插件管理器了 The Great Suspender：自动休眠标签页，避免 Chrome 使用的内存太过庞大 uBlock Origin：日常上网必须，屏蔽各种广告，比 Adblock 的名声好一些 Vue.js devtools：在 DevTool 中添加 Vuejs 选项卡，便于对 Vuejs 进行调试 WebRTC Network Limiter：阻止浏览器通过 WebRTC 泄露 IP 地址 WEB 前端助手(FeHelper)：貌似是百度的前端插件，但目前还没有什么流氓行为 快翻译：这个翻译插件是真心不错，某种意义上讲比 Chrome 自带的翻译都要好（#大雾） 扩展管理器（Extension Manager）：插件很少的时候还好，一多起来还是需要一个插件进行管理，快速启用和禁用一些插件，根据场景切换启用插件列表 Chrome 虽然已经很完美了，但 Google 却并非如此，最近 Chrome 开始变得越来越封闭了。首先是 Chrome 应用被删除，现在又禁止安装非官方商店中的第三方插件（目前仍可以通过开发者模式安装），真心越来越麻烦了。 附：这里提供一个快捷安装第三方插件的工具 Chrome 插件伴侣，方便解决 Chrome67 之后安装非商店插件的麻烦 IDEA：最好的 Java IDE[便携版], [部分开源], [跨平台] 官网, GitHub 社区版强烈建议升级至 2019.2.3，官方终于把智障的重构字符串查找引用默认关闭了，可喜可贺，可喜可贺 虽然说起最好的 IDE 什么的肯定是各种争论，但吾辈还是想说，在 Java 语言开发方面，IDEA 是目前最好的 IDE 了。 优点： 智能提示IDEA 在代码提示方面真的是无可比拟（相比于 Eclipse 来说），通常在还没有打完全部代码的时候，IDEA 就知道想要做什么了，然后给吾辈最常用的提示。并且，对于某些不太好的代码，IDEA 会提出一些建议甚至帮助进行安全的重构。一般而言吾辈认为，如果 IDEA 认为代码有问题，那么一般真的是代码有问题（确实有些情况下也不尽如此） 开箱即用虽然很不起眼，但这确实很重要。当下载完 IDEA 之后，几乎不需要安装什么插件，进行繁复的配置，就能开始工作了！因为 IDEA 帮我们准备好了绝大多数情况所需要的插件，很多配置也都有着良好的默认值。 插件系统或许 IDEA 的插件数量比不上 Eclipse，但 IDEA 的插件系统确实已经很完善了，至少，对于吾辈而言已然足够了。 界面很棒其实这个就是吸引吾辈入坑 IDEA 的原因之一，毕竟曾经其他人都在用着 Eclipse 的时候，吾辈看到了 IDEA 的黑色主题就被吸引了，然后到现在成了 IDEA 的死忠粉。 搜索功能非常强大的搜索功能，可以搜索类，文件，字段，以及全文搜索，包括正则表达式，文件后缀名等过滤功能。 调试或许有人说，现代的 IDE 基本上都有调试功能吧。但 IDEA 是不同的，它可以同时调试 Java 与 JavaScirpt 以及其他所集成的语言，这可真是强大无比。使用 JavaScript 发送一个请求，然后进入后台代码调试，得到 SQL 语句，去查询数据库，而这一切，都是在 IDEA 内部完成的！ 语言注入 这是 IDEA 比较高级的部分了，但这也是一个相当强大的功能。例如，有时候我们不得不手写 SQL 语句，或者在 JavaScript 中拼接 HTML 代码。难道我们必须要去对应的文件中编辑完之后复制过来么？不不不，语言注入就是可以为某一个代码片段注入为某一种语言，例如你可以为 Java 中的字符串注入 SQL 语言。在字符串中，你写 SQL 语句就有提示了！ 当然，语言注入也是要符合基本法的，必须是 IDEA 支持的语言才行，不过 IDEA 对主流语言的支持都还不错。 缺点： 硬件杀手IDEA 是出了名的吃内存（日常），吃磁盘（扫描项目建立索引）以及 CPU（索引）。 小病不断虽然 IDEA 很是强大，但各种小问题也是层出不穷。例如自定义的 Maven 到了新的项目就变成默认的了。。。 常用的 Plugin： Jrebel：做 Java Web 开发时真心免不了的 Plugin，能够有效解决 Web 容器（例如 Apache Tomcat）启动速度慢的问题（因为几乎所有的资源都能够热加载，热部署）。 LiveEdit：提供了 HTML/CSS/JavaScript 的实时预览和刷新功能。 MybatisX：Mybatis 的一个辅助开发插件，能够比较愉快的使用 Mybatis 框架了 Alibaba Java Coding Guidelines：Alibaba 出品的一个 Java 规范检查插件，能够规避一些不好的代码。 Translation：目前使用感觉最好的翻译插件，能够提供比较长的内容进行翻译（某些翻译插件只能翻译很短的内容）。 String Manipulation：字符串操作工具，可以方便对变量名或其他字符串修改为其他风格，例如将变量从 驼峰命名 （Java 标准命名） 修改为 下划线命名 （SQL 标准命名）或者 中缀线命名 （HTML/CSS 标准命名）。 其实这么多命名规范真心坑。。。 Custom Postfix Templates：提供自定义的后缀模板 GsonFormat：格式化 json 数据并转换为 Java 中的 POJO，其实对于吾辈而言主要是用来写爬虫时生成 json 数据对应的实体类 Markdown Navigator：IDEA 平台的 Markdown 写作插件，比自带的 Markdown Support 要好很多，主要在需要写的 Markdown 内容中含有代码时比较方便。 Maven Helper：一个 Apache Maven 的辅助操作插件 CodeGlance：在编辑器右侧显示一块代码缩略图，主要为了方便使用滑块进行上下滑动 Grep Console：自定义控制台的输出颜色 .ignore：在使用版本控制时，快速忽略某些文件/文件夹，目前支持广泛 CMD support：Cmd 支持，主要是方便在 IDEA 中直接运行 bat 脚本 NodeJS：集成了 NodeJS，可以直接在 IDEA 中运行 NodeJS Vue.js：集成 Vue.js 框架 Properties to YAML Converter：将 Properties 文件快速转换为 YAML 格式的配置文件 WakaTime：统计使用的编程语言，不同 IDE 的记录。官网 入门推荐教程（简体中文）：https://github.com/judasn/IntelliJ-IDEA-Tutorial drawio：开源免费的画图工具[便携版], [开源], [免费]，[跨平台] 官网, GitHub 目前能找到的最好的免费开源的画图解决方案，能够制作流程图、UML 图、组织结构图、泳道图等绘图。允许导出为图片/HTML/PDF 等各种格式，全平台免费可用的解决方案。 ProcessOn 是一个更加本地化的解决方案，并且添加了思维导图支持，只允许在线编辑导致了数据的最终权力不在自己手上，强制收费则让（小）团队使用成为一件可望而不可及的事情。注：此处不是故意黑 ProcessOn，因为吾辈也是它的付费用户，但在团队内推广使用确实会存在一些问题。 Cmder：Windows 上模拟 Linux 终端[便携版], [开源], [免费] 官网, GitHub 曾经有段时间吾辈也很迷信 Linux，觉得 Linux 适合开发者使用（事实上确实如此），但后来还是因为电脑的硬件原因（驱动问题，莫名其妙的卡死在登录页面）而回到了 Windows。而最让人念念不忘的就是 Linux Bash 终端了，相比于 Windows 自带的 Cmd/PowerShell 来说漂亮/强大了许多。当然，Windows 相比于 Linux 的最大优点就是稳定（不怕折腾，很难损坏系统本身）以及可视化（大部分的设计都有可视化界面进行配置）。然而有时候还是不得不使用命令行，而这时便显示出 Windows 的不足了，直到后来吾辈在偶然的机会碰到了 Cmder，发现了 Windows 上果然已经有了解决方案。 Cmder 是一个在 Windows 上模拟 Linux Bash 的终端模拟器，其实对于吾辈而言，主要是因为 Cmder 比较漂亮，集成了很多组件（例如 Git, SSH, Curl, Less, vim），而且还能在上面安装 Windows 下的包管理系统 Chocolatey。安装了包管理器之后就能直接使用 choco command options 命令安装软件了。 具体方法可以参考另外一篇文章：Windows 下 Cmder 安装 Chocolatey Git：Git 版本控制系统[便携版], [开源], [跨平台], [免费] 官网, GitHub 最初由 Linus 开发的一个分布式的版本控制系统，现在已经是最流行的版本控制系统之一了。说到 Git 不得不提 GitHub，GitHub 为 Git 实现了一个在线代码（或者其他）托管平台，相当好用而且托管开源项目是免费的。其实对于吾辈而言 Git 只是一个用于在本地操作 GitHub 的客户端罢了（很少有添加到 Git 而没有提交到 GitHub 的代码）。正是因为 GitHub 对 Git 的支持之好，所以吾辈才喜欢使用 Git 的。。。（#因果颠倒） Termius：简单漂亮的 SSH 连接工具[跨平台] 官网 非常漂亮的一个远程连接工具，免费版本仅支持 SSH 连接，而 SFTP 则是收费版本的功能，吾辈作为前端感觉算是足够了。 更强大的远程连接工具：Mobaxterm，功能非常强大但界面复古（Win7 时代的风格）。 HeidiSQL：SQL 客户端[便携版], [开源], [免费] 官网, GitHub 一个支持 MSSQL,MySQL 和 PostgreSQL 的开源数据库轻量级客户端。吾辈使用这个 SQL 客户端最重要的原因就是满足了吾辈对于数据库文件的导入/导出需求，谁让 IDEA 自带的 DataGrip 没有导出功能呢 附：吾辈很清楚 MySQL 有自带的导出命令，然而并不灵活。。。至少这个能够定义创建数据表，插入数据时使用的策略 freeFTPd：免费的 FTP/SFTP 服务端[开源], [免费] 官网 开源免费的 FTP/SFTP 服务端，非常适合在开发过程中模拟 FTP/SFTP 服务端使用。而且相比于 FileZilla 更加简单，也不会从一开始就出现错误需要解决。 日常Win + R：快速启动虽说 Windows 上也有快速启动工具，但 Windows 原生的快速启动已然足够使用了。将快捷方式命名为一个简单的名字，例如将 Google Chrome 命名为 GC，然后丢到当前用户目录下，然后使用 Win + R 弹出运行窗口，然后输入 GC，系统就会启动 Google Chrome 了。 当然，更好的方法是新建一个目录保存这些快速启动的快捷方式，然后将目录添加到环境变量中即可（Path）。 SSR：正常上网必须[便携版], [开源], [跨平台], [免费] GitHub 为了正常浏览网络折腾与支付一些代价都是微不足道的，貌似比 ShadowScoks 要稳定一点，不过据传闻作者最初并未打算将其开源。 Proxifier：代理一切 官网 虽然 SSR 可以连接代理服务器，同时还提供局域网内的 Socks5/HTTP 的代理，同时 Chrome 自动使用系统代理，所以我们可以方便的直接打开 Google 了。然而，并不是所有程序都像是 Chrome 浏览器那么友好，许多支持代理的程序都需要手动进行配置，更麻烦的地方是：很多程序不包含代理设置 – 尤其是开发相关的工具链。 先说几个具体的使用场景吧 进行 web 开发时，有些 npm 包（electron/SQLLite）会在 install 时从网络下载二进制文件，而因为众所周知的原因，它们的下载速度并不快，而 nodejs 也不能配置代理（事实上，即便配置了，也不是所有 npm 包都会读） 希望监听某些程序到底有没有发送网络请求，但却不能尽知 MacType：字体美化[开源], [免费] 官网, GitHub Windows 上的字体就是会出现锯齿（所谓的点阵字体真是坑），不过 MacType 可以缓解这一现象。它可以美化 Windows 对字体的渲染（绝大部分程序都不会出现问题），让吾辈在使用 Windows 的时候看着代码更舒服一点。 附: MacType 配置推荐 注册表加载 + Iof =&gt; XHei =&gt; Default附：字体个人推荐 Sarasa-Gothic，中文名是更纱黑体，包含 英/简中/繁中/日 四种语言 Joplin：适合开发者的笔记工具[便携版], [开源], [跨平台], [免费] 官网, GitHub 一个开源跨平台的笔记工具，使用的模式是本地程序 + 第三方服务同步的方式，编辑功能算是比较弱的，同步也不算是最好的，甚至于客户端也不算是最漂亮的。但是，它有一些非常显著的优点，甚至足以压倒以上的缺点。 主要优点： 搜索很快，非常快 可以使用外部编辑器打开 数据都在自己手里，提供一次性导出全部的功能 使用标准的 md，可以直接复制到其他平台 可以基于它进行二次开发 主要缺点： ui/ux 有点简陋 没有 vsc 插件导致使用外部编辑器也并不是非常方便 之前吾辈使用过 OneNote 和 Notion，两者都是非常好的笔记工具，但两者存在的一些缺点导致吾辈在使用一段时间后选择了放弃。 OneNote：主要问题是需要自己调整文本格式，不支持 markdown，以及富文本带来的分享问题。 Notion：主要问题是编辑体验不好，搜索非常慢，以及非标准 markdown 导致的分享问题。可以参考 Notion 使用体验 FDM：免费的多线程下载器[免费] 官网 全称 Free Download Manager，没什么太多好说的，但用起来还算不错，不会弹个窗口提示下载什么的，然而下载完成也没提示真的坑。。。 基本上满足吾辈日常的下载需求吧 提供客户端界面 多线程 集成 chrome 静默下载 下载后可以打开文件/文件夹 下载完成后没有弹窗提示 便携版（配置可携带化） Bandizip：压缩/解压缩[便携版], [免费] 官网 目前吾辈觉得最好的压缩/解压缩软件，具体压缩率/支持格式什么的吾辈也并非专业所以就不说了。但就使用体验上看 Bandizip 是要明显优于其他同类软件的（WinRAR, 7Zip）。 吾辈看到的优点： 预览压缩文件时左侧有文件夹树状列表，不需要再用鼠标点击进进出出的活塞运动了 自动解压，这个功能相当棒，可以智能解压压缩文件到文件夹中（如果压缩包里面只有一个文件夹就直接解压出来，否则会将压缩包里面的内容解压到一个与压缩包同名的文件夹中），这个功能看起来很不起眼，但吾辈在此之前并未找到（需要特别注意压缩包的内容） 注：最新版本强制更新还有广告，所以需要下载修改版或是旧版本，即便如此，它的自动解压仍然是非常优秀的解压功能。 ClipboardFusion：剪切板增强[便携版], [免费] 官网 Windows 剪切板增强工具，不得不说 Windows 上的工具确实很多。即使一个功能 Windows 本身支持的不好，然而总有其他的工具对其增强就是了。当然，吾辈需要的只是它的剪切板历史功能。 注：目前 Windows 已经自带了剪切板历史功能，但尚未支持搜索，同时在布局上也并不是特别友好（只能显示 4 条），轻量级用户可以选择这个（毕竟是自带，很方便）。注：之前在用 Ditto，但后面在吾辈电脑上出现了 bug，所以更换了其他工具，但并不是说它不好。参考：善用佳软评测 对其的介绍很全面，这里也就无需赘述了。 JPEGView：图片浏览[便携版], [免费] 官网 简单而且好用的图片查看器，相比于吾辈之前使用的 ImageView（中文名是 快眼看图王）更快，相比于 HoneyView 更见简洁和易操作。 主要优点如下： 操作简单：滚轮缩放，按住移动，ESC 关闭，Left/Right 查看上/下张图片 性能很高：实测打开速度比 imageview/honeyview 都要更快 借用知乎网友理想中的图片查看器需求图 理想中的图片查看器 Everything：文件快速搜索[便携版], [免费] 官网 用过 Windows Explorer（文件管理器）搜索文件的人都知道速度是有多慢，有了 Everything 之后这一切都是往事了。Everything 建立了全盘索引，可以对文件/文件夹进行实时搜索，速度之快简直和默认的文件管理器有天壤之别。当然，也能够根据正则表达式/类型进行高级查询。 Snipaste：截图/贴图[便携版], [免费] 官网, GitHub, GitBook 中文教程 很漂亮，很漂亮，很漂亮！重要的话说三遍，Snipaste 相比于其他的截图软件（FSCapture）在使用时感觉漂亮了很多，而且在使用时会方便很多（截图完成后直接就进行编辑，而不是跳转到编辑器中编辑截图），保存时自动保存并且复制到剪切板也很方便。总而言之就是细节做的相当棒！ PicGo：图床上传[开源], [跨平台], [免费] 官网, GitHub 使用 Markdown 写作必备的图床工具，解决了吾辈关于 GitHub 图床的几个痛点。 上传之前重命名 上传一键就好 上传后图片管理 ScreenToGif：录制 GIF 动态图[便携版], [开源], [免费] 官网, GitHub 虽然吾辈已经有了截图工具，但某些时候单靠截图并不能很好的表达意思，这时候就需要使用 Gif 去记录一系列的操作。这也是 ScreenToGif 的主要功能，将记录转换成 Gif 动态图。 嘛，虽然就使用上吾辈觉得还是有一些不便： 吸附窗口操作不太直观 保存时不能直接一键保存到指定目录并复制到剪切板 不过这也是目前吾辈能找到的比较好用的一个了 QuickLook：文件管理器预览增强[便携版], [开源], [免费] 官网, GitHub Windows Explorer 预览功能增强，支持很多种格式的文件/文件夹进行预览，包括但不限于文本、图像、视频、音频、压缩包、文件夹等。在按下空格时，就弹出一个预览窗口，感觉还是不错的。 Steam：国外的游戏平台[跨平台], [免费] 官网 来自于金三胖出品的非常著名的游戏平台，非常之多的游戏在该平台上发售。Steam 提供了非常多的支付方式，对国内用户相当友好（支付宝/微信/银联）。吾辈个人是只玩 Galgame 的，所以能入正支持到做 Galgame 的公司表示还是很开心的。唉，不过 Steam 也没能逃过魔掌呢。。。steam 落户上海浦东，吾辈表示转区势在必得了！ 附：Steam 上没有什么盗版游戏，所以想免费玩游戏的人就不用抱有什么幻想了 Mozilla FireFox：抱有期待的浏览器[开源], [跨平台], [免费] 官网, GitHub FireFox 是一个将 IE 拉下神坛的浏览器，一个开源的注重隐私的浏览器，但现在，它已然有些没落了。尤其是最近的 FireFox57 后为了提升性能更新了浏览器的整体架构，导致旧的扩展基本全部失效，恢复曾经的附加扩展生态却又不知道需要多久了。但是，这些问题，相比于隐私（信息收集）与自由（不限制扩展）来说，还是值得考虑一下的，确实挺期待 FireFox 之后发展的。 相比于其他的浏览器最大的特点就是隐私性和自由，收集隐私是完全可选的（国内的流氓浏览器信息收集简直过分），附加扩展基本不怎么限制（Chrome 限制扩展只能从官方商店直接安装）。 补充: 2019-06-18: 现在 Chrome 居然限制 uBlock Origin 这类的扩展了。。。 AIMP：本地音乐播放器[免費] 官网 一个非常漂亮的本地音乐播放器，貌似对歌曲音质的支持也很好（没什么感触，不玩耳机的萌新一只）。。。 PotPlayer：视频播放器[便携版], [免费] 官网 非常强大/漂亮的本地视频播放器，虽然也可以播放 URL 的视频资源，但主要还是播放本地视频，支持大多数常见格式而且对于 1080P 之上的画质支持非常棒！ qBittorrent：BT 下载[免费] 官网 不错的 BT 客户端，相比于大名鼎鼎的比特彗星，它没有广告，没有等级。对于热门种子的下载速度十分喜人，最高可以跑到 10M 左右，比特彗星永远没有跑到这么高过。。。 Dism++：Windows 的一个集中式的系统管理工具[便携版], [免费] 官网, GitHub 一个集中式管理 Windows 各种设置的 GUI 控制面板，能够方便的控制系统的各种设置（包括 Windows 10 的更新。。。），提供了一个简单但却还算可以的垃圾清理工具，感觉上很干净而且可以独立运行！ TreeSizeFree：以树状视图查看文件夹[便携版], [免费] 官网 用来以树状结构查看文件夹大小的工具，可以知道到底是哪些文件占用了硬盘及其大小。 renamer：文件/文件夹重命名[便携版], [免费] 官网 或许很多人想吐槽，重命名不就一个 F2 的事情么？是的，如果只需要重命名一个文件，F2 是最快捷的方式。但 rename 针对的是需要重命名数十，数百个文件的情况，这种时候手动重命名实在太过于糟糕了点！ waifu2x-caffe：图片清晰化[便携版], [免费], [开源], [跨平台] 官网, GitHub, GitHub Windows 可以让图片变得清晰，具体技术实现涉及机器学习等尖端技术（#dalao），可以说是很厉害了呢 Bandicam：视频录制 官网 还算好用的视频录制工具，可以很方便的进行视频的录制。特色功能： 绘制模式：在录制视频时进行频幕上的编辑 窗口依附：可以轻松指定需要录制的窗口 多格式输出：支持输出多种格式，按照不同分辨率，预置平台输出 添加水印：防盗利器，在视频中添加自己的 LOGO LockHunter：解除文件占用[免费] 官网 在删除文件/目录时，偶尔会遇到文件占用的问题，这个小工具直接集成到 MS Explorer 右键菜单中，可以方便查看占用程序，并关闭它。 rufus：USB 启动盘制作工具[便携版], [免费], [开源] 官网, GitHub 小巧方便的 USB 启动盘制作工具，完全的一键操作，下载 =&gt; 解压 =&gt; 运行 三连之后就是父母辈的人都能看懂的界面了！ Geek Uninstaller：深度卸载Loseless Audio Validator：音频校验[便携版], [跨平台], [开源], [免费] 官网 校验音乐是否是真的 Flac/Ape 无损音质的，其实作为一个外行人吾辈表示并不理解什么音频波形什么的。 ApowerMirror: 手机投屏[跨平台] 官网 在局域网内将手机屏幕投影到 PC 上，延迟性在半秒之内，基本上不影响使用。相比于 TeamViewer 的主要优点是不走中央服务器，延迟没那么高。而与之类似的 AirServer 却延迟很大，而且操作过快时就会崩溃。吾辈主要是在 PC 演示操作时，涉及到手机上的操作时，为了避免录屏中断或是后期拼接，故而需要将手机屏幕投影至 PC 方便持续录制。 HandBrake: 视频压缩工具[便携版], [跨平台], [免费], [开源] 官网 非常简单的视频压缩工具，对非专业人士非常友好，只要选择合适的预设，任何人都能批量将视频压缩。吾辈一般使用的是 MP4+Web Optimized，可以将 50M 的视频压缩到 10M 左右，同时支持删除音轨进一步缩小视频大小。 ref: HandBrake 免費影片壓縮軟體教學，將視頻的檔案縮小。（Mac ,Windows） 国内流氓QQ：国内不得不用的流氓 IM 官网 垃圾 QQ，毁我人生！好了，以上纯属吐槽不用在意。QQ 是腾讯的社交 IM，由于国内使用人数极其庞大，所以吾辈也不得不用的软件。一个软件居然会向系统添加驱动服务，而且禁用了还不让使用，真是厉害了呀。。。 百度网盘：国内事实上的垄断网盘[跨平台], [免费] 官网 国内事实上的大文件共享服务，目前对普通用户的限制非常大，逼得人不得不开通 SVip 才能使用，但同时也不能得到满速下载，在下载量较大后，之后的下载就会在服务端限速，同时百度网盘的客户端非常讨厌，即便是 SVip 也会看到广告。 Pass：以上国内的流氓吾辈使用的都是修改过的绿色版本，基本上没什么太多麻烦的东西了呢 名词解释便携版能够下载完解压之后无需安装即可直接使用的软件，所有的配置文件也都在程序运行的文件目录下（非用户目录下），理论上可以配置完成一次之后就可以用一辈子了（#笑） 开源开放程序源代码，任何人都可以自由浏览 部分开源指程序既有开源版本又有商业版本，所以标识为部分开源 跨平台至少兼容两个主流 PC 系统平台（Windows/MacOS #Linux 桌面在哭泣 #笑）才会标记为跨平台 免费所有功能都完全免费（或许有捐助请求）才会标识为免费，有收费的高级功能的程序不会被标记为免费，例如 IDEA 就不会被标识为免费 以上均为吾辈个人主观认知，参考一下就好。。。","categories":[],"tags":[]},{"title":"ambari","slug":"ambari安装","date":"2021-02-08T07:49:12.000Z","updated":"2024-03-14T14:55:29.475Z","comments":true,"path":"e29592b7.html","link":"","permalink":"https://imlike.cc/e29592b7.html","excerpt":"","text":"Ambari安装指南注意事项 目前centos 7 默认python 环境 为2.7 现在调用 中需要请求比如 https://master:8441/agent/v1/register/master 注册时 会出现问题 我没有去升级 参考了该链接 Ambari 2.4 在 CentOS 7.4 因 TLS_1.2 协商内部错误导致注册失败 1234yum updateyum install -y ntp &amp;&amp; systemctl enable ntpd &amp;&amp; systemctl start ntpdyum install -y unzipyum install -y gcc [X] 安装环境: 系统版本:CentOS 7.3 x86_64 JAVA版本:JDK-1.8.x 确保机器的 Python 版本大于或等于2.6 服务器列表: 192.168.68.17 master #Ambari Server 192.168.68.18 work1 #Ambari Agent,namenode2,journalnode1,datanode1,nodemanager1 192.168.68.20 work2 #Ambari Agent,journalnode2,datanode2,nodemanager2 192.168.68.21 work3 #Ambari Agent,journalnode3,datanode3,nodemanager3 SSL(需要的话可以升级) 需要openssl的1.0以上版本! 123yum install openssl或者yum upgrade openssl JDK yum install -y java-1.8.0-openjdk-headless.x86_64 或者下载RPM 包安装 建议安装Sun的JDK1.8版本! 安装完毕并配置java环境变量,在/etc/profile末尾添加如下代码: 如果是用yum 在线安装 的openjdk 安装地址在 /lib/jvm/ 下面 或者是下载 oraclejdk 着一般需要解压后 设置java_home 上述地址在初始化 ambari-server setup 时是需要用到的 export JAVA_HOME=/usr/java/defaultexport PATH=$JAVA_HOME/bin:$PATH 保存退出即可,然后执行source /etc/profile生效.在命令行执行java -version 如下代表JAVA安装成功. ssh 需要配置各个节点的免密码登录!首先在自己机器上使用ssh-keygen -t rsa ssh-copy-id root@work1 ssh-copy-id root@work2 ssh-copy-id root@work3 NTP(必要条件)集群的时钟要保证基本的一致.稍有不一致是可以容忍的,但是很大的不一致会 造成奇怪的行为. 运行 NTP 或者其他什么东西来同步你的时间.如果你查询的时候或者是遇到奇怪的故障,可以检查一下系统时间是否正确! crontab -e */10 * * * * ntpdate time.nist.gov ulimit和nproc Hdaoop会在同一时间使用很多的文件句柄.大多数linux系统使用的默认值1024是不能满足的,修改/etc/security/limits.conf文件为: 1234* soft nproc 16384* hard nproc 16384 * soft nofile 65536 * hard nofile 65536 修改host和虚拟机的名称192.168.68.17,192.168.68.18,192.168.68.20,192.168.68.21的 /etc/hosts 文件 在文件最后添加: 192.168.68.17 master192.168.68.18 work1192.168.68.20 work2192.168.68.21 work3 同时修改虚拟的名称 12345678hostnamectl set-hostname master vi &#x2F;etc&#x2F;hosts192.168.56.101 master192.168.56.102 work1192.168.56.103 work2vi &#x2F;etc&#x2F;sysconfig&#x2F;networkHOSTNAME&#x3D;masteruname -a 禁止 PackageKit12pkill -9 packagekitdvim &#x2F;etc&#x2F;yum&#x2F;pluginconf.d&#x2F;refresh-packagekit.conf 将内容改为 1enabled&#x3D;0 关闭防火墙和SELINUX12345#关闭service firewalld stop#随着系统启动自动关闭chkconfig firewalld off systemctl stop firewalld systemctl disable firewalld vim /etc/selinux/config 修改 SELINUX=disabled [X] 安装Ambari Server在T1上操作 [1] 添加yum资源库: wget -nv http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.7.3.0/ambari.repo -O /etc/yum.repos.d/ambari.repo yum clean all yum install ambari-server [2] 校验YUM仓库ambari:由于公司安全机制,这个地方可能不会安装成功执行yum repolist,如果有ambari字符串的话,就可以开始安装了! [3.1] YUM在线安装: ambari-server1yum install ambari-server 实验证明,网速巨慢,耐心…,等待安装完成! [3.2] YUM离线本地安装: ambari-server 先找一个网络好的环境,配置 1&#x2F;etc&#x2F;yum.conf 文件,修改 1keepcache 为 11 ,来保留安装包;再通过 1yum install ambari-server 安装成功后后;把相关的rpm文件复制到需要离线安装的环境上;例如: 123456ambari-server-2.1.2-377.x86_64.rpmambari-agent-2.1.2-377.x86_64.rpmpostgresql-8.4.20-3.el6_6.x86_64.rpmpostgresql-devel-8.4.20-3.el6_6.x86_64.rpmpostgresql-libs-8.4.20-3.el6_6.x86_64.rpmpostgresql-server-8.4.20-3.el6_6.x86_64.rpm 再执行yum localhost ./postgresql-8.4.20-3.el6_6.x86_64.rpm ./postgresql-server-8.4.20-3.el6_6.x86_64.rpm ./postgresql-libs-8.4.20-3.el6_6.x86_64.rpm ./postgresql-devel-8.4.20-3.el6_6.x86_64.rpm ./ambari-server-2.1.2-377.x86_64.rpm来进行离线安装! [5] 配置AMBARI-SERVER 1234##指定JAVA_HOMEambari-server setup -j &#x2F;usr&#x2F;java&#x2F;default##不指定JAVA_HOME执行ambari-server setup 选择n会按默认配置数据库类型：postgresql数据库：ambari用户名：ambari密码：bigdata等待安装完成 [6] 启动AMBARI-SERVER1234567891011121314151617181920## 初始化 ambari 包含配置数据库和 jdk环境ambari-server setup## 重置 ambari-server reset## 安装高版的 ambari ,后升级ambari-server upgrade##启动:ambari-server start##查看:ps -ef | grep Ambari##停止:ambari-server stop##帮助ambari-server help 然后访问：http://master:8080不出意外的话就安装完成了用户名/密码:admin/admin [6] 安装AMBARI-AGENT(可选 在web界面自动安装注册)在网络环境不好的地方或者sshd的端口不是标准的22时,可以在每个agent节点上先手工安装AMBARI-AGENT 1yum install ambari-agent 备注: Ambari安装的环境路径: 各台机器的安装目录: 12345&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;usr&#x2F;lib&#x2F;hbase&#x2F;usr&#x2F;lib&#x2F;zookeeper&#x2F;usr&#x2F;lib&#x2F;hcatalog&#x2F;usr&#x2F;lib&#x2F;hive Log路径, 这里需要看出错信息都可以在目录下找到相关的日志 12&#x2F;var&#x2F;log&#x2F;hadoop&#x2F;var&#x2F;log&#x2F;hbase 配置文件的路径 123&#x2F;etc&#x2F;hadoop&#x2F;etc&#x2F;hbase&#x2F;etc&#x2F;hive HDFS的存储路径 1&#x2F;hadoop&#x2F;hdfs FAQ 主机名格式必须是hoastname.domain,如master Zookeeper-Agent端修改Server指向的HOSTNAME, /etc/ambari-agent/conf/ambari-agent.ini,如修改过主机hostname 安装失败后或重新安装先执行ambari-server reset 后 ambari-setup 最后一步安装可能会失败,多数原因是下载包错误引起的,可重复安装直到成功,本人反复几个最终成功了,网络,网络,尤其就朝民,各种干扰! ssl 版本导致的agent注册失败 123#在agent的配置,主要针对ambari 2.6.2[security]force_https_protocol&#x3D;PROTOCOL_TLSv1_2 [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:579) 123# vim &#x2F;etc&#x2F;python&#x2F;cert-verification.cfg[https]verify&#x3D;disable 卸载hdp 确保防火墙关闭，能够ping通ambari-server服务所在机器的IP 升级openssl服务 1yum upgrade openssl 12343. 关闭 openssl 的检查 &#96;&#96;&#96;bash sed -i &#39;s&#x2F;verify&#x3D;platform_default&#x2F;verify&#x3D;disable&#x2F;&#39; &#x2F;etc&#x2F;python&#x2F;cert-verification.cfg 先停掉ambari-server，然后依次停掉各个ambari-agent 12ambari-server stopambari-agent stop 在所有ambari-agent节点上，修改ambari-agent.ini文件的[security]，新增force_https_protocol如下： 123vi /etc/ambari-agent/conf/ambari-agent.ini[security]force_https_protocol=PROTOCOL_TLSv1_2 依次开启ambari-agent，最后开启ambari-server 12ambari-server startambari-agent start 卡在安装界面看log不报错 这是由于 我们使用了远程的地址,下载比较慢 查看服务器后台 123456[root@localhost ~]# ps -ef|grep yumroot 8946 8942 0 14:01 ? 00:00:00 /bin/sh -c rpm -qa | grep smartsense- || yum -y install smartsense-hst || rpm -i /var/lib/ambari-agent/cache/stacks/HDP/3.0/services/SMARTSENSE/package/files/rpm/*.rpmroot 8949 8946 1 14:01 ? 00:00:00 /usr/bin/python /bin/yum -y install smartsense-hstroot 8993 3120 0 14:01 pts/0 00:00:00 grep --color=auto yum[root@localhost ~]# 发现 yum是在运行的 期间大概耗费20多分钟 补充搭建本地仓库 (目前使用的是在线仓库搭建)安装Nginx 编辑本地仓库 vi /etc/yum.repos.d/nginx.repo 12345[nginx]name&#x3D;nginx repobaseurl&#x3D;https:&#x2F;&#x2F;nginx.org&#x2F;packages&#x2F;centos&#x2F;$releasever&#x2F;$basearch&#x2F;gpgcheck&#x3D;0enabled&#x3D;1 yum repolist yum install nginx.x86_64 首先确定我们的版本 ambari 2.7.3.0 ,hdp HDP-3.1.4.0 链接 Timeline Service V1.5 Start 123Execution of &#39;curl -sS -L -w &#39;%&#123;http_code&#125;&#39; -X PUT -d &#39;&#39; -H &#39;Content-Length: 0&#39; &#39;http:&#x2F;&#x2F;work1:50070&#x2F;webhdfs&#x2F;v1&#x2F;ats&#x2F;done?op&#x3D;SETPERMISSION&amp;user.name&#x3D;hdfs&amp;permission&#x3D;755&#39;&#39; returned status_code&#x3D;403. 解决方案 https://community.cloudera.com/t5/Support-Questions/App-Timeline-Server-not-start/td-p/230460 1# su - hdfs As hdfs 1$ hdfs dfsadmin -safemode get From the above you will confirm the status in the error “safe mode” then 1$ hdfs dfsadmin -safemode leave https://liujia.anqun.org/index.php/archives/1211/ xfs_repair","categories":[],"tags":[]},{"title":"书写技术文档的规范结合阮一峰的博客介绍","slug":"书写技术文档的规范结合阮一峰的博客介绍","date":"2021-01-27T13:24:34.000Z","updated":"2024-03-14T14:55:29.483Z","comments":true,"path":"cadd68a.html","link":"","permalink":"https://imlike.cc/cadd68a.html","excerpt":"","text":"技术文档每天都会遇到,条理不够清晰,表达不够准确首先是很难让人理解,同时还会增加沟通的成本 我这边结合阮一峰的博客也结合我最近写文档的思路列一下两个方向是实践类(包括但不限于 部署,搭建),预研学习类(包括 新技术,技术选型,新趋势)实践类较为简单先来个实例网站华为云的部署文档1.t1 1.1 t2 1.1.1t3 a. b. c. 或者 - - - 这种来分开步骤和梳理逻辑同时遇到的问题或者是要集中介绍和梳理的内容可以放在后面开头问题这种一般先描述现象和问题,然后 列出解决方案不必纠结与 ,把问题讲清楚在md也尽量使用规范 引用 代码块 需要分开使用不要乱用,对应主要的部分可以加粗高亮 对于第二种预研学习类来个实例网站 阿里云 云数据库 hbase 先应该是基本概念 来介绍名词和这个技术或者服务的概览列出类似的产品服务,描述这个技术和产品的特点 依赖的环境和 版本给出实例代码和有实例代码或者 sample来","categories":[],"tags":[]},{"title":"Linux使用fdisk挂载和初始化磁盘以华为云服务器为例","slug":"Linux使用fdisk挂载和初始化磁盘以华为云服务器为例","date":"2021-01-25T02:49:17.000Z","updated":"2024-03-14T14:55:29.473Z","comments":true,"path":"ca3ef640.html","link":"","permalink":"https://imlike.cc/ca3ef640.html","excerpt":"","text":"先使用 fdisk -l 查看磁盘情况 磁盘看到已经能在系统上显示了 但是没有初始化和挂载分区,使用使用 df du 等,命令是看不到具体的情况 看到有 /dev/vdb没有被使用那就去初始化然后去使用 使用 fdisk /dev/vdb 来初始化先进入会有 (Command (m for help))步骤如下 n 是去初步去创建新磁盘 进入 一直按 enter 包括 primary,extend ,起始磁盘等都走默认 ,Partition number 默认为1 p 查看具体的分区情况 w 写入修改 下图为过程 fdisk -l 查看磁盘已经初始化 图片如下可以看到 分区已经形成,由于我们走默认使用只有一个 使用ext4格式化 /dev/vdb1 这个分区ext4 格式化后 使用 blkid /dev/vdb1 来查看 UUID 修改 /etc/fstab 来开机自动挂载分区1UUID&#x3D;0b3040e2-1367-4abb-841d-ddb0b92693df &#x2F;mnt&#x2F;sdc ext4 defaults 0 2 可以考这边内容 需要修改 UUID ,和挂载的文件夹 ,我这边是在根目录下面 创建的文件夹挂载的 以内容上仅为示例，具体请以实际情况为准，参数说明如下： 第一列为UUID，此处填写1中查询到的磁盘分区的UUID。 第二列为磁盘分区的挂载目录，可以通过df -TH命令查询。 第三列为磁盘分区的文件系统格式， 可以通过df -TH命令查询。 第四列为磁盘分区的挂载选项，此处通常设置为defaults即可。 第五列为Linux dump备份选项。 0表示不使用Linux dump备份。现在通常不使用dump备份，此处设置为0即可。 1表示使用Linux dump备份。 第六列为fsck选项，即开机时是否使用fsck检查磁盘。 0表示不检验。 挂载点为（/）根目录的分区，此处必须填写1。 根分区设置为1，其他分区只能从2开始，系统会按照数字从小到大依次检查下去。 使用mount -a加载**/etc/fstab**的配置mount -a 没有报错后 用df du 都可以看到你挂载的目录和对应的分区","categories":[],"tags":[]},{"title":"docker容器导入导出和配置","slug":"docker容器导入导出和配置","date":"2021-01-18T02:19:41.000Z","updated":"2024-03-14T14:55:29.476Z","comments":true,"path":"aba1fef8.html","link":"","permalink":"https://imlike.cc/aba1fef8.html","excerpt":"","text":"镜像导入导出目前的导入导出是 sava/load,export/import ,前者有完整的数据,后者类似于容器的快照,开始搭建时,想直接复制我们服务器中运行的容器,导出后在docker中运行后 一直报 No command specified” from re-imported docker image/container 后面有了解 sava/load 会有完整的数据 但是 export的tar包这个命令将导入镜像文件和参数 关于 gdal容器目前使用的gdal的容器是 osgeo/gdal:ubuntu-small-3.2.1 docker run -itd –privileged -v /usr/local/gdal/data/:/home –name gdal_python osgeo/gdal:ubuntu-small-3.2.1 容器的名称需要和上面 application-boot.yml的关联上,需要通过容器的主机 ssh 控制 docker 下的容器 需要复制 shp2pg.py 到 /usr/local/gdal/data/ 后面的版本是有添加 gdal的环境 123456789FROM osgeo/gdal:ubuntu-small-3.2.1MAINTAINER gdallikedemoRUN add-apt-repository ppa:openjdk-r/ppa &amp;&amp; \\ apt-get update &amp;&amp; \\ apt-get install -y java-1.8.0-openjdk &amp;&amp; \\ apt-get clean;COPY NAME.jar /home/NAME.jar# RUN nohup java -jar /home/NAME.jar &gt;/dev/null 2&gt;&amp;1 &amp;CMD [&quot;java&quot;,&quot;-jar&quot;,&quot;/home/NAME.jar&quot;] 搭建pgsql集群搭建集群 可以参考之前的文档 我这边只是列出清单 集群名称 pg_master pg_hba.conf( 配置网络和集群的IP,注意配置 所有从服务器的IP),postgresql.conf(配置数据库自己的服务和资源使用) pg_slaver1 同上,recovery.conf(设置恢复和通信) pg_slaver2 同上,recovery.conf(设置恢复和通信) tips: 建议在postgresql.conf 中的日志打开,日志的配置 logging_collector =on # Enable capturing of stderr and csvlog 日志会在本地的/data/postgresql/data 启动时挂载的点,会有log目录可以找到异常信息 尽量选择复制配置文件,像postgresql.conf这种标准配置文件主从有区别要注意 nginx拉取 nginx镜像后 123456789docker run --name nginx -p 80:80 --restart&#x3D;always -v \\&#x2F;data&#x2F;nginx&#x2F;html:&#x2F;etc&#x2F;nginx&#x2F;html&#x2F; \\-v &#x2F;data&#x2F;nginx&#x2F;conf&#x2F;nginx.conf:&#x2F;etc&#x2F;nginx&#x2F;nginx.conf \\-v &#x2F;data&#x2F;nginx&#x2F;conf.d&#x2F;default.conf:&#x2F;etc&#x2F;nginx&#x2F;conf.d&#x2F;default.conf \\nginx:1.19.6 一个要注意的是 网上都是nginx默认的挂载是/usr/share/nginx/html ,使用依照了 官方docker版本我挂载了数据,一直都有是404 ,后面看日志是 2021/01/13 21:56:42 [error] 21#21: *1 “/etc/nginx/html/index.html” is not found (2: No such file or directory), client: 172.17.0.1, server: localhost, request: “GET / HTTP/1.1”, host: “127.0.0.1” 172.17.0.1 - - [13/Jan/2021:21:56:42 +0000] “GET / HTTP/1.1” 404 153 “-“ “curl/7.29.0” 这边关联到/etc/nginx/html/ 后正常可以看到 welcome nginx的页面 geoserver目前使用的是官方的 geoserver的集群 主要 是要导出 yaml 的文件 将本地的geoserver 镜像导入 harber的私服中然后再 配置 k8s挂载卷然后编辑 挂载卷 harber的远程地址 推送镜像到harber首先 使用命令 docker tag IMAGE_ID NAME:TAG 将load 的镜像设置对应的标签方便后面使用k8s导入的配置文件好找到 push_im.sh文件中的 1234567891011# 发布最终版本# 版本号version&#x3D;latest# 打包镜像docker build -t NAME:$version .# 配置标签docker tag NAME:$version 192.168.7.120&#x2F;public&#x2F;NAME:$version# 推送镜像docker push 192.168.7.120&#x2F;public&#x2F;NAME:$version version 对应的字段要有 ,同时 harber 地址要改的 上面的这个 docker build -t NAME:$version . 是有个点的, 需要dockerfile文件的","categories":[],"tags":[]},{"title":"gis初入门","slug":"gis初入门","date":"2021-01-08T06:24:30.000Z","updated":"2024-03-14T14:55:29.477Z","comments":true,"path":"6a9c1835.html","link":"","permalink":"https://imlike.cc/6a9c1835.html","excerpt":"","text":"参考链接 https://segmentfault.com/a/1190000015674238 GIS是一个设计用来捕获、存储、操作（manipulate）、分析、管理和展示空间或图像数据的系统。 GIS 应用 = 基础底图 + 操作图层 + 任务 基础底图：可以简单地理解为就是地图，并且要求是一个不会频繁改变的地图。 操作图层：用户使用的主要图层，承载主要的待编辑、展示和分析的信息数据。 任务：如导航、地址编码等计算量比较大的分析任务。 GIS 数据和协议由于 ArcGIS 在行业内的龙头地位，其指定的一系列数据格式和交换协议，成为 GIS 业界的标准协议。目前制定 GIS 协议的标准化组织为开放地理空间协会（OGC），其制定的规范称为 OpenGIS（Open Geodata Interoperation Specification）。 相关的接口和协议非常多，以后再一一补齐。目前大部分 demo 系统，导入的数据格式为 shp 文件，往往是由甲方直接提供、或者是网上的公开 GIS 服务提供商可提供下载的，不需要我们自行转换数据格式。ArcGIS 可以直接识别并导入，入库后也使用标准接口协议开放服务，比如 WMS（网络地图服务）和 WFS（网络要素服务）。 免费开源 GIS 软件开源 GIS 软件，从大类来分，包含服务器版和桌面版（即桌面版）。显然我们最关注的是服务器版的 GIS 软件 / 组件。 服务器版的部署架构和 ArcGIS 基本类似，也是分 web、GIS Server、database 三大模块，并且可以分开部署。 参考链接https://blog.csdn.net/sxausgyy/article/details/8151019 要素模型(Feature)要素是GeoTools中的核心术语，它是描述地理空间数据对象的基本单位，它描述了一个现实世界中的客观地理实体，如:一条河流、一座桥梁都可以理解为要素。 栅格数据栅格数据是从上向下拍摄的地球照片，使用离散单元存储其值，图片的每个像素点都有一个指定值服务器本身存储，传输速度快，服务器压力小，多为png,jpg,gif数据量大，更新慢 矢量数据矢量数据是仅仅存储节点，比如箭头，线条，点等。存储的是对象的轮廓，而栅格数据适合表达对象的内容。矢量数据类型有点，线和多边形。数量小，数据更新快服务器压力大，技术要求高 在gist环境下回有图层的概念","categories":[],"tags":[]},{"title":"dockerfile使用","slug":"dockerfile使用","date":"2021-01-08T05:46:19.000Z","updated":"2024-03-14T14:55:29.476Z","comments":true,"path":"2ba9cf6c.html","link":"","permalink":"https://imlike.cc/2ba9cf6c.html","excerpt":"","text":"目前需要创建docker容器 之前用的是基于 Java环境的镜像 构件的,当前开发都后期需要gdal和其他的环境,仅仅基于java的 命令是远远不够用户的 首先gdal 官方提供了源,但是这个源没有java环境需要我们安装,他提供了多个Linux发行版本 包括Ubuntu和alpine ,起初基于Ubuntu small来搭建的 12345678910FROM osgeo&#x2F;gdal:ubuntu-small-3.2.1MAINTAINER gdallikedemoRUN add-apt-repository ppa:openjdk-r&#x2F;ppa &amp;&amp; \\ apt-get update &amp;&amp; \\ apt-get install -y java-1.8.0-openjdk &amp;&amp; \\ apt-get clean;COPY MY_JAR.jar &#x2F;home&#x2F;MY_JAR.jar# RUN nohup java -jar &#x2F;home&#x2F;MY_JAR.jar &gt;&#x2F;dev&#x2F;null 2&gt;&amp;1 &amp;CMD [&quot;java&quot;,&quot;-jar&quot;,&quot;&#x2F;home&#x2F;MY_JAR.jar&quot;] ,但是在添加源的时候又依赖于命令 add-resportiy 添加源,如果用 Ubuntu-full后可以看到文件明显增大到1g,明显不符合要求,我就换了alpine ,看网上介绍路由器和小型设备的,看了后就在这个版本上添加 123456789101112FROM osgeo/gdal:alpine-normal-3.2.0MAINTAINER gdallikedemo# 添加命令 安装javaRUN apk add --repository=http://dl-cdn.alpinelinux.org/alpine/edge/community &amp;&amp; \\ apk add openjdk8 ;# 声明端口EXPOSE 11003# 复制指定文件COPY MY_JAR.jar /home/MY_JAR.jar# RUN nohup java -jar /home/MY_JAR.jar &gt;/dev/null 2&gt;&amp;1 &amp;CMD [&quot;java&quot;,&quot;-jar&quot;,&quot;/home/MY_JAR.jar&quot;] 如果需要后面的服务也可以 参考该要求分层存储 镜像构建时，会一层层构建，前一层是后一层的基础。每一层构建完就不会再发生改变，后一层上的任何改变只发生在自己这一层。比如，删除前一层文件的操作，实际不是真的删除前一层的文件，而是仅在当前层标记为该文件已删除。在最终容器运行的时候，虽然不会看到这个文件，但是实际上该文件会一直跟随镜像。因此，在构建镜像的时候，需要额外小心，每一层尽量只包含该层需要添加的东西，任何额外的东西应该在该层构建结束前清理掉。 Dockerfile 中每一个指令都会建立一层，RUN 也不例外。每一个 RUN 的行为，就和刚才我们手工建立镜像的过程一样：新建立一层，在其上执行这些命令，执行结束后，commit 这一层的修改，构成新的镜像。 一定要注一个实现 不要多次run,尽量集合在一起执行","categories":[],"tags":[{"name":"docker","slug":"docker","permalink":"https://imlike.cc/tags/docker/"}]},{"title":"编写Linux启动文件并设置为自启动使用systemctl","slug":"编写Linux启动文件并设置为自启动使用systemctl","date":"2020-11-29T08:54:38.000Z","updated":"2024-03-14T14:55:29.486Z","comments":true,"path":"3e75944c.html","link":"","permalink":"https://imlike.cc/3e75944c.html","excerpt":"","text":"编写Linux启动文件并设置为自启动使用systemctl常见使用的格式模板 12345678910111213[Unit]Description&#x3D;Sakura Frp Client ServiceAfter&#x3D;network.target syslog.targetWants&#x3D;network.target[Service]Type&#x3D;simpleRestart&#x3D;on-failureRestartSec&#x3D;5sExecStart&#x3D; 执行步骤[Install]WantedBy&#x3D;multi-user.target 可以使用 下面的步骤的 1234567891011121314echo &quot;[Unit] Description=Sakura Frp Client Service After=network.target syslog.target Wants=network.target [Service] Type=simple Restart=on-failure RestartSec=5s ExecStart=$&#123;FRP_EXEC_DIR&#125;$&#123;FRP_EXEC&#125; -f $&#123;FRP_KEY&#125;:$&#123;FRP_TID&#125; [Install] WantedBy=multi-user.target &quot; &gt; /lib/systemd/system/frpc.service 后面可以使用 systemctl daemon-reload 启动 frpc 服务systemctl enable frpcsystemctl start frpc 还有一些其他的systemctl 的 命令 像 status 类似 123456789101112131415[Unit]Description&#x3D;Wants&#x3D;network-online.targetAfter&#x3D;network-online.target[Service]ExecStart&#x3D;User&#x3D;WorkingDirectory&#x3D;Restart&#x3D;on-failureRestartSec&#x3D;3StartLimitBurst&#x3D;10[Install]WantedBy&#x3D;multi-user.target","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"https://imlike.cc/tags/linux/"}]},{"title":"pgsql重要视图转载","slug":"pgsql重要视图转载","date":"2020-11-19T07:10:43.000Z","updated":"2024-03-14T14:55:29.480Z","comments":true,"path":"245877594.html","link":"","permalink":"https://imlike.cc/245877594.html","excerpt":"","text":"转载地址 PostgreSQL统计信息的几个重要视图 1、pg_stat_database 1234567891011121314151617181920-[ RECORD 1 ]--+------------------------------datid | 13156 #数据库的oiddatname | postgres #数据库名numbackends | 0 #访问当前数据库的连接数量xact_commit | 2357 #该数据库事务提交总量：和下面的rollback和作为TPS统计xact_rollback | 17 #该数据库事务rollback总量，如果特别多，需要看业务是否有问题了blks_read | 1946 #总磁盘物理读的块数，这里的read可能是从 cache中读取，如果很高需要结合blk_read_time看是否真的存在从磁盘读取的情况blks_hit | 103625 #从shared buffer命中块数tup_returned | 1413113 #对于表来说，是全表扫描的行数；对于索引是通过索引返回的索引行数，如果这个值明显大于tup_fetched，说明当前数据库存在大量的全表扫描。查看执行计划，这个是databas全局级别的tup_fetched | 36041 #指通过索引返回的行数tup_inserted | 104 #插入的行数tup_updated | 0 #更新的行数tup_deleted | 19 #删除的行数conflicts | 0 #与恢复冲突取消的查询次数，只会在备机上发生temp_files | 0 #产生临时文件的数量，如果这个值很高，需要调大work_memtemp_bytes | 0 #临时文件的大小deadlocks | 0 #死锁的数量，如果这个值很大说明业务逻辑有问题blk_read_time | 0 #数据库中花费在读取文件的时间，这个值很高说明内存较小，需要频繁从磁盘读入数据文件blk_write_time | 0 #数据库中花费在写数据文件的时间，pg中脏页一般写入page cache，如果这个值较高，则说明cache较小，操作系统的cache需要更积极的写入stats_reset | 2019-02-11 23:42:37.526743-08 #统计信息重置的时间 通过pg_stat_database可以大概了解数据库的历史情况。 比如tup_returned值明显大于tup_fetched，历史SQL语句很多是全表扫描，存在没有使用索引的SQL，可结合pg_stat_statments查找慢SQL，也可结合pg_stat_user_table找全表扫描次数和行数最多的表； 通过看tup_updated很高，可以说明数据库有频繁的更新，这个时候需要关注vaccum相关的指标和长事务，如果没有及时进行垃圾回收，会引起表膨胀； temp_files较高说明存在很多排序，hash,或者聚合这种操作，可以增大work_mem减少临时文件的产生，并且同时这些操作的性能也会有较大的提升。 2、pg_stat_user_tables 123456789101112131415161718192021222324yzs&#x3D;# select *from pg_stat_user_tables;-[ RECORD 1 ]-------+------------------------------relid | 16440 #表oidschemaname | public #模式名relname | t1 #表名seq_scan | 50 #这个表进行全表扫描的次数seq_tup_read | 1867763 #全表扫描的数据行数，如果这个值很大说明操作这个表的SQL语句很可能是全表扫描，需要结合执行计划分析idx_scan | #索引扫描的次数idx_tup_fetch | #通过索引扫描返回的行数n_tup_ins | 1130502 #插入的数据行数n_tup_upd | 0 #更新的数据行数n_tup_del | 81920 #删除的数据行数n_tup_hot_upd | 0 #hot update的数据行数，这个值与n_tup_upd接近说明更新性能较好，不需要更新索引n_live_tup | 655366 #活的行数量n_dead_tup | 0 #死记录个数n_mod_since_analyze | 6 #上次analyze的实际last_vacuum | 2019-04-07 00:22:00.955542-07 #上次手动vacuum的实际last_autovacuum | #上次autovacuum的实际last_analyze | #上次analyze时间last_autoanalyze | 2019-04-07 00:26:07.668391-07 #上次自动analyze时间vacuum_count | 2 #vacuum次数autovacuum_count | 0 #自动vacuum次数analyze_count | 0 #analyze次数autoanalyze_count | 10 #自动analyze次数 通过查询pg_stat_user_tables，可以基本清除哪些表的全表扫描次数较多，表中DML哪种操作多，也可以了解垃圾数据的数量。 3、pg_stat_user_indexes 12345678910yzs&#x3D;# select *from pg_stat_user_indexes;-[ RECORD 1 ]-+----------relid | 16447 #相关表的oidindexrelid | 16450 #索引的oidschemaname | public #模式名relname | t3 #表名indexrelname | t3_id_idx #索引名idx_scan | 0 #通过索引扫描的次数，如果该值很小，说明该索引很少被用到，可以考虑删除idx_tup_read | 0 #通过任意索引方法返回的索引行数idx_tup_fetch | 0 #通过索引方法返回的数据行数 可以知道当前哪些索引频繁使用，哪些是无效索引。无效索引可以删除掉，减少磁盘空间的使用和提升insert、delete、update的性能。 4、pg_statio_user_tables 12345678910111213yzs&#x3D;# select *from pg_statio_user_tables;-[ RECORD 1 ]---+--------relid | 16447 schemaname | publicrelname | t3heap_blks_read | 1 #从page cache或磁盘读取表的块数heap_blks_hit | 1 #从shared buffer命中的块数idx_blks_read | 0 #从page cache或磁盘读取的索引的块数idx_blks_hit | 0 #从shared buffer命中的索引块数toast_blks_read | #从page cache或磁盘读取的toast表的块数toast_blks_hit | #在shared buffer中命中toast表的块数tidx_blks_read | #从page cache或者磁盘中读入的toast表索引的块数tidx_blks_hit | #在shared buffer中命中toast表索引的块数 如果heap_blks_read、idx_blks_read很高，说明shared buffer较小，存在频繁从磁盘或者page cache读取到shared buffer中命中toast表的块数。 5、 pg_stat_bgwriter 12345678910111213yzs&#x3D;# select *from pg_stat_bgwriter;-[ RECORD 1 ]---------+------------------------------checkpoints_timed | 206 #指超过checkpoint_timeout的时间后触发的检查点次数checkpoints_req | 8 #手动触发checkpoint或者因为WAL文件数量达到max_wal_size时也会增加，如果这个值大于checkpoints_req说明checkpoint_timeout设置的不合理checkpoint_write_time | 306582 #从shared buffer 中write到page cache花费的时间checkpoint_sync_time | 367 #checkpoint调用fsync将脏数据刷到磁盘花费的时间，如果这个值很长，容易造成IO抖动，需要增加checkpoint_timeout或者checkpoint_completion_targetbuffers_checkpoint | 6671 #通过checkpoint写入脏块的数量buffers_clean | 0 #通过bgwriter写入块的数量maxwritten_clean | 0 #bgwriter超过bgwriter_lru_maxpages时停止的次数，如果这个值很高，需要增加bgwriter_lru_maxpagesbuffers_backend | 7953 #通过backend写入的块数量buffers_backend_fsync | 0 #backend需要fsync的次数buffers_alloc | 11613 #被分配的缓冲区数量stats_reset | 2019-02-11 23:42:35.273758-08 通过这个视图，可以判断checkpoint以及max_wal_size是否合理","categories":[],"tags":[]},{"title":"pgsql和mysql的对比和特性","slug":"pgsql和mysql的对比和特性","date":"2020-10-11T07:18:06.000Z","updated":"2024-03-14T14:55:29.480Z","comments":true,"path":"3192111075.html","link":"","permalink":"https://imlike.cc/3192111075.html","excerpt":"","text":"目前找到的资料和说法如下 后续还会继续补充 1. pgsql 的特点主要是 多线程的使用同时 提供提供了丰富的类型支持 json,varchar, 等,索引的类型也是有很多包括 b-tree,map,hash,gist这些索引 ,同时 处理的数据量也能保证数据量的读写,PostgreSQL支持物化视图 Pg 的列里有隐藏的乐观锁 version 字段, 默认的 repeatable read 级别就能保证并发更新的正确性, 1234567891011GIS 领域处于优势地位，因为它有丰富的几何类型，实际上不止几何类型，PG有大量字典、数组、bitmap 等数据类型PG 的可以使用函数和条件索引，这使得PG数据库的调优非常灵活，mysql就没有这个功能，条件索引在web应用中很重要。PG有极其强悍的 SQL 编程能力（9.x 图灵完备，支持递归！），有非常丰富的统计函数和统计语法支持，比如分析函数（ORACLE的叫法，PG里叫window函数），还可以用多种语言来写存储过程，对于R的支持也很好。这一点上MYSQL就差的很远，很多分析功能都不支持作者：方圆链接：https:&#x2F;&#x2F;www.zhihu.com&#x2F;question&#x2F;20010554&#x2F;answer&#x2F;15863274来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。PG不止支持传统数据类型：numeric、strings、date、decimal等，还支持非结构的数据类型：json、xml、hstore等以及网络数据类型、bit字符串，还有ARRAYS，地理数据类型。 2. mysql的特点主要的对内存的使用比较好 同时包含两个数据库引擎 inodb,MyISAM,目前主要是使用Inodeb来保证数据库的事务和读写问题,同时 MyISAM是有其他功能,主要提供的类型是字符串 ,索引的类型主要是 b-tree,到达百万后数据的瓶颈明显 MySQL 的事务隔离级别 repeatable read 并不能阻止常见的并发更新, 1234567891011121314进程模式与线程模式的对比PostgreSQL和oracle是进程模式，MySQL是线程模式。进程模式对多CPU利用率比较高。进程模式共享数据需要用到共享内存，而线程模式数据本身就是在进程空间内都是共享的，不同线程访问只需要控制好线程之间的同步。线程模式对资源消耗比较少。所以MySQL能支持远比oracle多的更多的连接。对于PostgreSQL的来说，如果不使用连接池软件，也存在这个问题，但PostgreSQL中有优秀的连接池软件软件，如pgbouncer和pgpool，所以通过连接池也可以支持很多的连接。作者：蛙课网链接：https:&#x2F;&#x2F;www.zhihu.com&#x2F;question&#x2F;20010554&#x2F;answer&#x2F;743955463来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。堆表与索引组织表的的对比Oracle支持堆表，也支持索引组织表PostgreSQL只支持堆表，不支持索引组织表Innodb只支持索引组织表索引组织表的优势：表内的数据就是按索引的方式组织，数据是有序的，如果数据都是按主键来访问，那么访问数据比较快。而堆表，按主键访问数据时，是需要先按主键索引找到数据的物理位置。索引组织表的劣势：索引组织表中上再加其它的索引时，其它的索引记录的数据位置不再是物理位置，而是主键值，所以对于索引组织表来说，主键的值不能太大，否则占用的空间比较大。对于索引组织表来说，如果每次在中间插入数据，可能会导致索引分裂，索引分裂会大大降低插入的性能。所以对于使用innodb来说，我们一般最好让主键是一个无意义的序列，这样插入每次都发生在最后，以避免这个问题。由于索引组织表是按一个索引树，一般它访问数据块必须按数据块之间的关系进行访问，而不是按物理块的访问数据的，所以当做全表扫描时要比堆表慢很多，这可能在OLTP中不明显，但在数据仓库的应用中可能是一个问题。作者：蛙课网链接：https:&#x2F;&#x2F;www.zhihu.com&#x2F;question&#x2F;20010554&#x2F;answer&#x2F;743955463来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","categories":[],"tags":[]},{"title":"（转载）经验：什么影响了数据库查询速度、什么影响了MySQL性能","slug":"（转载）经验：什么影响了数据库查询速度、什么影响了MySQL性能","date":"2020-10-11T07:15:20.000Z","updated":"2024-03-14T14:55:29.487Z","comments":true,"path":"3414469055.html","link":"","permalink":"https://imlike.cc/3414469055.html","excerpt":"","text":"平时时不时遇到数据库，CPU飚到100%，或者数据库突然查询时间变长，看不出来，或者无从下手的时候可以看一下下面转载的文章，也有优化的方向 目前来说，pgsql 这种数据对吧千万级的数据，如果不是特别复杂的查询，或者全表搜索这种，应该是最多2~3s 链接的原始地址 一、什么影响了数据库查询速度1.1 影响数据库查询速度的四个因素 1.2 风险分析QPS：*Queries Per Second* 意思是“每秒查询率”，是一台服务器每秒能够相应的查询次数，是对一个特定的查询服务器在规定时间内所处理流量多少的衡量标准。 TPS：是TransactionsPerSecond的缩写，也就是事务数/秒。它是软件测试结果的测量单位。客户机在发送请求时开始计时，收到服务器响应后结束计时，以此来计算使用的时间和完成的事务个数。 Tips：最好不要在主库上数据库备份，大型活动前取消这样的计划。 效率低下的sql：超高的QPS与TPS。 大量的并发：数据连接数被占满（max_connection默认100，一般把连接数设置得大一些）。并发量：同一时刻数据库服务器处理的请求数量 超高的CPU使用率：CPU资源耗尽出现宕机。 磁盘IO：磁盘IO性能突然下降、大量消耗磁盘性能的计划任务。解决：更快磁盘设备、调整计划任务、做好磁盘维护。 1.3 网卡流量：如何避免无法连接数据库的情况 减少从服务器的数量（从服务器会从主服务器复制日志） 进行分级缓存（避免前端大量缓存失效） 避免使用select * 进行查询 分离业务网络和服务器网络 1.4 大表带来的问题（重要）1.4.1 大表的特点 记录行数巨大，单表超千万 表数据文件巨大，超过10个G 1.4.2 大表的危害1.慢查询：很难在短时间内过滤出需要的数据 查询字区分度低 -&gt; 要在大数据量的表中筛选出来其中一部分数据会产生大量的磁盘io -&gt; 降低磁盘效率 2.对DDL影响： 建立索引需要很长时间： MySQL -v&lt;5.5 建立索引会锁表 MySQL -v&gt;=5.5 建立索引会造成主从延迟（mysql建立索引，先在组上执行，再在库上执行） 修改表结构需要长时间的锁表：会造成长时间的主从延迟(‘480秒延迟’) 1.4.3 如何处理数据库上的大表分库分表把一张大表分成多个小表 难点： 分表主键的选择 分表后跨分区数据的查询和统计 1.5 大事务带来的问题（重要）1.5.1 什么是事务 1.5.2事务的ACID属性1、原子性（atomicity)：全部成功，全部回滚失败。银行存取款。 2、一致性（consistent)：银行转账的总金额不变。 3、隔离性（isolation)： 隔离性等级： 未提交读(READ UNCOMMITED) 脏读,两个事务之间互相可见； **已提交读(READ COMMITED)**符合隔离性的基本概念,一个事务进行时，其它已提交的事物对于该事务是可见的，即可以获取其它事务提交的数据。 可重复读(REPEATABLE READ) InnoDB的默认隔离等级。事务进行时，其它所有事务对其不可见，即多次执行读，得到的结果是一样的！ 可串行化（SERIALIZABLE）在读取的每一行数据上都加锁，会造成大量的锁超时和锁征用，严格数据一致性且没有并发是可使用。 查看系统的事务隔离级别：show variables like ‘%iso%’;开启一个新事务：begin;提交一个事务：commit;修改事物的隔离级别：set session tx_isolation=’read-committed’; 4、持久性(DURABILITY)：从数据库的角度的持久性，磁盘损坏就不行了 redo log机制保证事务更新的一致性和持久性 1.5.3 大事务运行时间长，操作数据比较多的事务； 风险：锁定数据太多，回滚时间长，执行时间长。 锁定太多数据，造成大量阻塞和锁超时； 回滚时所需时间比较长，且数据仍然会处于锁定； 如果执行时间长，将造成主从延迟，因为只有当主服务器全部执行完写入日志时，从服务器才会开始进行同步，造成延迟。 解决思路： 避免一次处理太多数据，可以分批次处理； 移出不必要的SELECT操作，保证事务中只有必要的写操作。 二、什么影响了MySQL性能（非常重要）2.1 影响性能的几个方面 服务器硬件。 服务器系统（系统参数优化）。 存储引擎。 MyISAM：不支持事务，表级锁。 InnoDB: 支持事务，支持行级锁，事务ACID。 数据库参数配置。 数据库结构设计和SQL语句。（重点优化） 2.2 MySQL体系结构分三层：客户端-&gt;服务层-&gt;存储引擎 MySQL是插件式的存储引擎，其中存储引擎分很多种。只要实现符合mysql存储引擎的接口，可以开发自己的存储引擎! 所有跨存储引擎的功能都是在服务层实现的。 MySQL的存储引擎是针对表的，不是针对库的。也就是说在一个数据库中可以使用不同的存储引擎。但是不建议这样做。 2.3 InnoDB存储引擎MySQL5.5及之后版本默认的存储引擎：InnoDB。 2.3.1 InnoDB使用表空间进行数据存储。1show variables like &#39;innodb_file_per_table 如果innodb_file_per_table 为 ON 将建立独立的表空间，文件为tablename.ibd； 如果innodb_file_per_table 为 OFF 将数据存储到系统的共享表空间，文件为ibdataX（X为从1开始的整数）； .frm ：是服务器层面产生的文件，类似服务器层的数据字典，记录表结构。 2.3.2 (MySQL5.5默认)系统表空间与(MySQL5.6及以后默认)独立表空间 系统表空间无法简单的收缩文件大小，造成空间浪费，并会产生大量的磁盘碎片。 独立表空间可以通过optimeze table 收缩系统文件，不需要重启服务器也不会影响对表的正常访问。 如果对多个表进行刷新时，实际上是顺序进行的，会产生IO瓶颈。 独立表空间可以同时向多个文件刷新数据。 强烈建议对Innodb 使用独立表空间，优化什么的更方便，可控。 2.3.3 系统表空间的表转移到独立表空间中的方法1、使用mysqldump 导出所有数据库数据（存储过程、触发器、计划任务一起都要导出 ）可以在从服务器上操作。 2、停止MYsql 服务器，修改参数（my.cnf加入innodb_file_per_table），并删除Inoodb相关文件（可以重建Data目录）。 3、重启MYSQL，并重建Innodb系统表空间。 4、 重新导入数据。 或者 Alter table 同样可以的转移，但是无法回收系统表空间中占用的空间。 2.4 InnoDB存储引擎的特性2.4.1 特性一：事务性存储引擎及两个特殊日志类型：Redo Log 和 Undo Log Innodb 是一种事务性存储引擎。 完全支持事务的ACID特性。 支持事务所需要的两个特殊日志类型：Redo Log 和Undo Log Redo Log：实现事务的持久性(已提交的事务)。Undo Log：未提交的事务，独立于表空间，需要随机访问，可以存储在高性能io设备上。 Undo日志记录某数据被修改前的值，可以用来在事务失败时进行rollback；Redo日志记录某数据块被修改后的值，可以用来恢复未写入data file的已成功事务更新的数据。 InnoDB小知识点：InnoDB一棵B+树可以存放多少行数据？ 2.4.2 特性二：支持行级锁 InnoDB支持行级锁。 行级锁可以最大程度地支持并发。 行级锁是由存储引擎层实现的。 2.5 什么是锁2.5.1 锁 2.5.2 锁类型 2.5.3 锁的粒度MySQL的事务支持不是绑定在MySQL服务器本身，而是与存储引擎相关 将table_name加表级锁命令：lock table table_name write; 写锁会阻塞其它用户对该表的‘读写’操作，直到写锁被释放：unlock tables； 锁的开销越大，粒度越小，并发度越高。 表级锁通常是在服务器层实现的。 行级锁是存储引擎层实现的。innodb的锁机制，服务器层是不知道的 2.5.4 阻塞和死锁1.阻塞是由于资源不足引起的排队等待现象。 2.死锁是由于两个对象在拥有一份资源的情况下申请另一份资源，而另一份资源恰好又是这两对象正持有的，导致两对象无法完成操作，且所持资源无法释放。 关于MySQL锁的详解：学会MySQL中的索引和锁 2.6 如何选择正确的存储引擎参考条件： 事务 备份(Innobd免费在线备份) 崩溃恢复 存储引擎的特有特性 总结:Innodb大法好。 注意：尽量别使用混合存储引擎，比如回滚会出问题在线热备问题。 2.7 配置参数2.7.1 内存配置相关参数确定可以使用的内存上限。 内存的使用上限不能超过物理内存，否则容易造成内存溢出；**（对于32位操作系统，MySQL只能使用3G以下的内存。**） 确定MySQL的每个连接单独使用的内存。 1sort_buffer_size #定义了每个线程排序缓存区的大小，MySQL在有查询、需要做排序操作时才会为每个缓冲区分配内存（直接分配该参数的全部内存）；join_buffer_size #定义了每个线程所使用的连接缓冲区的大小，如果一个查询关联了多张表，MySQL会为每张表分配一个连接缓冲，导致一个查询产生了多个连接缓冲；read_buffer_size #定义了当对一张MyISAM进行全表扫描时所分配读缓冲池大小，MySQL有查询需要时会为其分配内存，其必须是4k的倍数；read_rnd_buffer_size #索引缓冲区大小，MySQL有查询需要时会为其分配内存，只会分配需要的大小。 注意：以上四个参数是为一个线程分配的，如果有100个连接，那么需要×100。 MySQL数据库实例： ①MySQL是单进程多线程（而oracle是多进程），也就是说MySQL实例在系统上表现就是一个服务进程，即进程； ②MySQL实例是线程和内存组成，实例才是真正用于操作数据库文件的； 一般情况下一个实例操作一个或多个数据库；集群情况下多个实例操作一个或多个数据库。 如何为缓存池分配内存： Innodb_buffer_pool_size，定义了Innodb所使用缓存池的大小，对其性能十分重要，必须足够大，但是过大时，使得Innodb 关闭时候需要更多时间把脏页从缓冲池中刷新到磁盘中； 总内存-（每个线程所需要的内存\\连接数）-系统保留内存* key_buffer_size，定义了MyISAM所使用的缓存池的大小，由于数据是依赖存储操作系统缓存的，所以要为操作系统预留更大的内存空间； 1select sum(index_length) from information_schema.talbes where engine&#x3D;&#39;myisam&#39; 注意：即使开发使用的表全部是Innodb表，也要为MyISAM预留内存，因为MySQL系统使用的表仍然是MyISAM表。 max_connections 控制允许的最大连接数，一般2000更大。 不要使用外键约束保证数据的完整性。 2.8 性能优化顺序从上到下： END 赏加个鸡腿支付宝微信","categories":[],"tags":[{"name":"转载","slug":"转载","permalink":"https://imlike.cc/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"java","slug":"java","permalink":"https://imlike.cc/tags/java/"}]},{"title":"es索引生命周期维护","slug":"es索引生命周期维护","date":"2020-09-08T07:14:34.000Z","updated":"2024-03-14T14:55:29.476Z","comments":true,"path":"2302551430.html","link":"","permalink":"https://imlike.cc/2302551430.html","excerpt":"","text":"keyword text hot-warm-cold架构将索引数据切分成hot/warm/cold的索引。hot索引负责最新数据的读写，可使用内存存储；warm索引负责较旧数据的读取，可使用内存或SSD存储；cold索引很少被读取，可使用大容量磁盘存储。随着时间的推移，数据不断从hot索引-&gt;warm索引-&gt;cold索引迁移。针对不同阶段的索引我们还可以调整索引的主分片数，副本数，单分片的segment数等等，更好的利用机器资源。 索引生命周期管理（Index Lifecycle Management ，简称ILM)作者：Monica2333链接：https://juejin.im/post/6844904131262431246来源：掘金著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061PUT _ilm&#x2F;policy&#x2F;my_policy&#123; &quot;policy&quot;: &#123; &quot;phases&quot;: &#123; &quot;hot&quot;: &#123; &quot;actions&quot;: &#123; &quot;rollover&quot;: &#123;&#x2F;&#x2F;rollover前距离索引的创建时间最大为7天 &quot;max_age&quot;: &quot;7d&quot;,&#x2F;&#x2F;rollover前索引的最大大小不超过50G &quot;max_size&quot;: &quot;50G&quot;,&#x2F;&#x2F;rollover前索引的最大文档数不超过1个（测试用） &quot;max_docs&quot;: 1, &#125; &#125; &#125;, &quot;warm&quot;: &#123;&#x2F;&#x2F;rollover之后进入warm阶段的时间不小于30天 &quot;min_age&quot;: &quot;30d&quot;, &quot;actions&quot;: &#123; &quot;forcemerge&quot;: &#123;&#x2F;&#x2F;强制分片merge到segment为1 &quot;max_num_segments&quot;: 1 &#125;, &quot;shrink&quot;: &#123;&#x2F;&#x2F;收缩分片数为1 &quot;number_of_shards&quot;: 1 &#125;, &quot;allocate&quot;: &#123;&#x2F;&#x2F;副本数为2 &quot;number_of_replicas&quot;: 2 &#125; &#125; &#125;, &quot;cold&quot;: &#123;&#x2F;&#x2F;rollover之后进入cold阶段的时间不小于60天 &quot;min_age&quot;: &quot;60d&quot;, &quot;actions&quot;: &#123; &quot;allocate&quot;: &#123; &quot;require&quot;: &#123;&#x2F;&#x2F;分配到cold 节点，ES可根据机器资源配置不同类型的节点 &quot;type&quot;: &quot;cold&quot; &#125; &#125; &#125; &#125;, &quot;delete&quot;: &#123;&#x2F;&#x2F;rollover之后进入cold阶段的时间不小于60天 &quot;min_age&quot;: &quot;90d&quot;, &quot;actions&quot;: &#123; &quot;delete&quot;: &#123;&#125; &#125; &#125; &#125; &#125;&#125;作者：Monica2333链接：https:&#x2F;&#x2F;juejin.im&#x2F;post&#x2F;6844904131262431246来源：掘金著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","categories":[],"tags":[]},{"title":"Ubuntu给外接设备修改分辨率","slug":"Ubuntu给外接设备修改分辨率","date":"2020-08-18T07:13:33.000Z","updated":"2024-03-14T14:55:29.475Z","comments":true,"path":"2232115083.html","link":"","permalink":"https://imlike.cc/2232115083.html","excerpt":"","text":"参考链接 How to Set A Custom Screen Resolution in Ubuntu Desktop 目前正常 dp-1上面有2560x1440 2k的分辨率但是有时候显示不出来,导致不能显示最优的效果 1234567891011121314151617DP-1 connected 2560x1440+0+0 (normal left inverted right x axis y axis) 597mm x 336mm 2560x1440 59.95 + 74.97* 1920x1080 60.00 50.00 59.94 1920x1080i 60.00 50.00 59.94 1280x1440 59.91 1680x1050 59.88 1280x1024 75.02 60.02 1440x900 59.90 1280x960 60.00 1280x720 60.00 50.00 59.94 1024x768 75.03 70.07 60.00 832x624 74.55 800x600 72.19 75.00 60.32 56.25 720x576 50.00 720x480 60.00 59.94 640x480 75.00 72.81 60.00 59.94 720x400 70.08 cvt 2560 1440 来看分辨率的最优设置 编辑 ~/.profile的文件来实现 1234sudo xrandr --newmode &quot;2560x1440_74.97&quot; 118.25 1600 1696 1856 2112 900 903 908 934 -hsync +vsyncsudo xrandr --newmode &quot;2560x1440_60.00&quot; 312.25 2560 2752 3024 3488 1440 1443 1448 1493 -hsync +vsyncsudo xrandr --addmode DP-1 &quot;2560x1440_74.97&quot;","categories":[],"tags":[]},{"title":"Jenkins的遇到的坑","slug":"Jenkins的遇到的坑","date":"2020-08-04T07:12:38.000Z","updated":"2024-03-14T14:55:29.473Z","comments":true,"path":"1581320389.html","link":"","permalink":"https://imlike.cc/1581320389.html","excerpt":"","text":"maven本地仓库地址Jenkins本地仓库依赖的common和工具包,是独立安装的,使用在解析上述依赖时会出现问题,需要在Jenkins所在的本地 maven install 一下,或者在用sh写脚本让执行 更新完plugin部分job看不到的情况重启还是进一步操作后出现,不是project,job 看不到的情况 网上看到的是配置没到到 jenkins_home/workspace 下面的project, 但是我看Jenkins的配置是对的,在对应的文件夹下面是有配置 网上给的Jenkins迁移job的问题我看了不是这个问题 我遇到的是job下面的config.xml maven2-module失败导致的最后解决方案是重启tomcat不是Jenkins是,重启Jenkins是 URL/jenkins/restart, 看到tomcat的日志出现解析 jobs下面的各个job下面的config.xml 出问题 maven2-module ,对于非maven的项目就可以直接解析, 原来是 Maven Integration 插件不存在导致,原有的maven项目不能解析和显示","categories":[],"tags":[{"name":"jenkins","slug":"jenkins","permalink":"https://imlike.cc/tags/jenkins/"},{"name":"maven","slug":"maven","permalink":"https://imlike.cc/tags/maven/"}]},{"title":"树莓派下面搭建pgsql","slug":"树莓派下面搭建pgsql","date":"2020-06-19T07:09:18.000Z","updated":"2024-03-14T14:55:29.484Z","comments":true,"path":"1327663525.html","link":"","permalink":"https://imlike.cc/1327663525.html","excerpt":"","text":"搭建pgsql的环境查看的文档 怎么在一台树莓派上安装 Postgres 数据库 树莓派中安装MySQL 5.7感慨是准备是安装MySQL的,树莓派的环境 : Linux raspberrypi 4.19.97-v7l+ #1294 2020 armv7l 目前直接用 sudo apt-get install mysql-service 但是不行返回结果 没有可用的软件包 mysql-server，但是它被其它的软件包引用了。这可能意味着这个缺失的软件包可能已被废弃，或者只能在其他发布源中找到然而下列软件包会取代它：mariadb-server-10.0 目前来说直接安装的路被堵死 wget http://ftp.debian.org/debian/pool/main/m/mysql-5.7/libmysqlclient-dev_5.7.18-1_armhf.debwget http://ftp.debian.org/debian/pool/main/m/mysql-5.7/libmysqlclient20_5.7.18-1_armhf.debwget http://ftp.debian.org/debian/pool/main/m/mysql-5.7/libmysqld-dev_5.7.18-1_armhf.debwget http://ftp.debian.org/debian/pool/main/m/mysql-5.7/mysql-client-5.7_5.7.18-1_armhf.debwget http://ftp.debian.org/debian/pool/main/m/mysql-5.7/mysql-client-core-5.7_5.7.18-1_armhf.debwget http://ftp.debian.org/debian/pool/main/m/mysql-5.7/mysql-server-5.7_5.7.18-1_armhf.debwget http://ftp.debian.org/debian/pool/main/m/mysql-5.7/mysql-server-core-5.7_5.7.18-1_armhf.debwget http://ftp.debian.org/debian/pool/main/m/mysql-defaults/mysql-common_5.8+1.0.2_all.deb sudo dpkg -i mysql-common_5.8+1.0.2_all.debsudo dpkg -i mysql-client-core-5.7_5.7.18-1_armhf.debsudo dpkg -i mysql-client-5.7_5.7.18-1_armhf.debsudo dpkg -i mysql-server-core-5.7_5.7.18-1_armhf.debsudo dpkg -i mysql-server-5.7_5.7.18-1_armhf.deb上面的命令是直接在debian上下载 arm版本的MySQL但是现在访问网站已经找不到对于的deb包 而且在MySQL官网也没找到官方下载链接 arm的包 访问http://ftp.debian.org/debian/pool/main/m/mysql-5.7/ 拿到对于的deb包版本为5.7.26-1,比上面的高 wget http://ftp.debian.org/debian/pool/main/m/mysql-5.7/libmysqlclient-dev_5.7.26-1+b1_armhf.debwget http://ftp.debian.org/debian/pool/main/m/mysql-5.7/libmysqlclient20_5.7.26-1+b1_armhf.debwget http://ftp.debian.org/debian/pool/main/m/mysql-5.7/libmysqld-dev_5.7.26-1+b1_armhf.debwget http://ftp.debian.org/debian/pool/main/m/mysql-5.7/mysql-client-5.7_5.7.26-1+b1_armhf.debwget http://ftp.debian.org/debian/pool/main/m/mysql-5.7/mysql-client-core-5.7_5.7.26-1+b1_armhf.debwget http://ftp.debian.org/debian/pool/main/m/mysql-5.7/mysql-server-5.7_5.7.26-1+b1_armhf.debwget http://ftp.debian.org/debian/pool/main/m/mysql-5.7/mysql-server-core-5.7_5.7.26-1+b1_armhf.debwget http://ftp.debian.org/debian/pool/main/m/mysql-defaults/mysql-common_5.8+1.0.2_all.deb sudo dpkg -i mysql-common_5.8+1.0.2_all.debsudo dpkg -i mysql-client-core-5.7_5.7.26-1+b1_armhf.debsudo dpkg -i mysql-client-5.7_5.7.26-1+b1_armhf.debsudo dpkg -i mysql-server-core-5.7_5.7.26-1+b1_armhf.debsudo dpkg -i mysql-server-5.7_5.7.26-1+b1_armhf.deb安装后显示 dpkg: 依赖关系问题使得 mysql-server-core-5.7 的配置工作不能继续：mysql-server-core-5.7 依赖于 libaio1 (&gt;= 0.3.93)；然而：未安装软件包 libaio1。mysql-server-core-5.7 依赖于 libc6 (&gt;= 2.29)；然而：系统中 libc6:armhf 的版本为 2.28-10+rpi1。mysql-server-core-5.7 依赖于 libstdc++6 (&gt;= 9)；然而：系统中 libstdc++6:armhf 的版本为 8.3.0-6+rpi1。 依赖太多找不到目前放弃安装MySQL使用官方推进的Mariadbsudo apt-get install mariadb-server 当服务器安装完成后，你需要运行： sudo mysql_secure_installation来为你的数据库设置安全。 主要这个用途是设置你 root 的密码，root 的远程访问权限，是否允许匿名访问，是否允许远程访问等。 如果你只是测试在本地使用这个数据库，你可以不用设置这个。 但是我们还是建议你设置。 当前使用pi这个用户无法登陆Mariadb的需要用root登陆 CREATE USER ‘pi’@’%’ IDENTIFIED BY ‘123’;GRANT USAGE ON . TO ‘pi’@’%’;GRANT ALL PRIVILEGES ON . TO ‘pi’@’%’ IDENTIFIED BY ‘123’ WITH GRANT OPTION;FLUSH PRIVILEGES; 重启服务器的命令是： root@raspberrypi:~# service mariadb restart 安装pgsql主要参考下面的博客文章 怎么在一台树莓派上安装 Postgres 数据库 安装pgsql还是简单的 首先输入命令 apt-get install postgresql 先安装后软件已经有客户端了,同时会带也会初始化的用户 : postgres Postgres 用户去配置数据库： sudo su postgres 初始化一个新的用户(pi) createuser pi -P –interactive PostgreSQL 配置允许远程连接： 1、 编辑 PostgreSQL 配置文件 /etc/postgresql/9.6/main/postgresql.conf ，取消 listen_addresses 行的注释，并把它的值从 localhost 改变成 *。然后保存并退出。 2、 编辑 pg_hba 配置文件 /etc/postgresql/9.6/main/postgresql.conf，将 127.0.0.1/32 改变成 0.0.0.0/0 （对于IPv4）和将 ::1/128 改变成 ::/0 （对于 IPv6）。然后保存并退出。 3、 重启 PostgreSQL 服务： sudo service postgresql restart。 sudo service postgresql restart","categories":[],"tags":[]},{"title":"知识管理工具","slug":"知识管理工具","date":"2020-05-31T00:49:06.000Z","updated":"2024-03-14T14:55:29.485Z","comments":true,"path":"803410763.html","link":"","permalink":"https://imlike.cc/803410763.html","excerpt":"","text":"为什么要知识管理 知识管理就是(Personal Knowledge Management),在当前一个知识爆炸的时代,无效信息不经过整理,过几天就会只存在收藏夹 不能被接受的知识是无效,这里的知识是要能被使用或者是可以传授的,不是读书感受,类似与一种综合的笔记 知识和信息不能混为一谈，信息是免费的，但毫无目标的获取信息，就无法将信息转化为知识。因此学习必须要有目标，如果你用有限的时间去学无限的知识，你将被知识“淹没”，我们要学习那些给自己带来最大提升的知识。 时间是一个最稀缺的资源，在学习知识的过程中，必须考虑知识获取的“成本”问题，这个成本除了金钱之外，最重要的是时间成本。因此，学习知识的工具需要满足：“花费时间较短，获得价值较高的知识”。 对于纸质书籍的发现和评论，豆瓣读书是一个理想的工具，在使用之前，先标记自己读过的较为喜欢的书籍，不建议标记娱乐性书籍，这样能保证“豆瓣猜”的准确性，之后豆瓣就能根据你的阅读兴趣找到相关的书籍，你可以根据其他人对书籍的评价来决定是否购买。期刊杂志也是同样的道理。 总的来说，网络和电子读物适合浅阅读，具有深度和专业的知识还是需要阅读纸质书籍，你付出了金钱，却节省了时间。 平时工作学习中遇到问题，可以直接使用搜索引擎来寻找答案，通过搜索引擎也可以学到一些你不知道的小知识。 ​ 学习和积累的基础就是能对要学习的知识点可以罗列出来同时加入自己的认识和理解,不仅有利用后面复习和及时查看 同时文档可以导出和打印 方便查看 知识管理工具用一个产品就可以做文档（Google docs）、管理项目进度（Trello）、做知识文档（Confluence、Github Wiki）、团队协同文档（Dropbox Paper、Quip）、搭建个人博客、做 在线笔记、知识管理（Evernote），而且不用科学上网基础功能不用像 Office 365一样付费，即使付费也可走万能的淘宝，之前还有小伙伴买到永久会员资格知识库管理,文档和知识库,团队的wiki , 当前的知识管理工具常见类型 思维导图型 主要构成是方块和连线以及备注构成的图 类似的鱼骨图 各种云笔记(印象笔记,有道云笔记,为知笔记)全部是在web端的操作 文档型的笔记,由文件夹,子文件夹,文档(富文本,支持插入office或者思维导图) wiki,javadoc,Confluence类 存在条理清晰的目录和内容也是富文本在公司内部使用比较多 notion,语雀 等综合类 这类知识管理工具都是以page(语雀以知识库中的文档和目录来体现)),notion是万物都可以引用,page内加page同时可以引用别人的内容 Notion 自称是一款「将笔记、知识库和任务管理无缝整合的协作平台」。它具有无限的层级和相互链接的组织弹性，给笔记间的关系提供了足够自由的组织方式；它排版灵活，能够把笔记里的内容按块（Block）进行组织和拖拽，甚至可以做出 Trello 看板进行项目管理；它的内容类型丰富，可以嵌入图片、网页、文档甚至视频，几乎无所不能。 https://sspai.com/post/39694 少数派: Notion：重新定义数字笔记 怎么获取和筛选知识 搜索引擎多个搜索引擎搜索,加插件屏蔽到搜索结果中的广告 http://s.uc.cn/ uc的多重搜索 多吉搜索 外网一手资料 各种wiki和简单入门的网站 博客 和论坛都是好去处 …. 培养积累和学习的习惯 ….","categories":[],"tags":[{"name":"知识管理","slug":"知识管理","permalink":"https://imlike.cc/tags/%E7%9F%A5%E8%AF%86%E7%AE%A1%E7%90%86/"}]},{"title":"ubuntu输出的分辨率","slug":"ubuntu输出的分辨率","date":"2020-05-26T05:38:11.000Z","updated":"2024-03-14T14:55:29.482Z","comments":true,"path":"3468817301.html","link":"","permalink":"https://imlike.cc/3468817301.html","excerpt":"","text":"xrandr 之前会默认的1080p,我这个显示器是2k的分辨率,使用thunderbolt转接槽接的HDMI接口 但是有时候(长时间不工作息屏,先关机后关显示器电源)时会出现显示不出来2k的选项, 2560x1440 59.95 + 74.97* sudo xrandr –newmode “1920x1080_60.00” 173.00 1920 2048 2248 2576 1080 1083 1088 1120 -hsync +vsync sudo xrandr –addmode VGA1 “1920x1080_60.00” sudo xrandr –output VGA1 –mode “1920x1080_60.00”","categories":[],"tags":[{"name":"ubuntu","slug":"ubuntu","permalink":"https://imlike.cc/tags/ubuntu/"},{"name":"thunderbolt","slug":"thunderbolt","permalink":"https://imlike.cc/tags/thunderbolt/"}]},{"title":"Git/GitHub 中文术语表","slug":"GitGitHub 中文术语表","date":"2020-05-26T00:08:22.000Z","updated":"2024-03-14T14:55:29.473Z","comments":true,"path":"19126373.html","link":"","permalink":"https://imlike.cc/19126373.html","excerpt":"","text":"​ 转载Linux中国关于 Git/GitHub 中文术语表 ,可以参考一下不至于混乱 Git 和 GitHub 已经成为了开发者的基础工具，尤其是参与开源软件开发时经常会使用它们。但是在 Git 和 GitHub 使用过程中遇到的很多术语并没有标准的或约定俗成的中文译名，因此，我们根据 GitHub 、Git 等文档，并结合我们的翻译惯例，收集整理了 Git 和 GitHub 中常用术语的中文译名及其解释。 这里值得注意是术语有复刻、挂钩、议题、星标、变基、仓库等，这些术语之前要么经常中英文混杂使用，要么中文译法不确定，我们根据多年的翻译和开发经验，在 GitHub 译法的基础上进行斟酌，整理了如下的术语表供大家使用参考。此外，“复刻”这个翻译应该是我们 LCTT 首倡的；而“议题”这个对 issue 的译法也比之前的一些其它译法更为精准；“仓库”一词还有存储库、版本库等译法，但是仓库一词似乎更加合适。 受让人assignee分配到某个议题的用户。 追溯blameGit 中的“追溯”功能描述对文件每行的最新修改，一般会显示修订、作者和时间。这很有用，例如，可以跟踪何时添加了功能，或者哪个提交导致了特定漏洞。 分支branch分支是仓库的平行版本。它包含在仓库中，但不影响主要或 master 分支，可让你自由工作而不中断“即时”版本。在执行所需的更改后，可以将分支合并回 master 分支以发布更改。 检出checkout你可以在命令行上使用 git checkout 创建新分支，将当前的工作分支更改为不同的分支，甚至使用 git checkout [branchname] [path to file]从不同的分支切换到不同版本的文件。“检出”操作会使用对象数据库中的树对象或 blob 更新工作树的全部或部分，以及更新索引和 HEAD（如果整个工作树指向新分支）。 优选cherry-picking从一系列更改（通常是提交）中选择一部分更改，并在不同的代码库上将它们记录为新的更改系列。在 Git 中，这通过 git cherry-pick 命令来执行，在另一个分支上解压缩现有提交引入的更改，并根据当前分支的提示将其记录为新提交。 清洁clean工作树在对应当前头部引用的版本时是清洁的。另请参阅“脏”。 克隆clone克隆是指存在于计算机上而非网站服务器其他位置的仓库副本，或者是复制的操作。在克隆时，可在首选编辑器中编辑文件，使用 Git 跟踪更改而无需保持在线。你克隆的仓库仍与远程版本连接，以便当你在线时将本地更改推送到远程，以保持同步。 行为准则code of conduct为如何参与社区制定标准的文档。 代码所有者code owner被指定为部分仓库代码所有者的个人。当有人打开对代码所有者拥有的代码进行更改的拉取请求（非草稿模式）时，会自动申请代码所有者审查。 协作者collaborator协作者是受仓库所有者邀请参与，对仓库拥有读取和写入权限的人。 提交commit提交或“修订”是对一个文件（或一组文件）的个别更改。在进行提交以保存工作时，Git 会创建唯一的 ID（也称为 “SHA” 或“哈希”），用于记录提交的特定更改以及提交者和提交时间。提交通常包含一条提交消息，其中简要说明所做的更改。 提交作者commit author进行提交的用户。 提交 IDcommit ID也称为 SHA。用于识别提交的 40 字符校验和的哈希。 提交消息commit message随附于提交的简短描述性文字，用于沟通提交引入的更改。 持续集成continuous integration也称为 CI。在个人对 GitHub 上配置的仓库提交更改后运行自动化构建和测试的过程。CI 是软件开发中一种帮助检测错误的常用最佳实践。 贡献指南contribution guidelines说明人们应如何参与项目的文档。 贡献contributionsGitHub 上的特定活动。 贡献者contributor贡献者是指对仓库没有协作者权限但参与过项目，并且他们打开的拉取请求已合并到仓库的人员。 默认分支default branch仓库中的基本分支，除非你指定不同的分支，否则会自动对它完成所有拉取请求和代码提交。此分支通常称为 master。 游离的 HEADdetached HEAD如果你操作的是游离的 HEAD，Git 将会警告你，这意味着 Git 不指向某个分支，并且你的任何提交都不会出现在提交历史记录中。例如，在检出并非任何特定分支最新提交的任意提交时，你操作的是“游离的 HEAD”。 差异diff差异是指两个提交之间的更改或保存的更改之间的区别，它将从视觉上描述文件自上次提交后添加或删除的内容。 脏dirty工作树如果包含尚未提交到当前分支的更改，将被视为“脏”。 快进fast-forward快进是一种特殊类型的合并，在其中你有修订以及“合并”另一个分支的更改作为现有分支的子系。在这种情况下，你无法进行新的合并提交，而只是更新此修订。这在远程仓库的远程跟踪分支中经常发生。 功能分支feature branch用于试验新功能或修复未正式使用的议题的分支。也称为主题分支。 围栏代码块fenced code block你可以在代码块前后使用三个反引号 `````，通过 GitHub Flavored Markdown 创建缩进代码块。 获取fetch在使用 git fetch 时，你将从远程仓库添加更改到本地工作分支，而不提交它们。与 git pull 不同，提取可让你在更改提交到本地分支之前先进行审查。 跟进（用户）following (users)获取关于另一个用户的贡献和活动的通知。 强制推送force push一种使用本地更改覆盖远程仓库的 Git 推送，不管是否冲突。 复刻fork复刻是其他用户仓库在你的帐户上的个人副本。复刻允许你自由更改项目而不影响原始上游仓库。你也可以在上游仓库中打开拉取请求，并使复刻同步最新的更改，因为两个仓库仍然互相连接。 gitfile一种普通的 .git 文件，始终位于工作树的根部，指向 Git 目录，包含整个 Git 仓库及其元数据。你可以在命令行上使用 git rev-parse --git-dir 查看仓库（实际仓库）的此文件。 HEAD当前分支。 挂钩hook在多个 Git 命令正常执行时，对可选脚本进行标注以允许开发者添加功能或检查。通常，挂钩允许预先验证和潜在中止命令，并且允许在操作完成后再发事后通知。 实例instance组织包含在其配置和控制的虚拟机中的 GitHub 私有副本。 议题issue议题是提议的与仓库相关的改进、任务或问题。（对于公共仓库）任何人都可创建议题，然后由仓库协作者调解。每个议题都包含自己的讨论线程。你也可以使用标签将议题归类并分配到某人。 密钥指纹key fingerprint用于识别较长公钥的简短字节系列。 关键词keyword用在拉取请求中时关闭议题的特定文字。 标签label议题或拉取请求的标记。仓库随附一系列默认标签，但用户也可创建自定义标签。 LFSGit Large File Storage。一种开源 Git 扩展，用于对大文件进行版本控制。 许可证license一种可随附于项目的文档，告知们能够对你的源代码执行哪些操作，不能执行哪些操作。 行注释line comment拉取请求内特定代码行上的评论。 主干master默认开发分支。只要创建 Git 仓库，就会创建一个名为 master 的分支，并且它会变为活动的分支。大多数情况下，这包含本地开发，但纯属惯例，而非必需。 提及mention一种通过在用户名前加上 @ 符号来发送给用户的通知。GitHub 上组织中的用户也可成为可提及的团队一部分。 合并merge合并是从一个分支（在相同的仓库中或来自一个分叉）提取更改，然后将其应用到另一个分支。这通常是作为“拉取请求”（可视为请求合并）或通过命令行完成。如果没有冲突的更改，可通过 GitHub.com web 界面使用拉取请求完成合并，或始终通过命令行完成。 合并冲突merge conflict合并的分支之间发生的差异。当人们对同一文件的同一行进行不同的更改时，或者一个人编辑某文件而另一个人删除该文件时，就会发生合并冲突。必须解决合并冲突后才可合并分支。 合并请求merge request合并请求（MR）是 GitLab 上类似于 GitHub 上的拉取请求的概念。 里程碑milestone一种跟踪仓库中议题或拉取请求组进度的方式。 镜像mirror仓库的新副本。 非快进non-fast-forward当仓库的本地副本未与上游仓库同步时，你在推送本地更改之前需要提取上游更改。 通知notificationweb 或电子邮件（根据你的设置）传送的更新，提供你感兴趣的活动的相关信息。 外部协作者outside collaborator已被授予访问一个或多个组织的仓库但对组织没有其他访问权限的用户，且不属于组织的成员。 开源open source开源软件是可供任何人自由使用、修改和共享（以修改和未修改的形式）的软件。今天，“开源”的概念通常扩展到软件以外，代表一种协作原则，其中工作材料在线供任何人分叉、修改、讨论和参与。 源origin默认上游仓库。大多数项目至少有一个它们跟踪的上游项目。默认情况下，源用于该目的。 所有者owner对组织有完全管理权限的组织成员。 私有贡献private contributions对私有（与公共相对）仓库的贡献。 私有仓库private repository私有仓库仅对仓库所有者和所有者指定的协作者可见。 生产分支production branch包含可使用或部署到应用程序或站点的最终更改的分支。 个人资料profile显示 GitHub 上用户活动相关信息的页面。 受保护分支protected branch受保护分支在仓库管理员选择保护的分支上禁止多项 Git 功能。必要检查未通过或必需审查未批准，不能对它们执行强制推送、删除和更改合并，或者不能从 GitHub web 界面上传文件到其中。受保护分支通常是默认分支。 公共贡献public contributions对公共（与私有相对）仓库的贡献。 公共仓库public repository公共仓库可供任何人查看，包括不是 GitHub 用户的人员。 拉取pull拉取是指提取与合并更改。例如，如果有人编辑了你操作的远程文件，你要将这些更改拉取到本地副本，以使其保持最新。另请参阅“提取”。 拉取权限pull access读取权限的同义词。 拉取请求pull request拉取请求（PR）是提议更改用户提交的仓库，然后被仓库协作者接受或拒绝。像议题一样，每个拉取请求都有自己的论坛。 拉取请求审查pull request review拉取请求中协作者批准更改或在拉取请求合并之前申请进一步更改的评论。 推送push推送是指将提交的更改发送到 GitHub.com 上的远程仓库。例如，如果你在本地更改内容，便可推送这些更改，让其他人访问。 推送分支push a branch成功将分支推送到远程仓库后，使用本地分支中的更改来更新远程分支。在你“推送分支”时，Git 将会到远程仓库中搜索分支的头部引用，并验证它是分支本地头部引用的直系原型。在验证后，Git 将拉取所有对象（从本地头部引用可获取，而远程仓库中缺失）到远程对象数据库，然后更新远程头部引用。如果远程头部不是本地头部的原型，推送将会失败。 推送权限push access写入权限的同义词。 读取权限read access对仓库的权限级别，允许用户拉取或者读取仓库中的信息。所有公共仓库向所有 GitHub 用户授予读取权限。拉取权限的同义词。 自述文件README包含仓库中文件相关信息的文本文件，通常是仓库访问者看到的第一个文件。自述文件连同仓库许可证、参与指南以及行为准则，帮助你交流要求和管理项目的参与。 变基rebase将一系列更改从一个分支重新应用到不同的基本分支，并将该分支的头部重置为结果。 发布releaseGitHub 封装软件并向用户提供软件的方式。 远程remote这是托管于服务器（很可能是 GitHub.com）上的仓库或分支版本。远程版本可以连接到本地克隆，以使更改保持同步。 远程仓库remote repository用于跟踪同一个项目但储存在其他位置的仓库。 远程 URLremote URL存储代码的位置：GitHub、其他用户分支甚至不同服务器 上的仓库。 副本replica为主要 GitHub Enterprise 实例提供冗余的 GitHub Enterprise 实例。 仓库repository仓库是 GitHub 最基本的元素，最容易被想象成项目的文件夹。一个仓库包含所有项目文件（包括文档），并且存储每个文件的修订历史记录。仓库可有多个协作者，也可以是公共仓库或私有仓库。 仓库维护员repository maintainer管理仓库的人员。此人可帮助对议题分类，以及使用标签和其他功能管理仓库的工作，也可负责更新自述文件和参与文件。 解决resolve手动修复自动合并失败的操作。 还原revert恢复 GitHub 上的拉取请求时，新拉取请求会自动打开，其中有一个提交用于从原始合并的拉取请求恢复合并提交。在 Git 中，你可以使用 git revert 恢复提交。 审查review审查允许对仓库具有访问权限的其他人评论拉取请求中提议的更改、审批更改或在拉取请求合并之前请求进一步更改。 服务挂钩service hook也称为“Web 挂钩”。Web 挂钩是一种通知方式，只要仓库或组织上发生特定操作，就会发送通知到外部 web 服务器。 压扁squash将多个提交合并为一个。也是 Git 命令。 暂存实例staging instance在修改应用到实际 GitHub Enterprise 实例之前测试修改的一种方式。 状态status拉取请求中的可视表现形式，表示你的提交符合你参与的仓库所设定的条件。 星标star仓库的书签或赞赏表示。星标是项目受欢迎程度排名的手动方式。 主题分支topic branch开发者用来识别开发概念行的常规 Git 分支。由于分支很容易并且便宜，因此往往适合拥有多个小分支，每个小分支包含定义明确的概念，或者渐进但相关的更改。也可称为“特征分支”。 上游upstream在谈论分支或分叉时，原始仓库的主要分支通常被称为“上游”，因为它是其他更改的主要来源。你操作的分支/分叉则称为“下游”。也称为“源”。 上游分支upstream branch合并到所述分支的默认分支（或所述分支变基到的分支）。它通过 branch.&lt;name&gt;.remote 和 branch.&lt;name&gt;.merge 配置。如果 A 的上游分支是源/B，有时我们会说“A 在跟踪源/B”。 查看watch你可以关注仓库或议题，以便在议题或拉取请求有更新时接收通知。 web 挂钩webhooksWeb 挂钩可让你构建或设置订阅 GitHub.com 上特定事件的 GitHub 应用程序。Web 挂钩提供一种通知方式，只要仓库或组织中发生特定操作，就会发送通知到外部 web 服务器。也称为“服务挂钩”。 写入权限write access对仓库的权限级别，可让用户推送或写入更改到仓库。","categories":[],"tags":[{"name":"git","slug":"git","permalink":"https://imlike.cc/tags/git/"}]},{"title":"将博客搬至CSDN","slug":"搬家","date":"2020-05-21T06:50:31.000Z","updated":"2024-03-14T14:55:29.484Z","comments":true,"path":"1341101137.html","link":"","permalink":"https://imlike.cc/1341101137.html","excerpt":"","text":"将博客搬至CSDN,方便搜索引擎检索","categories":[],"tags":[{"name":"搬家","slug":"搬家","permalink":"https://imlike.cc/tags/%E6%90%AC%E5%AE%B6/"}]},{"title":"activiti零零碎碎的记录","slug":"activiti零零碎碎的记录","date":"2020-05-10T13:15:14.000Z","updated":"2024-03-14T14:55:29.475Z","comments":true,"path":"3807663978.html","link":"","permalink":"https://imlike.cc/3807663978.html","excerpt":"","text":"熟悉app创建app然后是用发布 processes可以看出来 对于每个用户会有任务 同时这个同时称为activiti workflow(工作流) deploy的可能 processengineConfiguration -&gt;processEngine-&gt;respositoryService taskSerivice 对于任务的查询 identityService 用户先关的查询 formService runtimeService ManagementService HistoryService 对于上面的接口 ProcessEngines.getDefaultProcessEngine()会在第一次调用时 初始化并创建一个流程引擎，以后再调用就会返回相同的流程引擎。 使用对应的方法可以创建和关闭所有流程引擎：ProcessEngines.init() 和 ProcessEngines.destroy()。 RepositoryService可能是使用Activiti引擎时最先接触的服务。 它提供了管理和控制发布包和流程定义的操作。 这里不涉及太多细节，流程定义是BPMN 2.0流程的java实现。 它包含了一个流程每个环节的结构和行为。 发布包是Activiti引擎的打包单位。一个发布包可以包含多个BPMN 2.0 xml文件和其他资源。 开发者可以自由选择把任意资源包含到发布包中。 既可以把一个单独的BPMN 2.0 xml文件放到发布包里，也可以把整个流程和相关资源都放在一起。 （比如，’hr-processes’实例可以包含hr流程相关的任何资源）。 可以通过RepositoryService来部署这种发布包。 发布一个发布包，意味着把它上传到引擎中，所有流程都会在保存进数据库之前分析解析好。 从这点来说，系统知道这个发布包的存在，发布包中包含的流程就已经可以启动了。 RuntimeService正好是完全相反的。它负责启动一个流程定义的新实例。 如上所述，流程定义定义了流程各个节点的结构和行为。 流程实例就是这样一个流程定义的实例。对每个流程定义来说，同一时间会有很多实例在执行。 RuntimeService也可以用来获取和保存流程变量。 这些数据是特定于某个流程实例的，并会被很多流程中的节点使用 （比如，一个排他网关常常使用流程变量来决定选择哪条路径继续流程）。 Runtimeservice也能查询流程实例和执行。 执行对应BPMN 2.0中的’token’。基本上执行指向流程实例当前在哪里。 最后，RuntimeService可以在流程实例等待外部触发时使用，这时可以用来继续流程实例。 流程实例可以有很多暂停状态，而服务提供了多种方法来’触发’实例， 接受外部触发后，流程实例就会继续向下执行。 它是Activiti这类BPMN引擎的核心功能之一。 所有与任务有关的功能都包含在TaskService中： 查询分配给用户或组的任务 创建独立运行任务。这些任务与流程实例无关。 手工设置任务的执行者，或者这些用户通过何种方式与任务关联。 认领并完成一个任务。认领意味着一个人期望成为任务的执行者， 即这个用户会完成这个任务。完成意味 着“做这个任务要求的事情”。 通常来说会有很多种处理形式。 IdentityService非常简单。它可以管理（创建，更新，删除，查询…）群组和用户。 请注意， Activiti执行时并没有对用户进行检查。 例如，任务可以分配给任何人，但是引擎不会校验系统中是否存在这个用户。 这是Activiti引擎也可以使用外部服务，比如ldap，活动目录，等等。 FormService是一个可选服务。即使不使用它，Activiti也可以完美运行， 不会损失任何功能。这个服务提供了启动表单和任务表单两个概念。 启动表单会在流程实例启动之前展示给用户， 任务表单会在用户完成任务时展示。Activiti支持在BPMN 2.0流程定义中设置这些表单。 这个服务以一种简单的方式将数据暴露出来。再次重申，它时可选的， 表单也不一定要嵌入到流程定义中。 HistoryService提供了Activiti引擎手机的所有历史数据。 在执行流程时，引擎会保存很多数据（根据配置），比如流程实例启动时间，任务的参与者， 完成任务的时间，每个流程实例的执行路径，等等。 这个服务主要通过查询功能来获得这些数据。 ManagementService在使用Activiti的定制环境中基本上不会用到。 它可以查询数据库的表和表的元数据。另外，它提供了查询和管理异步操作的功能。 Activiti的异步操作用途很多，比如定时器，异步操作， 延迟暂停、激活，等等。后续，会讨论这些功能的更多细节。","categories":[],"tags":[{"name":"activiti","slug":"activiti","permalink":"https://imlike.cc/tags/activiti/"}]},{"title":"ubuntu软件的配置","slug":"ubuntu软件的配置","date":"2020-05-10T12:55:01.000Z","updated":"2024-03-14T14:55:29.482Z","comments":true,"path":"3494110652.html","link":"","permalink":"https://imlike.cc/3494110652.html","excerpt":"","text":"nginx 12345678910server &#123; listen 80; server_name localhost [www.example.com](http://www.example.com/); root /Users/yangyi/www; #全局定义，表示在该server下web的根目录 client_max_body_size 100M; keepalive_timeout 20; index index.php index.html index.htm; charset utf-8; access_log logs/host.access.log main; #用来指定此虚拟主机的访问日志存放路径，输出格式为main。 error_log logs/host.error.log error; #错误日志存放路径，输出格式为error。 error_page 404 /404.html; #状态码为404时的时候的网页地址,还可定义500,502之类的 .... &#125; 以上一些配置为在该server下具有全局性，例如 nginx允许跨域配置:return 204;上是针对预检请求（preflight request）,后面add_header 是应对的CROS浏 123456789location &#x2F; &#123; if ($request_method &#x3D; &#39;OPTIONS&#39;) &#123; return 204; &#125; add_header Access-Control-Allow-Origin *; add_header Access-Control-Allow-Methods &#39;GET, POST, OPTIONS&#39;; add_header Access-Control-Allow-Headers &#39;DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Authorization&#39;;&#125; docker 123456docker run -d -p 80:8001 --name kodexplorer -v &lt;path&#x2F;to&#x2F;data&gt;:&#x2F;var&#x2F;www&#x2F;html xaljer&#x2F;kodexplorerdocker run -d -p 8001:8001 --name yangxuan8282&#x2F;kodexplorer -v &quot;$PWD&quot;:&#x2F;var&#x2F;www&#x2F;html yangxuan8282&#x2F;kodexplorerdocker run --name&#x3D;my_portainer -d -p 80:8001 -v &#x2F;var&#x2F;run&#x2F;docker.sock:&#x2F;var&#x2F;run&#x2F;docker.sockdocker run --name&#x3D;kodexplorer -d -p 8001:8001 -v &#x2F;var&#x2F;run&#x2F;docker.sock:&#x2F;var&#x2F;run&#x2F;docker.sockdocker run --privileged&#x3D;true -d -p 8001:80 --name kod-dockerfile-1 kod:v8 npm 123456789cnpm intall hexo-generator-search --savecnpm intall hexo-prism-plugin --savecnpm install hexo-all-minifier --savecnpm i -S hexo-prism-plugin --savenpm config set proxy http:&#x2F;&#x2F;127.0.0.1:7890npm config listnpm config set registry npm install --registry&#x3D;https:&#x2F;&#x2F;registry.npm.taobao.org postgresql 1. 修改postgresql.conf postgresql.conf存放位置在/etc/postgresql/9.x/main下，这里的x取决于你安装PostgreSQL的版本号，编辑或添加下面一行，使PostgreSQL可以接受来自任意IP的连接请求。 listen_addresses = ‘*’ 2. 修改pg_hba.conf pg_hba.conf，位置与postgresql.conf相同，虽然上面配置允许任意地址连接PostgreSQL，但是这在pg中还不够，我们还需在pg_hba.conf中配置服务端允许的认证方式。任意编辑器打开该文件，编辑或添加下面一行。 # TYPE DATABASE USER CIDR-ADDRESS METHOD host all all 0.0.0.0/0 md5","categories":[],"tags":[{"name":"ubuntu","slug":"ubuntu","permalink":"https://imlike.cc/tags/ubuntu/"},{"name":"linux","slug":"linux","permalink":"https://imlike.cc/tags/linux/"},{"name":"软件配置","slug":"软件配置","permalink":"https://imlike.cc/tags/%E8%BD%AF%E4%BB%B6%E9%85%8D%E7%BD%AE/"}]},{"title":"网站推荐","slug":"navigation","date":"2020-05-09T23:49:23.000Z","updated":"2024-03-14T14:55:29.479Z","comments":true,"path":"2974411664.html","link":"","permalink":"https://imlike.cc/2974411664.html","excerpt":"","text":"## 导航类 免费视频网站、APP和高清电影下载合集爱达杂货铺 聚BT - 聚合最优质的BT、磁力资源BT搜索 磁力搜索 云盘搜索 影视APP 在线影视 磁力影视 种子搜索 免费的在线影视网站汇总 - 不死鸟 - 分享为王 龙轩导航-做个有用的导航 BT磁力搜索网址_BTMOO ## 临时文件传输(少用百度云) 奶牛快传 蓝奏云 Firefox Send 文叔叔 ## bt磁力网站 bt 磁力搜索网站 比特球云盘 btfox 麻花影视采集 剧汇 TV 萝莉岛 APP bt 磁力网站大全 网页在线工具 ## 资讯类 今日热榜官网 鱼塘热榜 月光博客-关注互联网和搜索引擎的IT科技博客 Solidot: 奇客的资讯，重要的东西 阮一峰的网络日志 FT中文网 - 全球财经精粹 International Business, World News &amp; Global Stock Market Analysis–CNBC HK, China, Asia news &amp; opinion from SCMP’s global editionSouth China Morning Post 南华早报 ## 视频播放和vip解析 低端影视 渧渧哩哩 速影TV - 免费快速在线电影网 独播库 云播TV-云播,云影院,云播放,云点播,最新电影,最新电视剧 电影蜜蜂-高清电影资源最新电影下载 完美看看-海量高清电影、电视剧、动漫免费在线观看 kiwi视频解析 搜搜GIF - 解析尽在掌握","categories":[],"tags":[{"name":"网站","slug":"网站","permalink":"https://imlike.cc/tags/%E7%BD%91%E7%AB%99/"},{"name":"推荐","slug":"推荐","permalink":"https://imlike.cc/tags/%E6%8E%A8%E8%8D%90/"}]},{"title":"acitiviti的NativeQuery","slug":"acitiviti的NativeQuery","date":"2020-05-03T09:06:08.000Z","updated":"2024-03-14T14:55:29.475Z","comments":true,"path":"2572672426.html","link":"","permalink":"https://imlike.cc/2572672426.html","excerpt":"","text":"有写过activiti activiti数据库的分类 ​ 之前有Query接口但是自定义程度太低,针对复杂的查询还是需要**NativeQuery **还是需要用下面是我转载的一篇文章 2017.2.21 activiti实战–第十三章–流量数据查询与跟踪（一）查询接口介绍及运行时数据查询 第十三章 流量数据查询与跟踪 本章讲解运行时与历史数据的查询方法。主要包含三种：标准查询，Native查询，CustomSql查询。 13.1 Query API Activiti的查询API分为： 1234561 标准查询 以Java对象的方式，创建一个指定类型的Query对象，然后链式编程查询。 缺点：不支持复杂的查询。2 Nativit查询 采用标准SQL的方式查询，因此支持复杂的查询。 缺点：仅支持部分对象的查询。 （1）Query接口与NativeQuery Query接口： 12345671 （I）Query&lt;...&gt;2 asc() T3 desc() T4 count() long5 singleResult() U6 list() List&lt;U&gt;7 listPage(int,int) List&lt;U&gt; NativeQuery接口（可以看到与Query无区别）： 12345671 （I）NativeQuery&lt;...&gt;2 asc() T3 desc() T4 count() long5 singleResult() U6 list() List&lt;U&gt;7 listPage(int,int) List&lt;U&gt; Query接口的子接口： 12345678910111213141516 1 UserQuery 2 GroupQuery 3 4 JobQuery 5 ProcessDefinitionQuery 6 DeploymentQuery 7 ModelQuery 8 TaskQuery 9 ExecutionQuery10 ProcessInstanceQuery1112 HistroricProcessInstanceQuery13 HistroricTaskInstanceQuery14 HistroricActivityInstanceQuery15 HistroricDetailQuery16 HistroricVariableInstanceQuery NativeQuery接口的子接口（正如前面所说，NativeQuery只支持部分流程对象）： 12345678910111213141516 1 UserQuery 2 GroupQuery 3 4 JobQuery 5 ProcessDefinitionQuery 6 DeploymentQuery 7 ModelQuery 8 NativeTaskQuery 9 NativeExecutionQuery10 NativeProcessInstanceQuery1112 NativeHistroricProcessInstanceQuery13 NativeHistroricTaskInstanceQuery14 NativeHistroricActivityInstanceQuery15 HistroricDetailQuery16 HistroricVariableInstanceQuery （2）使用xxxService创建查询 运行时与历史数据涉及到的Service有：RuntimeService，TaskService，HistoryService。 这里以TaskService为例： 1231 （I）TaskService2 createTaskQuery（） TaskQuery3 createNativeTaskQuery（） NativeTaskQuery 13.2 运行时数据查询 运行时涉及到的Service是：RuntimeService，TaskService。 （1）流程对象对应的数据库表 Activiti提供了通过流程对象获取对应表名的接口，比如： 121 String tableName &#x3D; managementService.getTableName(Task.Class)&#x2F;&#x2F;此时返回的tableName &#x3D; &quot;ACT_RU_TASK&quot; 下面列出流程对象对应的数据库表名。 类名 表名 **.task.Task ACT_RU_TASK **.runtime.Job ACT_RU_JOB **.runtime.Execution ACT_RU_EXECUTION **.runtime.ProcessInstance ACT_RU_EXECUTION **.repository.ProcessDefinition ACT_RE_PROCDEF **.repository.Deployment ACT_RE_DEPLOYMENT **.history.HistoricProcessInstance ACT_HI_PROCINST **.history.HistoricActivityInstance ACT_HI_ACTINST **.history.HistoricDetail ACT_HI_DETAIL **.history.HistoricVariableUpdate ACT_HI_DETAIL **.history.HistoricFormProperty ACT_HI_DETAIL **.history.HistoricTaskInstance ACT_HI_TASKINST **.history.HistoricVariableInstance ACT_HI_VARINST （2）任务查询 任务查询的时候，一般都需要满足分页查询的要求。所以标准查询无法做到，采用Native查询。从前面的接口介绍中可以看出，NativeTaskQuery是有的。 12341 List&lt;Task&gt; tasks &#x3D; taskService.createNativeTaskQuery()2 .sql(&quot;SELECT * FROM ACT_RU_TASK T WHERE T.NAME_ &#x3D; #&#123;taskName&#125;&quot; ) &#x2F;&#x2F;此处的表明也可通过managementService.getTableName(Task.Class)获得3 .parameter(&quot;taskName&quot;,&quot;人事审批&quot;)4 .listPage()；&#x2F;&#x2F;使用list()则不分页 如果不需要分页，采用标准查询也可以完成task的查询功能。 如果要条件查询，只需要修改sql语句，把 = 换做 like 即可。 （2）查询参与的流程 需求：查询已经办理过的流程，以及跟踪流程的办理情况。 使用标准查询： 12341 &#x2F;&#x2F;第9章时已经介绍过ProcessInstance与Execution的关系为一对多2 List&lt;ProcessInstance&gt; processInstanceList &#x3D; runtimeService.createProcessInstanceQuery().list();34 List&lt;Execution&gt; executionList &#x3D; runtimeService.createExecutionQuery().list(); 但是ProcessInstanceQuery与ExecutionQuery均没有提供”查询某个用户相关的流程”这个接口。所以这时需要用到Native查询。 native查询的用法前面已经讲过。sql语句如下： 123456781 --用于native查询的sql语句2 SELECT RES.* FROM ACT_RU_EXECUTION RES3 LEFT JOIN ACT_HI_TASKINST ART4 ON ART.PROC_INST_ID_ &#x3D; RES.PROC_INST_ID_5 WHERE ART.ASSIGNEE_ &#x3D; #&#123;userId&#125;6 AND ACT_ID_ IS NOT NULL7 AND IS_ACTIVE_ &#x3D; &#39;TRUE&#39;8 ORDER BY START_TIME_ DESC","categories":[],"tags":[{"name":"activiti","slug":"activiti","permalink":"https://imlike.cc/tags/activiti/"}]},{"title":"ubuntu目前经常用到的命令","slug":"ubuntu目前经常用到的命令","date":"2020-04-25T07:55:33.000Z","updated":"2024-03-14T14:55:29.482Z","comments":true,"path":"1501012444.html","link":"","permalink":"https://imlike.cc/1501012444.html","excerpt":"","text":"最近常用的方法 Linux下软链接 12345sudo ln -s /usr/local/node-v12.16.2-linux-x64/node /usr/local/bin/nodesudo ln -s /usr/local/node-v12.16.2-linux-x64/npm /usr/local/bin/npmecho &#x27;export PATH=&quot;/usr/local/node-v12.16.2-linux-x64/bin:$PATH&quot;&#x27; &gt;&gt; ~/.zshrcsudo chown -R leek node-v12.16.2-linux-x64 更改用户权限 终端走代理 12export http_proxy&#x3D;http:&#x2F;&#x2F;127.0.0.1:7890 export ALL_PROXY&#x3D;socks5:&#x2F;&#x2F;127.0.0.1:7891","categories":[],"tags":[{"name":"ubuntu","slug":"ubuntu","permalink":"https://imlike.cc/tags/ubuntu/"},{"name":"开发","slug":"开发","permalink":"https://imlike.cc/tags/%E5%BC%80%E5%8F%91/"}]},{"title":"树莓派天翼云盘目录索引","slug":"树莓派天翼云盘目录索引","date":"2020-04-19T08:36:28.000Z","updated":"2024-03-14T14:55:29.485Z","comments":true,"path":"759123676.html","link":"","permalink":"https://imlike.cc/759123676.html","excerpt":"","text":"或许是最好用的天翼云目录索引程序-sharelist sharelist 项目地址 1 该项目需要先有nodejs的环境,注意树莓派arm版本的 npm config set registry https://registry.npm.taobao.org npm install -g pm2 123456| ShareList is installing ...install.sh: 29: install.sh: npm: not foundinstall.sh: 30: install.sh: npm: not foundinstall.sh: 32: install.sh: pm2: not foundinstall.sh: 33: install.sh: pm2: not foundinstall.sh: 34: install.sh: pm2: not found 中间出现问题安装完npm还有这个问题 123456789101112131415161718192021222324252627282930313233343536#!/bin/bashPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/binecho &quot;+============================================================+&quot;echo &quot;| ShareList Installer |&quot;echo &quot;| |&quot;echo &quot;| &lt;reruin@gmail.com&gt; |&quot;echo &quot;|------------------------------------------------------------|&quot;echo &quot;| https://reruin.net |&quot;echo &quot;+============================================================+&quot;echo &quot;&quot;echo -e &quot;\\n| ShareList is installing ... &quot;# depsif [ -n &quot;$(command -v apt-get)&quot; ]then apt-get install -y curl &gt;/dev/null 2&gt;&amp;1 curl -sL https://deb.nodesource.com/setup_8.x | bash - &gt;/dev/null 2&gt;&amp;1 apt-get install -y nodejs &gt;/dev/null 2&gt;&amp;1elif [ -n &quot;$(command -v yum)&quot; ]then yum install -y curl &gt;/dev/null 2&gt;&amp;1 curl --silent --location https://rpm.nodesource.com/setup_8.x | bash - &gt;/dev/null 2&gt;&amp;1 yum install -y nodejs &gt;/dev/null 2&gt;&amp;1finpm installnpm install pm2 -gpm2 start app.js --name sharelist --env prodpm2 savepm2 startupecho -e &quot;|\\n| Success: ShareList has been installed\\n|&quot; 看了下是在29行出问题的 pm2 list pm2 show applicationName pm2 start 会运行当前目录下的app.js 加入后管理按钮就在页面下面","categories":[],"tags":[{"name":"树莓派","slug":"树莓派","permalink":"https://imlike.cc/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/"},{"name":"天翼云","slug":"天翼云","permalink":"https://imlike.cc/tags/%E5%A4%A9%E7%BF%BC%E4%BA%91/"}]},{"title":"vue中axios中post跨域SpringBoot获取不到RequestBody数据","slug":"vue中axios中post跨域SpringBoot获取不到RequestBody数据","date":"2020-04-11T18:59:20.000Z","updated":"2024-03-14T14:55:29.482Z","comments":true,"path":"552952049.html","link":"","permalink":"https://imlike.cc/552952049.html","excerpt":"","text":"目前使用axios实现vue中与后台数据交互,目前GET,DELETE都是正常使用,但是会有跨域的问题,我就是临时在返回头加上了Access-Control-Allow-Origin:”*” 但是一直由于POST提交数据后台数据一直获取不到RequestBody,我之前使用postman测试都是可以通过的,显示status failure后台也根本不报错,因为获取不到 开始是以为是我格式问题换了几种数据 123456789101112131415161718192021axios.post(&#x27;http://localhost:8080/xxx/model/&#x27; + path, modelIds).then(response = &gt; &#123;this.searchContent()&#125;).catch(function (error) &#123; console.log(error);&#125;);axios.post(&#x27;http://localhost:8080/xxx/process/model/27501&#x27;, &#123; comment: &#x27;Fred&#x27;, userName: &#x27;Flintstone&#x27;&#125;).then(function (response) &#123; console.log(response);&#125;).catch(function (error) &#123; console.log(error);&#125;);const options = &#123; method: &#x27;POST&#x27;, headers: &#123;&#x27;content-type&#x27;: &#x27;application/json&#x27;&#125;, data: modelIds, url: &#x27;http://localhost:8080/xxx/model/&#x27; + path&#125;;axios(options) 还有就是PUT提交时 application/x-www-form-urlencoded,后台用RequestParam,或者不写都可以接受,对象的话要set,get方法,我刚开始用的form-data,SpringBoot是获取不到数据的 我目前用的是nginx,代理","categories":[],"tags":[{"name":"vue","slug":"vue","permalink":"https://imlike.cc/tags/vue/"},{"name":"axios","slug":"axios","permalink":"https://imlike.cc/tags/axios/"}]},{"title":"hexo的next主题换到Yelee(Yilia)","slug":"hexo的next主题换到Yelee(Yilia)","date":"2020-04-10T20:00:39.000Z","updated":"2024-03-14T14:55:29.478Z","comments":true,"path":"1815339691.html","link":"","permalink":"https://imlike.cc/1815339691.html","excerpt":"","text":"原有的主题是已经配过valine的 配置valine, 需要注意下面的配置 本主题基于主题 Hexo-Theme-Yilia 修改而来，在此再次感谢原作者 Litten。修复了一些 bugs，改变了大量的样式，添加了不少特性。对原主题百般折腾后，发觉变动越来越大， 索性就发布个新主题了，主题随我微博名 “夜Yelee” 。个人喜欢简洁的样式，重视内容的浏览， 同时希望作为个人网站的博客，能稍微凸显出博主个性。各种修改折腾大抵基于以上考虑。 layout/_partial/post/nav.ejs12345678910111213141516171819&lt;% if (theme.valine &amp;&amp; theme.valine.appid &amp;&amp; theme.valine.appkey)&#123; %&gt;&lt;section id=&quot;comments&quot; class=&quot;comments&quot; style=&quot;margin:30px;padding:15px;background:#fff&#125; &quot;&gt; &lt;style&gt; @media screen and (max-width:800px) &#123; .comments &#123; margin: auto; padding: 10px; background: #fff &#125; &#125; &lt;/style&gt; &lt;%- partial(&#x27;../comments/valine&#x27;, &#123; key: post.slug, title: post.title, url: config.url+url_for(post.path) &#125;) %&gt;&lt;/section&gt;&lt;% &#125; %&gt; layout/_partial/comments/valine.ejs123456789101112131415161718192021222324252627282930&lt;div id=&quot;vcomment&quot; class=&quot;comment&quot;&gt;&lt;/div&gt;&lt;!--载入js，在 body之前插入即可--&gt;&lt;!--Leancloud 操作库:--&gt;&lt;script src=&quot;//cdn1.lncld.net/static/js/3.0.4/av-min.js&quot;&gt;&lt;/script&gt;&lt;!--Valine 的核心代码库--&gt;&lt;script src=&#x27;//cdn.jsdelivr.net/npm/valine/dist/Valine.min.js&#x27;&gt;&lt;/script&gt;&lt;script&gt; var notify = &#x27;&lt;%= theme.valine.notify %&gt;&#x27; == true ? true : false; var verify = &#x27;&lt;%= theme.valine.verify %&gt;&#x27; == true ? true : false; new Valine(&#123; // AV 对象来自上面引入av-min.js(老司机们不要开车➳♡゛扎心了老铁) av: AV, el: &#x27;#vcomment&#x27;, emoticon_url: &#x27;https://cloud.panjunwen.com/alu&#x27;, emoticon_list: [&quot;狂汗.png&quot;, &quot;不说话.png&quot;, &quot;汗.png&quot;, &quot;坐等.png&quot;, &quot;献花.png&quot;, &quot;不高兴.png&quot;, &quot;中刀.png&quot;, &quot;害羞.png&quot;, &quot;皱眉.png&quot;, &quot;小眼睛.png&quot;, &quot;暗地观察.png&quot; ], app_id: &#x27;&lt;%= theme.valine.appid %&gt;&#x27;, app_key: &#x27;&lt;%= theme.valine.appkey %&gt;&#x27;, placeholder: &#x27;&lt;%= theme.valine.placeholder %&gt;&#x27; &#125;); if (window.location.hash) &#123; var checkExist = setInterval(function () &#123; if (document.getElementById(window.location.hash.replace(&quot;#&quot;, &quot;&quot;))) &#123; location.href = window.location.hash; clearInterval(checkExist); &#125; &#125;, 250); &#125;&lt;/script&gt; 起初参考MonoLogueChi博客,不知道什么原因:( 在主页的界面的每个标题下面都有一个留言的窗口 于是我将加到 ARTICLE.EJS 换了地方 themes/hexo-theme-yelee/layout/_partial/post/nav.ejs ,注意换了路径需要将 &lt;%- partial(‘../comments/valine’, {key: post.slug,title: post.title,url: config.url+url_for(post.path)}) %&gt; 中的../POST/valine相对地址也跟着换掉 自动生成摘要官方推荐的文章摘要格式 方法一: &lt;!-- more --&gt;123&gt; title: Hello World&gt; date: 2015-12-03 00:00:00&gt; ---&gt; &lt;Excerpt in index | 首页摘要&gt;&gt;&lt;!-- more --&gt;&gt;&lt;The rest of contents | 余下全文&gt;&gt; &lt;!-- more --&gt; 之前最好不要有空格等字符； 方法二: description in Front-matter12&gt; title: Hello World&gt; date: 2015-12-03 00:00:00&gt; +description: &quot;Welcome to Hexo! This is your very first post.&quot;&gt;---&gt; &lt;Contents&gt;&gt; 通过 description 添加的摘要只能为纯文本； 但是很复杂每次写的时候都要添加对于标签重复劳动,对于之前的文章都要这样去设置很机械,后面发现了 有 hexo-auto-excerpt 可以使用 npm install –save hexo-auto-excerpt 将layout/_partial/article.ejs，在div.article-entry中的代码换成下面的 12345678910111213141516171819&lt;% if (index)&#123; %&gt; &lt;% if (post.excerpt) &#123; %&gt; &lt;%- post.excerpt %&gt; &lt;% &#125; else if (theme.auto_excerpt.enable) &#123; %&gt; &lt;% var br_position &#x3D; 0 %&gt; &lt;% for (var br_count &#x3D; 0; br_count &lt; theme.auto_excerpt.lines; br_count++) &#123; %&gt; &lt;% br_position &#x3D; post.content.indexOf(&#39;\\n&#39;,br_position + 1) %&gt; &lt;% if(br_position &lt; 0) &#123; break &#125; %&gt; &lt;% &#125; %&gt; &lt;% if(br_position &gt; 0) &#123; %&gt; &lt;% show_all_content &#x3D; false %&gt; &lt;p&gt;&lt;%- post.content.substring(0, br_position + 1) %&gt;&lt;p&gt; &lt;% &#125; %&gt; &lt;% &#125; %&gt; &lt;% &#125; else &#123; %&gt; &lt;%- partial(&#39;toc&#39;, &#123; post: post&#125;) %&gt; &lt;%- post.content %&gt; &lt;% &#125; %&gt; &lt;&#x2F;div&gt; _config.yml 添加下面代码 123auto_excerpt: enable: true lines: 3 hexo博文摘要生成方案 Yelee(Yilia)打赏和调节版本署名的样式yelee没有打赏功能目前需要自己实现目前需要修改 _config.yml 1234567891011121314151617181920212223242526#打赏donate: enable: true #设定打赏功能是否可用 reward_wording: 加个鸡腿 wechat: /pics/weixin.png #微信二维码图片的路径 alipay: /pics/alipay.jpg #支付宝二维码图片的路径# 打赏# 打赏type设定：0-关闭打赏； 1-文章对应的md文件里有reward:true属性，才有打赏； 2-所有文章均有打赏reward_type: 2# 打赏wordingreward_wording: &#x27;加个鸡腿&#x27;# 支付宝二维码图片地址，跟你设置头像的方式一样。比如：/assets/img/alipay.jpgalipay: /pics/weixin.png# 微信二维码图片地址weixin: /pics/alipay.jpg# rss配置feed: type: atom path: atom.xml limit: 20 hub: content: content_limit: 140 content_limit_delim: &#x27; &#x27; order_by: -date icon: icon.png layout/_partial/post/nav.ejs 12345678910111213141516171819202122232425262728293031323334&lt;% if (((theme.reward_type &#x3D;&#x3D;&#x3D; 2 &amp;&amp; !post.reward) || (theme.reward_type &#x3D;&#x3D;&#x3D; 1 &amp;&amp; post.reward)) &amp;&amp; !index)&#123; %&gt;&lt;link rel&#x3D;&quot;stylesheet&quot; type&#x3D;&quot;text&#x2F;css&quot; href&#x3D;&quot;&#x2F;share&#x2F;reward.css&quot; &#x2F;&gt;&lt;div class&#x3D;&quot;page-reward&quot;&gt; &lt;a href&#x3D;&quot;javascript:;&quot; class&#x3D;&quot;page-reward-btn tooltip-top&quot;&gt; &lt;div class&#x3D;&quot;tooltip tooltip-east&quot;&gt; &lt;span class&#x3D;&quot;tooltip-item&quot;&gt; 赏 &lt;&#x2F;span&gt; &lt;span class&#x3D;&quot;tooltip-content&quot;&gt; &lt;span class&#x3D;&quot;tooltip-text&quot;&gt; &lt;span class&#x3D;&quot;tooltip-inner&quot;&gt; &lt;p class&#x3D;&quot;reward-p&quot;&gt;&lt;i class&#x3D;&quot;icon icon-quo-left&quot;&gt;&lt;&#x2F;i&gt;&lt;%&#x3D; theme.reward_wording%&gt;&lt;i class&#x3D;&quot;icon icon-quo-right&quot;&gt;&lt;&#x2F;i&gt;&lt;&#x2F;p&gt; &lt;div class&#x3D;&quot;reward-box&quot;&gt; &lt;% if(theme.alipay) &#123;%&gt; &lt;div class&#x3D;&quot;reward-box-item&quot;&gt; &lt;img class&#x3D;&quot;reward-img&quot; src&#x3D;&quot;&lt;%&#x3D; theme.alipay%&gt;&quot;&gt; &lt;span class&#x3D;&quot;reward-type&quot;&gt;支付宝&lt;&#x2F;span&gt; &lt;&#x2F;div&gt; &lt;% &#125; %&gt; &lt;% if(theme.weixin) &#123;%&gt; &lt;div class&#x3D;&quot;reward-box-item&quot;&gt; &lt;img class&#x3D;&quot;reward-img&quot; src&#x3D;&quot;&lt;%&#x3D; theme.weixin%&gt;&quot;&gt; &lt;span class&#x3D;&quot;reward-type&quot;&gt;微信&lt;&#x2F;span&gt; &lt;&#x2F;div&gt; &lt;% &#125; %&gt; &lt;&#x2F;div&gt; &lt;&#x2F;span&gt; &lt;&#x2F;span&gt; &lt;&#x2F;span&gt; &lt;&#x2F;div&gt; &lt;&#x2F;a&gt;&lt;&#x2F;div&gt;&lt;% &#125; %&gt; 最后是在source下的share文件夹下保存css文件 地址../share/reward.css 修改版本署名的样式 /themes/hexo-theme-yelee/layout/_partial/post/nav.ejs 1234567891011121314&lt;div class&#x3D;&quot;is_copyright&quot;&gt;&lt;ui class&#x3D;post-copyright&gt; &lt;li&gt;&lt;strong&gt;&lt;%&#x3D; __(&#39;copyright_info.title&#39;) %&gt;:&lt;&#x2F;strong&gt;&lt;a href&#x3D;&quot;&lt;%- url_for(post.path) %&gt;&quot;&gt;&lt;%&#x3D; post.title %&gt;&lt;&#x2F;a&gt;&lt;&#x2F;li&gt; &lt;li&gt;&lt;strong&gt;&lt;%&#x3D; __(&#39;copyright_info.author&#39;) %&gt;:&lt;&#x2F;strong&gt;&lt;a href&#x3D;&quot;&#x2F;&quot; title&#x3D;&quot;&lt;%&#x3D; __(&#39;tooltip.back2home&#39;) %&gt;&quot;&gt;&lt;%&#x3D;theme.author%&gt;&lt;&#x2F;a&gt; &lt;&#x2F;li&gt; &lt;li&gt;&lt;strong&gt;&lt;%&#x3D; __(&#39;copyright_info.date&#39;) %&gt;:&lt;&#x2F;strong&gt;&lt;%&#x3D; post.date.format(&quot;YYYY-MM-DD, HH:mm:ss&quot;) %&gt; &lt;&#x2F;li&gt; &lt;li&gt; &lt;strong&gt;&lt;%&#x3D; __(&#39;copyright_info.updated&#39;) %&gt;:&lt;&#x2F;strong&gt;&lt;%&#x3D; post.updated.format(&quot;YYYY-MM-DD, HH:mm:ss&quot;) %&gt; &lt;&#x2F;li&gt; &lt;li&gt; &lt;strong&gt;&lt;%&#x3D; __(&#39;copyright_info.url&#39;) %&gt;:&lt;&#x2F;strong&gt; &lt;a class&#x3D;&quot;post-url&quot; href&#x3D;&quot;&lt;%- url_for(post.path) %&gt;&quot; title&#x3D;&quot;&lt;%&#x3D; post.title %&gt;&quot;&gt;&lt;%&#x3D; post.permalink %&gt;&lt;&#x2F;a&gt; &lt;strong class&#x3D;&quot;copy-path&quot; data-clipboard-text&#x3D;&quot;&lt;%&#x3D; __(&#39;copyright_info.from&#39;) %&gt; &lt;%&#x3D; post.permalink %&gt; &lt;%&#x3D; __(&#39;copyright_info.by&#39;) %&gt; &lt;%&#x3D;theme.author%&gt;&quot; title&#x3D;&quot;&lt;%&#x3D; __(&#39;tooltip.copyPath&#39;) %&gt;&quot;&gt; &lt;i class&#x3D;&quot;fa fa-clipboard&quot;&gt;&lt;&#x2F;i&gt;&lt;&#x2F;strong&gt; &lt;script&gt; var clipboard &#x3D; new Clipboard(&#39;.copy-path&#39;); &lt;&#x2F;script&gt; &lt;&#x2F;li&gt; &lt;li&gt; &lt;strong&gt;&lt;%&#x3D; __(&#39;copyright_info.license&#39;) %&gt;:&lt;&#x2F;strong&gt;&lt;i class&#x3D;&quot;fa fa-creative-commons&quot;&gt;&lt;&#x2F;i&gt; &lt;a rel&#x3D;&quot;license&quot; href&#x3D;&quot;http:&#x2F;&#x2F;creativecommons.org&#x2F;licenses&#x2F;by-nc-sa&#x2F;4.0&#x2F;&quot; title&#x3D;&quot;CC BY-NC-SA 4.0 International&quot; target &#x3D; &quot;_blank&quot;&gt;&quot;&lt;%&#x3D; __(&#39;copyright_info.cc&#39;) %&gt;&quot;&lt;&#x2F;a&gt; &lt;%&#x3D; __(&#39;copyright_info.notice&#39;) %&gt; &lt;&#x2F;li&gt; &lt;&#x2F;ui&gt;&lt;&#x2F;div&gt; 代码区前后有很多空行1234.article-entrypre, code font-family: font-mono, monospace, font-chs font-size: 1em## 后面加这几个.line:after content: &#x27;&#x27; display: inline-block; 解决链接 添加统计字数12345678&lt;% if (post.tags &amp;&amp; post.tags.length)&#123; %&gt;-&lt;div class&#x3D;&quot;article-tag tagcloud&quot;&gt; &lt;div class&#x3D;&quot;article-tag tagcloud&quot; style&#x3D;&quot;display: flex; flex-wrap: wrap&quot;&gt; &lt;%- list_tags(post.tags, &#123; show_count: false, class: &#39;article-tag&#39; &#125;) %&gt; &lt;span class&#x3D;&quot;post-count&quot;&gt;总字数&lt;%&#x3D; wordcount(post.content) %&gt;&lt;&#x2F;span&gt; &lt;span class&#x3D;&quot;post-count&quot;&gt;预计阅读&lt;%&#x3D; min2read(post.content) %&gt;分钟&lt;&#x2F;span&gt; &lt;&#x2F;div&gt; &lt;% &#125; %&gt;","categories":[],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://imlike.cc/tags/hexo/"},{"name":"Yelee","slug":"Yelee","permalink":"https://imlike.cc/tags/Yelee/"},{"name":"Yilia","slug":"Yilia","permalink":"https://imlike.cc/tags/Yilia/"},{"name":"hexo主题","slug":"hexo主题","permalink":"https://imlike.cc/tags/hexo%E4%B8%BB%E9%A2%98/"}]},{"title":"使用yandex搭建自定义域名的邮件服务","slug":"使用yandex搭建自定义域名的邮件服务","date":"2020-04-10T19:36:20.000Z","updated":"2024-03-14T14:55:29.483Z","comments":true,"path":"2045142583.html","link":"","permalink":"https://imlike.cc/2045142583.html","excerpt":"","text":"2019年最新Yandex免费域名邮箱申请使用全攻略 我注册的域名是免费的 注册ml域名的地址下面的注册好的列表 注册不成功显示 Not available 可以使用这个入口http://www.point.ml/en/index.html?lang=en 刚开始用的是https://www.bajiedns.com/这个dns,但是配置后依照没反应 dns.la,he dns.com 这两个都要手机号码而且后面操作也麻烦,我后面选择dnspod之前用的就是这个后面是要变收费 ,看页面也没提免费解析, 1 .修改nameservr(NS)-&gt;f1g1ns1.dnspod.net ,f1g1ns2.dnspod.net. 2 .验证这个域名是你的在yandex哪会有 yandex-verification 的部分 3 .解析域名到mx.yandex.com 这样才能使用他们的邮件服务 4 .添加一个TXT记录，子域名为“mail._domainkey”，值为“v=DKIM1; k=rsa; t=s jjj”。 上述配置完后就可以使用 adduser 创建用户后,注意创建后也要在网页https://mail.yandex.com登陆同意协议后方能使用 123IMAP：imap.yandex.com 端口：993(SSL)POP3：pop.yandex.com 端口：995(SSL)SMTP：smtp.yandex.com 端口：465(SSL)","categories":[],"tags":[{"name":"mail","slug":"mail","permalink":"https://imlike.cc/tags/mail/"},{"name":"yandex","slug":"yandex","permalink":"https://imlike.cc/tags/yandex/"},{"name":"域名","slug":"域名","permalink":"https://imlike.cc/tags/%E5%9F%9F%E5%90%8D/"},{"name":"邮件服务","slug":"邮件服务","permalink":"https://imlike.cc/tags/%E9%82%AE%E4%BB%B6%E6%9C%8D%E5%8A%A1/"}]},{"title":"ubuntu进程(daemon)管理,启动和停止","slug":"ubuntu进程(daemon)管理,启动和停止","date":"2020-04-06T21:01:16.000Z","updated":"2024-03-14T14:55:29.482Z","comments":true,"path":"1520204912.html","link":"","permalink":"https://imlike.cc/1520204912.html","excerpt":"","text":"systemctl list-units –all –type=service systemctl list-units –all –type=service –no-pager Systemd 入门教程：命令篇 ystemd 是 Linux 系统工具，用来启动守护进程，已成为大多数发行版的标准配置。 正确使用的一种，利用systemctl命令。 您的系统可能使用其他服务初始化命令，例如常用服务。 但是，当命令systemctl可用时，使用list-units选项应该会产生良好的结果。 添加了一些额外的参数，这些参数可以自行解释。 Linux启动过程也可以设置每次登录自动执行脚本，在/etc/profile.d/目录下新建sh脚本，/etc/profile会遍历/etc/profile.d/*.sh 另外，几个脚本的区别：（1） /etc/profile： 此文件为系统的每个用户设置环境信息,当用户第一次登录时,该文件被执行. 并从/etc/profile.d目录的配置文件中搜集shell的设置。 （2） /etc/bashrc: 为每一个运行bash shell的用户执行此文件.当bash shell被打开时,该文件被读取（即每次新开一个终端，都会执行bashrc）。 （3） ~/.bash_profile: 每个用户都可使用该文件输入专用于自己使用的shell信息,当用户登录时,该文件仅仅执行一次。默认情况下,设置一些环境变量,执行用户的.bashrc文件。 （4） ~/.bashrc: 该文件包含专用于你的bash shell的bash信息,当登录时以及每次打开新的shell时,该该文件被读取。 （5） /.bash_logout: 当每次退出系统(退出bash shell)时,执行该文件. 另外,/etc/profile中设定的变量(全局)的可以作用于任何用户,而/.bashrc等中设定的变量(局部)只能继承 /etc/profile中的变量,他们是”父子”关系。 （6） /.bash_profile: 是交互式、login 方式进入 bash 运行的/.bashrc 是交互式 non-login 方式进入 bash 运行的通常二者设置大致相同，所以通常前者会调用后者。","categories":[],"tags":[{"name":"ubuntu","slug":"ubuntu","permalink":"https://imlike.cc/tags/ubuntu/"}]},{"title":"树莓派内网穿透","slug":"树莓派内网穿透","date":"2020-04-05T00:54:08.000Z","updated":"2024-03-14T14:55:29.484Z","comments":true,"path":"2070834639.html","link":"","permalink":"https://imlike.cc/2070834639.html","excerpt":"","text":"下面是我了解的提供内网穿透的网站,也可以使用比较传统的花生壳 Sakura Frp 我用的这个 Sunny-Ngrok 支持ngrok,frp两种客户端 还是推荐的 natapp 第一个pass 要手机号码注册而且后面使用其服务还要实名(支付宝验证) 网云穿内网穿透 这个看介绍还不错 但是没有树莓派的软件,树莓派目前的 linux-arm不能用64位的,它提供的是wyc_linux_64所以就放弃的,但是它有提供Java版,但是要配置jre环境就算了 大概使用Sakura Frp步骤 下载对应的软件 这个是注意事项 1由于客户端更新，所有用户将需要重新下载最新版本的客户端，请老用户访问左侧软件下载获取最新版本客户端。**为了避免滥用，所有国内节点将不会转发未实名用户的HTTP流量（特指使用TCP隧道建立的对HTTP协议web服务器的映射），如希望使用国内节点建站请实名认证或加群(747976298)联系我(Q:2792020)并提供已备案的域名然后使用HTTP&#x2F;HTTPS隧道。** https://qianqu.me/frp/frpc_linux_arm 下载树莓派的链接 创建隧道 启动 1234chmod +x frpc_linux_arm 将文件传输到你创建的文件下面 sudo .&#x2F;frpc_linux_arm 完成后在平级的文件夹下面会有frp_user.txt的配置文件 然后在终端下面会显示 本地：127.0.0.1:80 请把域名: yun.imlike.cc 的CNAME记录更改为: us-la-kr.sakurafrp.com (或更改A记录为: 98.126.219.155) 此时直接访问us-la-kr.sakurafrp.com 是503 Service Unavailable错误,还有就是上面提到的告示,使用国内的节点需要通过站长的通过否则也会导致页面错误","categories":[],"tags":[{"name":"树莓派","slug":"树莓派","permalink":"https://imlike.cc/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/"},{"name":"内网穿透","slug":"内网穿透","permalink":"https://imlike.cc/tags/%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/"}]},{"title":"树莓派nginx代理转发到本机端口","slug":"树莓派nginx代理转发到本机端口","date":"2020-04-03T16:09:12.000Z","updated":"2024-03-14T14:55:29.484Z","comments":true,"path":"981520166.html","link":"","permalink":"https://imlike.cc/981520166.html","excerpt":"","text":"sudo apt-get update sudo apt-get install nginx 安装成功后 sudo /etc/init.d/nginx start 启动一下访问树莓派的IP能访问成功就可以了 选择/etc/nginx/nginx.conf 1234567891011121314151617181920212223242526272829http &#123; server&#123; listen 80; server_name yun.imlike.cc; location &#x2F; &#123; # 一些基本反向代理配置以及获取请求真是IP地址 proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-Ip $remote_addr; proxy_set_header X-Forwarded-For $remote_addr; proxy_pass http:&#x2F;&#x2F;192.168.2.189:8001; &#125; # access_log &#x2F;home&#x2F;wwwlogs&#x2F;git.aboutme.ink.log; &#125; server&#123; listen 80; server_name portal.imlike.cc; location &#x2F; &#123; # 一些基本反向代理配置以及获取请求真是IP地址 proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-Ip $remote_addr; proxy_set_header X-Forwarded-For $remote_addr; proxy_pass http:&#x2F;&#x2F;192.168.2.189:9000; &#125; # access_log &#x2F;home&#x2F;wwwlogs&#x2F;git.aboutme.ink.log; &#125;&#125; sudo /etc/init.d/nginx restart 重启一下 优化访问目前是已经知道树莓派的IP的有两种优化的 使用host改本地访问 目前我将host改为 192.168.2.189 yun.imlike.cc192.168.2.189 portal.imlike.cc 这样这两个域名将会解析到192.168.2.189IP上,由于由于nginx上面的配置会解析 server_name对应的yun.imlike.cc,portal.imlike.cc效果如下 使用路由器改变DNS解析,使其解析到树莓派IP 该方式还没实现,原理是一样的","categories":[],"tags":[{"name":"nginx","slug":"nginx","permalink":"https://imlike.cc/tags/nginx/"},{"name":"树莓派","slug":"树莓派","permalink":"https://imlike.cc/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/"}]},{"title":"树莓派docker搭建配置","slug":"树莓派docker搭建配置","date":"2020-04-02T23:42:52.000Z","updated":"2024-03-14T14:55:29.484Z","comments":true,"path":"1469044090.html","link":"","permalink":"https://imlike.cc/1469044090.html","excerpt":"","text":"搭建docker环境如果你过去安装过 docker，先删掉: 1sudo apt-get remove docker docker-engine docker.io 首先安装依赖: 1sudo apt-get install apt-transport-https ca-certificates curl gnupg2 software-properties-common 信任 Docker 的 GPG 公钥: 1curl -fsSL https:&#x2F;&#x2F;download.docker.com&#x2F;linux&#x2F;debian&#x2F;gpg | sudo apt-key add - 如果你是树莓派或其它ARM架构计算机，请运行: 1echo &quot;deb [arch&#x3D;armhf] https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;docker-ce&#x2F;linux&#x2F;debian \\ $(lsb_release -cs) stable&quot; | \\ sudo tee &#x2F;etc&#x2F;apt&#x2F;sources.list.d&#x2F;docker.list 最后安装 1sudo apt-get updatesudo apt-get install docker-ce 使用脚本自动安装在测试或开发环境中 Docker 官方为了简化安装流程，提供了一套便捷的安装脚本，Raspbian 系统上可以使用这套脚本安装，另外可以通过 --mirror 选项使用国内源进行安装： 123$ curl -fsSL get.docker.com -o get-docker.sh$ sudo sh get-docker.sh --mirror Aliyun#$ sudo sh get-docker.sh --mirror AzureChinaCloud 执行这个命令后，脚本就会自动的将一切准备工作做好，并且把 Docker CE 的稳定(stable)版本安装在系统中。 启动 Docker CE1$ sudo systemctl enable docker$ sudo systemctl start docker 建立 docker 用户组默认情况下，docker 命令会使用 Unix socket 与 Docker 引擎通讯。而只有 root 用户和 docker 组的用户才可以访问 Docker 引擎的 Unix socket。出于安全考虑，一般 Linux 系统上不会直接使用 root 用户。因此，更好地做法是将需要使用 docker 的用户加入 docker 用户组。 建立 docker 组： 1$ sudo groupadd docker 将当前用户加入 docker 组： 1$ sudo usermod -aG docker $USER 退出当前终端并重新登录，进行如下测试。 测试 Docker 是否安装正确1$ docker run hello-worldUnable to find image &#39;hello-world:latest&#39; locallylatest: Pulling from library&#x2F;hello-world4ee5c797bcd7: Pull complete Digest: sha256:f9dfddf63636d84ef479d645ab5885156ae030f611a56f3a7ac7f2fdd86d7e4eStatus: Downloaded newer image for hello-world:latestHello from Docker!This message shows that your installation appears to be working correctly.To generate this message, Docker took the following steps:1. The Docker client contacted the Docker daemon.2. The Docker daemon pulled the &quot;hello-world&quot; image from the Docker Hub. (arm32v7)3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading.4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal.To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bashShare images, automate workflows, and more with a free Docker ID: https:&#x2F;&#x2F;hub.docker.com&#x2F;For more examples and ideas, visit: https:&#x2F;&#x2F;docs.docker.com&#x2F;get-started&#x2F; docker-compose的安装开始是找的国内的镜像,但是下载一直是8k的文件不能执行打开就是文件下载失败这种内容,可能是我这个网络问题 1curl -L https:&#x2F;&#x2F;get.daocloud.io&#x2F;docker&#x2F;compose&#x2F;releases&#x2F;download&#x2F;1.25.4&#x2F;docker-compose-&#96;uname -s&#96;-&#96;uname -m&#96; &gt; &#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-composechmod +x &#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-compose 后面我找到别的资源 至此Docker就安装好了，再安装docker compose。先看有没有pip或pip3，有就直接卸了。然后安装： Bash 12$ apt-get remove python-pip python3-pip$ wget https:&#x2F;&#x2F;bootstrap.pypa.io&#x2F;get-pip.py$ python get-pip.py$ python3 get-pip.py 然后安装一些依赖库 1apt install python-dev python3-dev libevent-dev python-scipy libssl-dev libffi-devapt install apt-transport-https ca-certificates curl gnupg2 software-properties-commonpip install cryptography 查看pip版本: 1root@raspberrypi:~# pip3 --versionpip 19.1.1 from &#x2F;usr&#x2F;local&#x2F;lib&#x2F;python3.5&#x2F;dist-packages&#x2F;pip (python 3.5)root@raspberrypi:~# 安装docker compose这里通过pip的方式安装，所以先安装pip 1$ pip3 install docker-compose 如果你报了什么 1TypeError: unsupported operand type(s) for -&#x3D;: &#39;Retry&#39; and &#39;int&#39; 这样的错误，多半是用了上古时代的pip，参考上述方式卸载重装，然后重启一下。如果是下面这样的错 1pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host&#x3D;&#39;www.piwheels.org&#39;, port&#x3D;443): Read timed out. 是由于网络连接超时，重新执行命令就好，或者用国内镜像下载。可以多试几次 安装好了查看版本： 1root@raspberrypi:~# docker-compose --versiondocker-compose version 1.24.1, build 4667896root@raspberrypi:~# sudo pip3 install docker-compose 设置docker镜像1234567891011121314sudo mkdir -p &#x2F;etc&#x2F;dockersudo tee &#x2F;etc&#x2F;docker&#x2F;daemon.json &lt;&lt;-&#39;EOF&#39;&#123; &quot;registry-mirrors&quot;: [ &quot;https:&#x2F;&#x2F;1nj0zren.mirror.aliyuncs.com&quot;, &quot;https:&#x2F;&#x2F;docker.mirrors.ustc.edu.cn&quot;, &quot;http:&#x2F;&#x2F;f1361db2.m.daocloud.io&quot;, &quot;https:&#x2F;&#x2F;registry.docker-cn.com&quot; ]&#125;EOFsudo systemctl daemon-reloadsudo systemctl restart docker 镜像加速器 镜像加速器地址 专属加速器？ 其它加速？ Docker 中国官方镜像 https://registry.docker-cn.com Docker Hub DaoCloud 镜像站 http://f1361db2.m.daocloud.io 可登录，系统分配 Docker Hub Azure 中国镜像 https://dockerhub.azk8s.cn Docker Hub、GCR、Quay 科大镜像站 https://docker.mirrors.ustc.edu.cn Docker Hub、GCR、Quay 阿里云 https://.mirror.aliyuncs.com 需登录，系统分配 Docker Hub 七牛云 https://reg-mirror.qiniu.com Docker Hub、GCR、Quay 网易云 https://hub-mirror.c.163.com Docker Hub 腾讯云 https://mirror.ccs.tencentyun.com 作者：y0ngb1n链接：https://juejin.im/post/5cd2cf01f265da0374189441来源：掘金著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 ### docker命令 1234567891011121314151617181920#查看 Docker 版本docker -&#96;&#96;vsudo# 仓库&#x2F;镜像:版本（留空的话默认为 latest）docker pullsudo docker run # 加参数，用来创建容器查看运行容器sudo docker ps #查看所有下载的镜像sudo docker images #进入容器终端sudo docker exec -i -t ha &#x2F;bin&#x2F;bash #实时查看10行的 ha 日志sudo docker logs -f -t --tail 10 ha #重启sudo systemctl daemon-reload#设置 Docker 开机启动sudo systemctl enable docker#开启 Docker 服务sudo systemctl start docker 常用配置和工具命令12345678910111213141516171819202122#查看 Docker 版本docker -vsudo docker pull 仓库&#x2F;镜像:版本（留空的话默认为 latest）sudo docker run 加参数，用来创建容器#查看运行容器sudo docker ps#查看所有下载的镜像sudo docker images#进入容器终端sudo docker exec -i -t ha &#x2F;bin&#x2F;bash#实时查看10行的 ha 日志sudo docker logs -f -t --tail 10 ha#重启 systemctl 守护进程sudo systemctl daemon-reload#设置 Docker 开机启动sudo systemctl enable docker#开启 Docker 服务sudo systemctl start docker #下载 Docker 图形化界面 portainersudo docker pull portainer&#x2F;portainer#创建 portainer 容器sudo docker volume create portainer_data#运行portainersudo docker run -d -p 9000:9000 --name portainer --restart always -v &#x2F;var&#x2F;run&#x2F;docker.sock:&#x2F;var&#x2F;run 部署docker images,docker ps docker pull nextcloud docker pull portainer/portainer 1docker run -ti -d --name kevin-portainer -p 9000:9000 --restart&#x3D;always -v &#x2F;var&#x2F;run&#x2F;docker.sock:&#x2F;var&#x2F;run&#x2F;docker.sock portainer&#x2F;portainerdocker run -d -p 9000:9000 -p 8000:8000 --name portainer --restart unless-stopped -v &#x2F;var&#x2F;run&#x2F;docker.sock:&#x2F;var&#x2F;run&#x2F;docker.sock portainer&#x2F;portainer 123 docker volume create portainer_data docker run -d -p 8000:8000 -p 9000:9000 --name=portainer --restart=always -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer该容器命名为my_portainer，将容器中portainer默认端口9000，映射到宿主机端口9000，使用socket映射 /var/run/docker.sock:/var/run/docker.sock，指定volume映射（即将portainer_volume对应的宿主机目录挂载到容器的/data目录），容器基于portainer/portainer镜像启动 12mkdir &#x2F;home&#x2F;nextclouddocker run --name nextcloud -p 8080:80 -v &#x2F;home&#x2F;nextcloud:&#x2F;var&#x2F;www&#x2F;html&#x2F;data -d nextcloud参数说明：–name：设定容器名称为nextcloud-p 8080:80 ：端口映射，将宿主机8080端口映射到容器中的80端口-v &#x2F;data&#x2F;nextcloud:&#x2F;var&#x2F;www&#x2F;html&#x2F;data将容器中项目的data目录映射到本地&#x2F;data&#x2F;nextcloud目录下方便配置和数据保存-d 是以后台形式启动 https://cloud.tencent.com/developer/article/1371476 中间找不文件就选择下面这个命令 1cd &#x2F;var&#x2F;lib&#x2F;find . -name &quot;config.php&quot;","categories":[],"tags":[{"name":"docker","slug":"docker","permalink":"https://imlike.cc/tags/docker/"},{"name":"树莓派","slug":"树莓派","permalink":"https://imlike.cc/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/"}]},{"title":"树莓派设置用户和ssh,VN连接","slug":"树莓派设置用户和ssh,VN连接","date":"2020-04-02T21:38:17.000Z","updated":"2024-03-14T14:55:29.485Z","comments":true,"path":"2385605860.html","link":"","permalink":"https://imlike.cc/2385605860.html","excerpt":"","text":"启动后的树莓派页面 ssh 连接使用private-key首先是需要你已经生成了本地的秘钥,然后将本地的公钥加入到对方的authorized_keys中ssh-copy-id传过去 否则可以直接用用户和密码登陆 1cat ~&#x2F;.ssh&#x2F;id_rsa.pub | ssh USERNAME@IP-ADDRESS &#39;cat &gt;&gt; .ssh&#x2F;authorized_keys&#39;ssh-copy-id YOU-USERNAME@IP-ADDRESS 设置国内源https://mirror.tuna.tsinghua.edu.cn/help/raspbian/ 树莓派选择Debian 10 (buster) 1234# 编辑 &#96;&#x2F;etc&#x2F;apt&#x2F;sources.list&#96; 文件，删除原文件所有内容，用以下内容取代：deb http:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;raspbian&#x2F;raspbian&#x2F; buster main non-free contribdeb-src http:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;raspbian&#x2F;raspbian&#x2F; buster main non-free contrib# 编辑 &#96;&#x2F;etc&#x2F;apt&#x2F;sources.list.d&#x2F;raspi.list&#96; 文件，删除原文件所有内容，用以下内容取代：deb http:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;raspberrypi&#x2F; buster main ui 设置期间的问题 sudo root 报su: 鉴定故障 1entos7 su 鉴定故障解决方法Linux(fedora\\centos\\ubuntu等)终端窗口运行 su 输入密码，提示鉴定失败。根据网上查询的原因是：安装linux时，未输入ROOT密码；或新建用户，勾选“设为管理员”，仅为新用户设置密码。未设置ROOT密码，系统检测失败。亲测解决方法如下：打开终端 ：sudo passwd root输入当前用户密码，然后输入设置的root密码。(重新设置root密码） sudo chmod -777 /home 1sudo: &#x2F;etc&#x2F;sudoers 可被任何人写sudo: 没有找到有效的 sudoers 资源，退出sudo: 无法初始化策略插件 1&#x2F;etc&#x2F;sudoers下面的权限修改回去需要直接修改说不定就直接出什么问题了sudo su - chmod 777 &#x2F;etc&#x2F;sudoers echo &#39;username ALL&#x3D;(ALL) ALL&#39; &gt;&gt; &#x2F;etc&#x2F;sudoers Ubuntu vi 上下左右变ABCD问题解决方法 1一.执行命令 sudo apt-get remove vim-common 二 . sudo apt-get install vimcp &#x2F;etc&#x2F;vim&#x2F;vimrc ~&#x2F;.vimrc &#x2F;&#x2F;没网用这个","categories":[],"tags":[{"name":"树莓派","slug":"树莓派","permalink":"https://imlike.cc/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/"},{"name":"Linux","slug":"Linux","permalink":"https://imlike.cc/tags/Linux/"}]},{"title":"vue和element-ui初步使用","slug":"vue和element-ui初步使用","date":"2020-04-01T21:13:13.000Z","updated":"2024-03-14T14:55:29.482Z","comments":true,"path":"3181160514.html","link":"","permalink":"https://imlike.cc/3181160514.html","excerpt":"","text":"是一套用于构建用户界面的渐进式框架。与其它大型框架不同的是，Vue 被设计为可以自底向上逐层应用。Vue 的核心库只关注视图层，不仅易于上手，还便于与第三方库或既有项目整合。另一方面，当与现代化的工具链以及各种支持类库结合使用时，Vue 也完全能够为复杂的单页应用提供驱动。是一种 MVVM (Model-View-ViewModel) 使用vue首先是需要导入vue.js,下面就是简单的vue对象 1![processInstance](&#x2F;home&#x2F;leek&#x2F;桌面&#x2F;processInstance.png)&lt;div id&#x3D;&quot;app&quot;&gt; &lt;h2&gt;&#123;&#123;name&#125;&#125; 显示&lt;&#x2F;h2&gt;&lt;&#x2F;div&gt;&lt;script src&#x3D;&quot;.&#x2F;node_modules&#x2F;vue&#x2F;dist&#x2F;vue.js&quot;&gt;&lt;&#x2F;script&gt;&lt;script type&#x3D;&quot;text&#x2F;javascript&quot;&gt; &#x2F;&#x2F; 生成一个Vue实例 var app &#x3D; new Vue(&#123; el:&quot;#app&quot;, &#x2F;&#x2F; el,即element。要渲染的的页面元素 data:&#123; &#x2F;&#x2F; 数据 name:&quot;like&quot; &#125; &#125;)&lt;&#x2F;script&gt; 标签绑定数据 常见数据绑定的熟悉是 v-text 绑定的值直接显示不解析 v-html 内容按普通 HTML 插入 - 不会作为 Vue 模板进行编译 v-show 是否显示 v-if 内容按普通 HTML 插入 - 不会作为 Vue 模板进行编译 v-for 1&lt;div v-for&#x3D;&quot;(item, index) in items&quot;&gt;&lt;&#x2F;div&gt;&lt;div v-for&#x3D;&quot;(val, key) in object&quot;&gt;&lt;&#x2F;div&gt; &lt;div v-for&#x3D;&quot;(val, name, index) in object&quot;&gt;&lt;&#x2F;div&gt; v-on 可以用@缩写@click,@node-click v-bind 可以用:缩写 1.prop - 作为一个 DOM property 绑定而不是作为 attribute 绑定。(差别在哪里？).camel - (2.1.0+) 将 kebab-case 特性名转换为 camelCase. (从 2.1.0 开始支持).sync (2.3.0+) 语法糖，会扩展成一个更新父组件绑定值的 v-on 侦听器。 vue的声明周期 1var app &#x3D; new Vue(&#123; el: &quot;#app&quot;, data: &#123; num: 0 &#125;, beforeCreate() &#123; console.log(&quot;创建前&quot;) &#125;, created() &#123; console.log(&quot;创建&quot;) &#125;, beforeMount() &#123; console.log(&quot;计数前&quot;) &#125;, mounted() &#123; console.log(&quot;计数了&quot;) &#125;,&#125;) element-UI相关的 element-UI网站快速成型工具:一套为开发者、设计师和产品经理准备的基于 Vue 2.0 的桌面端组件库 网站 element-UI 目前我看到的标签(表格,)标签1-&gt;el-pagination 用于分页 下面是它的相关参数Attributes | 参数 | 说明 | 类型 | 可选值 | 默认值 || ——————- | ———————————————————— | ——– | ———————————————————— | ————————————– || small | 是否使用小型分页样式 | boolean | — | false || background | 是否为分页按钮添加背景色 | boolean | — | false || page-size | 每页显示条目个数，支持 .sync 修饰符 | number | — | 10 || total | 总条目数 | number | — | — || page-count | 总页数，total 和 page-count 设置任意一个就可以达到显示页码的功能；如果要支持 page-sizes 的更改，则需要使用 total 属性 | Number | — | — || pager-count | 页码按钮的数量，当总页数超过该值时会折叠 | number | 大于等于 5 且小于等于 21 的奇数 | 7 || current-page | 当前页数，支持 .sync 修饰符 | number | — | 1 || layout | 组件布局，子组件名用逗号分隔 | String | sizes, prev, pager, next, jumper, -&gt;, total, slot | ‘prev, pager, next, jumper, -&gt;, total’ || page-sizes | 每页显示个数选择器的选项设置 | number[] | — | [10, 20, 30, 40, 50, 100] || popper-class | 每页显示个数选择器的下拉框类名 | string | — | — || prev-text | 替代图标显示的上一页文字 | string | — | — || next-text | 替代图标显示的下一页文字 | string | — | — || disabled | 是否禁用 | boolean | — | false || hide-on-single-page | 只有一页时是否隐藏 | boolean | — | - | ¶ Events| 事件名称 | 说明 | 回调参数 || ————– | ———————————- | ——– || size-change | pageSize 改变时会触发 | 每页条数 || current-change | currentPage 改变时会触发 | 当前页 || prev-click | 用户点击上一页按钮改变当前页后触发 | 当前页 || next-click | 用户点击下一页按钮改变当前页后触发 | 当前页 | 标签2-&gt;el-tooltip 用于显示提示 Attributes 参数 说明 类型 可选值 默认值 effect 默认提供的主题 String dark/light dark content 显示的内容，也可以通过 slot#content 传入 DOM String — — placement Tooltip 的出现位置 String top/top-start/top-end/bottom/bottom-start/bottom-end/left/left-start/left-end/right/right-start/right-end bottom value / v-model 状态是否可见 Boolean — false disabled Tooltip 是否可用 Boolean — false offset 出现位置的偏移量 Number — 0 transition 定义渐变动画 String — el-fade-in-linear visible-arrow 是否显示 Tooltip 箭头，更多参数可见Vue-popper Boolean — true popper-options popper.js 的参数 Object 参考 popper.js 文档 { boundariesElement: ‘body’, gpuAcceleration: false } open-delay 延迟出现，单位毫秒 Number — 0 manual 手动控制模式，设置为 true 后，mouseenter 和 mouseleave 事件将不会生效 Boolean — false popper-class 为 Tooltip 的 popper 添加类名 String — — enterable 鼠标是否可进入到 tooltip 中 Boolean — true hide-after Tooltip 出现后自动隐藏延时，单位毫秒，为 0 则不会自动隐藏 number — 0 tabindex Tooltip 组件的 tabindex number — 0 标签3-&gt;el-table 用于显示表格,目前主要是用这个Table Attributes 参数 说明 类型 可选值 默认值 data 显示的数据 array — — height Table 的高度，默认为自动高度。如果 height 为 number 类型，单位 px；如果 height 为 string 类型，则这个高度会设置为 Table 的 style.height 的值，Table 的高度会受控于外部样式。 string/number — — max-height Table 的最大高度。合法的值为数字或者单位为 px 的高度。 string/number — — stripe 是否为斑马纹 table boolean — false border 是否带有纵向边框 boolean — false size Table 的尺寸 string medium / small / mini — fit 列的宽度是否自撑开 boolean — true show-header 是否显示表头 boolean — true highlight-current-row 是否要高亮当前行 boolean — false current-row-key 当前行的 key，只写属性 String,Number — — row-class-name 行的 className 的回调方法，也可以使用字符串为所有行设置一个固定的 className。 Function({row, rowIndex})/String — — row-style 行的 style 的回调方法，也可以使用一个固定的 Object 为所有行设置一样的 Style。 Function({row, rowIndex})/Object — — cell-class-name 单元格的 className 的回调方法，也可以使用字符串为所有单元格设置一个固定的 className。 Function({row, column, rowIndex, columnIndex})/String — — cell-style 单元格的 style 的回调方法，也可以使用一个固定的 Object 为所有单元格设置一样的 Style。 Function({row, column, rowIndex, columnIndex})/Object — — header-row-class-name 表头行的 className 的回调方法，也可以使用字符串为所有表头行设置一个固定的 className。 Function({row, rowIndex})/String — — header-row-style 表头行的 style 的回调方法，也可以使用一个固定的 Object 为所有表头行设置一样的 Style。 Function({row, rowIndex})/Object — — header-cell-class-name 表头单元格的 className 的回调方法，也可以使用字符串为所有表头单元格设置一个固定的 className。 Function({row, column, rowIndex, columnIndex})/String — — header-cell-style 表头单元格的 style 的回调方法，也可以使用一个固定的 Object 为所有表头单元格设置一样的 Style。 Function({row, column, rowIndex, columnIndex})/Object — — row-key 行数据的 Key，用来优化 Table 的渲染；在使用 reserve-selection 功能与显示树形数据时，该属性是必填的。类型为 String 时，支持多层访问：user.info.id，但不支持 user.info[0].id，此种情况请使用 Function。 Function(row)/String — — empty-text 空数据时显示的文本内容，也可以通过 slot=&quot;empty&quot; 设置 String — 暂无数据 default-expand-all 是否默认展开所有行，当 Table 包含展开行存在或者为树形表格时有效 Boolean — false expand-row-keys 可以通过该属性设置 Table 目前的展开行，需要设置 row-key 属性才能使用，该属性为展开行的 keys 数组。 Array — default-sort 默认的排序列的 prop 和顺序。它的prop属性指定默认的排序的列，order指定默认排序的顺序 Object order: ascending, descending 如果只指定了prop, 没有指定order, 则默认顺序是ascending tooltip-effect tooltip effect 属性 String dark/light show-summary 是否在表尾显示合计行 Boolean — false sum-text 合计行第一列的文本 String — 合计 summary-method 自定义的合计计算方法 Function({ columns, data }) — — span-method 合并行或列的计算方法 Function({ row, column, rowIndex, columnIndex }) — — select-on-indeterminate 在多选表格中，当仅有部分行被选中时，点击表头的多选框时的行为。若为 true，则选中所有行；若为 false，则取消选择所有行 Boolean — true indent 展示树形数据时，树节点的缩进 Number — 16 lazy 是否懒加载子节点数据 Boolean — — load 加载子节点数据的函数，lazy 为 true 时生效，函数第二个参数包含了节点的层级信息 Function(row, treeNode, resolve) — — tree-props 渲染嵌套数据的配置选项 Object — { hasChildren: ‘hasChildren’, children: ‘children’ } ¶ Table Events 事件名 说明 参数 select 当用户手动勾选数据行的 Checkbox 时触发的事件 selection, row select-all 当用户手动勾选全选 Checkbox 时触发的事件 selection selection-change 当选择项发生变化时会触发该事件 selection cell-mouse-enter 当单元格 hover 进入时会触发该事件 row, column, cell, event cell-mouse-leave 当单元格 hover 退出时会触发该事件 row, column, cell, event cell-click 当某个单元格被点击时会触发该事件 row, column, cell, event cell-dblclick 当某个单元格被双击击时会触发该事件 row, column, cell, event row-click 当某一行被点击时会触发该事件 row, column, event row-contextmenu 当某一行被鼠标右键点击时会触发该事件 row, column, event row-dblclick 当某一行被双击时会触发该事件 row, column, event header-click 当某一列的表头被点击时会触发该事件 column, event header-contextmenu 当某一列的表头被鼠标右键点击时触发该事件 column, event sort-change 当表格的排序条件发生变化的时候会触发该事件 { column, prop, order } filter-change 当表格的筛选条件发生变化的时候会触发该事件，参数的值是一个对象，对象的 key 是 column 的 columnKey，对应的 value 为用户选择的筛选条件的数组。 filters current-change 当表格的当前行发生变化的时候会触发该事件，如果要高亮当前行，请打开表格的 highlight-current-row 属性 currentRow, oldCurrentRow header-dragend 当拖动表头改变了列的宽度的时候会触发该事件 newWidth, oldWidth, column, event expand-change 当用户对某一行展开或者关闭的时候会触发该事件（展开行时，回调的第二个参数为 expandedRows；树形表格时第二参数为 expanded） row, (expandedRows \\ expanded) ¶ Table Methods 方法名 说明 参数 clearSelection 用于多选表格，清空用户的选择 — toggleRowSelection 用于多选表格，切换某一行的选中状态，如果使用了第二个参数，则是设置这一行选中与否（selected 为 true 则选中） row, selected toggleAllSelection 用于多选表格，切换所有行的选中状态 - toggleRowExpansion 用于可展开表格与树形表格，切换某一行的展开状态，如果使用了第二个参数，则是设置这一行展开与否（expanded 为 true 则展开） row, expanded setCurrentRow 用于单选表格，设定某一行为选中行，如果调用时不加参数，则会取消目前高亮行的选中状态。 row clearSort 用于清空排序条件，数据会恢复成未排序的状态 — clearFilter 不传入参数时用于清空所有过滤条件，数据会恢复成未过滤的状态，也可传入由columnKey组成的数组以清除指定列的过滤条件 columnKey doLayout 对 Table 进行重新布局。当 Table 或其祖先元素由隐藏切换为显示时，可能需要调用此方法 — sort 手动对 Table 进行排序。参数prop属性指定排序列，order指定排序顺序。 prop: string, order: string","categories":[],"tags":[{"name":"vue","slug":"vue","permalink":"https://imlike.cc/tags/vue/"}]},{"title":"VUE初级教程","slug":"VUE初级教程","date":"2020-03-30T18:08:13.000Z","updated":"2024-03-14T14:55:29.475Z","comments":true,"path":"1664911364.html","link":"","permalink":"https://imlike.cc/1664911364.html","excerpt":"","text":"使用Vue.js和Axios从第三方API获取请求数据 axios之Vue请求初始化数据放在Created还是Mounted？ axios介绍基于Promise的HTTP客户端，用于浏览器和node.js axios-github 制作的XMLHttpRequest从浏览器 让HTTP从node.js的请求 支持Promise API 拦截请求和响应 转换请求和响应数据 取消请求 JSON数据的自动转换 客户端支持以防止XSRF vue的初始化过程 1var app &#x3D; new Vue(&#123; el: &quot;#app&quot;, data: &#123; num: 0 &#125;, beforeCreate() &#123; console.log(&quot;创建前&quot;) &#125;, created() &#123; console.log(&quot;创建&quot;) &#125;, beforeMount() &#123; console.log(&quot;计数前&quot;) &#125;, mounted() &#123; console.log(&quot;计数了&quot;) &#125;, &#125;) 之前赋值不到vue的data里最开始放在 created(),mounted()都可以赋值过去 12var mydata&#x3D;&quot;&quot; ;var app &#x3D; new Vue(&#123; el: &quot;#app&quot;, data: &#123; num: 0, tableDataA:mydata &#125;, beforeCreate() &#123; console.log(&quot;创建前&quot;) &#125;, created() &#123; console.log(&quot;创建&quot;) &#125;, beforeMount() &#123; console.log(&quot;计数前&quot;) &#125;, mounted() &#123; console.log(&quot;计数了&quot;) &#125;, &#125;)axios.get(&#39;http:&#x2F;&#x2F;localhost:8081&#x2F;BackSpace&#x2F;test&#39;) .then(function (response) &#123; &#x2F;&#x2F; mydata&#x3D;response.data 刚开始使用的这个 但是一直赋值不过去 &#x2F;&#x2F;这个方法也不行,this不能指代当前这个app对象 this.tableDataA&#x3D;response.data&#125;) .catch(function (error) &#123; console.log(error);&#125;); 成功的版本 1axios.get(&#39;http:&#x2F;&#x2F;localhost:8081&#x2F;BackSpace&#x2F;test&#39;).then(response &#x3D;&gt; &#123;this.tableDataA&#x3D;response.data&#125;).catch(function (error) &#123;console.log(error);&#125;);","categories":[],"tags":[{"name":"vue","slug":"vue","permalink":"https://imlike.cc/tags/vue/"}]},{"title":"VScode优化和插件推荐","slug":"VScode优化和插件推荐","date":"2020-03-28T21:38:31.000Z","updated":"2024-03-14T14:55:29.475Z","comments":true,"path":"2425517015.html","link":"","permalink":"https://imlike.cc/2425517015.html","excerpt":"","text":"Auto Close Tag 自动闭合标签Auto Rename Tag 尾部闭合标签同步修改Live Server 实时预览Bracket Pair Colorizer 用不同颜色高亮显示匹配的括号Highlight Matching Tag 高亮显示匹配标签Vscode-icons VSCode 文件图标优化字段的字体 需要编辑 Editor: Font Family 下我的设置‘Droid Sans Mono’, ‘monospace’, monospace, ‘Droid Sans Fallback’","categories":[],"tags":[{"name":"vscode","slug":"vscode","permalink":"https://imlike.cc/tags/vscode/"}]},{"title":"Linux下搭建vue环境以及出现No command 'vue'found","slug":"Linux下搭建vue环境以及出现No command 'vue'found","date":"2020-03-27T00:47:43.000Z","updated":"2024-03-14T14:55:29.473Z","comments":true,"path":"3693735390.html","link":"","permalink":"https://imlike.cc/3693735390.html","excerpt":"","text":"搭建nodejs环境1234561. 下载node的Linux版2. 解压后放在&#x2F;user&#x2F;local&#x2F; 下或者其他位置3. 设置国内的淘宝源 npm config set registry&#x3D;https:&#x2F;&#x2F;registry.npm.taobao.org 或者使用npm install -g cnpm --registry&#x3D;https:&#x2F;&#x2F;registry.npm.taobao.org安装完成后同样要建软链接，否则在使用 cnpm时会报No command &#39;cnpm&#39; found的错误。sudo ln -s &#x2F;home&#x2F;formularoom&#x2F;env&#x2F;node-v8.1.2-linux-x64&#x2F;bin&#x2F;cnpm &#x2F;usr&#x2F;local&#x2F;bin&#x2F;cnpm 常见npm设置命令 npm config get/set xxxx ,其中xxx代指prefix ,registry等,其对于的配置文件地址 ~/.npmrc 下载vuejs1npm install -g vue-cli 出现 No command ‘vue’found由于 /usr/local/bin下面没有对于命令所以显示找不到这个命令,需要建立软链接 sudo ln -s /usr/local/node-v8.9.3-linux-x64/bin/vue /usr/local/bin/vue Linux下Vue开发环境搭建一篇全搞定","categories":[],"tags":[{"name":"npm","slug":"npm","permalink":"https://imlike.cc/tags/npm/"},{"name":"vue","slug":"vue","permalink":"https://imlike.cc/tags/vue/"}]},{"title":"提交文件MultipartFile后ZipEntry.size一直为-1","slug":"提交文件MultipartFile后ZipEntry.size一直为-1","date":"2020-03-16T23:18:32.000Z","updated":"2024-03-14T14:55:29.484Z","comments":true,"path":"3269460028.html","link":"","permalink":"https://imlike.cc/3269460028.html","excerpt":"","text":"先看一下代码 12345678910111213141516171819202122232425262728293031323334353637public static void importModels (MultipartFile file) throws IOException, InvalidFormatException &#123; ZipInputStream zipInputStream = new ZipInputStream(file.getInputStream(), Charset.defaultCharset()); BufferedInputStream bs = new BufferedInputStream(zipInputStream); Map&lt;String, Map&gt; stringMapMap; Map&lt;String, String&gt; modelJsons = new HashMap&lt;&gt;(); ZipEntry zipEntry; String zipFileName; while ((zipEntry = zipInputStream.getNextEntry()) != null) &#123; if (zipEntry.isDirectory()) &#123; // do nothing &#125; else if (zipEntry.getName().endsWith(&quot;json&quot;)) &#123; String name = zipEntry.getName(); long size = zipEntry.getSize(); if (size == -1) &#123; ByteArrayOutputStream baos = new ByteArrayOutputStream(); while (true) &#123; int bytes = zipInputStream.read(); if (bytes == -1) &#123; break; &#125; baos.write(bytes); &#125; baos.close(); System.out.println(String.format(&quot;Name:%s,Content:%s&quot;, name, new String(baos.toByteArray()))); &#125; else &#123; // ZipEntry的size正常 byte[] bytes = new byte[(int) zipEntry.getSize()]; zipInputStream.read(bytes, 0, (int) zipEntry.getSize()); System.out.println(String.format(&quot;Name:%s,Content:%s&quot;, name, new String(bytes))); &#125; &#125; else if (zipEntry.getName().endsWith(&quot;xlxs&quot;)) &#123; //do other things &#125; else &#123; return; &#125; &#125; return; &#125; 文件提交后台是提交后天后文件并不是在本地,网上之前有提供zipfile后读取在我这边是不可能实现 12345678while ((ze &#x3D; zin.getNextEntry()) !&#x3D; null) &#123; BufferedReader bufferedReader &#x3D; new BufferedReader(new InputStreamReader(zin)); BufferedReader reader &#x3D; new BufferedReader(new InputStreamReader(file.getInputStream())); String str; while ((str &#x3D; reader.readLine()) !&#x3D; null) &#123; System.out.println(str); &#125; &#125; 上面的读取的是整个Zip文件,和要求不符 上传Zip文件不解压读取文件内容时ZipEntry的size为-1的问题","categories":[],"tags":[{"name":"spring","slug":"spring","permalink":"https://imlike.cc/tags/spring/"}]},{"title":"ubuntu导致不能复制剪切板","slug":"ubuntu导致不能复制剪切板","date":"2020-03-03T21:20:16.000Z","updated":"2024-03-14T14:55:29.482Z","comments":true,"path":"3534202997.html","link":"","permalink":"https://imlike.cc/3534202997.html","excerpt":"","text":"剪切板不能共享 以Ubuntu为主机(host)的系统,在vmware的虚拟机(guest)一直无法复制剪切板上的文字,网上看到的都是给Linux的虚拟机安装 vmware tools 的过程,刚开始以为是Linux主机导致 vmware workspace无法安装 因为我看到所有Windows安装VMware Workstation的软件在有vmware tools后都有重新安装 tools的 所以后面我都以为是系统缘故导致的 确定原因和解决方案最近有不文档需要复制,我就准备重新下载安装VMware Workstation链接地址 1Name: VMware-Workstation-Full-15.5.1-15018445.x86_64.bundle发行日期: 2019-11-12内部版本号: 15018445This Workstation bundle includes the full product and VMware Tools for Linux 64-bit operating systems.MD5SUM: dfaa7c3b858df839db6d006437142e98SHA1SUM: c82e66f28fb5cc6fac4c38c55727cc45b916d107SHA256SUM: 490f8fe8d874c27601c155c1b2c0055ad0d4f14feb32fa9f18643659a147d3c6 发现其实是附带了tools,然后我就找到Download VMware Tools 10.3.10 的链接,然后下载解压下面是目录结构 给对于虚拟机安装上后就可以复制了下面是 Linux的安装步骤方法1 Linux的安装步骤方法2 需要注意的的tools安装文件已经在上面压缩包里面都有了,所以直接解压执行省略步骤方法前面的几个步骤","categories":[],"tags":[{"name":"ubuntu","slug":"ubuntu","permalink":"https://imlike.cc/tags/ubuntu/"},{"name":"vmware","slug":"vmware","permalink":"https://imlike.cc/tags/vmware/"}]},{"title":"activiti用法和结构","slug":"activiti用法和结构","date":"2020-02-17T23:34:02.000Z","updated":"2024-03-14T14:55:29.475Z","comments":true,"path":"745403789.html","link":"","permalink":"https://imlike.cc/745403789.html","excerpt":"","text":"activiti零零碎碎的记录 包括processengineConfiguration -&gt;processEngine-&gt;respositoryService 下面的各个类和方法 支持的事件类型​ 下面是引擎中可能出现的所有activiti事件类型。每个类型都对应org.activiti.engine.delegate.event.ActivitiEventType中的一个枚举值。 Table 3.2. 支持的事件** 事件名称 描述 事件类型 ENGINE_CREATED 监听器监听的流程引擎已经创建完毕，并准备好接受API调用。 org.activiti...ActivitiEvent ENGINE_CLOSED 监听器监听的流程引擎已经关闭，不再接受API调用。 org.activiti...ActivitiEvent ENTITY_CREATED 创建了一个新实体。实体包含在事件中。 org.activiti...ActivitiEntityEvent ENTITY_INITIALIZED 创建了一个新实体，初始化也完成了。如果这个实体的创建会包含子实体的创建，这个事件会在子实体都创建/初始化完成后被触发，这是与ENTITY_CREATED的区别。 org.activiti...ActivitiEntityEvent ENTITY_UPDATED 更新了已存在的实体。实体包含在事件中。 org.activiti...ActivitiEntityEvent ENTITY_DELETED 删除了已存在的实体。实体包含在事件中。 org.activiti...ActivitiEntityEvent ENTITY_SUSPENDED 暂停了已存在的实体。实体包含在事件中。会被ProcessDefinitions, ProcessInstances 和 Tasks抛出。 org.activiti...ActivitiEntityEvent ENTITY_ACTIVATED 激活了已存在的实体，实体包含在事件中。会被ProcessDefinitions, ProcessInstances 和 Tasks抛出。 org.activiti...ActivitiEntityEvent JOB_EXECUTION_SUCCESS 作业执行成功。job包含在事件中。 org.activiti...ActivitiEntityEvent JOB_EXECUTION_FAILURE 作业执行失败。作业和异常信息包含在事件中。 org.activiti...ActivitiEntityEvent and org.activiti...ActivitiExceptionEvent JOB_RETRIES_DECREMENTED 因为作业执行失败，导致重试次数减少。作业包含在事件中。 org.activiti...ActivitiEntityEvent TIMER_FIRED 触发了定时器。job包含在事件中。 org.activiti...ActivitiEntityEvent JOB_CANCELED 取消了一个作业。事件包含取消的作业。作业可以通过API调用取消， 任务完成后对应的边界定时器也会取消，在新流程定义发布时也会取消。 org.activiti...ActivitiEntityEvent ACTIVITY_STARTED 一个节点开始执行 org.activiti...ActivitiActivityEvent ACTIVITY_COMPLETED 一个节点成功结束 org.activiti...ActivitiActivityEvent ACTIVITY_SIGNALED 一个节点收到了一个信号 org.activiti...ActivitiSignalEvent ACTIVITY_MESSAGE_RECEIVED 一个节点收到了一个消息。在节点收到消息之前触发。收到后，会触发ACTIVITY_SIGNAL或ACTIVITY_STARTED，这会根据节点的类型（边界事件，事件子流程开始事件） org.activiti...ActivitiMessageEvent ACTIVITY_ERROR_RECEIVED 一个节点收到了一个错误事件。在节点实际处理错误之前触发。 事件的activityId对应着处理错误的节点。 这个事件后续会是ACTIVITY_SIGNALLED或ACTIVITY_COMPLETE， 如果错误发送成功的话。 org.activiti...ActivitiErrorEvent UNCAUGHT_BPMN_ERROR 抛出了未捕获的BPMN错误。流程没有提供针对这个错误的处理器。 事件的activityId为空。 org.activiti...ActivitiErrorEvent ACTIVITY_COMPENSATE 一个节点将要被补偿。事件包含了将要执行补偿的节点id。 org.activiti...ActivitiActivityEvent VARIABLE_CREATED 创建了一个变量。事件包含变量名，变量值和对应的分支或任务（如果存在）。 org.activiti...ActivitiVariableEvent VARIABLE_UPDATED 更新了一个变量。事件包含变量名，变量值和对应的分支或任务（如果存在）。 org.activiti...ActivitiVariableEvent VARIABLE_DELETED 删除了一个变量。事件包含变量名，变量值和对应的分支或任务（如果存在）。 org.activiti...ActivitiVariableEvent TASK_ASSIGNED 任务被分配给了一个人员。事件包含任务。 org.activiti...ActivitiEntityEvent TASK_CREATED 创建了新任务。它位于ENTITY_CREATE事件之后。当任务是由流程创建时， 这个事件会在TaskListener执行之前被执行。 org.activiti...ActivitiEntityEvent TASK_COMPLETED 任务被完成了。它会在ENTITY_DELETE事件之前触发。当任务是流程一部分时，事件会在流程继续运行之前， 后续事件将是ACTIVITY_COMPLETE，对应着完成任务的节点。 org.activiti...ActivitiEntityEvent TASK_TIMEOUT 任务已超时，在TIMER_FIRED事件之后，会触发用户任务的超时事件， 当这个任务分配了一个定时器的时候。 org.activiti...ActivitiEntityEvent PROCESS_COMPLETED 流程已结束。在最后一个节点的ACTIVITY_COMPLETED事件之后触发。 当流程到达的状态，没有任何后续连线时， 流程就会结束。 org.activiti...ActivitiEntityEvent MEMBERSHIP_CREATED 用户被添加到一个组里。事件包含了用户和组的id。 org.activiti...ActivitiMembershipEvent MEMBERSHIP_DELETED 用户被从一个组中删除。事件包含了用户和组的id。 org.activiti...ActivitiMembershipEvent MEMBERSHIPS_DELETED 所有成员被从一个组中删除。在成员删除之前触发这个事件，所以他们都是可以访问的。 因为性能方面的考虑，不会为每个成员触发单独的MEMBERSHIP_DELETED事件。 org.activiti...ActivitiMembershipEvent 单元测试​ 业务流程是软件项目的一部分，它也应该和普通的业务流程一样进行测试： 使用单元测试。 因为Activiti是一个嵌入式的java引擎， 为业务流程编写单元测试和写普通单元测试完全一样。 ​ Activiti支持JUnit 3和4进行单元测试。使用JUnit 3时， 必须集成org.activiti.engine.test.ActivitiTestCase。 它通过保护的成员变量提供ProcessEngine和服务， 在测试的setup()中， 默认会使用classpath下的activiti.cfg.xml初始化流程引擎。 想使用不同的配置文件，可以重写*getConfigurationResource()*方法。 如果配置文件相同的话，对应的流程引擎会被静态缓存， 就可以用于多个单元测试。 ​ 继承了ActivitiTestCase你，可以在测试方法上使用 org.activiti.engine.test.Deployment注解。 测试执行前，与测试类在同一个包下的， 格式为testClassName.testMethod.bpmn20.xml的资源文件，会被部署。 测试结束后，发布包也会被删除，包括所有相关的流程实例，任务，等等。 Deployment注解也可以直接设置资源的位置。 参考Javadocs获得更多信息。 ​ 把这些放在一起，JUnit 3测试看起来像这样。 12345678910public class MyBusinessProcessTest extends ActivitiTestCase &#123; @Deployment public void testSimpleProcess() &#123; runtimeService.startProcessInstanceByKey(&quot;simpleProcess&quot;); Task task &#x3D; taskService.createTaskQuery().singleResult(); assertEquals(&quot;My Task&quot;, task.getName()); taskService.complete(task.getId()); assertEquals(0, runtimeService.createProcessInstanceQuery().count()); &#125;&#125; ​ 要想在使用JUnit 4编写单元测试时获得同样的功能， 可以使用org.activiti.engine.test.ActivitiRule。 通过它，可以通过getter方法获得流程引擎和各种服务。 和 ActivitiTestCase一样（参考上面章节），使用这个Rule 也会启用org.activiti.engine.test.Deployment注解（参考上面章节使用和配置的介绍）， 它会在classpath下查找默认的配置文件。 如果配置文件相同的话，对应的流程引擎会被静态缓存， 就可以用于多个单元测试。 ​ 下面的代码演示了JUnit 4单元测试并使用了ActivitiRule的例子。 12345678910111213141516public class MyBusinessProcessTest &#123; @Rule public ActivitiRule activitiRule &#x3D; new ActivitiRule(); @Test @Deployment public void ruleUsageExample() &#123; RuntimeService runtimeService &#x3D; activitiRule.getRuntimeService(); runtimeService.startProcessInstanceByKey(&quot;ruleUsage&quot;); TaskService taskService &#x3D; activitiRule.getTaskService(); Task task &#x3D; taskService.createTaskQuery().singleResult(); assertEquals(&quot;My Task&quot;, task.getName()); taskService.complete(task.getId()); assertEquals(0, runtimeService.createProcessInstanceQuery().count()); &#125;&#125; ​ 当集成Spring时，使用标准的Activiti测试工具类是非常容易的对业务流程进行测试。 下面的例子展示了如何在一个典型的基于Spring单元测试测试业务流程： 1234567891011121314151617181920212223242526272829import org.junit.Rule;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.test.context.ContextConfiguration;import org.springframework.test.context.junit4.SpringJUnit4ClassRunner;@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(&quot;classpath:org&#x2F;activiti&#x2F;spring&#x2F;test&#x2F;junit4&#x2F;springTypicalUsageTest-context.xml&quot;)public class MyBusinessProcessTest &#123; @Autowired private RuntimeService runtimeService; @Autowired private TaskService taskService; @Autowired @Rule public ActivitiRule activitiSpringRule; @Test @Deployment public void simpleProcessTest() &#123; runtimeService.startProcessInstanceByKey(&quot;simpleProcess&quot;); Task task &#x3D; taskService.createTaskQuery().singleResult(); assertEquals(&quot;My Task&quot;, task.getName()); taskService.complete(task.getId()); assertEquals(0, runtimeService.createProcessInstanceQuery().count()); &#125;&#125;","categories":[],"tags":[{"name":"activiti","slug":"activiti","permalink":"https://imlike.cc/tags/activiti/"}]},{"title":"activiti数据库的分类","slug":"activiti数据库的分类","date":"2020-02-14T00:10:04.000Z","updated":"2024-03-14T14:55:29.475Z","comments":true,"path":"2197510555.html","link":"","permalink":"https://imlike.cc/2197510555.html","excerpt":"","text":"Activiti数据库表结构 Activiti工作流总共包含23张数据表，所有的表名默认以“ACT_”开头。 并且表名的第二部分用两个字母表明表的用例，而这个用例也基本上跟Service API匹配。 ACT_GE_* : “GE”代表“General”（通用），用在各种情况下； ACT_HI_* : “HI”代表“History”（历史），这些表中保存的都是历史数据，比如执行过的流程实例、变量、任务，等等。Activit默认提供了4种历史级别： none: 不保存任何历史记录，可以提高系统性能； activity：保存所有的流程实例、任务、活动信息； audit：也是Activiti的默认级别，保存所有的流程实例、任务、活动、表单属性； full：最完整的历史记录，除了包含audit级别的信息之外还能保存详细，例如：流程变量。 对于几种级别根据对功能的要求选择，如果需要日后跟踪详细可以开启full。 ACT_ID_* : “ID”代表“Identity”（身份），这些表中保存的都是身份信息，如用户和组以及两者之间的关系。如果Activiti被集成在某一系统当中的话，这些表可以不用，可以直接使用现有系统中的用户或组信息； ACT_RE_* : “RE”代表“Repository”（仓库），这些表中保存一些‘静态’信息，如流程定义和流程资源（如图片、规则等）； ACT_RU_* : “RU”代表“Runtime”（运行时），这些表中保存一些流程实例、用户任务、变量等的运行时数据。Activiti只保存流程实例在执行过程中的运行时数据，并且当流程结束后会立即移除这些数据，这是为了保证运行时表尽量的小并运行的足够快； Activiti数据表清单: 表分类 表名 解释 一般数据 ACT_GE_BYTEARRAY 通用的流程定义和流程资源 ACT_GE_PROPERTY 系统相关属性 流程历史记录 ACT_HI_ACTINST 历史的流程实例 ACT_HI_ATTACHMENT 历史的流程附件 ACT_HI_COMMENT 历史的说明性信息 ACT_HI_DETAIL 历史的流程运行中的细节信息 ACT_HI_IDENTITYLINK 历史的流程运行过程中用户关系 ACT_HI_PROCINST 历史的流程实例 ACT_HI_TASKINST 历史的任务实例 ACT_HI_VARINST 历史的流程运行中的变量信息 用户用户组表 ACT_ID_GROUP 身份信息-组信息 ACT_ID_INFO 身份信息-组信息 ACT_ID_MEMBERSHIP 身份信息-用户和组关系的中间表 ACT_ID_USER 身份信息-用户信息 流程定义表 ACT_RE_DEPLOYMENT 部署单元信息 ACT_RE_MODEL 模型信息 ACT_RE_PROCDEF 已部署的流程定义 运行实例表 ACT_RU_EVENT_SUBSCR 运行时事件 ACT_RU_EXECUTION 运行时流程执行实例 ACT_RU_IDENTITYLINK 运行时用户关系信息 ACT_RU_JOB 运行时作业 ACT_RU_TASK 运行时任务 ACT_RU_VARIABLE 运行时变量表","categories":[],"tags":[{"name":"activiti","slug":"activiti","permalink":"https://imlike.cc/tags/activiti/"}]},{"title":"虚拟机内部连vpn","slug":"虚拟机内部连vpn","date":"2020-02-03T17:22:27.000Z","updated":"2024-03-14T14:55:29.486Z","comments":true,"path":"571302738.html","link":"","permalink":"https://imlike.cc/571302738.html","excerpt":"","text":"最近由于众所周知的原因,目前只能在家里远程办公,由于目前主力系统是Ubuntu,使用vmware horizon client 连接公司Windows虚拟主机会闪退,地址 : /tmp/vmware-USERNAME/vmware-horizon-client-xxxx.log,网上搜了一下看不到解决方案,那就只有退而求其次在虚拟机上连接. 使用虚拟机一直创建vpn一直报619错误,我之前本地主机连接是没问题的,现在连不上以为我用的win7系统的某些服务是不是没开启,看了一下都开启 后面是我本地主机连上vpn,然后虚拟主机使用这个连接vmware horizon client,同时我发现可能是我VMware workspace软件配置的网卡有问题,所以将网卡由NAT转为桥接 参考链接","categories":[],"tags":[{"name":"ubuntu","slug":"ubuntu","permalink":"https://imlike.cc/tags/ubuntu/"},{"name":"vpn","slug":"vpn","permalink":"https://imlike.cc/tags/vpn/"}]},{"title":"ZUUL转发后导致文件乱码","slug":"ZUUL转发后导致文件乱码","date":"2020-01-01T04:09:50.000Z","updated":"2024-03-14T14:55:29.475Z","comments":true,"path":"230759535.html","link":"","permalink":"https://imlike.cc/230759535.html","excerpt":"","text":"spring zuul how it works Zuul探究(二——Zuul的工作原理 ZuulServlet的执行流程分析 zuul源码解析 —— zuul整体架构 Spring Cloud Netflix Zuul源码分析之请求处理篇-上 dispatcherservlet -&gt; doDispatch( -&gt; getHandler(HandlerExecutionChain) { 该过程会通过getHandlerInternal方法根据 lookupPath(请求路径)、request(请求)调用ZuulHandlerMapping.lookupHandler方法查找匹配的handler。如果没有 } ) -&gt;doService SimpleControllerHandlerAdapter.handle zuulcontroller .handleRequest ZuulServlet.service{//Zuul的主要流程发生在ZuulServlet中，它的调用流程如下： DispatcherServlet.doServiceDispatcherServlet.doDispatchSimpleControllerHandlerAdapter.handleZuulController.handleRequestServletWrappingController.handleRequestInternalZuulServlet.service 1RibbonRoutingFilterpublic Object run() &#123; RequestContext context &#x3D; RequestContext.getCurrentContext(); this.helper.addIgnoredHeaders(); try &#123; RibbonCommandContext commandContext &#x3D; buildCommandContext(context); ClientHttpResponse response &#x3D; forward(commandContext); setResponse(response); return response; &#125; catch (ZuulException ex) &#123; throw new ZuulRuntimeException(ex); &#125; catch (Exception ex) &#123; throw new ZuulRuntimeException(ex); &#125; &#125; 栈信息 1java.lang.Thread.State: RUNNABLE at org.springframework.cloud.netflix.zuul.filters.route.RibbonRoutingFilter.getRequestBody(RibbonRoutingFilter.java:199) at org.springframework.cloud.netflix.zuul.filters.route.RibbonRoutingFilter.buildCommandContext(RibbonRoutingFilter.java:126) at org.springframework.cloud.netflix.zuul.filters.route.RibbonRoutingFilter.run(RibbonRoutingFilter.java:105) at com.netflix.zuul.ZuulFilter.runFilter(ZuulFilter.java:112) at com.netflix.zuul.FilterProcessor.processZuulFilter(FilterProcessor.java:193) at com.netflix.zuul.FilterProcessor.runFilters(FilterProcessor.java:157) at com.netflix.zuul.FilterProcessor.route(FilterProcessor.java:118) at com.netflix.zuul.ZuulRunner.route(ZuulRunner.java:96) at com.netflix.zuul.http.ZuulServlet.route(ZuulServlet.java:116) at com.netflix.zuul.http.ZuulServlet.service(ZuulServlet.java:81) at org.springframework.web.servlet.mvc.ServletWrappingController.handleRequestInternal(ServletWrappingController.java:157) at org.springframework.cloud.netflix.zuul.web.ZuulController.handleRequest(ZuulController.java:44) at org.springframework.web.servlet.mvc.SimpleControllerHandlerAdapter.handle(SimpleControllerHandlerAdapter.java:50) at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:963) at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:897) at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:970) at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:872) at javax.servlet.http.HttpServlet.service(HttpServlet.java:648) at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:846) at javax.servlet.http.HttpServlet.service(HttpServlet.java:729) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:230) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) at org.springframework.boot.web.filter.ApplicationContextHeaderFilter.doFilterInternal(ApplicationContextHeaderFilter.java:55) at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilterInternal(WebRequestTraceFilter.java:108) at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) at cn.com.betasoft.dxm.sso.ext.client.filter.SessionValidationFilter.doFilter(SessionValidationFilter.java:98) at cn.com.betasoft.btrm.portal.config.ExcludeSupportFilter.doFilter(ExcludeSupportFilter.java:21) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) at org.jasig.cas.client.util.AssertionThreadLocalFilter.doFilter(AssertionThreadLocalFilter.java:54) at cn.com.betasoft.btrm.portal.config.ExcludeSupportFilter.doFilter(ExcludeSupportFilter.java:21) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) at org.jasig.cas.client.util.HttpServletRequestWrapperFilter.doFilter(HttpServletRequestWrapperFilter.java:75) at cn.com.betasoft.btrm.portal.config.ExcludeSupportFilter.doFilter(ExcludeSupportFilter.java:21) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) at org.jasig.cas.client.authentication.AuthenticationFilter.doFilter(AuthenticationFilter.java:107) at cn.com.betasoft.btrm.portal.config.ExcludeSupportFilter.doFilter(ExcludeSupportFilter.java:21) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) at org.jasig.cas.client.validation.AbstractTicketValidationFilter.doFilter(AbstractTicketValidationFilter.java:201) at cn.com.betasoft.btrm.portal.config.ExcludeSupportFilter.doFilter(ExcludeSupportFilter.java:21) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) at org.jasig.cas.client.session.SingleSignOutFilter.doFilter(SingleSignOutFilter.java:76) at cn.com.betasoft.btrm.portal.config.ExcludeSupportFilter.doFilter(ExcludeSupportFilter.java:21) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) at org.springframework.security.web.authentication.logout.LogoutFilter.doFilter(LogoutFilter.java:116) at cn.com.betasoft.btrm.portal.config.ExcludeSupportFilter.doFilter(ExcludeSupportFilter.java:21) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) at cn.com.betasoft.btrm.common.serviceconfig.ServiceAssertionConfig$1.doFilter(ServiceAssertionConfig.java:53) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:99) at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:105) at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:81) at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:197) at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) at org.springframework.boot.actuate.autoconfigure.MetricsFilter.doFilterInternal(MetricsFilter.java:106) at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) at org.springframework.web.filter.CorsFilter.doFilterInternal(CorsFilter.java:96) at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:198) at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:96) at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:474) at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:140) at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:79) at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:87) at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:349) at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:783) at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:66) at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:798) at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1434) at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:49) - locked &lt;0x2a0d&gt; (a org.apache.tomcat.util.net.NioEndpoint$NioSocketWrapper) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) at java.lang.Thread.run(Thread.java:748)","categories":[],"tags":[{"name":"spring","slug":"spring","permalink":"https://imlike.cc/tags/spring/"}]},{"title":"springboot 读取外部文件","slug":"springboot 读取外部文件","date":"2020-01-01T04:09:50.000Z","updated":"2024-03-14T14:55:29.481Z","comments":true,"path":"1470057129.html","link":"","permalink":"https://imlike.cc/1470057129.html","excerpt":"","text":"external location for image store in Spring Boot 2.0 web app? 之前有个需求是需要有个urlpatren像 staticFiles/xxx.pdf 请求的url 对应服务器端的某个文件夹可以直接访问, WebMvcConfigurer 需要实现或者继承WebMvcConfigurerAdapter 1@Configurationpublic class MvcConfig extends WebMvcConfigurerAdapter &#123; @Override public void addResourceHandlers(ResourceHandlerRegistry registry) &#123; registry .addResourceHandler(&quot;&#x2F;btrm&#x2F;framework&#x2F;item&#x2F;static&#x2F;**&quot;) .addResourceLocations(&quot;file:D:\\\\SVNCODE\\\\btrm\\\\&quot;) .setCachePeriod(200); super.addResourceHandlers(registry); &#125;&#125;","categories":[],"tags":[{"name":"spring","slug":"spring","permalink":"https://imlike.cc/tags/spring/"}]},{"title":"申请教育邮箱和使用微软 OneDrive API 和腾讯云 SCF 免费搭建个人网盘","slug":"申请教育邮箱和使用微软 OneDrive API 和腾讯云 SCF 免费搭建个人网盘","date":"2019-09-28T02:03:19.000Z","updated":"2024-03-14T14:55:29.485Z","comments":true,"path":"18764620.html","link":"","permalink":"https://imlike.cc/18764620.html","excerpt":"","text":"参考注册地址 注册的学校是(Red Rocks Community College官网)注册申请时需要用到 参考注册地址2 国西南学院(Southwestern College) 为什么要申请教育邮箱优点如下 $ 15 Amazon AWS 优惠券 50美元的 Digital Ocean 优惠券。它会在添加到您的帐户（仅限新帐户）后的12个月内过期。 Bitnami：提供为期一年的Business 3计划，通常每月收费49美元。 Crowdflower：访问Crowdflower平台。 DNSimple：提供两年的个人托管DNS计划，通常每月收取5美元。 GitHub：在您还是学生时提供无限的私人存储库。通常，GitHub每月收费7美元。 25美元的HackHands信用， 这是一项实时的编程帮助，每天24/7可用。 一套 Microsoft Azure云服务 和开发人员工具：学生期间，Microsoft Azure，Visual Studio社区和其他Microsoft开发人员工具。 NameCheap.com：提供为期一年的.ME域名注册，每年费用为18.99美元–以及为期一年的SSL证书，费用为每年10美元。 Orchestrate：当您是学生时，访问每月费用为49美元的开发者帐户。它是一个完整的数据库产品组合，包括通过API进行的搜索，时间序列事件，地理位置和图形查询。 SendGrid：当您是学生时，每月为学生计划提供15,000封免费电子邮件。 Travis CI：当您是学生时，访问每月收费69美元的Private版本。 LastPass Apple .EDU电子邮件地址从Apple购买产品，可以帮助您节省很多钱。 微软 OneDrive API 和腾讯云 SCF 免费目前可以使用onedrive 5t 使用软件搭建自己网盘 oneDrive搭建个人网盘","categories":[],"tags":[{"name":"onedrive","slug":"onedrive","permalink":"https://imlike.cc/tags/onedrive/"},{"name":"腾讯云","slug":"腾讯云","permalink":"https://imlike.cc/tags/%E8%85%BE%E8%AE%AF%E4%BA%91/"}]},{"title":"Gnome插件安装和优化","slug":"Gnome插件安装和优化","date":"2019-09-21T22:32:53.000Z","updated":"2024-03-14T14:55:29.473Z","comments":true,"path":"2127448358.html","link":"","permalink":"https://imlike.cc/2127448358.html","excerpt":"","text":"1sudo apt install -y sudo apt install gnome-tweak-tool sudo apt install gnome-shell-extensions sudo apt install chrome-gnome-shell sudo apt install gtk2-engines-pixbuf sudo apt install libxml2-utils 之前一直都是安装在~/.local/share/gnome-shell/extensions 之前都是从github 下载后 手动安装 现在有些插件安装后 优化工具一直没有显示 ,之前 使用 ~/.local/share/gnome-shell/extensions 没有权限 sudo chmod 777 extensions 给予权限 不然无法使用chrome插件(GNOME Shell integration ) 来安装 现在chrome插件安装 (GNOME Shell integration) 对于 Google Chrome、Chromium 和 Vivaldi： Chrome Web 商店 对于 Firefox： Mozilla Addons 对于 Opera： Opera Addons 1“尽管 GNOME Shell 集成扩展正在运行，但未检测到本地主机连接器。请参阅文档以获取有关安装连接器的信息。”sudo apt install chrome-gnome-shell 安装玩后访问 https://extensions.gnome.org/extension 就会有选择按钮了 农历插件1sudo apt-get update安装依赖sudo apt-get install gir1.2-lunar-date-2.0 拓展的列表和 2.1 安装 Dash to dock . Hide top bar ——全屏自动隐藏顶栏,缺少这个 GNOME3 没法用 Clipboard Indicator —— 剪切板 Media player indicator —— 显示音乐播放器的状态 Battery status —— 显示电池电量的百分比 Netspeed ——在顶栏上显示网速 Workspace indicator —— 在顶栏显示当前示工作区的序号 Activities-configurator ——活动，顶栏颜色，透明等 User themes ——启用自定义的 shell 主题 Lunar Calendar ——阴历(依赖 typelib-1_0-LunarDate-2_0) Openweather ——天气 System-monitor ——资源监控 (依赖 NetworkManager-devel libgtop-devel) topicon plus —— 通知图标 2.2通知栏的效果[](../img/2019-09-22 14-48-58 的屏幕截图.png) 3 字体1单独放在一个文件夹里(fonts),然后sudo cp -r 连着文件夹复制到&#x2F;usr&#x2F;share&#x2F;fonts,(sudo cp -r xxx&#x2F;fonts &#x2F;usr&#x2F;share&#x2F;fonts (xxx请换为路径))再 chmod 755 -R &#x2F;usr&#x2F;share&#x2F;fonts&#x2F;fonts , 最后依次执行:sudo mkfontscalesudo mkfontdirsudo fc-cache [写给入坑大蜥蜴且选择GNOME的小白]) Gnome 配置 如何使用 GNOME Shell 扩展","categories":[],"tags":[{"name":"ubuntu","slug":"ubuntu","permalink":"https://imlike.cc/tags/ubuntu/"},{"name":"gnome","slug":"gnome","permalink":"https://imlike.cc/tags/gnome/"}]},{"title":"Ubuntu下安装和初始化postgres","slug":"Ubuntu下安装和初始化postgres","date":"2019-09-12T17:00:06.000Z","updated":"2024-03-14T14:55:29.474Z","comments":true,"path":"18764621.html","link":"","permalink":"https://imlike.cc/18764621.html","excerpt":"","text":"安装postgres1sudo apt-get updatesudp apt-get postgresql postgresql-contrib PostgreSQL安装完成后默认是已经启动的，但是也可以通过下面的方式来手动启动服务 1sudo &#x2F;etc&#x2F;init.d&#x2F;postgresql start 开启sudo &#x2F;etc&#x2F;init.d&#x2F;postgresql stop 关闭sudo &#x2F;etc&#x2F;init.d&#x2F;postgresql restart 重启 配置环境 创建数据库用户ricky，并指定为超级用户 1sudo -u postgres createuser --superuser username 配置用户的密码 1sudo -u postgres psql 进入控制台 为刚创建的用户创建数据库 1sudo -u postgres createdb --owner&#x3D;dbname 登录数据库 1psql -U username -d dbname -h 127.0.0.1 -p 5432 修改用户数据1修改用户密码 1. sudo -u postgres psql 2. ALTER USER username WITH PASSWORD &#96;&#96;&#39;postgres&#39;&#96;&#96;;tips: 密码postgres要用引号引起来 命令最后有分号 修改linux系统postgres用户的密码PostgreSQL会创建一个默认的linux用户postgres，修改该用户密码的方法如下： 步骤一：删除用户postgres的密码 1&#96;sudo&#96; &#96;passwd&#96; &#96;-d postgres&#96; 步骤二：设置用户postgres的密码 1&#96;sudo&#96; &#96;-u postgres &#96;&#96;passwd&#96; 系统提示输入新的密码 1&#96;Enter new UNIX password:&#96;&#96;Retype new UNIX password:&#96;&#96;passwd&#96;&#96;: password updated successfully&#96; 参考文档 参考文档","categories":[],"tags":[{"name":"ubuntu","slug":"ubuntu","permalink":"https://imlike.cc/tags/ubuntu/"},{"name":"postgres","slug":"postgres","permalink":"https://imlike.cc/tags/postgres/"}]},{"title":"aria2使用rpc下载百度云","slug":"aria2使用rpc下载百度云","date":"2019-09-05T03:30:18.000Z","updated":"2024-03-14T14:55:29.475Z","comments":true,"path":"3861112470.html","link":"","permalink":"https://imlike.cc/3861112470.html","excerpt":"","text":"1.下载并安装github地址-&gt;https://github.com/aria2/aria2 这边提供一下deb 包的下载地址 https://ugetdm.com/go/ubuntu-64-bionic-aria2-download 或者 sudo apt-get install aria2 可以尝试在命令行 使用 1aria2c --conf-path&#x3D;&lt;配置地址&gt; -D # aria2c -c -s 5 http:&#x2F;&#x2F;example.org&#x2F;mylinux.iso -c : 断点续传 -s : 使用线程数 2、Download from 2 sources # aria2c http:&#x2F;&#x2F;a&#x2F;f.iso ftp:&#x2F;&#x2F;b&#x2F;f.iso 下载相关的命令 11、Download from WEB # aria2c -c -s 5 http://example.org/mylinux.iso-c : 断点续传-s : 使用线程数2、Download from 2 sources# aria2c http://a/f.iso ftp://b/f.iso3、Download using 2 connections per host# aria2c -x2 http://a/f.iso4、BitTorrent Magnet URI# aria2c &#x27;magnet:?xt=urn:btih:248D0A1CD08284299DE78D5C1ED359BB46717D8C&#x27;5、Metalink# aria2c http://example.org/mylinux.metalink6、Download URIs found in text file# aria2c -i uris.txt7、BT# aria2c /tmp/CentOS-6.3-i386-bin-DVD1to2.torrent# aria2c http://mirrors.163.com/centos/6.6/isos/x86_64/CentOS-6.6-x86_64-minimal.torrent 2.配置Aria2一般地址是在 ~/.aria2/aria2.conf rpc设置为true 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384#用户名#rpc-user&#x3D;user#密码#rpc-passwd&#x3D;passwd#上面的认证方式不建议使用,建议使用下面的token方式#设置加密的密钥#rpc-secret&#x3D;token#允许rpcenable-rpc&#x3D;true#允许所有来源, web界面跨域权限需要rpc-allow-origin-all&#x3D;true#允许外部访问，false的话只监听本地端口rpc-listen-all&#x3D;true#RPC端口, 仅当默认端口被占用时修改rpc-listen-port&#x3D;6800#最大同时下载数(任务数), 路由建议值: 3max-concurrent-downloads&#x3D;5#断点续传continue&#x3D;true#同服务器连接数max-connection-per-server&#x3D;16#最小文件分片大小, 下载线程数上限取决于能分出多少片, 对于小文件重要min-split-size&#x3D;10M#单文件最大线程数, 路由建议值: 5split&#x3D;10#下载速度限制max-overall-download-limit&#x3D;0#单文件速度限制max-download-limit&#x3D;0#上传速度限制max-overall-upload-limit&#x3D;0#单文件速度限制max-upload-limit&#x3D;0#断开速度过慢的连接#lowest-speed-limit&#x3D;0#验证用，需要1.16.1之后的release版本#referer&#x3D;*#文件保存路径, 默认为当前启动位置dir&#x3D;&#x2F;home&#x2F;leek&#x2F;save&#x2F;download#文件缓存, 使用内置的文件缓存, 如果你不相信Linux内核文件缓存和磁盘内置缓存时使用, 需要1.16及以上版本#disk-cache&#x3D;0#另一种Linux文件缓存方式, 使用前确保您使用的内核支持此选项, 需要1.15及以上版本(?)#enable-mmap&#x3D;true#文件预分配, 能有效降低文件碎片, 提高磁盘性能. 缺点是预分配时间较长#所需时间 none &lt; falloc ? trunc « prealloc, falloc和trunc需要文件系统和内核支持file-allocation&#x3D;preallocbt-tracker&#x3D;udp:&#x2F;&#x2F;62.138.0.158:6969&#x2F;announce,udp:&#x2F;&#x2F;188.241.58.209:6969&#x2F;announce,udp:&#x2F;&#x2F;93.158.213.92:1337&#x2F;announce,udp:&#x2F;&#x2F;62.210.97.59:1337&#x2F;announce,udp:&#x2F;&#x2F;151.80.120.113:2710&#x2F;announce,udp:&#x2F;&#x2F;151.80.120.113:2710&#x2F;announce,udp:&#x2F;&#x2F;165.231.0.116:80&#x2F;announce,udp:&#x2F;&#x2F;208.83.20.20:6969&#x2F;announce,udp:&#x2F;&#x2F;37.235.174.46:2710&#x2F;announce,udp:&#x2F;&#x2F;185.181.60.67:80&#x2F;announce,udp:&#x2F;&#x2F;54.37.235.149:6969&#x2F;announce,udp:&#x2F;&#x2F;89.234.156.205:451&#x2F;announce,udp:&#x2F;&#x2F;35.156.19.129:6969&#x2F;announce,udp:&#x2F;&#x2F;159.100.245.181:6969&#x2F;announce,udp:&#x2F;&#x2F;51.15.40.114:80&#x2F;announce,udp:&#x2F;&#x2F;142.44.243.4:1337&#x2F;announce,udp:&#x2F;&#x2F;176.113.71.19:6961&#x2F;announce,udp:&#x2F;&#x2F;5.206.54.49:6969&#x2F;announce,udp:&#x2F;&#x2F;184.105.151.164:6969&#x2F;announce,http:&#x2F;&#x2F;184.105.151.164:6969&#x2F;announce,udp:&#x2F;&#x2F;51.254.249.186:6969&#x2F;announce,udp:&#x2F;&#x2F;185.83.215.123:6969&#x2F;announce,udp:&#x2F;&#x2F;194.182.176.251:6969&#x2F;announce,udp:&#x2F;&#x2F;212.1.226.176:2710&#x2F;announce,udp:&#x2F;&#x2F;212.47.227.58:6969&#x2F;announce,udp:&#x2F;&#x2F;95.211.168.204:2710&#x2F;announce,udp:&#x2F;&#x2F;46.148.18.250:2710&#x2F;announce,udp:&#x2F;&#x2F;165.227.0.51:6969&#x2F;announce,udp:&#x2F;&#x2F;91.217.91.21:3218&#x2F;announce,udp:&#x2F;&#x2F;190.146.242.80:7070&#x2F;announce,udp:&#x2F;&#x2F;149.129.65.231:2710&#x2F;announce,udp:&#x2F;&#x2F;15.188.67.15:6969&#x2F;announce,udp:&#x2F;&#x2F;185.97.255.30:6969&#x2F;announce,udp:&#x2F;&#x2F;51.15.134.218:2710&#x2F;announce,udp:&#x2F;&#x2F;52.58.128.163:6969&#x2F;announce,udp:&#x2F;&#x2F;45.86.70.144:6969&#x2F;announce,udp:&#x2F;&#x2F;109.72.83.214:80&#x2F;announce,udp:&#x2F;&#x2F;217.76.183.53:80&#x2F;announce,udp:&#x2F;&#x2F;46.148.18.254:2710&#x2F;announce,udp:&#x2F;&#x2F;23.95.32.130:6969&#x2F;announce,udp:&#x2F;&#x2F;207.241.231.226:6969&#x2F;announce,udp:&#x2F;&#x2F;207.241.226.111:6969&#x2F;announce,udp:&#x2F;&#x2F;207.180.241.35:2710&#x2F;announce,https:&#x2F;&#x2F;47.240.44.4:443&#x2F;announce,http:&#x2F;&#x2F;79.133.36.244:6969&#x2F;announce,http:&#x2F;&#x2F;91.217.91.21:3218&#x2F;announce,http:&#x2F;&#x2F;93.158.213.92:1337&#x2F;announce,http:&#x2F;&#x2F;51.15.134.218:2710&#x2F;announce,http:&#x2F;&#x2F;153.92.6.117:80&#x2F;announce,http:&#x2F;&#x2F;139.99.45.37:80&#x2F;announce,http:&#x2F;&#x2F;78.30.254.12:2710&#x2F;announce,http:&#x2F;&#x2F;91.207.136.85:80&#x2F;announce,http:&#x2F;&#x2F;52.203.65.11:80&#x2F;announce,udp:&#x2F;&#x2F;95.211.168.204:2710&#x2F;announce,udp:&#x2F;&#x2F;176.113.68.67:6961&#x2F;announce,udp:&#x2F;&#x2F;104.244.79.229:6969&#x2F;announce,udp:&#x2F;&#x2F;51.15.3.74:6969&#x2F;announce,udp:&#x2F;&#x2F;218.5.42.53:2710&#x2F;announce,udp:&#x2F;&#x2F;185.83.214.123:6969&#x2F;announce,udp:&#x2F;&#x2F;80.211.26.119:9696&#x2F;announce,udp:&#x2F;&#x2F;139.99.100.97:8080&#x2F;announce,https:&#x2F;&#x2F;212.237.22.141:443&#x2F;announce,http:&#x2F;&#x2F;182.150.53.61:8080&#x2F;announce,http:&#x2F;&#x2F;95.107.48.115:80&#x2F;announce,http:&#x2F;&#x2F;37.235.174.46:2710&#x2F;announce,http:&#x2F;&#x2F;176.113.71.19:6961&#x2F;announce,http:&#x2F;&#x2F;176.113.68.67:6961&#x2F;announce,http:&#x2F;&#x2F;34.92.10.197:6789&#x2F;announce,http:&#x2F;&#x2F;95.216.171.255:6969&#x2F;announce,http:&#x2F;&#x2F;62.210.202.61:80&#x2F;announce,http:&#x2F;&#x2F;104.244.79.229:80&#x2F;announce,http:&#x2F;&#x2F;176.123.5.118:80&#x2F;announce,http:&#x2F;&#x2F;51.79.71.167:80&#x2F;announce,http:&#x2F;&#x2F;185.83.214.123:6969&#x2F;announce,http:&#x2F;&#x2F;62.210.177.88:1096&#x2F;announce,http:&#x2F;&#x2F;51.68.122.172:80&#x2F;announce## BT&#x2F;PT下载相关 ### 当下载的是一个种子(以.torrent结尾)时, 自动开始BT任务, 默认:true#follow-torrent&#x3D;true# BT监听端口, 当端口被屏蔽时使用, 默认:6881-6999listen-port&#x3D;51413# 单个种子最大连接数, 默认:55#bt-max-peers&#x3D;55# 打开DHT功能, PT需要禁用, 默认:trueenable-dht&#x3D;false# 打开IPv6 DHT功能, PT需要禁用#enable-dht6&#x3D;false# DHT网络监听端口, 默认:6881-6999#dht-listen-port&#x3D;6881-6999# 本地节点查找, PT需要禁用, 默认:false#bt-enable-lpd&#x3D;false# 种子交换, PT需要禁用, 默认:trueenable-peer-exchange&#x3D;false# 每个种子限速, 对少种的PT很有用, 默认:50K#bt-request-peer-speed-limit&#x3D;50K# 客户端伪装, PT需要peer-id-prefix&#x3D;-TR2770-user-agent&#x3D;Transmission&#x2F;2.77peer-agent&#x3D;Transmission&#x2F;2.77# 当种子的分享率达到这个数时, 自动停止做种, 0为一直做种, 默认:1.0seed-ratio&#x3D;0# 强制保存会话, 即使任务已经完成, 默认:false# 较新的版本开启后会在任务完成后依然保留.aria2文件#force-save&#x3D;false# BT校验相关, 默认:true#bt-hash-check-seed&#x3D;true# 继续之前的BT任务时, 无需再次校验, 默认:falsebt-seed-unverified&#x3D;true# 保存磁力链接元数据为种子文件(.torrent文件), 默认:falsebt-save-metadata&#x3D;true 3.rpc调用 下载百度云测试 rpc可用性 http://127.0.0.1:6800/jsonrpc?jsoncallback=1 有返回既能用否则可能是服务没有起起来 使用的chrome 油猴插件 百度网盘超级助手选择pandownload -&gt; 调用baiduwp.com 的下载接口直接下载 -&gt; 点击具体文件后选择使用Aria2下载 参考链接 关于aria2最完整的一篇 https://binux.blog/2012/12/aria2-examples/","categories":[],"tags":[]},{"title":"floccus利用坚果云WebDAV跨浏览器同步数据","slug":"floccus利用坚果云WebDAV跨浏览器同步数据","date":"2019-08-29T04:11:41.000Z","updated":"2024-03-14T14:55:29.477Z","comments":true,"path":"4202359184.html","link":"","permalink":"https://imlike.cc/4202359184.html","excerpt":"","text":"floccus介绍1Floccus 是一款可以通过 Nextcloud，WebDAV 或本地文件（以及任何文件同步解决方案）跨浏览器同步书签的插件，支持 Chrome、Firefox、Kiwi 等浏览器（不支持 iOS）项目地址: https:&#x2F;&#x2F;github.com&#x2F;marcelklehr&#x2F;floccusWebDAV ：目前支持 「坚果云、nextcloud &#x2F; owncloud，box」 等 WebDAV 服务器Nextcloud 书签 ：利用 Nextcloud 自带的书签应用程序进行同步，可通过 Web 访问。(需要nextcloud v12 以上版本)。本地文件同步 ：利用 LoFloccus 4 软件将书签同步到本地, 再利用 Dropbox，Syncthing，rsync 等进行跨设备同步。 配置坚果云 尽量和和我的坚果云文件夹同级为了后面填写书签同步路径,然后上传一个bookmarks.xbel文件到刚才新建的文件夹里面，没有的点这里下载。 接着坚果云“账户信息”-“安全选项”-“第三方应用管理”，创建一个WebDAV应用，获取密码 1### 安装Firefox chrome 插件 Chrome扩展地址（离线地址）、Firefox扩展地址 最后一个注意的 不然只能同步一部分的","categories":[],"tags":[{"name":"webdav","slug":"webdav","permalink":"https://imlike.cc/tags/webdav/"},{"name":"坚果云","slug":"坚果云","permalink":"https://imlike.cc/tags/%E5%9D%9A%E6%9E%9C%E4%BA%91/"}]},{"title":"POST与PUT区别RESTFUL风格中","slug":"POST与PUT区别RESTFUL风格中","date":"2019-08-28T04:34:09.000Z","updated":"2024-03-14T14:55:29.474Z","comments":true,"path":"4265857413.html","link":"","permalink":"https://imlike.cc/4265857413.html","excerpt":"","text":"1 HTTP协议详解转载地址 http://blog.yikaiba.com/Home/Article/index/id/156.html HTTP是HyperText Transfer Protocol（超文本传输协议）的缩写。它的发展是万维网协会（WorldWide Web Consortium）和Internet工作小组IETF（Internet Engineering Task Force）合作的结果，（他们）最终发布了一系列的RFC，RFC 1945定义了HTTP/1.0版本。其中最著名的就是RFC 2616。RFC 2616定义了今天普遍使用的一个版本——HTTP 1.1。 HTTP协议（HyperText Transfer Protocol，超文本传输协议）是用于从WWW服务器传输超文本到本地浏览器的传送协议。它可以使浏览器更加高效，使网络传输减少。它不仅保证计算机正确快速地传输超文本文档，还确定传输文档中的哪一部分，以及哪部分内容首先显示(如文本先于图形)等。 HTTP协议通常承载于TCP协议之上，有时也承载于TLS或SSL协议层之上，这个时候，就成了我们常说的HTTPS。如下图所示： 默认HTTP的端口号为80，HTTPS的端口号为443。 HTTP协议永远都是客户端发起请求，服务器回送响应。见下图： 这样就限制了使用HTTP协议，无法实现在客户端没有发起请求的时候，服务器将消息推送给客户端。 HTTP协议是一个无状态的协议，同一个客户端的这次请求和上次请求是没有对应关系。 一次HTTP操作称为一个事务，其工作过程可分为四步： 1）首先客户机与服务器需要建立连接。只要单击某个超级链接，HTTP的工作开始。 2）建立连接后，客户机发送一个请求给服务器，请求方式的格式为：统一资源标识符（URL）、协议版本号，后边是MIME信息包括请求修饰符、客户机信息和可能的内容。 3）服务器接到请求后，给予相应的响应信息，其格式为一个状态行，包括信息的协议版本号、一个成功或错误的代码，后边是MIME信息包括服务器信息、实体信息和可能的内容。 4）客户端接收服务器所返回的信息通过浏览器显示在用户的显示屏上，然后客户机与服务器断开连接。 如果在以上过程中的某一步出现错误，那么产生错误的信息将返回到客户端，有显示屏输出。对于用户来说，这些过程是由HTTP自己完成的，用户只要用鼠标点击，等待信息显示就可以了。 2 HTTP协议详解之请求篇 http请求由三部分组成，分别是：请求行、消息报头、请求正文 请求行以一个方法符号开头，以空格分开，后面跟着请求的URI和协议的版本，格式如下：Method Request-URIHTTP-Version CRLF 其中 Method表示请求方法；Request-URI是一个统一资源标识符；HTTP-Version表示请求的HTTP协议版本；CRLF表示回车和换行（除了作为结尾的CRLF外，不允许出现单独的CR或LF字符）。 请求方法（所有方法全为大写）有多种，各个方法的解释如下： GET 请求获取Request-URI所标识的资源 POST 在Request-URI所标识的资源后附加新的数据 HEAD 请求获取由Request-URI所标识的资源的响应消息报头 PUT 请求服务器存储一个资源，并用Request-URI作为其标识 DELETE 请求服务器删除Request-URI所标识的资源 TRACE 请求服务器回送收到的请求信息，主要用于测试或诊断 CONNECT 保留将来使用 OPTIONS 请求查询服务器的性能，或者查询与资源相关的选项和需求 应用举例： GET方法：在浏览器的地址栏中输入网址的方式访问网页时，浏览器采用GET方法向服务器获取资源，eg:GET /form.html HTTP/1.1 (CRLF) POST方法要求被请求服务器接受附在请求后面的数据，常用于提交表单。 HEAD方法与GET方法几乎是一样的，对于HEAD请求的回应部分来说，它的HTTP头部中包含的信息与通过GET请求所得到的信息是相同的。利用这个方法，不必传输整个资源内容，就可以得到Request-URI所标识的资源的信息。该方法常用于测试超链接的有效性，是否可以访问，以及最近是否更新。 3 HTTP协议详解之响应篇 在接收和解释请求消息后，服务器返回一个HTTP响应消息。 HTTP响应也是由三个部分组成，分别是：状态行、消息报头、响应正文 1、状态行格式如下： HTTP-VersionStatus-Code Reason-Phrase CRLF 其中，HTTP-Version表示服务器HTTP协议的版本；Status-Code表示服务器发回的响应状态代码；Reason-Phrase表示状态代码的文本描述。 状态代码有三位数字组成，第一个数字定义了响应的类别，且有五种可能取值： 1xx：指示信息–表示请求已接收，继续处理 2xx：成功–表示请求已被成功接收、理解、接受 3xx：重定向–要完成请求必须进行更进一步的操作 4xx：客户端错误–请求有语法错误或请求无法实现 5xx：服务器端错误–服务器未能实现合法的请求 2、常见状态代码、状态描述、说明： 请求收到，继续处理 100——客户必须继续发出请求 101——客户要求服务器根据请求转换HTTP协议版本 操作成功收到，分析、接受 200——交易成功 201——提示知道新文件的URL 202——接受和处理、但处理未完成 203——返回信息不确定或不完整 204——请求收到，但返回信息为空 205——服务器完成了请求，用户代理必须复位当前已经浏览过的文件 206——服务器已经完成了部分用户的GET请求 完成此请求必须进一步处理 300——请求的资源可在多处得到 301——删除请求数据 302——在其他地址发现了请求数据 303——建议客户访问其他URL或访问方式 304——客户端已经执行了GET，但文件未变化 305——请求的资源必须从服务器指定的地址得到 306——前一版本HTTP中使用的代码，现行版本中不再使用 307——申明请求的资源临时性删除 请求包含一个错误语法或不能完成 400——错误请求，如语法错误 401——未授权 HTTP 401.1 - 未授权：登录失败 HTTP 401.2 - 未授权：服务器配置问题导致登录失败 HTTP 401.3 - ACL 禁止访问资源 HTTP 401.4 - 未授权：授权被筛选器拒绝 HTTP 401.5 - 未授权：ISAPI 或 CGI 授权失败 402——保留有效ChargeTo头响应 403——禁止访问 HTTP 403.1 禁止访问：禁止可执行访问 HTTP 403.2 - 禁止访问：禁止读访问 HTTP 403.3 - 禁止访问：禁止写访问 HTTP 403.4 - 禁止访问：要求 SSL HTTP 403.5 - 禁止访问：要求 SSL128 HTTP 403.6 - 禁止访问：IP 地址被拒绝 HTTP 403.7 - 禁止访问：要求客户证书 HTTP 403.8 - 禁止访问：禁止站点访问 HTTP 403.9 - 禁止访问：连接的用户过多 HTTP 403.10 - 禁止访问：配置无效 HTTP 403.11 - 禁止访问：密码更改 HTTP 403.12 - 禁止访问：映射器拒绝访问 HTTP 403.13 - 禁止访问：客户证书已被吊销 HTTP 403.15 - 禁止访问：客户访问许可过多 HTTP 403.16 - 禁止访问：客户证书不可信或者无效 HTTP 403.17 - 禁止访问：客户证书已经到期或者尚未生效 404——没有发现文件、查询或URl 405——用户在Request-Line字段定义的方法不允许 406——根据用户发送的Accept拖，请求资源不可访问 407——类似401，用户必须首先在代理服务器上得到授权 408——客户端没有在用户指定的饿时间内完成请求 409——对当前资源状态，请求不能完成 410——服务器上不再有此资源且无进一步的参考地址 411——服务器拒绝用户定义的Content-Length属性请求 412——一个或多个请求头字段在当前请求中错误 413——请求的资源大于服务器允许的大小 414——请求的资源URL长于服务器允许的长度 415——请求资源不支持请求项目格式 416——请求中包含Range请求头字段，在当前请求资源范围内没有range指示值，请求也不包含If-Range请求头字段 417——服务器不满足请求Expect头字段指定的期望值，如果是代理服务器，可能是下一级服务器不能满足请求长。 服务器执行一个完全有效请求失败 HTTP 500 - 内部服务器错误 HTTP 500.100 - 内部服务器错误 -ASP 错误 HTTP 500-11 服务器关闭 HTTP 500-12 应用程序重新启动 HTTP 500-13 - 服务器太忙 HTTP 500-14 - 应用程序无效 HTTP 500-15 - 不允许请求 global.asa Error 501 - 未实现 HTTP 502 - 网关错误 HTTP 500.100 - 内部服务器错误 -ASP 错误 HTTP 500-11 服务器关闭 HTTP 500-12 应用程序重新启动 HTTP 500-13 - 服务器太忙 HTTP 500-14 - 应用程序无效 HTTP 500-15 - 不允许请求 global.asa Error 501 - 未实现 HTTP 502 - 网关错误 4 API设计的基本要求 一个被普遍承认和遵守：RESTful设计原则。它被Roy Felding提出（在他的”基于网络的软件架构“论文中第五章）。而REST的核心原则是将你的API拆分为逻辑上的资源。这些资源通过http被操作（GET ,POST,PUT,DELETE）。 显然从API用户的角度来看，”资源“应该是个名词。即使你的内部数据模型和资源已经有了很好的对应，API设计的时候你仍然不需要把它们一对一的都暴露出来。这里的关键是隐藏内部资源，暴露必需的外部资源。 一旦定义好了要暴露的资源，你可以定义资源上允许的操作，以及这些操作和你的API的对应关系： · GET /tickets # 获取ticket列表 · GET /tickets/12 # 查看某个具体的ticket · POST /tickets # 新建一个ticket · PUT /tickets/12 # 更新ticket 12. · DELETE /tickets/12 #删除ticekt 12 可以看出使用REST的好处在于可以充分利用http的强大实现对资源的CURD功能。而这里你只需要一个endpoint：/tickets,再没有其他什么命名规则和url规则了，cool！ 但是有的endpoint，需要使用复数使得你的URL更加规整。这让API使用者更加容易理解，对开发者来说也更容易实现。 如何处理关联？关于如何处理资源之间的管理REST原则也有相关的描述： · GET /tickets/12/messages- Retrieves list of messages forticket #12 · GET /tickets/12/messages/5- Retrieves message #5 forticket #12 · POST /tickets/12/messages- Creates a new message inticket #12 · PUT /tickets/12/messages/5- Updates message #5 for ticket#12 · PATCH /tickets/12/messages/5- Partially updates message#5 for ticket #12 · DELETE /tickets/12/messages/5- Deletes message #5 forticket #12 其中，如果这种关联和资源独立，那么我们可以在资源的输出表示中保存相应资源的endpoint。然后API的使用者就可以通过点击链接找到相关的资源。如果关联和资源联系紧密。资源的输出表示就应该直接保存相应资源信息。（例如这里如果message资源是独立存在的，那么上面 GET/tickets/12/messages就会返回相应message的链接；相反的如果message不独立存在，他和ticket依附存在，则上面的API调用返回直接返回message信息） 5 get和post区别 常用的请求方式是GET和POST. GET方式：是以实体的方式得到由请求URI所指定资源的信息，如果请求URI只是一个数据产生过程，那么最终要在响应实体中返回的是处理过程的结果所指向的资源，而不是处理过程的描述。 POST方式：用来向目的服务器发出请求，要求它接受被附在请求后的实体，并把它当作请求队列中请求URI所指定资源的附加新子项，Post被设计成用统一的方法实现下列功能： 1）对现有资源的解释； 2）向电子公告栏、新闻组、邮件列表或类似讨论组发信息； 3）提交数据块； 4）通过附加操作来扩展数据库 。 从上面描述可以看出，Get是向服务器发索取数据的一种请求；而Post是向服务器提交数据的一种请求，要提交的数据位于信息头后面的实体中。 GET与POST方法有以下区别： 1） 在客户端，Get方式在通过URL提交数据，数据在URL中可以看到；POST方式，数据放置在HTMLHEADER内提交。 2） GET方式提交的数据最多只能有1024字节，而POST则没有此限制。 3） 安全性问题。正如在（1）中提到，使用 Get 的时候，参数会显示在地址栏上，而 Post 不会。所以，如果这些数据是中文数据而且是非敏感数据，那么使用 get；如果用户输入的数据不是中文字符而且包含敏感数据，那么还是使用 post为好。 4） 安全的和幂等的。所谓安全的意味着该操作用于获取信息而非修改信息。幂等的意味着对同一 URL 的多个请求应该返回同样的结果。完整的定义并不像看起来那样严格。换句话说，GET 请求一般不应产生副作用。从根本上讲，其目标是当用户打开一个链接时，她可以确信从自身的角度来看没有改变资源。比如，新闻站点的头版不断更新。虽然第二次请求会返回不同的一批新闻，该操作仍然被认为是安全的和幂等的，因为它总是返回当前的新闻。反之亦然。POST 请求就不那么轻松了。POST 表示可能改变服务器上的资源的请求。仍然以新闻站点为例，读者对文章的注解应该通过 POST 请求实现，因为在注解提交之后站点已经不同了（比方说文章下面出现一条注解）。 6 put和post区别 有的观点认为，应该用POST来创建一个资源，用PUT来更新一个资源；有的观点认为，应该用PUT来创建一个资源，用POST来更新一个资源；还有的观点认为可以用PUT和POST中任何一个来做创建或者更新一个资源。这些观点都只看到了风格，争论起来也只是争论哪种风格更好，其实，用PUT还是POST，不是看这是创建还是更新资源的动作，这不是风格的问题，而是语义的问题。 举一个简单的例子，假如有一个博客系统提供一个Web API，模式是这样http://superblogging/blogs/{blog-name}，很简单，将{blog-name} 替换为我们的blog名字，往这个URI发送一个HTTP PUT或者POST请求，HTTP的body部分就是博文，这是一个很简单的REST API例子。我们应该用 PUT方法还是POST方法？取决于这个REST服务的行为是否是idempotent的，假如我们发送两个http://superblogging/blogs/post/Sample请求，服 务器端是什么样的行为？如果产生了两个博客帖子，那就说明这个服务不是idempotent的，因为多次使用产生了副作用了嘛；如果后一个请求把第一个 请求覆盖掉了，那这个服务就是idempotent的。前一种情况，应该使用POST方法，后一种情况，应该使用PUT方法。","categories":[],"tags":[{"name":"post","slug":"post","permalink":"https://imlike.cc/tags/post/"},{"name":"put","slug":"put","permalink":"https://imlike.cc/tags/put/"}]},{"title":"免费短信接码网站","slug":"免费短信接码网站","date":"2019-08-28T04:18:12.000Z","updated":"2024-03-14T14:55:29.483Z","comments":true,"path":"124395117.html","link":"","permalink":"https://imlike.cc/124395117.html","excerpt":"","text":"国内平台展开目录 https://www.pdflibr.com https://www.visitorsms.com/cn https://www.becmd.com http://www.114sim.com https://yunduanxin.net http://www.smszk.com http://z-sms.com http://www.shejiinn.com https://sms.cngrok.com 国外平台展开目录 https://ch.freephonenum.com https://smsreceivefree.com https://zh.mytrashmobile.com https://www.receive-sms-online.info https://receiveasms.com https://sms-online.co/receive-free-sms https://receive-sms.com 引用链接 在线邮箱展开目录 9em https://9em.org 9em.org 来自不死鸟微信群内大佬，这是一个 10 分钟邮箱服务，邮箱后缀为 [@9em.org](mailto:@9em.org)` 9em 取自 九条命 Email 在屏幕右上角输入一个你想要的邮箱前缀，就能收取邮件需要注意的是这个邮箱对所有人都是公开的，并且邮件仅在服务器保存十分钟，十分钟后会自动删除 maildrop https://maildrop.cc 打开 maildrop 会自动生成一个临时的邮箱，如果你不想更改地址，可以直接点右上角的 Go 进入收件箱页面，也可以在右上角修改邮箱前缀地址，在进入临时收件箱页面。 bccto http://bccto.me 10 分钟邮箱，可以自定义也可以随机申请的 10 分钟临时邮箱，支持自定义域名，可以将自己的域名绑定 bccto 服务。 10minutemail https://10minutemail.net 一个 10 分钟邮箱 10minutemail https://10minutemail.com 10 分钟邮箱，老牌网站 07 年稳定至今。 小部分网站网站封锁了 10minutemail 的域名，可能无法接收，可以当作备用。 666 临时邮箱 https://www.666email.com/ UI 很简洁，提供了 2 个域名选择，可以自定义也可以随机生成。 anyalias https://anyalias.comanyalias 是一个可以保护隐私的邮箱服务，可以生成一个自定义的邮箱然后转发给你的真实邮箱，从而达到保护隐私的效果，更多介绍查看 anyalias.com：无限制的免费电子邮件地址 eskiimo https://eskiimo.com 匿名发送邮件给任何人 在线办公展开目录https://shimo.im 石墨文档 https://www.notion.so notion https://yuque.com 语雀 https://docs.qq.com 腾讯文档 https://tableconvert.com 一个免费的在线表格转换工具 TableConvert","categories":[],"tags":[{"name":"资源","slug":"资源","permalink":"https://imlike.cc/tags/%E8%B5%84%E6%BA%90/"}]},{"title":"Ubuntu开发软件安装","slug":"Ubuntu开发软件安装","date":"2019-08-08T06:07:34.000Z","updated":"2024-03-14T14:55:29.474Z","comments":true,"path":"1625103035.html","link":"","permalink":"https://imlike.cc/1625103035.html","excerpt":"","text":"2020-06 2020-05-10 sudo apt install fonts-wqy-microhei 2020-03-20 Ubuntu开发软件安装 kchmviewer是用与 查看 chm,Windows下是有各种软件Linux下下载过几个软件不是对中文支持不好或者就是容易崩溃的 DbVisualizer Pro 10.0.26 [Build #3028]是用与数据库连接客户端,相比比较dbevaer是各个自定义功能和方法还方便很多,就是显示信息也是很方便,免费版就是不能直接在表里编辑数据其他完全够用,需要激活我用的最新版找了好久链接地址 ZSH这真的是个神器,在终端上编辑和检测输入语法会非常方便,各种插件自定义后完全可以发现全新的世界,同时可以考虑. 酸酸乳ssr,v2ray连接工具之前用的是Qv2ray.v2.1.2.linux-x64.qt5.13.2.AppImage 好用非常方便用过的最好的连接工具了 Linux 版SecureCRT/SecureFX 8.5.41）下载secureCRT和secureFX**官网下载地址：https://www.vandyke.com/download/index.htmlscrt-sfx-8.5.4-1942.ubuntu16-64.x86_64.deb secureCRT和secureFX 前面是ssh连接 后面是FTP的连接 2）进行安装 sudo dpkg -i scrt-sfx-8.5.4-1942.ubuntu16-64.x86_64.deb如果安装过程中因为缺少依赖而安装失败，只需要通过命令：apt-get install -f即可完成依赖的安装 3）secureCRT和secureFX的License破解破解文件下载：https://share.weiyun.com/5Mc38uB 密码：yetp32 1**sudo perl securecrt_forgeek_crack.pl &#x2F;usr&#x2F;bin&#x2F;SecureCRT**启动SecureCRT，根据执行结果填写license信息即可3.1) 运行破解secureFX脚本**sudo perl securefx_forgeek_crack.pl &#x2F;usr&#x2F;bin&#x2F;SecureFX**启动SecureFX，根据执行结果填写license信息即可 参考连接 找了好久我这个算是比较新的 网上的大部分都不行 用的一个是都激活不了secureFX 现在可以","categories":[],"tags":[{"name":"ubuntu","slug":"ubuntu","permalink":"https://imlike.cc/tags/ubuntu/"}]},{"title":"Hexo提交百度和Google收录站点","slug":"Hexo提交百度和Google收录站点","date":"2019-08-06T05:27:50.000Z","updated":"2024-03-14T14:55:29.473Z","comments":true,"path":"3423376587.html","link":"","permalink":"https://imlike.cc/3423376587.html","excerpt":"","text":"使用sitemap 提交网站123456安装插件npm install hexo-generator-baidu-sitemap --savenpm install hexo-generator-sitemap --savel# 自动生成_config.ymsitemapsitemap: path: sitemap.xmlbaidusitemap: path: baidusitemap.xml 1 百度提交1https:&#x2F;&#x2F;ziyuan.baidu.com&#x2F;linksubmit&#x2F;index浏览器打开 your-url&#x2F;baidusitemap.xml 看下有没有文件 2 Google提交12https:&#x2F;&#x2F;search.google.com&#x2F;search-console&#x2F;sitemaps浏览器打开your-url&#x2F;sitemap.xml 看下有没有文件提交站点地图 使用 hexo-baidu-url-submit 自动提交 github地址 1234567891011npm install hexo-baidu-url-submit --savebaidu_url_submit: count: 1000 ## 提交最新的一个链接 host: alili.tech## 在百度站长平台中注册的域名 token: xxxxx ## 请注意这是您的秘钥， 所以请不要把博客源代码发布在公众仓库里! path: baidu_urls.txt ## 文本文档的地址， 新链接会保存在此文本文档里deploy 配置deploy:- type: baidu_url_submitter # 百度在上面的图片点击自动提交 接口调用地址： http:&#x2F;&#x2F;data.zz.baidu.com&#x2F;urls?site&#x3D;XXX &amp;token&#x3D;XXX前面XXX是你的网站 后面XXX是token的值","categories":[],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://imlike.cc/tags/hexo/"}]},{"title":"Linux命令记录","slug":"Linux命令记录","date":"2019-08-05T00:08:58.000Z","updated":"2024-03-14T14:55:29.473Z","comments":true,"path":"2125313135.html","link":"","permalink":"https://imlike.cc/2125313135.html","excerpt":"","text":"最近常用的Linux 命令 查询相关123find .|xargs grep -ri &quot;XXX&quot; -l 查找目录下的所有文件中包含某个字符串 XXX 1 lsof -i:端口号 # lsof -i:8000 COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME nodejs 26993 root 10u IPv4 37999514 0t0 TCP *:8000 (LISTEN) 2 netstat -tunlp | grep 端口号 -t (tcp) 仅显示tcp相关选项 -u (udp)仅显示udp相关选项 -n 拒绝显示别名，能显示数字的全部转化为数字 -l 仅列出在Listen(监听)的服务状态 -p 显示建立相关链接的程序名","categories":[],"tags":[{"name":"ubuntu","slug":"ubuntu","permalink":"https://imlike.cc/tags/ubuntu/"}]},{"title":"搭建云播基于cloud-torrent","slug":"搭建云播基于cloud-torrent","date":"2019-08-04T03:39:42.000Z","updated":"2024-03-14T14:55:29.484Z","comments":true,"path":"1352061511.html","link":"","permalink":"https://imlike.cc/1352061511.html","excerpt":"","text":"基于Aria2 搭建离线云播之前撸了个谷歌云的服务器都没怎么用 现在突然记起来了随便搭建了 刚开始是想搭建基于参考链接 离线技术来自于 Aria2 但是没有安装成功我是 Debian9 没有能安装 脚本里面的是 1安装锐速 serverspeeder-all.sh 而且启动里面的 mysql 还有thinkPHP 所以不上去是不是有什么其他错误 换一种搭建方案基于cloud-torrent后面是想看看还有没有其他的搭建方案 搜了一下果然是有比较完善和封装比较好的服务 参考链接 1wget -N --no-check-certificate https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;ToyoDAdoubiBackup&#x2F;doubi&#x2F;master&#x2F;cloudt.sh &amp;&amp; chmod +x cloudt.sh &amp;&amp; bash cloudt.sh github地址 说一下中间的坑 因为选择想有绑定时有域名和IP的两种选项我刚好有域名就绑了个子域名后面等后面安装后启动就一直报 1Can&#39;t assign requested address 我以为是端口占用 杀了对于进程但是不行 我就卸载了对于软件 重新安装 在绑定端口时选择了8080 但是还是不星 但是8080是没有服务占用的 所以我换成我服务器的IP地址就行了 应该是它需要监听域名的请求但是和本机IP 不能绑定 目前的将我子域名直接解析到对于的云播地址就了 速度不错 最后分享应该安卓端也可用的点我下载","categories":[],"tags":[{"name":"ubuntu","slug":"ubuntu","permalink":"https://imlike.cc/tags/ubuntu/"},{"name":"云播","slug":"云播","permalink":"https://imlike.cc/tags/%E4%BA%91%E6%92%AD/"}]},{"title":"搭建orientDB","slug":"搭建orientDB","date":"2019-08-01T05:28:50.000Z","updated":"2024-03-14T14:55:29.484Z","comments":true,"path":"2702077458.html","link":"","permalink":"https://imlike.cc/2702077458.html","excerpt":"","text":"1234apt-get update apt-get default_jrecd &#x2F;optwget https:&#x2F;&#x2F;s3.us-east-2.amazonaws.com&#x2F;orientdb3&#x2F;releases&#x2F;3.0.22&#x2F;orientdb-3.0.22.tar.gz tar zxvf 3.0.22&#x2F;orientdb-3.0.22.tar.gz cp -r 3.0.22&#x2F;orientdb-3.0.22.tar.gz orientDB cd orientDB cd bin .&#x2F;server.sh","categories":[],"tags":[{"name":"orientDb","slug":"orientDb","permalink":"https://imlike.cc/tags/orientDb/"}]},{"title":"折腾几个小时的Ubuntu启动出错","slug":"折腾几个小时的Ubuntu启动出错","date":"2019-07-29T05:07:31.000Z","updated":"2024-03-14T14:55:29.484Z","comments":true,"path":"3716506550.html","link":"","permalink":"https://imlike.cc/3716506550.html","excerpt":"","text":"Ubuntu出错今天莫名其妙的Ubuntu 出错 ,刚开始显示错误如下 刚开始显示 1“Couldn’t get size: 0x800000000000000e”加载一段时间后显示 failed start kernel 吓到我了以为关机前是有过升级的 不知道有没有升级内核补丁 就被这个误导了查了好久 引导恢复这个弄完后就是 使用 boot-repair 但是按照网上的添加源后 使用apt-get安装完全不行一直提示什么Pakage万能安装成功提示 apt -get update 莫名其妙 准备直接找 boot-repair.deb 包千辛万苦找到后不能安装 真的是醉了 后面也是人肉在网上找的关联包 gawk_4.1.3+dfsg-0.1_amd64.deb 先安装这个后面再安装boot-repair_4ppa65_all.deb 对了我的是用U盘进的Ubuntu16 所以下载链接 最终解决其实不是上述问题 在看到有 emergency mode 后我尝试在网搜了一下 原来是我fsb 文件的问题 仔细看打印的日志也正好是这个意思 我的某个分区没有成功挂载 ctrl+d 后直接指明的对应的分区 我是注释掉了 /etc/fstab 文件里的出错的分区 不知道是什么时候添加的 好坑 参考博客","categories":[],"tags":[{"name":"ubuntu","slug":"ubuntu","permalink":"https://imlike.cc/tags/ubuntu/"}]},{"title":"hexo优化","slug":"hexo优化","date":"2019-07-25T04:09:35.000Z","updated":"2024-03-14T14:55:29.477Z","comments":true,"path":"878762119.html","link":"","permalink":"https://imlike.cc/878762119.html","excerpt":"","text":"hexo 的 themes文件夹下存在git等文件导致URI malformed参考链接 遇到过生成页面后出现一堆代码，或者是 CSS 错误，或者类似 URIError: URI malformed 这样的错误，查了半天，发现是vim 修改了 ejs 文件后，自动生成了带 ~ 后缀的备份文件，这将导致 Hexo 生成的时候一起包含进去出错。删掉所有的备份文件即可。 12345678910111213URIError: URI malformed at decodeURIComponent (&lt;anonymous&gt;) at &#x2F;home&#x2F;leek&#x2F;bei&#x2F;make-blog&#x2F;node_modules&#x2F;hexo-server&#x2F;lib&#x2F;middlewares&#x2F;route.js:17:28 at call (&#x2F;home&#x2F;leek&#x2F;bei&#x2F;make-blog&#x2F;node_modules&#x2F;connect&#x2F;index.js:239:7) at next (&#x2F;home&#x2F;leek&#x2F;bei&#x2F;make-blog&#x2F;node_modules&#x2F;connect&#x2F;index.js:183:5) at &#x2F;home&#x2F;leek&#x2F;bei&#x2F;make-blog&#x2F;node_modules&#x2F;hexo-server&#x2F;lib&#x2F;middlewares&#x2F;header.js:9:5 at call (&#x2F;home&#x2F;leek&#x2F;bei&#x2F;make-blog&#x2F;node_modules&#x2F;connect&#x2F;index.js:239:7) at next (&#x2F;home&#x2F;leek&#x2F;bei&#x2F;make-blog&#x2F;node_modules&#x2F;connect&#x2F;index.js:183:5) at Function.handle (&#x2F;home&#x2F;leek&#x2F;bei&#x2F;make-blog&#x2F;node_modules&#x2F;connect&#x2F;index.js:186:3) at Server.app (&#x2F;home&#x2F;leek&#x2F;bei&#x2F;make-blog&#x2F;node_modules&#x2F;connect&#x2F;index.js:51:37) at Server.emit (events.js:310:20) at parserOnIncoming (_http_server.js:784:12) at HTTPParser.parserOnHeadersComplete (_http_common.js:119:17) 需要在hexo的配置文件添加 123skip_render:- daohang&#x2F;*- .git&#x2F;* 如果还存在问题可以考虑 对于markdown文件中备注代码块格式加了ejs的备注 tags 多标签 multi category and multi tag -2020年3月31日1&#x2F;&#x2F;文章标签，可空，多标签请用格式[tag1,tag2,tag3]，如下，注意!!&quot;tags:&quot;后面有个空格tags: [tag1, tag2, tag3, ...]&#x2F;&#x2F; 下面这种方式似乎仅限于hexo 3.0tags:- npm - vue 注意tags: 后面不要有空格,-与标签名之间应该有空格 How about supporting multi category and multi tag 设置date的格式 变量 描述 :year 文章的发表年份（4 位数） :month 文章的发表月份（2 位数） :i_month 文章的发表月份（去掉开头的零） :day 文章的发表日期 (2 位数) :i_day 文章的发表日期（去掉开头的零） :title 文件名称 :post_title 文章标题 :id 文章 ID :category 分类。如果文章没有分类，则是 default_category 配置信息 github地址 可以参考一下代码风格 为网页设置统一风格的URLgithub地址 1npm install hexo-abbrlink --save 安装 该插件 后将_config.yml permalink: :abbrlink.html # 生成唯一链接 永久链接（Permalinks）abbrlink: alg: crc32 # 算法：crc16(default) and crc32 rep: dec # 进制：dec(default) and hex permalink_defaults: :year&#x2F;:month&#x2F;:day&#x2F;:title&#x2F; 默认是是这样拼接URL 参考链接","categories":[],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://imlike.cc/tags/hexo/"}]},{"title":"检查Ubuntu开机的相关操作","slug":"检查Ubuntu开机的相关操作","date":"2019-07-25T03:34:00.000Z","updated":"2024-03-14T14:55:29.485Z","comments":true,"path":"2802364883.html","link":"","permalink":"https://imlike.cc/2802364883.html","excerpt":"","text":"开机启动软件都会放在这个目录下/etc/init.d/ systemd-analyze blame 可以分析开机占用时间比较多点 进程 sudo systemd-analyze plot &gt; boot.svg 可以想数据导出成 svg 文件 或者直接查看 开机日志 /var/log/boot.log","categories":[],"tags":[{"name":"ubuntu","slug":"ubuntu","permalink":"https://imlike.cc/tags/ubuntu/"}]},{"title":"搜狗输入法linux版  两个图标在托盘 tray","slug":"搜狗输入法linux版  两个图标在托盘 tray","date":"2019-07-23T13:03:31.000Z","updated":"2024-03-14T14:55:29.484Z","comments":true,"path":"7f98c9f2.html","link":"","permalink":"https://imlike.cc/7f98c9f2.html","excerpt":"","text":"托盘水平 起初是因为 fcitx和搜狗冲突了 11 .)ps -rf |grep fcitx kill -9 对应进程 2.) killall fcitx 开机后就还行 然后准备加到开机启动脚本里面 查了一下 可以是 /etc/ini.d 或者local.c 下 然后着手开始干,但是注销用户登录后还是不行,当时就蒙了主要是后面调用不出来搜狗了 有应该是fcitx 上面的汉语输入法 而且很丑,然后就接着查找资料 最后发现有人的教程里面提到fcitx的配置文件(fcitx-config-gtk3) 可以设置取消界面,就压根不知道为什么要这么配置参考链接","categories":[],"tags":[{"name":"ubuntu","slug":"ubuntu","permalink":"https://imlike.cc/tags/ubuntu/"},{"name":"搜狗输入法","slug":"搜狗输入法","permalink":"https://imlike.cc/tags/%E6%90%9C%E7%8B%97%E8%BE%93%E5%85%A5%E6%B3%95/"}]},{"title":"解决deepin-wine 下微信黑窗口问题","slug":"解决deepin-wine 下微信黑窗口问题","date":"2019-07-23T04:53:40.000Z","updated":"2024-03-14T14:55:29.486Z","comments":true,"path":"835633961.html","link":"","permalink":"https://imlike.cc/835633961.html","excerpt":"","text":"之前一直是有deepin-wine 下微信黑窗口问题 一直拖着没解决 比较不知道怎么去解决当时是有写过一个临时优化的点击链接 参考的的链接当实现的效果是可以但是有反复 后面就没怎么弄 一直是在输入666 这可以可以消除 但是每次弹出来好烦 看到留言区的 123vim /opt/deepinwine/apps/Deepin-WeChat/run.sh向下面追加wmctrl -l -G -p -x | grep wechat.exe.Wine | grep -v 微信| awk &#x27;&#123;print $1&#125;&#x27; | xargs -r -i xdotool windowunmap &#123;&#125; 我修改后是可以跑通的 方法二参考链接 注意要 121. sudo chmod +x /opt/deepinwine/apps/Deepin-WeChat/runrun.sh 给执行权限2. 根据deepin-wine 的配置和路径来 我这边是没成功的不知道为什么","categories":[],"tags":[{"name":"ubuntu","slug":"ubuntu","permalink":"https://imlike.cc/tags/ubuntu/"},{"name":"deepin","slug":"deepin","permalink":"https://imlike.cc/tags/deepin/"}]},{"title":"docker 简单使用","slug":"docker 简单使用","date":"2019-07-17T04:15:32.000Z","updated":"2024-03-14T14:55:29.476Z","comments":true,"path":"4262021452.html","link":"","permalink":"https://imlike.cc/4262021452.html","excerpt":"","text":"离线安装docker 依赖检测失败 centos7rpm -ivp *.rpm 现象但一直报 依赖检测失败以为是系统版本不对用命令查询 cat /etc/redhat-release 但是都是centos7 小版本有区别,这边之前都是在本地虚拟机上安装没出现这样的问题,以为是真的依赖不上,但是我使用的命令为 yum download –downloadonly=/root/download docker 这个命令应该是会把所有依赖的软件都下载下来的 解决方案rpm -ivh –nodeps –force *.rpm 强制执行 完全卸载docker如果已安装不想要docker、docker-engine、docker-ce相关的软件包，则卸载掉：sudo yum -y remove docker docker-common docker-selinux docker-engine docker-engine-selinux container-selinux docker-ce #删除所有的镜像、容器、数据卷、配置文件等sudo rm -rf /var/lib/docker 如果已安装不想要docker、docker-engine、docker-ce相关的软件包则卸载掉：sudo yum -y remove docker docker-common docker-selinux docker-engine docker-engine-selinux container-selinux docker-ce #删除所有的镜像、容器、数据卷、配置文件等sudo rm -rf /var/lib/docker rpm -qa|grep docker yum list installed|grep docker","categories":[],"tags":[{"name":"docker","slug":"docker","permalink":"https://imlike.cc/tags/docker/"}]},{"title":"PHP学习笔记","slug":"PHP学习笔记","date":"2019-07-08T05:45:12.000Z","updated":"2024-03-14T14:55:29.474Z","comments":true,"path":"1439793898.html","link":"","permalink":"https://imlike.cc/1439793898.html","excerpt":"","text":"PHP学习笔记1PHP（全称：PHP：Hypertext Preprocessor，即“PHP：超文本预处理器”）是一种开源的通用计算机脚本语言，尤其适用于网络开发并可嵌入HTML中使用。PHP的语法借鉴吸收C语言、Java和Perl等流行计算机语言的特点，易于一般程序员学习。PHP的主要目标是允许网络开发人员快速编写动态页面，但PHP也被用于其他很多领域。[1]PHP最初是由勒多夫在1995年开始开发的；现在PHP的标准由the PHP Group[2]维护。PHP以PHP License作为许可协议，不过因为这个协议限制了PHP名称的使用，所以和开放源代码许可协议GPL不兼容。[3]PHP的应用范围相当广泛，尤其是在网页程序的开发上。一般来说PHP大多运行在网页服务器上，透过运行PHP代码来产生用户浏览的网页。PHP可以在多数的服务器和操作系统上运行，而且使用PHP完全是免费的。根据2013年4月的统计数据，PHP已经被安装在超过2亿4400万个网站和210万台服务器上[4]。PHP在Windows上有专门的官方移植编译项目，并且分多个VC编译器版本和线程安全特性来提供不同的版本支持。----维基百科 引用链接 PHP常见错误，我认为主要分为3块： 1、语法错误1Parse error: syntax error, unexpected T_VARIABLE in E:\\gstbook\\user.php on line 268 一般是少分号，在上一行或者下一行都有可能Parse error: syntax error, unexpected &#39;.&#39; in F:\\www\\blog\\article.php on line 100 多个“.”，类似的还有多个“;”等等 Notice: Undefined variable: a in ...使用了一个没有被定义的变量a 1Parse error: syntax error, unexpected T_ELSE in if else if else 句式逻辑错误 Parse error: syntax error, unexpected $end in一般是大括号不匹配 Parse error: syntax error, unexpected T_STRING in一般是忘了写句末分号 Parse error: syntax error, unexpected T_PRIVATE in一般是private函数位置写到了调用自己函数的内部 Warning: Invalid argument supplied for foreach() inforeach 的第一个参数必须是一个数组 Warning: Wrong parameter count for a in函数参数个数不对 1Warning: session_start() [function.session-start]: Cannot send session cache limiter - headers already sent (output started at F:\\www\\blog\\index.php:10) in F:\\www\\blog\\incl?s\\config.inc.php on line 10Warning: annot modify header information - headers already sent by (output started at F:\\www\\blog\\index.php:10) in F:\\www\\blog\\incl?s\\config.inc.php on line 12 session前面不能有输出，同意的cookie也是这样的 Fatal error: Cannot redeclare函数已经定义过，不能重新再定义；类也不能同名 2、SQL语句错误Warning: mysql_fetch_assoc(): supplied argument is not a valid MySQL result resource in说明前面执行SQL 返回失败，一般可能是SQL语句错误 ，如分析不出来，可以Mysql_error () 看具体是哪个地方错了 No database selected一般是没有连接数据库 Warning: mysql_connect() [function.mysql-connect]: Access denied for user ‘root’@&#39;localhost’ (using password: YES) in ….. on line 3Access denied for user ‘root’@&#39;localhost’ (using password: YES)数据库用户名的密码错误 Unknown database ‘cms07261′数据库不存在 Table ‘cms0726.test’ doesn’t exist表不存在 Unknown column ‘abc’ in ‘field list’字段不存在 Column count doesn’t match val count at row 1sql语句中 字段值的个数比字段名个数不一样 3、模板错误Fatal error: Smarty error: [in message_list.html line 22]: syntax error: ‘foreach: item’ must be a variable name (literal string)foreach 后面的item 后面必须跟一个变量名，不加$，是一个字符串 Fatal error: Smarty error: [in admin/tpl/class_detail.html line 14]: syntax error: unrecognized tag: cs.st?nt_id (Smarty_Compiler.class.php, line 436) in D:\\www\\web\\xmphp\\inc\\smarty\\Smarty.class.php on line 1088smarty语法错误：循环数组的时候，少$ Warning: Smarty error: unable to read resource: “message_list1.html” in模板文件找不到，看是否是文件名写错或文件存放到别的目录中去了。 Fatal error: Smarty error: [in order_detail.html line 171]: syntax error: unclosed tag \\&#123;elseif&#125; (opened line 157). (Smarty_Compiler.class.php, line 317) in F:\\www\\gstbook\\xmphp\\inc\\smarty\\Smarty.class.php on line 1088html页面语法错误，少结束标签&lt;%/if%&gt;以后遇到问题，我会及时更新！ 附上PHP错误报表: 序号代码错误报告 1 error_reporting(Integer) 此设置是每个脚本默认错误报告设置。它的参数可以是这里列出的任意常量，表示所有错误的E_ALL或者类似E_ALL&amp;E_NOTICE的一个逻辑表达式。 2 display_error(Boolean) 此设置控制错误是否作为PHP输出的一部分显示出来，默认设置是on。 3 display_startup_errors(Boolean) 此设置控制在PHP启动时是否显示错误，默认设置是off，而且是用来调试C扩展。 4 error_prepend_string(String) 这个字符串在浏览器中显示错误信息之前直接显示出来。 5 track_errors(Boolean) 当这个设置开启时，一旦一个错误出现，变量$php_errormsg在PHP作用域中被定义。这个变量包含错误信息。 6 html_errors(Boolean) 此设置控制是否在错误信息中采用HTML格式。默认情况下显示的是HTML错误，CLI版本的PHP除外。 7 xmlrpc_erroes(Boolean) 此设置控制错误信息是否启用作为XML－RPC故障显示。 8 xmlrpc_error_number(Integer) 这个XML-RPC故障代码是启用xmlrpc_errors时使用的。 9 log_errors(Boolean) 此设置控制错误是否记录。日志的地址是通过error_log设置决定的。默认情况下，错误被记录到Web服务器的错误日志中。 10 log_error_max_len(Integer) 此设置是当启用log_errors时控制记录的错误信息的最大长度。超过这个长度的信息也会被记录下来，但是会被截断。 11 error_log(String) 此设置决定错误记录的地方。在默认情况下，它们被传送到Web服务器的错误记录机制中，但必须指定一个文件名，或者指定Syslog让系统日志来记录。 12 ignore_report_errors(Boolean) 当启用它时，这个设置让PHP不在同一行中重复显示一样的错误信息。 13 ignore_repeated_source(Boolean) 当启用时，PHP将不显示与前一个显示的错误来自同一个文件同一行错误。如果它没有被启用，它就不起作用。 2 、PHP错误是通过错误级别进行划分的，范围从通告到严重错误，这个错误级别报告错误的严重性。大部分错误可以用常用的错误处理器来获取，但其中一些则不可恢复。 PHP错误级别表 序号 错误代码 错误级别 1 E_ERROR 严重错误，不可恢复。例如内存不足、位置异常或类重复声明错误 2 E_WARNING 一个经常发生的错误类型。表示一些操作错误。例如丢失函数参数，连接数据库失败，除数为0等错误。 3 E_PARSE 编译时发生的解析错误，会导致在PHP执行前退出。意味着如果一个文件因为解析错误运行失败，程序将停止运行。 4 E_STRICT 错误级别是唯一不包含在E_ALL常量中。这是为了让PHP4到PHP5的迁移更加容易；你可以在PHP5软件系统中运行PHP4代码。 5 E_NOTICE 运行代码在操作一些未知的动作，例如读取为定义的变量。 6 E_CORE_ERROR 内部错误，扩展启动失败导致的，会导致PHP运行退出。 7 E_COMPILE_ERROR 编译错误，与E_PARSE不同，这个错误会导致PHP运行退出。 8 E_WARNING_ERROR 编译时的警告，可能是使用者发生语法错误。 9 E_USER_ERROR 用户定义错误导致PHP退出执行。用户定义错误(E_USER_*)不是来自PHP本身，而是包含在脚本中。 10 E_USER_WARNING 用户定义错误不会导致PHP退出执行。脚本可以使用它来通知一个执行失败，执行失败PHP也会用E_WARNING来通知。 11 E_USER_NOTICE 用户定义的通告可以用来在脚本中可能存在错误。","categories":[],"tags":[{"name":"php","slug":"php","permalink":"https://imlike.cc/tags/php/"}]},{"title":"我目前的Ubuntu软件","slug":"我目前的Ubuntu软件","date":"2019-07-05T19:51:23.000Z","updated":"2024-03-14T14:55:29.484Z","comments":true,"path":"78752914.html","link":"","permalink":"https://imlike.cc/78752914.html","excerpt":"","text":"整理一下我用了这么久后觉得还可以使用的软件 目前这台电脑没有用与实际开发但是开发工具还是都有的而且还有deb包的,方便;) 浏览器: chrome firefox (常见插件包括 tempermonkey ,ublock , stylus ,SwitchyOmega, https everywhere ) 开发工具 : IDEA, VScode, VMware ZSH, sublime_text ,DBeaver ,SSH(securet )+ftp(secureFX) 截屏工具 livepath(录屏软件),flamesflash(截屏),peek(录GIF) 办公工具 : Xmind WPS typore,foxit(PDF) 同步工具 onenote 坚果云 影音播放: spotify(推荐 版权多) 网易云音乐, VLC(要自己找主题和配置快捷键不然真的是反人类) 下载工具 uget(已经不行了 arai) ,motrix ,Xtreme download, 其他: ciliber(电子书编辑和阅读) xchm(chm阅读) ,搜狗 deepin-wine(安装 微信 迅雷),SSR(electrol ssr),安卓模拟器(Genymotion 选择个人用户)","categories":[],"tags":[{"name":"ubuntu","slug":"ubuntu","permalink":"https://imlike.cc/tags/ubuntu/"}]},{"title":"ubuntu优化－onedrive","slug":"ubuntu优化－onedrive","date":"2019-06-26T15:17:14.000Z","updated":"2024-03-14T14:55:29.482Z","comments":true,"path":"1097254589.html","link":"","permalink":"https://imlike.cc/1097254589.html","excerpt":"","text":"之前是准备用onedrive-dev 但是涉及到太多的以来还有步骤很复杂 有看到其他 123456Easy to configure:$ cat ~&#x2F;.config&#x2F;onedrive&#x2F;configsync_dir &#x3D; &quot;~&#x2F;OneDrive&quot;skip_file &#x3D; &quot;.*|~*|thumbs.db|Games&#x2F;*.iso&quot;skip_dir &#x3D; &quot;.*|Music|Movies&#x2F;FullHD&quot; 12345678910So simple, You already know how to use it:$ onedrive -h Usage: onedrive [OPTION]... no option Sync and exit.-m --monitor Keep monitoring for local and remote changes. --resync Forget the last saved state, perform a full sync. -v --verbose Print more details, useful for debugging. -h --help This help information. 配置也很简单 onedrive –download 获取授权链接 登录 onedrive的账号就行了 将返回的URL贴到命令行后 火车如果长时间没相应可以直接ctrc+c 后重新运行 记录一下chmod的用法 12345678910111213141516171819202122232425262728293031323334353637查看linux文件的权限：ls -l 文件名称查看linux文件夹的权限：ls -ld 文件夹名称（所在目录）修改文件及文件夹权限：sudo chmod -（代表类型）×××（所有者）×××（组用户）×××（其他用户）常用修改权限的命令：sudo chmod 600 ××× （只有所有者有读和写的权限）sudo chmod 644 ××× （所有者有读和写的权限，组用户只有读的权限）sudo chmod 700 ××× （只有所有者有读和写以及执行的权限）sudo chmod 666 ××× （每个人都有读和写的权限）sudo chmod 777 ××× （每个人都有读和写以及执行的权限）查看linux文件的权限：ls -l 文件名称查看linux文件夹的权限：ls -ld 文件夹名称（所在目录）修改文件及文件夹权限：sudo chmod -（代表类型）×××（所有者）×××（组用户）×××（其他用户）常用修改权限的命令：sudo chmod 600 ××× （只有所有者有读和写的权限）sudo chmod 644 ××× （所有者有读和写的权限，组用户只有读的权限）sudo chmod 700 ××× （只有所有者有读和写以及执行的权限）sudo chmod 666 ××× （每个人都有读和写的权限）sudo chmod 777 ××× （每个人都有读和写以及执行的权限） aria 下载123aria2c -c -s10 -k1M -x16 --enable-rpc&#x3D;false -o &#39;要保存的文件名&#39; --header &quot;User-Agent: Mozilla&#x2F;5.0 (X11; Linux x86_64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;61.0.3163.100 Safari&#x2F;537.36&quot; --header &quot;Cookie: 你的cookie内容&quot; &quot;文件的下载地址&quot;使用cookie 来实现百度云下载,想一些网站会识别cookie判断是否能下载","categories":[],"tags":[]},{"title":"配置on-my-zsh","slug":"配置on-my-zsh","date":"2019-06-17T03:43:25.000Z","updated":"2024-03-14T14:55:29.487Z","comments":true,"path":"37058318.html","link":"","permalink":"https://imlike.cc/37058318.html","excerpt":"","text":"补充 添加 找到 ~/.zshrc 有一行 plugins=(git)，想加什么插件就把名字放里面就是了， 1.) autojump 按照官方文档介绍，需要使用如下命令安装，而不是一些博客中的介绍 ~/.oh-my-zsh/plugins/autojump/autojump.plugin.zsh 安装好之后，记得source ~/.zshrc，然后你就可以通过j+目录名快速进行目录跳转。支持目录名的模糊匹配和自动补全。 j -stat：可以查看历史路径库 2.) extract 解压文件用的，所有的压缩文件，都可以直接x filename，不用记忆参数 当然，如果你想要用tar命令，可以使用tar -加tab键，zsh会列出参数的含义。 使用zsh 提交命令行输入的输入效率 1# 安装 Zshsudo apt install zsh# 将 Zsh 设置为默认 Shellchsh -s &#x2F;bin&#x2F;zsh# 可以通过 echo $SHELL 查看当前默认的 Shell，如果没有改为 &#x2F;bin&#x2F;zsh，那么需要重启 Shell。 安装Git1sudo apt install git 9. zsh(也可以安装fish)1sudo apt install zsh# 切换到zshchsh -s &#x2F;bin&#x2F;zsh 安装Oh-my-zsh 1sh -c &quot;$(wget https:&#x2F;&#x2F;raw.github.com&#x2F;robbyrussell&#x2F;oh-my-zsh&#x2F;master&#x2F;tools&#x2F;install.sh -O -)&quot; 安装插件highlight，高亮语法 1cd ~&#x2F;.oh-my-zsh&#x2F;custom&#x2F;pluginsgit clone git:&#x2F;&#x2F;github.com&#x2F;zsh-users&#x2F;zsh-syntax-highlighting.git 在Oh-my-zsh的配置文件中~/.zshrc中添加插件 1plugins&#x3D;( [plugins...] zsh-syntax-highlighting) 最后使配置生效 1source ~&#x2F;.zshrc ~/.oh-my-zsh 配置文件 ~/.zshrc plugins=(git autojump zsh-syntax-highlighting​ ) # 光标移动 快捷键 描述 Ctrl + a 移动光标到行首 Ctrl + e 移动光标到行尾 Alt + b 移动光标后退一个单词（词首） Alt + f 移动光标前进一个单词（词首） Ctrl + f 光标前进一个字母 Ctrl + b 光标后退一个字母 Ctrl + xx 当前位置与行首之间光标切换 剪切粘贴 快捷键 描述 Ctrl + k 删除从光标到行尾 Ctrl + u 删除从光标到行首 Ctrl + w 从光标向前删除一个单词 Alt + d 从光标向后删除一个单词 Ctrl + d 删除光标下一个字母 Ctrl + h 删除光标前一个字母 Alt + t swap(当前单词, 上一个单词) Ctrl + t swap(当前字母, 上一个字母) Ctrl + y 粘贴上一次删除的文本 历史命令 快捷键 描述 Ctrl + r 向后搜索历史命令 Ctrl + g 退出搜索 Ctrl + p 历史中上一个命令 Ctrl + n 历史中下一个命令 Alt + . 上一个命令的最后一个单词 终端指令 快捷键 描述 Ctrl + l 清屏 Ctrl + s 停止输出（在Zsh中为向前搜索历史命令） Ctrl + q 继续输出 Ctrl + c 终止当前命令 Ctrl + z 挂起当前命令 Ctrl + d 结束输入（产生一个EOF）","categories":[],"tags":[{"name":"ubuntu","slug":"ubuntu","permalink":"https://imlike.cc/tags/ubuntu/"},{"name":"zsh","slug":"zsh","permalink":"https://imlike.cc/tags/zsh/"}]},{"title":"Ubuntu下 intellj idea maven下载","slug":"Ubuntu下 intellj idea maven下载","date":"2019-06-08T00:53:16.000Z","updated":"2024-03-14T14:55:29.474Z","comments":true,"path":"3350959125.html","link":"","permalink":"https://imlike.cc/3350959125.html","excerpt":"","text":"bindsym XF86AudioNext exec playerctl next xmodmap -pke | grep XF86Audio 123sudo apt-get install mdbus2dbus-send --print-reply --dest=org.mpris.MediaPlayer2.spotify /org/mpris/MediaPlayer2 org.mpris.MediaPlayer2.Player.PlayPause 转载来自 –解决Ubuntu和idea快捷键冲突 1、Gnome 快捷键修改Ctrl + Alt + ←Ctrl + Alt + → 12gsettings set org.gnome.desktop.wm.keybindings switch-to-workspace-right &quot;[]&quot;gsettings set org.gnome.desktop.wm.keybindings switch-to-workspace-left &quot;[]&quot; 2、Fcitx 输入法快捷键修改Ctrl + Alt + BCtrl + Alt + S点击右上角语言栏，点配置 &gt; 全局配置 &gt; 显示高级选项，点选冲突按键，按 Esc 即可。 opt/apache-maven-3.6.0/bin/mvn dependency:resolve -Dclassifier=sources 直接使用本机自己的maven 该问题应该是idea 调用 maven出来问题 1iptables -A INPUT -p tcp --dport 3306 -j ACCEPT","categories":[],"tags":[{"name":"ubuntu","slug":"ubuntu","permalink":"https://imlike.cc/tags/ubuntu/"},{"name":"intellj","slug":"intellj","permalink":"https://imlike.cc/tags/intellj/"}]},{"title":"hexo常见命令","slug":"hexo常见命令","date":"2019-04-12T20:37:35.000Z","updated":"2024-03-14T14:55:29.477Z","comments":true,"path":"3173742102.html","link":"","permalink":"https://imlike.cc/3173742102.html","excerpt":"","text":"npm install hexo-generator-feed –save 生成 RSS组件 npm install hexo-wordcount –save 统计字数 npm install hexo-deployer-git –save git 出现 error deployer not found:git 的错误处理 留言部分的多说和畅眼都有自己的问题不是很好用 hexo中的这个valine 第三方留言还是比较稳定比gtiment也要好 npm install leancloud-storage –save # Install leancloud’s js-sdk npm install valine –save hexot添加valine留言系统,之前的留言系统包括多说,畅言都有一部分问题很不好用,valine算比较好用而且好操作的的 hexot添加valine留言 hexo clean &amp;&amp; hexo d -g` 部署和清理缓存 hexo new *** “标题名”* 是保存地址 会自动生成一个markdown文件 开头是 12345---title: hexo常见命令date: 2019-04-13 12:37:35tags: hexo--- 123YAMLException: can not read a block mapping entry; a multiline key may not be an implicit key at 出现上述问题是YAML格式:后面是必须要有空格的title: Hexohexo new page *** 可以生成对应的页面 如about pags","categories":[],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://imlike.cc/tags/hexo/"}]},{"title":"Ubuntu优化--smb局域网分享","slug":"Ubuntu优化--smb局域网分享","date":"2019-03-27T00:53:16.000Z","updated":"2024-03-14T14:55:29.474Z","comments":true,"path":"2693474031.html","link":"","permalink":"https://imlike.cc/2693474031.html","excerpt":"","text":"samba实现文件共享 Linux系统与Windows系统之间共享文件资源，小文件可以使用lrzsz，大文件可以使用samba。 Ubuntu是有自带的文件分享的 分享后 我是在vmvare上装了Windows的虚拟机经常需要两个局域网上共享文件 用Linux的分享手机也可以使用samba 协议的APP访问局域网 内的文件，还可以正常流媒体在线播放还是比较方便的的 netstat -an | grep 3306 tcp 0 0 0.0.0.0:3306 0.0.0.0:* LISTENtcp 0 0 127.0.0.1:3306 127.0.0.1:52102 ESTABLISHEDtcp 0 0 127.0.0.1:3306 127.0.0.1:52104 ESTABLISHEDtcp6 0 0 127.0.0.1:52104 127.0.0.1:3306 ESTABLISHEDtcp6 0 0 127.0.0.1:52102 127.0.0.1:3306 ESTABLISHED bind-address = 127.0.0.1 原本的是这样的 bind-addres = 0.0.0.0 后面有人说需要 其实主要上面的这个的就行了 /etc/mysql/mysql.conf.d/mysqld.cnf 1iptables -A INPUT -p tcp --dport 3306 -j ACCEPT","categories":[],"tags":[{"name":"ubuntu","slug":"ubuntu","permalink":"https://imlike.cc/tags/ubuntu/"}]},{"title":"Ubuntu折腾--优化wine 微信小黑框处理","slug":"Ubuntu折腾--优化wine 微信小黑框处理","date":"2019-03-27T00:53:16.000Z","updated":"2024-03-14T14:55:29.475Z","comments":true,"path":"3419088444.html","link":"","permalink":"https://imlike.cc/3419088444.html","excerpt":"","text":"解决gnome-dash-duck部分软件不能添加入收藏命令行输入启动gnome gsettings get org.gnome.shell favorite-apps 后可以在dash栏直接添加当前运行的问 wine 微信小黑框处理 chatcontactmenuwmctrl -l -G -p -x 显示当前窗口 xdotool windowunmap 0x0340000a 异常对应的窗口 0x01a0000a 0 2415 0 0 1920 1080 desktop_window.Nautilus lee-XPS 桌面0x02400084 0 2992 2734 -12 595 434 gedit.Gedit lee-XPS *note (~/桌面) - gedit0x0340000a 0 3200 0 56 1 1 wechat.exe.Wine lee-XPS ChatContactMenu0x0340000c 0 3200 1860 1020 60 60 wechat.exe.Wine lee-XPS0x03400009 0 3200 1220 56 1231 1052 wechat.exe.Wine lee-XPS 微信 flameshot 的截屏软件/usr/bin/flameshot guignome-screenshot -ac 更新 Linux压缩包乱码 压缩文件乱码的解决方案 指定编码解压 1unzip -O GBK zip 或者unzip -O cp936 本质问题还是zip格式的缺陷，没有字段标志出文件名的编码格式。ZIP在压缩与解压缩的时候默认使用了系统的本地编码，如windows中文环境下为GBK， 日文 环境下是JIS，linux默认编码为UTF8等；那么在不同系统环境下，只要压缩与解压缩的编码不一致，就会出现乱码。 构建desktop文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225[Desktop Entry]Version&#x3D;1.0Name&#x3D;Firefox Web BrowserName[ar]&#x3D;متصفح الويب فَيَرفُكْسName[ast]&#x3D;Restolador web FirefoxName[bn]&#x3D;ফায়ারফক্স ওয়েব ব্রাউজারName[ca]&#x3D;Navegador web FirefoxName[cs]&#x3D;Firefox Webový prohlížečName[da]&#x3D;Firefox - internetbrowserName[el]&#x3D;Περιηγητής FirefoxName[es]&#x3D;Navegador web FirefoxName[et]&#x3D;Firefoxi veebibrauserName[fa]&#x3D;مرورگر اینترنتی FirefoxName[fi]&#x3D;Firefox-selainName[fr]&#x3D;Navigateur Web FirefoxName[gl]&#x3D;Navegador web FirefoxName[he]&#x3D;דפדפן האינטרנט FirefoxName[hr]&#x3D;Firefox web preglednikName[hu]&#x3D;Firefox webböngészőName[it]&#x3D;Firefox Browser WebName[ja]&#x3D;Firefox ウェブ・ブラウザName[ko]&#x3D;Firefox 웹 브라우저Name[ku]&#x3D;Geroka torê FirefoxName[lt]&#x3D;Firefox interneto naršyklėName[nb]&#x3D;Firefox NettleserName[nl]&#x3D;Firefox webbrowserName[nn]&#x3D;Firefox NettlesarName[no]&#x3D;Firefox NettleserName[pl]&#x3D;Przeglądarka WWW FirefoxName[pt]&#x3D;Firefox Navegador WebName[pt_BR]&#x3D;Navegador Web FirefoxName[ro]&#x3D;Firefox – Navigator InternetName[ru]&#x3D;Веб-браузер FirefoxName[sk]&#x3D;Firefox - internetový prehliadačName[sl]&#x3D;Firefox spletni brskalnikName[sv]&#x3D;Firefox webbläsareName[tr]&#x3D;Firefox Web TarayıcısıName[ug]&#x3D;Firefox توركۆرگۈName[uk]&#x3D;Веб-браузер FirefoxName[vi]&#x3D;Trình duyệt web FirefoxName[zh_CN]&#x3D;Firefox 网络浏览器Name[zh_TW]&#x3D;Firefox 網路瀏覽器Comment&#x3D;Browse the World Wide WebComment[ar]&#x3D;تصفح الشبكة العنكبوتية العالميةComment[ast]&#x3D;Restola pela RedeComment[bn]&#x3D;ইন্টারনেট ব্রাউজ করুনComment[ca]&#x3D;Navegueu per la webComment[cs]&#x3D;Prohlížení stránek World Wide WebuComment[da]&#x3D;Surf på internettetComment[de]&#x3D;Im Internet surfenComment[el]&#x3D;Μπορείτε να περιηγηθείτε στο διαδίκτυο (Web)Comment[es]&#x3D;Navegue por la webComment[et]&#x3D;Lehitse veebiComment[fa]&#x3D;صفحات شبکه جهانی اینترنت را مرور نماییدComment[fi]&#x3D;Selaa Internetin WWW-sivujaComment[fr]&#x3D;Naviguer sur le WebComment[gl]&#x3D;Navegar pola redeComment[he]&#x3D;גלישה ברחבי האינטרנטComment[hr]&#x3D;Pretražite webComment[hu]&#x3D;A világháló böngészéseComment[it]&#x3D;Esplora il webComment[ja]&#x3D;ウェブを閲覧しますComment[ko]&#x3D;웹을 돌아 다닙니다Comment[ku]&#x3D;Li torê bigereComment[lt]&#x3D;Naršykite interneteComment[nb]&#x3D;Surf på nettetComment[nl]&#x3D;Verken het internetComment[nn]&#x3D;Surf på nettetComment[no]&#x3D;Surf på nettetComment[pl]&#x3D;Przeglądanie stron WWWComment[pt]&#x3D;Navegue na InternetComment[pt_BR]&#x3D;Navegue na InternetComment[ro]&#x3D;Navigați pe InternetComment[ru]&#x3D;Доступ в ИнтернетComment[sk]&#x3D;Prehliadanie internetuComment[sl]&#x3D;Brskajte po spletuComment[sv]&#x3D;Surfa på webbenComment[tr]&#x3D;İnternet&#39;te GezininComment[ug]&#x3D;دۇنيادىكى توربەتلەرنى كۆرگىلى بولىدۇComment[uk]&#x3D;Перегляд сторінок ІнтернетуComment[vi]&#x3D;Để duyệt các trang webComment[zh_CN]&#x3D;浏览互联网Comment[zh_TW]&#x3D;瀏覽網際網路GenericName&#x3D;Web BrowserGenericName[ar]&#x3D;متصفح ويبGenericName[ast]&#x3D;Restolador WebGenericName[bn]&#x3D;ওয়েব ব্রাউজারGenericName[ca]&#x3D;Navegador webGenericName[cs]&#x3D;Webový prohlížečGenericName[da]&#x3D;WebbrowserGenericName[el]&#x3D;Περιηγητής διαδικτύουGenericName[es]&#x3D;Navegador webGenericName[et]&#x3D;VeebibrauserGenericName[fa]&#x3D;مرورگر اینترنتیGenericName[fi]&#x3D;WWW-selainGenericName[fr]&#x3D;Navigateur WebGenericName[gl]&#x3D;Navegador WebGenericName[he]&#x3D;דפדפן אינטרנטGenericName[hr]&#x3D;Web preglednikGenericName[hu]&#x3D;WebböngészőGenericName[it]&#x3D;Browser webGenericName[ja]&#x3D;ウェブ・ブラウザGenericName[ko]&#x3D;웹 브라우저GenericName[ku]&#x3D;Geroka torêGenericName[lt]&#x3D;Interneto naršyklėGenericName[nb]&#x3D;NettleserGenericName[nl]&#x3D;WebbrowserGenericName[nn]&#x3D;NettlesarGenericName[no]&#x3D;NettleserGenericName[pl]&#x3D;Przeglądarka WWWGenericName[pt]&#x3D;Navegador WebGenericName[pt_BR]&#x3D;Navegador WebGenericName[ro]&#x3D;Navigator InternetGenericName[ru]&#x3D;Веб-браузерGenericName[sk]&#x3D;Internetový prehliadačGenericName[sl]&#x3D;Spletni brskalnikGenericName[sv]&#x3D;WebbläsareGenericName[tr]&#x3D;Web TarayıcıGenericName[ug]&#x3D;توركۆرگۈGenericName[uk]&#x3D;Веб-браузерGenericName[vi]&#x3D;Trình duyệt WebGenericName[zh_CN]&#x3D;网络浏览器GenericName[zh_TW]&#x3D;網路瀏覽器Keywords&#x3D;Internet;WWW;Browser;Web;ExplorerKeywords[ar]&#x3D;انترنت;إنترنت;متصفح;ويب;وبKeywords[ast]&#x3D;Internet;WWW;Restolador;Web;EsploradorKeywords[ca]&#x3D;Internet;WWW;Navegador;Web;Explorador;ExplorerKeywords[cs]&#x3D;Internet;WWW;Prohlížeč;Web;ExplorerKeywords[da]&#x3D;Internet;Internettet;WWW;Browser;Browse;Web;Surf;NettetKeywords[de]&#x3D;Internet;WWW;Browser;Web;Explorer;Webseite;Site;surfen;online;browsenKeywords[el]&#x3D;Internet;WWW;Browser;Web;Explorer;Διαδίκτυο;Περιηγητής;Firefox;Φιρεφοχ;ΙντερνετKeywords[es]&#x3D;Explorador;Internet;WWWKeywords[fi]&#x3D;Internet;WWW;Browser;Web;Explorer;selain;Internet-selain;internetselain;verkkoselain;netti;surffaaKeywords[fr]&#x3D;Internet;WWW;Browser;Web;Explorer;Fureteur;Surfer;NavigateurKeywords[he]&#x3D;דפדפן;אינטרנט;רשת;אתרים;אתר;פיירפוקס;מוזילה;Keywords[hr]&#x3D;Internet;WWW;preglednik;WebKeywords[hu]&#x3D;Internet;WWW;Böngésző;Web;Háló;Net;ExplorerKeywords[it]&#x3D;Internet;WWW;Browser;Web;NavigatoreKeywords[is]&#x3D;Internet;WWW;Vafri;Vefur;Netvafri;FlakkKeywords[ja]&#x3D;Internet;WWW;Web;インターネット;ブラウザ;ウェブ;エクスプローラKeywords[nb]&#x3D;Internett;WWW;Nettleser;Explorer;Web;Browser;NettsideKeywords[nl]&#x3D;Internet;WWW;Browser;Web;Explorer;Verkenner;Website;Surfen;OnlineKeywords[pt]&#x3D;Internet;WWW;Browser;Web;Explorador;NavegadorKeywords[pt_BR]&#x3D;Internet;WWW;Browser;Web;Explorador;NavegadorKeywords[ru]&#x3D;Internet;WWW;Browser;Web;Explorer;интернет;браузер;веб;файрфокс;огнелисKeywords[sk]&#x3D;Internet;WWW;Prehliadač;Web;ExplorerKeywords[sl]&#x3D;Internet;WWW;Browser;Web;Explorer;Brskalnik;SpletKeywords[tr]&#x3D;İnternet;WWW;Tarayıcı;Web;Gezgin;Web sitesi;Site;sörf;çevrimiçi;taraKeywords[uk]&#x3D;Internet;WWW;Browser;Web;Explorer;Інтернет;мережа;переглядач;оглядач;браузер;веб;файрфокс;вогнелис;переглядKeywords[vi]&#x3D;Internet;WWW;Browser;Web;Explorer;Trình duyệt;Trang webKeywords[zh_CN]&#x3D;Internet;WWW;Browser;Web;Explorer;网页;浏览;上网;火狐;Firefox;ff;互联网;网站;Keywords[zh_TW]&#x3D;Internet;WWW;Browser;Web;Explorer;網際網路;網路;瀏覽器;上網;網頁;火狐Exec&#x3D;firefox %uTerminal&#x3D;falseX-MultipleArgs&#x3D;falseType&#x3D;ApplicationIcon&#x3D;firefoxCategories&#x3D;GNOME;GTK;Network;WebBrowser;MimeType&#x3D;text&#x2F;html;text&#x2F;xml;application&#x2F;xhtml+xml;application&#x2F;xml;application&#x2F;rss+xml;application&#x2F;rdf+xml;image&#x2F;gif;image&#x2F;jpeg;image&#x2F;png;x-scheme-handler&#x2F;http;x-scheme-handler&#x2F;https;x-scheme-handler&#x2F;ftp;x-scheme-handler&#x2F;chrome;video&#x2F;webm;application&#x2F;x-xpinstall;StartupNotify&#x3D;trueActions&#x3D;new-window;new-private-window;[Desktop Action new-window]Name&#x3D;Open a New WindowName[ar]&#x3D;افتح نافذة جديدةName[ast]&#x3D;Abrir una ventana nuevaName[bn]&#x3D;Abrir una ventana nuevaName[ca]&#x3D;Obre una finestra novaName[cs]&#x3D;Otevřít nové oknoName[da]&#x3D;Åbn et nyt vindueName[de]&#x3D;Ein neues Fenster öffnenName[el]&#x3D;Νέο παράθυροName[es]&#x3D;Abrir una ventana nuevaName[fi]&#x3D;Avaa uusi ikkunaName[fr]&#x3D;Ouvrir une nouvelle fenêtreName[gl]&#x3D;Abrir unha nova xanelaName[he]&#x3D;פתיחת חלון חדשName[hr]&#x3D;Otvori novi prozorName[hu]&#x3D;Új ablak nyitásaName[it]&#x3D;Apri una nuova finestraName[ja]&#x3D;新しいウィンドウを開くName[ko]&#x3D;새 창 열기Name[ku]&#x3D;Paceyeke nû vekeName[lt]&#x3D;Atverti naują langąName[nb]&#x3D;Åpne et nytt vinduName[nl]&#x3D;Nieuw venster openenName[pt]&#x3D;Abrir nova janelaName[pt_BR]&#x3D;Abrir nova janelaName[ro]&#x3D;Deschide o fereastră nouăName[ru]&#x3D;Новое окноName[sk]&#x3D;Otvoriť nové oknoName[sl]&#x3D;Odpri novo oknoName[sv]&#x3D;Öppna ett nytt fönsterName[tr]&#x3D;Yeni pencere açName[ug]&#x3D;يېڭى كۆزنەك ئېچىشName[uk]&#x3D;Відкрити нове вікноName[vi]&#x3D;Mở cửa sổ mớiName[zh_CN]&#x3D;新建窗口Name[zh_TW]&#x3D;開啟新視窗Exec&#x3D;firefox -new-window[Desktop Action new-private-window]Name&#x3D;Open a New Private WindowName[ar]&#x3D;افتح نافذة جديدة للتصفح الخاصName[ca]&#x3D;Obre una finestra nova en mode d&#39;incògnitName[cs]&#x3D;Otevřít nové anonymní oknoName[de]&#x3D;Ein neues privates Fenster öffnenName[el]&#x3D;Νέο ιδιωτικό παράθυροName[es]&#x3D;Abrir una ventana privada nuevaName[fi]&#x3D;Avaa uusi yksityinen ikkunaName[fr]&#x3D;Ouvrir une nouvelle fenêtre de navigation privéeName[he]&#x3D;פתיחת חלון גלישה פרטית חדשName[hu]&#x3D;Új privát ablak nyitásaName[it]&#x3D;Apri una nuova finestra anonimaName[nb]&#x3D;Åpne et nytt privat vinduName[ru]&#x3D;Новое приватное окноName[sl]&#x3D;Odpri novo okno zasebnega brskanjaName[sv]&#x3D;Öppna ett nytt privat fönsterName[tr]&#x3D;Yeni bir pencere açName[uk]&#x3D;Відкрити нове вікно у потайливому режиміName[zh_TW]&#x3D;開啟新隱私瀏覽視窗Exec&#x3D;&#x2F;opt&#x2F;firefox&#x2F;firefox# Exec&#x3D;&#x2F;home&#x2F;leek&#x2F;soft&#x2F;firefox&#x2F;firefox %u# Exec&#x3D;&#x2F;home&#x2F;leek&#x2F;soft&#x2F;firefox&#x2F;firefox -new-window# Exec&#x3D;&#x2F;home&#x2F;leek&#x2F;soft&#x2F;firefox&#x2F;firefox -private-window Firefox.com 官网下载国际版 解压后可直接运行Firefox 文件夹下的firefox bin 添加对应的脚步文件 打开 ubuntu 安装开机界面 Linux 下载神器 motrix","categories":[],"tags":[{"name":"ubuntu","slug":"ubuntu","permalink":"https://imlike.cc/tags/ubuntu/"},{"name":"wine","slug":"wine","permalink":"https://imlike.cc/tags/wine/"}]},{"title":"java实现图片压缩","slug":"java实现图片压缩","date":"2019-03-24T04:39:19.494Z","updated":"2024-03-14T14:55:29.478Z","comments":true,"path":"3632233999.html","link":"","permalink":"https://imlike.cc/3632233999.html","excerpt":"","text":"开始时是直接对流进行处理但是会有异常,虽然thumbnailator可以填写，而图片对于现在的很多手机来说，拍摄出来的都是高清图片，分辨率也是相当的高，当然占用的存储空间也就大了,但不同的场景是需要不同的,微信的图片显示就是先显示压缩图然后可以选着看原图 123456&lt;!--图片压缩--&gt; &lt;dependency&gt; &lt;groupId&gt;net.coobird&lt;&#x2F;groupId&gt; &lt;artifactId&gt;thumbnailator&lt;&#x2F;artifactId&gt; &lt;version&gt;0.4.8&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; 1234567891011121314151617181920 &#x2F;&#x2F; 由于直接对流进行操作导致问题 所以是保存文件到本地后压缩 File fileSize2 &#x3D; new File(path_store + &quot;&#x2F;size2&#x2F;&quot; + &quot;m2&quot; + fileName); File fileSize3 &#x3D; new File(path_store + &quot;&#x2F;small&#x2F;&quot; + &quot;m3&quot; + fileName); File fileSize4 &#x3D; new File(path_store + &quot;&#x2F;size4&#x2F;&quot; + &quot;m4&quot; + fileName); String size2 &#x3D; fileSize2.toString(); String size3 &#x3D; fileSize3.toString(); String size4 &#x3D; fileSize4.toString();&#x2F;&#x2F;通过session保存值 request.getSession().setAttribute(&quot;size2&quot;,size2); request.getSession().setAttribute(&quot;size3&quot;,size3); request.getSession().setAttribute(&quot;size4&quot;,size4); Thumbnails.of(st).scale(0.2f).toFile(fileSize2); Thumbnails.of(st).scale(0.5f).toFile(fileSize3); Thumbnails.of(st).scale(1f).toFile(fileSize4); String rimage &#x3D; &quot;img&#x2F;product&#x2F;product&#x2F;&quot; + fileName; request.getSession().setAttribute(&quot;rimge&quot;, rimage); &#x2F;&#x2F;删除临时文件 item.delete(); &#x2F;&#x2F; response.getWriter().write(&quot;success&quot;+rimage);","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://imlike.cc/tags/java/"}]},{"title":"Ajax运用的归纳整理","slug":"Ajax运用的归纳整理","date":"2019-03-24T00:53:16.000Z","updated":"2024-03-14T14:55:29.473Z","comments":true,"path":"6f432c85.html","link":"","permalink":"https://imlike.cc/6f432c85.html","excerpt":"","text":"Ajax请求包括常见三种 本人日常使用是完整版,但是最近发现ajax中发post请求也是很快的而且很方便,将多种方法都试一遍线顺便整理一下 | GET请求 | $.get(url,[data],[callback],[type]) | | POST请求 | $.post(url,[data],[callback],[type]) | | AJAX请求 | `$.ajax([settings]) POST请求语法格式$.post(url, [data], [callback], [type])其中，参数说明如下：| 参数| 说明 || url| 请求的服务器端url地址 ||data|发送给服务器端的请求参数，格式可以是key=value也可以是js对象||callback| 当请求成功后的回掉函数，可以在函数体中编写我们的逻辑代码||type| 预期的返回数据的类型，取值可以是 xml, html, script, json, text, _defaul等 123456789101112&quot;user&#x2F;login&quot;,&#x2F;&#x2F;请求地址 $(&quot;#loginForm&quot;).serialize(),&#x2F;&#x2F;表单所有数据 function (resultInfo) &#123; if(resultInfo.flag)&#123; &#x2F;&#x2F;登录成功，跳转到首页 location.href&#x3D;&quot;index.html&quot;; &#125;else&#123; &#x2F;&#x2F;失败，显示消息 $(&quot;#errorMsg&quot;).text(resultInfo.errorMsg); &#125; &#125;, &quot;json&quot;);$(&quot;#errorMsg&quot;).text(&quot;请输入用户名或密码&quot;); 现在感觉发post请求,同这个会更简单且更加清晰,只需要url,[data],[callback],[type]),清晰明了,各个部分以’,’分开, AJAX请求我之前是完全的ajax请求格式太多但是功能也更加全面 引用自w3school jQuery ajax 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051 类型：Object 可选。AJAX 请求设置。所有选项都是可选的。async 类型：Boolean 默认值: true。默认设置下，所有请求均为异步请求。如果需要发送同步请求，请将此选项设置为 false。 注意，同步请求将锁住浏览器，用户其它操作必须等待请求完成才可以执行。beforeSend(XHR) 类型：Function 发送请求前可修改 XMLHttpRequest 对象的函数，如添加自定义 HTTP 头。 XMLHttpRequest 对象是唯一的参数。 这是一个 Ajax 事件。如果返回 false 可以取消本次 ajax 请求。cache 类型：Boolean 默认值: true，dataType 为 script 和 jsonp 时默认为 false。设置为 false 将不缓存此页面。 jQuery 1.2 新功能。complete(XHR, TS) 类型：Function 请求完成后回调函数 (请求成功或失败之后均调用)。 参数： XMLHttpRequest 对象和一个描述请求类型的字符串。 这是一个 Ajax 事件。contentType 类型：String 默认值: &quot;application&#x2F;x-www-form-urlencoded&quot;。发送信息至服务器时内容编码类型。 默认值适合大多数情况。如果你明确地传递了一个 content-type 给 $.ajax() 那么它必定会发送给服务器（即使没有数据要发送）。context 类型：Object 这个对象用于设置 Ajax 相关回调函数的上下文。也就是说，让回调函数内 this 指向这个对象（如果不设定这个参数，那么 this 就指向调用本次 AJAX 请求时传递的 options 参数）。比如指定一个 DOM 元素作为 context 参数，这样就设置了 success 回调函数的上下文为这个 DOM 元素。$.ajax(&#123; url: &quot;test.html&quot;, context: document.body, success: function()&#123; $(this).addClass(&quot;done&quot;); &#125;&#125;); route 123456789101112131415161718192021222324252627282930313233343536 var cid&#x3D;getParameter(&quot;cid&quot;) $.ajax(&#123; url:&quot;&#x2F;route&#x2F;showPage&quot;,&#x2F;&#x2F;请求路径 data:&#123;&quot;currentPage&quot;:currentPage, &quot;cid&quot;:cid&#125;,&#x2F;&#x2F;发送参数 dataType:&quot;json&quot;, &#x2F;&#x2F;接收类型确认 success:function (res) &#123; toShowRouteDetail(res) &#125; &#125;)&#125;ShowRoute(1)&#x2F;&#x2F;显示旅游线路function toShowRouteDetail(res) &#123; $(&quot;#show2Route&quot;).empty() for(var i&#x3D;0;i&lt;res.listRoute.length;i++)&#123; var temp&#x3D;$(&quot;&lt;li&gt;\\n&quot; + &quot; &lt;div class&#x3D;\\&quot;img\\&quot;&gt;&lt;img width&#x3D;&#39;290px&#39; src&#x3D;\\&quot;&quot;+res.listRoute[i].rimage+&quot;\\&quot; alt&#x3D;\\&quot;\\&quot;&gt;&lt;&#x2F;div&gt;\\n&quot; + &quot; &lt;div class&#x3D;\\&quot;text1\\&quot;&gt;\\n&quot; + &quot; &lt;p&gt;&quot;+res.listRoute[i].rname+&quot;&lt;&#x2F;p&gt;\\n&quot; + &quot; &lt;br&#x2F;&gt;\\n&quot; + &quot; &lt;p&gt;&quot;+res.listRoute[i].routeIntroduce+&quot;&lt;&#x2F;p&gt;\\n&quot; + &quot; &lt;&#x2F;div&gt;\\n&quot; + &quot; &lt;div class&#x3D;\\&quot;price\\&quot;&gt;\\n&quot; + &quot; &lt;p class&#x3D;\\&quot;price_num\\&quot;&gt;\\n&quot; + &quot; &lt;span&gt;&amp;yen;&lt;&#x2F;span&gt;\\n&quot; + &quot; &lt;span&gt;&quot;+res.listRoute[i].price+&quot;&lt;&#x2F;span&gt;\\n&quot; + &quot; &lt;span&gt;起&lt;&#x2F;span&gt;\\n&quot; + &quot; &lt;&#x2F;p&gt;\\n&quot; + &quot; &lt;p&gt;&lt;a href&#x3D;\\&quot;&#x2F;route_detail.html?rid&#x3D;&quot;+res.listRoute[i].rid+&quot;\\&quot;&gt;查看详情&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;\\n&quot; + &quot; &lt;&#x2F;div&gt;\\n&quot; + &quot; &lt;&#x2F;li&gt;&quot;); $(&quot;#show2Route&quot;).append(temp) &#125; GET请求$.get(url,[data],[callback],[type data方式一：key=value&amp;key=value方式二：{key:value,key:value…}| callback | 回调函数：当请求成功后触发的函数 ||type|返回参数类型：取值可以是xml,html,script, json,text,_defaul等 | 1234567891011121314ajaxGetFn() &#123;&#x2F;&#x2F;Ajax的GET请求： &#x2F;&#x2F;方式一：key&#x3D;value&amp;key&#x3D;value形式传值 &#x2F;&#x2F; $.get(&quot;&#x2F;getDemo&quot;,&quot;name&#x3D;&#39;张三&#39;&amp;age&#x3D;18&quot;,function (res) &#123; &#x2F;&#x2F; &#x2F;&#x2F;处理响应数据,并在浏览器上显示 &#x2F;&#x2F; alert(res); &#x2F;&#x2F; &#125;,&quot;text&quot;); &#x2F;&#x2F;方式二：&#123;key:value,key:value&#125;形式传值 $.get(&quot;&#x2F;getDemo&quot;,&#123;&quot;name&quot;:&quot;张三&quot;,&quot;age&quot;:18&#125;,function (res) &#123; &#x2F;&#x2F;处理响应数据 alert(res); &#125;,&quot;text&quot;);&#125;","categories":[],"tags":[]},{"title":"S6新特性 和VUE的个人理解","slug":"S6新特性 和VUE的个人理解","date":"2019-03-24T00:53:16.000Z","updated":"2024-03-14T14:55:29.474Z","comments":true,"path":"4288700677.html","link":"","permalink":"https://imlike.cc/4288700677.html","excerpt":"","text":"es6新特性ES6新特性介绍阮一峰的翻译的ECMAScript 6 入门-图书 123增加了 const let修饰还有map reduce indexof的替换者includes包括template(模板字符串（template string）是增强版的字符串，)在模 板字符串中嵌入变量，需要将变量名写在${}之中​ 结构{temp.feid}​ array.forEach(v = &gt; console.log(v));箭头表达式​ 反引号 ` 来创建字符串反问号创建字符串​ 美元符号加花括号包裹的变量${vraible}​​ 遍历器（Iterator）遍历的是一种规则数据的部署 使用for…of循环，遍历某种数据结构时，该循环会自动去寻找Iterator接口。在ES6中，有三类数据结构原生具备Iterator接口：数组、某些类似数组的对象、Set和Map结构。 S6模块通过export命令显式指定输出的代码，输入时也采用静态命令的形式。​ export xxx from xxx​ import xxxx from xxxx如果只是需要key:value的数据结构，记得使用Map，因为Map有内建的遍历机制。使用import取代require，使用export取代module.exports。这样可以在编译时就完成模块编译，效率要比CommonJS模块高。 // CommonJS模块 1&#123; stat, exists, readFile &#125;&#x2F;&#x2F; 等同于let _fs &#x3D; require(&#39;fs&#39;);let stat &#x3D; _fs.stat;let exists &#x3D; _fs.exists;let readfile &#x3D; _fs.readfile; 无序列表// ES6模块 1&#123; stat, exists, readFile &#125; from &#39;fs&#39;; vue子模块 vue是基于MVVM设计的js框架.需要注意子模块和父模块的使用而点vue文件需要在webpack是Data 是将将原本的值以变成了方法一return来实现值处理和返回值 还有保证在其他引用的方法的部分不会随着变化独立性更高 vue包括vue vue-cli vuex 而vue是可以首先是是它的生命周期包括create,mount注意在html里面可以用v-if foreach 1el data compute methods watch包括组件元素vue.componet(&quot;name&quot;,&#123; template &#125;)var a&#x3D; (&#123;template:return &#123;&#123; xx:kkkk &#125;&#125;) 将你输入的值遍历你的值是&lt;里面 :name=”值”&gt; 1234567891011121314151617181920212223242526272829&lt;ol&gt; &lt;!-- 现在我们为每个 todo-item 提供 todo 对象 todo 对象是变量，即其内容可以是动态的。 我们也需要为每个组件提供一个“key”，稍后再 作详细解释。 --&gt; &lt;todo-item v-for&#x3D;&quot;item in groceryList&quot; v-bind:todo&#x3D;&quot;item&quot; v-bind:key&#x3D;&quot;item.id&quot;&gt; &lt;&#x2F;todo-item&gt; &lt;&#x2F;ol&gt;Vue.component(&#39;todo-item&#39;, &#123; props: [&#39;todo&#39;], template: &#39;&lt;li&gt;&#123;&#123; todo.text &#125;&#125;&lt;&#x2F;li&gt;&#39;&#125;)var app7 &#x3D; new Vue(&#123; el: &#39;#app-7&#39;, data: &#123; groceryList: [ &#123; id: 0, text: &#39;蔬菜&#39; &#125;, &#123; id: 1, text: &#39;奶酪&#39; &#125;, &#123; id: 2, text: &#39;随便其它什么人吃的东西&#39; &#125; ] &#125;&#125;) 还包括后面是变量或者表达式没加冒号,后米娜是字符串v-bind 绑定了vue数据不是普通字符串 :xxx 比如classv-on:xxx 绑定事件 @click=””_ dirname 是绝对路径绑定时v-model_直接取值使用(或$是在vue实例中因为vue内部与API冲突./ 可以解释为组件能导入@/是webpack设置的路径别名在外界使用组件时需要注册组件 vueload 1、父组件可以使用 props 把数据传给子组件。2、子组件可以使用 $emit 触发父组件的自定义事件。 vm.$emit( event, arg ) //触发当前实例上的事件 vm.$on( event, fn );//监听event事件后运行 fn；https://www.tangshuang.net/3507.htmlv-bind用于绑定html元素上的attribution 不是双向绑定v-model主要是用在表单元素中，它实现了双向绑定。","categories":[],"tags":[{"name":"vue","slug":"vue","permalink":"https://imlike.cc/tags/vue/"},{"name":"es6","slug":"es6","permalink":"https://imlike.cc/tags/es6/"}]},{"title":"SpringCloud","slug":"SpringCloud","date":"2019-03-24T00:53:16.000Z","updated":"2024-03-14T14:55:29.474Z","comments":true,"path":"2527336709.html","link":"","permalink":"https://imlike.cc/2527336709.html","excerpt":"","text":"补充eureka闭源的消息添加dubbo与springcloud对比 关于之前关于SpringBoot的介绍是关于SpringBoot的概述 Spring boot专注于快速、方便集成的单个个体，Spring Cloud是关注全局的服务治理框架；spring boot使用了默认大于配置的理念，很多集成方案已经帮你选择好了，能不配置就不配置，Spring Cloud很大的一部分是基于Spring boot来实现。来自纯洁的微笑 ##SpringCloud官网介绍Spring Cloud为开发人员提供了快速构建分布式系统中一些常见模式的工具（例如配置管理，服务发现，断路器，智能路由，微代理，控制总线，一次性令牌，全局锁定，领导选举，分布式会话，集群状态）用于微服务时各个系统的同步和相互通信确保一致性. 目前的版本情况| Component | Edgware.SR5 | Finchley.SR1 | Finchley.BUILD-SNAPSHOT || ————————- | ————– | ————- | ———————– || spring-cloud-aws | 1.2.3.RELEASE | 2.0.0.RELEASE | 2.0.1.BUILD-SNAPSHOT || spring-cloud-bus | 1.3.3.RELEASE | 2.0.0.RELEASE | 2.0.1.BUILD-SNAPSHOT || spring-cloud-cli | 1.4.1.RELEASE | 2.0.0.RELEASE | 2.0.1.BUILD-SNAPSHOT || spring-cloud-commons | 1.3.5.RELEASE | 2.0.1.RELEASE | 2.0.2.BUILD-SNAPSHOT || spring-cloud-contract | 1.2.6.RELEASE | 2.0.1.RELEASE | 2.0.2.BUILD-SNAPSHOT || spring-cloud-config | 1.4.5.RELEASE | 2.0.1.RELEASE | 2.0.2.BUILD-SNAPSHOT || spring-cloud-netflix | 1.4.6.RELEASE | 2.0.1.RELEASE | 2.0.2.BUILD-SNAPSHOT || spring-cloud-security | 1.2.3.RELEASE | 2.0.0.RELEASE | 2.0.1.BUILD-SNAPSHOT || spring-cloud-cloudfoundry | 1.1.2.RELEASE | 2.0.0.RELEASE | 2.0.1.BUILD-SNAPSHOT || spring-cloud-consul | 1.3.5.RELEASE | 2.0.1.RELEASE | 2.0.2.BUILD-SNAPSHOT || spring-cloud-sleuth | 1.3.5.RELEASE | 2.0.1.RELEASE | 2.0.2.BUILD-SNAPSHOT || spring-cloud-stream | Ditmars.SR4 | Elmhurst.SR1 | Elmhurst.BUILD-SNAPSHOT || spring-cloud-zookeeper | 1.2.2.RELEASE | 2.0.0.RELEASE | 2.0.1.BUILD-SNAPSHOT || spring-boot | 1.5.16.RELEASE | 2.0.4.RELEASE | 2.0.4.BUILD-SNAPSHOT || spring-cloud-task | 1.2.3.RELEASE | 2.0.0.RELEASE | 2.0.1.BUILD-SNAPSHOT || spring-cloud-vault | 1.1.2.RELEASE | 2.0.1.RELEASE | 2.0.2.BUILD-SNAPSHOT || spring-cloud-gateway | 1.0.2.RELEASE | 2.0.1.RELEASE | 2.0.2.BUILD-SNAPSHOT || spring-cloud-openfeign | | 2.0.1.RELEASE | 2.0.2.BUILD-SNAPSHOT || spring-cloud-function | 1.0.1.RELEASE | 1.0.0.RELEASE | 1.0.1.BUILD-SNAPSHOT | 配置spring微服务springcloud是spring提供的一台服务而spring boot是简化配置文件统一版本提高开发效率能快速开启配置信息可以在application.yml中配置类似脚手架也能 1231.@ConfigurationProperties(locations&#x3D;“classpath:config&#x2F;user.properties”, prefix&#x3D;“XXX”)将配置文件导入,2.选择@PropertySource导入外部文件3.@EnableConfigurationProperties这个注解注释调，不让springboot自动配置，使用我们的手动配置 @ComponentScan(“com.lee.sh.mapper”)添加组件扫描component service controller SpringCould认识SpringCould中涉及到涉及的组件来自于netflix的eureka ribbon hystrix zuul都是在application.yml上面可以完成,然后在启动器上面要开启,就可以完成 可以实现集群解决访问量大的问题实现master 还是要辨析eureka和zookeeper的用途差异 Spring Cloud ZookeeperZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务，是Google的Chubby一个开源的实现，是Hadoop和Hbase的重要组件。它是一个为分布式应用提供一致性服务的软件，提供的功能包括：配置维护、域名服务、分布式同步、组服务等。ZooKeeper的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。 Netflix Eureka服务中心，云端服务发现，一个基于 REST 的服务，用于定位服务，以实现云端中间层服务发现和故障转移。这个可是springcloud最牛鼻的小弟，服务中心，任何小弟需要其它小弟支持什么都需要从这里来拿，同样的你有什么独门武功的都赶紧过报道，方便以后其它小弟来调用；它的好处是你不需要直接找各种什么小弟支持，只需要到服务中心来领取，也不需要知道提供支持的其它小弟在哪里，还是几个小弟来支持的，反正拿来用就行，服务中心来保证稳定性和质量。 情况来说目","categories":[],"tags":[{"name":"springcloud","slug":"springcloud","permalink":"https://imlike.cc/tags/springcloud/"}]},{"title":"Ubuntu优化-输入中文和deepin-Win","slug":"Ubuntu优化-输入中文和deepin-Win","date":"2019-03-24T00:53:16.000Z","updated":"2024-03-14T14:55:29.474Z","comments":true,"path":"2878480242.html","link":"","permalink":"https://imlike.cc/2878480242.html","excerpt":"","text":"将Deepin-ThunderSpeed ，Deepin-Wechart上在/usr/share/application 完善添加管理员权限 同时 、vi /etc/sudoers 将对于是sh脚本授予管理员权限 使用的deepin-wine deepin-wine 可以直接使用IDM 目前市面所有的下载工具 Linux可以寥寥无几像 Ｕget XMD 的用过之后还是不行，但是windows版安装和都很不方便,目前只使用 微信和迅雷 但是下载方面还是很慢 最近突然发现手里还珍藏了之前绿色版的IDM 赶紧的试试不错 123456789101112131415161718192021Desktop Entry]Name&#x3D;IdeaIUComment&#x3D;IdeaIUExec&#x3D;&#x2F;home&#x2F;admin-m&#x2F;soft&#x2F;idea2&#x2F;idea-IU-172.3317.76&#x2F;bin&#x2F;idea.shIcon&#x3D;&#x2F;home&#x2F;admin-m&#x2F;soft&#x2F;idea2&#x2F;idea-IU-172.3317.76&#x2F;bin&#x2F;idea.pngTerminal&#x3D;falseType&#x3D;ApplicationCategories&#x3D;Application;Development;[Desktop Entry] 每个desktop文件都以这个标签开始，说明这是一个Desktop Entry文件Version &#x3D; 1.0 标明Desktop Entry的版本（可选）Name &#x3D; IdeaIU 程序名称（必须），这里以创建一个idea的快捷方式为例GenericName &#x3D; IdeaIU 程序通用名称（可选）Comment &#x3D;IdeaIU 程序描述（可选）Exec &#x3D; 程序的启动命令（必选），可以带参数运行Icon &#x3D; 设置快捷方式的图标（可选），当Type为Application，此项有效Terminal &#x3D; false 是否在终端中运行（可选），当Type为Application，此项有效Type &#x3D; Application desktop的类型（必选），常见值有“Application”和“Link”Categories &#x3D; GNOME;Application;Network; &#x2F;&#x2F;注明在菜单栏中显示的类别（可选）完成 desktop 需要在权限上显示 不要懒，完成好计划好的 提升效率，多想想法子 sh wechat下面添加 1export XMODIFIERS&#x3D;&quot;@im&#x3D;fcitx&quot;export GTK_IM_MODULE&#x3D;&quot;fcitx&quot;export QT_IM_MODULE&#x3D;&quot;fcitx&quot; 设置开机主题 针对Ubuntu18 部分通知没有 可以使用 topicons plus 实现wine通知的","categories":[],"tags":[{"name":"ubuntu","slug":"ubuntu","permalink":"https://imlike.cc/tags/ubuntu/"}]},{"title":"Ubuntu安装软件部分","slug":"Ubuntu安装软件部分","date":"2019-03-24T00:53:16.000Z","updated":"2024-03-14T14:55:29.474Z","comments":true,"path":"2690029183.html","link":"","permalink":"https://imlike.cc/2690029183.html","excerpt":"","text":"系统安装完后后面肯定就是安装软件，开始是在谷歌中国上安装chrome，之后是安装最新的Firefox，安装是搜狗了，这种跟着晚上的步骤就可以了，同时可以添加host你懂的加速，然后是撤换国内的镜像源提升下载速度，完成后可以安装markdownd的软件，下载uget添加aira的插件，在chrome 和Firefox上安装配套的插件就可以调用uget下载速度反正比浏览器的速度要快的，这里主要如果没配置好的的话浏览器是不能调用uget的1，） 出现 unable connect 这个是uget的解决方案 123456789原因：缺少uget-integrator，出错在设置uGet为chrome的默认下载插件1 sudo add-apt-repository ppa:slgobinath/uget-chrome-wrapper2 sudo apt update3 sudo apt install uget-chrome-wrapper楼主也遇见了这个问题，经过多方查找发现：2018/5/10更新2018年uGet团队已将uget-chrome-wrapper采用官方插件并重新命名为uget-integrator 因此新的uget-extension插件不再支持uget-chrome-wrapper 解决方案：Ubuntu和Linux Mint1 sudo add-apt-repository ppa:uget-team/ppa2 sudo apt update3 sudo apt install uget-integratorwget https://raw.githubusercontent.com/ugetdm/ugetintegrator/master/install/linux/install_uget_integrator.sh2 chmod +x install_uget_integrator.sh3 ./install_uget_integrator.sh 2，）如果是搜狗看不到图标可以使用 在终端收入fcitx 查看启动情况，工开始出现有事搜狗启动异常的情况，要切换我就直接卸载了很多次然后又是安装了一大堆，都是一大堆问题，如果你是和我一样的话可以试试我的方法，先将下载fcitx 然后安装搜狗，必要的话按住Ctrl+h键将有sogou和fcitx的删掉，如果又不显示3，）对于网易云音乐安装是比较简单的 就是会出现不能自动登录的情况，每次都要启动都要登录我是参考这个链接 123456有些用户有登录不能保持的情况（打开需要重新登录），这种情况多半是sudo方法后的后遗症：使得配置文件的权限属于root。网易云音乐配置文件存在于两个地方，正常情况下的权限如下图：~&#x2F;.config&#x2F;netease-cloud-music (~ 代表当前用户的家目录)~&#x2F;.cache&#x2F;netease-cloud-music （这是缓存目录，是可以直接删的）友情提示，如果你没有 ll 命令，可以 ls -al 代替其中fancy是我的用户名，如果你的文件权限是root，那么就需要修改为你可以访问的状态，执行下面的命令就行啦：# 修改文件的所属用户和组，以及权限find ~&#x2F;.config&#x2F;netease-cloud-music -exec sudo chown $USER:$USER &#123;&#125; +find ~&#x2F;.config&#x2F;netease-cloud-music -exec sudo chmod 755 &#123;&#125; + find ~&#x2F;.cache&#x2F;netease-cloud-music -exec sudo chown $USER:$USER &#123;&#125; +find ~&#x2F;.cache&#x2F;netease-cloud-music -exec sudo chmod 755 &#123;&#125; + 时候就可以保持登录状态了4，）就是安装deepin-wine了参考了这位兄弟 这里有个地方要主要提供上面的下载链接是有问题的安装时会出现安装包损坏，用下面那个链接我出现安装后点图标没有后面是在 /opt/deepinwine/apps/Deepin-WeChat 这个路径下运行的","categories":[],"tags":[{"name":"ubuntu","slug":"ubuntu","permalink":"https://imlike.cc/tags/ubuntu/"},{"name":"软件","slug":"软件","permalink":"https://imlike.cc/tags/%E8%BD%AF%E4%BB%B6/"}]},{"title":"idea配置同步","slug":"idea配置同步","date":"2019-03-24T00:53:16.000Z","updated":"2024-03-14T14:55:29.478Z","comments":true,"path":"4293079570.html","link":"","permalink":"https://imlike.cc/4293079570.html","excerpt":"","text":"项目目录 .idea 目录和 demo.iml 和我们开发无关，是IDEA工具自己使用的 out 目录是存储编译后的.class文件 src 目录是存储我们编写的.java源文件 右单击模块,show in explorer :快速进入代码所在文件夹 常用快捷键 Alt+Enter 导入包，自动修正代码(重要) Ctrl+Y 删除光标所在行 Ctrl+D 复制光标所在行的内容，插入光标位置下面 Ctrl+Alt+L 格式化代码 (重要) Ctrl+/ 单行注释(记住) Ctrl+Shift+/ 选中代码注释，多行注释，再按取消注释(记住) Alt+Ins 自动生成代码，toString，get，set等方法 Alt+Shift+上下箭头 移动当前代码行 当前用的到的 psvm: main方法的快捷生成方式 sout: 输出语句 fori: for语句 100.for : for (int i = 0; i &lt; 100; i++) 修改快捷键 在IDEA工具中， Ctrl+空格 的快捷键，可以帮助我们补全代码，但是这个快捷键和Windows中的输入法切换快捷键冲突，需要修改IDEA中的快捷键。 File-&gt;Settings-&gt;keymap-&gt;Main menu-&gt;code-&gt;Completion-&gt;Basic 导入和关闭项目 关闭IDEA中已经存在的项目， File-&gt;Close Project 在IDEA的启动界面上，点击 OPEN ，选择项目目录即可 因为以后大家要看老师的代码,最好专门建立一个文件夹,存放 方法 回顾-方法的定义和调用 9*9乘法表 概念:方法也可以称为函数,是完成特定功能的代码块.(25m)","categories":[],"tags":[{"name":"intellj","slug":"intellj","permalink":"https://imlike.cc/tags/intellj/"}]},{"title":"kibana完成的对Elasticsraech的处理","slug":"kibana完成的对Elasticsraech的处理","date":"2019-03-24T00:53:16.000Z","updated":"2024-03-14T14:55:29.479Z","comments":true,"path":"9bad30c2.html","link":"","permalink":"https://imlike.cc/9bad30c2.html","excerpt":"","text":"创建索引,然后才能对索引进行映射配置包括映射配置的映射字段,查看映射关系,字段属性的详解PUT demo/_mapping/log{“properties”: { “name”:{ “type”:”text”, “index”: true, “store”: true, “analyzer”: “standard” }, “images”: { “type”: “keyword”, “index”: “false” }, “price”: { “type”: “float” }}}PUT demo{“settings”: { “number_of_replicas”: 1, “number_of_shards”: 5}}POST demo/log{“name”:”王大海”,“images”:”http://image.leyou.com/12479122.jpg&quot;, “price”:2699.00, “hobby”:” 大 桌球 “}GET _search{“query”: { “match_all”: {}}}GET demo/_mapping","categories":[],"tags":[{"name":"kibana","slug":"kibana","permalink":"https://imlike.cc/tags/kibana/"},{"name":"elasticsraech","slug":"elasticsraech","permalink":"https://imlike.cc/tags/elasticsraech/"}]},{"title":"spring 整合回顾","slug":"spring 整合回顾","date":"2019-03-24T00:53:16.000Z","updated":"2024-03-14T14:55:29.481Z","comments":true,"path":"857262000000000.html","link":"","permalink":"https://imlike.cc/857262000000000.html","excerpt":"","text":"spring 整合回顾注意spring的对bean的管理必须是建立在你声明的情况下,不然是不会管理的,还有针对配置文件必须要将给类使用也是必须要注解声明在spring整合中要注意包括webapp下的web,xml需要配置 1234dispatch 服务启动时要跟着启动拦截器保证你拦截到.do或者其他的值(在springmvc配置文件中)中文乱码确保能读取spring-xxx.xml的这类文件 !!配置监听器过程启动时启动ioc容器!! 将重要的文件什么会员才能下载的jsp文件都需要放在WEB-INF下面,确保能完成,还有一部分静态资源可以放在static文件夹下面,html的话可以放在主路径下面, 然后就是核心配置文件的配置包括resource下面spring核心配置文件 注入DataSource bean开启配置文件扫描 spring_MVC 12345678910111213141516配置文件:开启扫描 原本是在WEB-INF下面现在是都放在web下面注解开启 配置拦截路径和指向哪里 .jsp 还有静态文件全部放在controller下开启restcontroller respestMapping&lt;!--重要别忘了解决静态资源被拦截的问题--&gt;&lt;mvc:default-servlet-handler&#x2F;&gt; 转换器啊cover &lt;mvc:annotation-driven&gt; &lt;mvc:message-converters&gt; &lt;bean class&#x3D;&quot;org.springframework.http.converter.StringHttpMessageConverter&quot;&gt; &lt;constructor-arg name&#x3D;&quot;defaultCharset&quot; value&#x3D;&quot;UTF-8&quot;&#x2F;&gt; &lt;&#x2F;bean&gt; &lt;&#x2F;mvc:message-converters&gt;&lt;&#x2F;mvc:annotation-driven&gt; mybatis 123456789导入数据库连接池的bean 开启包扫描 注解开启 注解开发扫描mybatis sql的文件 sql必须要1),名字和接口名一样 2),@paremater提供 3).对象里面还有对象的复杂模型 4). !! 配置基包!! 1234567891011mybatis导入插件&lt;plugins&gt; &lt;plugin interceptor&#x3D;&quot;com.github.pagehelper.PageInterceptor&quot;&#x2F;&gt; &lt;plugin interceptor&#x3D;&quot;com.github.abel533.mapperhelper.MapperInterceptor&quot;&gt; &lt;!--指定主键自增回写方法,默认值MYSQL,详细说明请看文档 --&gt; &lt;property name&#x3D;&quot;IDENTITY&quot; value&#x3D;&quot;MYSQL&quot; &#x2F;&gt; &lt;!--通用Mapper接口，多个通用接口用逗号隔开 --&gt; &lt;property name&#x3D;&quot;mappers&quot; value&#x3D;&quot;com.github.abel533.mapper.Mapper&quot; &#x2F;&gt; &lt;&#x2F;plugin&gt; &lt;&#x2F;plugins&gt;","categories":[],"tags":[{"name":"spring","slug":"spring","permalink":"https://imlike.cc/tags/spring/"}]},{"title":"springAOP使用动态代理的简单实现","slug":"springAOP使用动态代理的简单实现","date":"2019-03-24T00:53:16.000Z","updated":"2024-03-14T14:55:29.481Z","comments":true,"path":"2810139133.html","link":"","permalink":"https://imlike.cc/2810139133.html","excerpt":"","text":"动态代理以及.Cglib动态代理通过动态代理和cglib来理解spring的AOP(Aspect Orient Programing)动态 动态代理以及.Cglib动态代理通过动态代理和cglib来理解spring的AOP(Aspect Orient Programing)动态代理是通过JDK实现了反射是–出现的 1). 代理设计模式：为其他对象提供一种代理以控制对这个对象的访问。 类加载器 概述 JDK 提供的代码. 运行时代码. 引导类加载器 扩展代码. 扩展类加载器 自己编写的代码.（以及第三方包) 应用类加载器 三种类加载器 引导类加载器（Bootstrap ClassLoader） 负责加载$JAVA_HOME中jre/lib/rt.jar里所有的class，由C++实现，不是ClassLoader子类 扩展类加载器（Extension ClassLoader） 负责加载java平台中扩展功能的一些jar包，包括$JAVA_HOME中jre/lib/*.jar（jre/lib/ext) 应用类加载器：（Application ClassLoader） 加载编写的代码 继承关系 引导类加载器由C++实现，不是ClassLoader子类（属于JVM的一部分） 扩展类加载器是引导类加载器子类 应用类加载器是扩展类加载器子类 双亲委托机制: 一个class文件只会加载一次，在内存有且只有一个Class对象 如果一个类加载器收到了类加载的请求，它首先不会自己去尝试加载这个类，而是把这个请求委派给父类加载器去完成，每一个层次的加载器都是如此，因此所有的类加载请求都会传给顶层的启动类加载器 只有当父加载器反馈自己无法完成该加载请求（该加载器的搜索范围中没有找到对应的类）时，子加载器才会尝试自己去加载。 动态代理 在运行时动态的创建代理类对象（无需定义一个代理类） java动态代理机制以巧妙的方式实现了代理模式的设计理念 作用：在被调用方法时，对委托者（被代理对象）进行 拦截和控制 结构 代理类对象 proxy （本质是一个匿名类对象） 委托类对象（被代理的对象） 接口（必须要有），被拦截的是接口中的方法 代码含义 Proxy.newProxyInstance(loader, interfaces, new InvocationHandler(){}）； 参数loader：代理类对象是运行时动态创建的，因为代理类在代码中并没有定义，需要在运行时动态加载，故需要类加载器（和被代理类加载器一样即可） 参数interfaces：接口用来动态的创建代理类，本质上这个代理类（匿名类）是接口的实现类，拥有接口中的所有方法（这个参数是复数，一般将委托类实现的所有接口传入） InvocationHandler：调用处理器，本质是接口回调2.重写 public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { proxy：代理类对象本身（几乎没作用） method：当前代理类对象执行的方法（拦截） args：当前方法所需要的参数 返回值：当前方法的返回值 final CustomerDao customerDao= new CustomerDaoImpl(); 123456789101112131415161718192021ClassLoader classLoader &#x3D; customerDao.getClass().getClassLoader();Class&lt;?&gt;[] interfaces &#x3D; customerDao.getClass().getInterfaces();InvocationHandler h &#x3D;new InvocationHandler() &#123; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.print(method.getName()+&quot; &quot;); if(args&#x3D;&#x3D;null)&#123; System.out.println(&quot;没有参数不增强&quot;); &#125;else &#123; Object invoke &#x3D; method.invoke(customerDao, args); System.out.println(&quot;方法增强&quot;); &#125; return null; &#125;&#125;;CustomerDao case &#x3D;(CustomerDao) Proxy.newProxyInstance(classLoader, interfaces,h);case.update(); 2).Cglib动态代理​ 本质是生成这个目标类的子类实现功能final CustomerDao customerDao=new CustomerDaoImpl(); 1234567891011121314151617181920ClassLoader classLoader &#x3D; customerDao.getClass().getClassLoader();Class&lt;?&gt;[] interfaces &#x3D; customerDao.getClass().getInterfaces();InvocationHandler h &#x3D;new InvocationHandler() &#123; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; Annotation[] annos &#x3D; method.getAnnotations(); for (Annotation anno : annos) &#123; if (anno.equals(&quot;Deprecated&quot;))&#123; System.out.println(&quot;注解标注增强功能&quot;); method.invoke(customerDao, args); &#125; &#125; return null; &#125;&#125;;CustomerDao case &#x3D;(CustomerDao) Proxy.newProxyInstance(classLoader, interfaces,h);case.change(&quot;ff&quot;);","categories":[],"tags":[{"name":"spring","slug":"spring","permalink":"https://imlike.cc/tags/spring/"}]},{"title":"springboot搭建","slug":"springboot搭建","date":"2019-03-24T00:53:16.000Z","updated":"2024-03-14T14:55:29.481Z","comments":true,"path":"1278275595.html","link":"","permalink":"https://imlike.cc/1278275595.html","excerpt":"","text":"Spring将很多魔法带入了Spring应用程序的开发之中，其中最重要的是以下四个核心。Spring Boot干货系列这位博主上面有不少干货 自动配置：针对很多Spring应用程序常见的应用功能，Spring Boot能自动提供相关配置 起步依赖：告诉Spring Boot需要什么功能，它就能引入需要的库。 命令行界面：这是Spring Boot的可选特性，借此你只需写代码就能完成完整的应用程序，无需传统项目构建。 Actuator：让你能够深入运行中的Spring Boot应用程序，一探究竟。详细的我们就不展开，等你爱上后自然会去深入的了解，后续章节我们会一一展开介绍。接下来让我们开搞吧。我已经迫不及待的要尝尝Spring Boot的味道了 现在就将我把问题记录一下 1).报missing servletwebServerFactory bean 是我将pom.xml配置文件打乱了我的文件,开始以为是技术论坛中大家常说的应用入口类缺少@SpringBootApplication标注，或者是Spring bean缺少@EnableAutoConfiguration标注等问题。maven的依赖问题没弄好.有冲突导致的问题 2.)配置properties时IDEA报错 这个由于idea预先读取配置文件中未出现前缀为jdbc的所以一直报错,且默认读取application.ym;(properties),要解决只能通过 1.@ConfigurationProperties(locations=”classpath:config/user.properties”, prefix=”XXX”)将配置文件导入,2.选择@PropertySource导入外部文件3.@EnableConfigurationProperties这个注解注释调，不让springboot自动配置，使用我们的手动配置","categories":[],"tags":[{"name":"springboot","slug":"springboot","permalink":"https://imlike.cc/tags/springboot/"}]},{"title":"spring处理响应的过程","slug":"spring处理响应的过程","date":"2019-03-24T00:53:16.000Z","updated":"2024-03-14T14:55:29.481Z","comments":true,"path":"3343205230.html","link":"","permalink":"https://imlike.cc/3343205230.html","excerpt":"","text":"springMVC的运行原理梳理是将 将浏览器发送过来信息处理已Model Vever Conctroller 将不同功能区分 1234dispacherServlethandlermapperninghandlerAdapater --&gt;handler(Controller)Viewer","categories":[],"tags":[{"name":"spring","slug":"spring","permalink":"https://imlike.cc/tags/spring/"}]},{"title":"ubuntu优化-修改软件权限挂着磁盘","slug":"ubuntu优化-修改软件权限挂着磁盘","date":"2019-03-24T00:53:16.000Z","updated":"2024-03-14T14:55:29.482Z","comments":true,"path":"1312199450.html","link":"","permalink":"https://imlike.cc/1312199450.html","excerpt":"","text":"sublime_text sudo chmod +x subl shell脚步 /usr/bin/subl 1Exec&#x3D;bash -c &quot;LD_PRELOAD&#x3D;&#x2F;opt&#x2F;sublime_text&#x2F;libsublime-imfix.so exec &#x2F;opt&#x2F;sublime_text&#x2F;sublime_text -n&quot; 完善 1# &#x2F;dev&#x2F;nvme0n1p1:beiUUID&#x3D;&quot;31777183-00e9-459a-898f-4405b03e2c1e&quot; PARTUUID&#x3D;&quot;a174f198-ae87-4ca7-b340-f4a6e8fda28f&quot; UUID&#x3D;31777183-00e9-459a-898f-4405b03e2c1e &#x2F;home&#x2F;leek&#x2F;device ext4 defaults 0 1 sudo mount -a /etc/fstab 12345678910111213&lt;fs spec&gt; &lt;fs file&gt; &lt;fs vfstype&gt; &lt;fs mntops&gt; &lt;fs freq&gt; &lt;fs passno&gt;具体说明，以挂载&#x2F;dev&#x2F;sdb1为例：&lt;fs spec&gt;：分区定位，可以给UUID或LABEL，例如：UUID&#x3D;6E9ADAC29ADA85CD或LABEL&#x3D;software&lt;fs file&gt;：具体挂载点的位置，例如：&#x2F;data&lt;fs vfstype&gt;：挂载磁盘类型，linux分区一般为ext4，windows分区一般为ntfs&lt;fs mntops&gt;：挂载参数，一般为defaults&lt;fs freq&gt;：磁盘检查，默认为0&lt;fs passno&gt;：磁盘检查，默认为0，不需要检查作者：a11en0链接：https:&#x2F;&#x2F;www.jianshu.com&#x2F;p&#x2F;336758411dbf来源：简书简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。","categories":[],"tags":[{"name":"ubuntu","slug":"ubuntu","permalink":"https://imlike.cc/tags/ubuntu/"}]},{"title":"使用fileupload","slug":"使用fileupload","date":"2019-03-24T00:53:16.000Z","updated":"2024-03-14T14:55:29.483Z","comments":true,"path":"7e2892da.html","link":"","permalink":"https://imlike.cc/7e2892da.html","excerpt":"","text":"使用commons-fileupload插件来帮我们处理上传后的数据而让我们自己手动处理的话,也是可以的,但是十分麻烦,因为我们需要将所有的请求体获取到, 首先maven下载fileupload和commons-io可以这两个类可以对浏览器的发送请求头保存,并能实现结束 1234567891011&lt;!--upload 文件--&gt; &lt;dependency&gt; &lt;groupId&gt;commons-fileupload&lt;&#x2F;groupId&gt; &lt;artifactId&gt;commons-fileupload&lt;&#x2F;artifactId&gt; &lt;version&gt;1.3.1&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;commons-io&lt;&#x2F;groupId&gt; &lt;artifactId&gt;commons-io&lt;&#x2F;artifactId&gt; &lt;version&gt;2.5&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; 浏览器端的代码 #html # 12345678910&lt;br&gt;&lt;!--&lt;form enctype&#x3D;&quot;multipart&#x2F;form-data&quot;&gt;--&gt;&lt;div class&#x3D;&quot;modal-header&quot;&gt;&lt;button type&#x3D;&quot;button&quot; class&#x3D;&quot;close&quot; data-dismiss&#x3D;&quot;modal&quot; aria-label&#x3D;&quot;Close&quot;&gt;&lt;span aria-hidden&#x3D;&quot;true&quot;&gt;&amp;times;&lt;&#x2F;span&gt;&lt;&#x2F;button&gt;&lt;h4 class&#x3D;&quot;modal-title&quot; id&#x3D;&quot;myModalLabel&quot;&gt;拖入文件上传&lt;&#x2F;h4&gt;&lt;&#x2F;div&gt;&lt;div class&#x3D;&quot;form-group&quot;&gt;&lt;input id&#x3D;&quot;file-1&quot; type&#x3D;&quot;file&quot; class&#x3D;&quot;file&quot; data-overwrite-initial&#x3D;&quot;false&quot;data-min-file-count&#x3D;&quot;1&quot;&gt;&lt;&#x2F;div&gt; js123456789101112131415161718192021222324252627&#x2F;&#x2F;文件上传 $(&quot;#file-1&quot;).fileinput(&#123; language: &#39;zh&#39;, &#x2F;&#x2F;设置语言 uploadUrl: &quot;adminAddRoute?methodName&#x3D;updatePic&quot;, &#x2F;&#x2F;上传的地址 showUpload: true, &#x2F;&#x2F;是否显示上传按钮 showCaption: true,&#x2F;&#x2F;是否显示标题 browseClass: &quot;btn btn-primary&quot;, &#x2F;&#x2F;按钮样式 previewId, index uploadExtraData: function () &#123; var data &#x3D; &#123; parentId: $(&quot;#fileType&quot;).val() &#125;; return data; &#125;, &#39;theme&#39;: &#39;explorer&#39;, maxFileCount: 1, &#x2F;&#x2F;表示允许同时上传的最大文件个数 enctype: &#39;multipart&#x2F;form-data&#39;, validateInitialCount: true, msgFilesTooMany: &quot;选择上传的文件数量(&#123;n&#125;) 超过允许的最大数值&#123;m&#125;！&quot;, allowedFileExtensions : [&#39;jpg&#39;, &#39;png&#39;,&#39;gif&#39;], overwriteInitial: false, maxFileSize: 1000, maxFilesNum: 1, &#x2F;&#x2F;allowedFileTypes: [&#39;image&#39;, &#39;video&#39;, &#39;flash&#39;], slugCallback: function(filename) &#123; return filename.replace(&#39;(&#39;, &#39;_&#39;).replace(&#39;]&#39;, &#39;_&#39;); &#125; &#125;); 将html文档放在需要实现的地方即可 java后台实现1234567891011121314151617181920212223242526272829303132333435String path_temp &#x3D; this.getServletContext().getRealPath(&quot;temp&quot;); &#x2F;&#x2F;DiskFileItemFactory factory &#x3D; new DiskFileItemFactory(1024*1024, new File(path_temp)); DiskFileItemFactory factory &#x3D; new DiskFileItemFactory(); factory.setSizeThreshold(1024*1024); factory.setRepository(new File(path_temp)); &#x2F;&#x2F;2、创建文件上传的核心类 ServletFileUpload upload &#x3D; new ServletFileUpload(factory); &#x2F;&#x2F;设置上传文件的名称的编码 upload.setHeaderEncoding(&quot;UTF-8&quot;); List&lt;FileItem&gt; fileItems &#x3D; upload.parseRequest(request); FileItem item &#x3D; fileItems.get(0); &#x2F;&#x2F;文件上传项 &#x2F;&#x2F;文件的名 String fileName1 &#x3D; item.getName(); System.out.println(&quot;上传文件名称 &#x3D; &quot; + fileName1); String uuid &#x3D; UUID.randomUUID().toString().replace(&quot;-&quot;, &quot;&quot;); String fileName &#x3D; uuid + &quot;.jpg&quot;; &#x2F;&#x2F;获得上传文件的内容 InputStream in &#x3D; item.getInputStream(); &#x2F;&#x2F;设置图片保存名字和路径 String path_store &#x3D; this.getServletContext().getRealPath(&quot;img&#x2F;product&quot;); &#x2F;&#x2F; if(path_store) String st &#x3D; path_store + &quot;&#x2F;&quot; + fileName; OutputStream outSize2 &#x3D; new FileOutputStream(st); &#x2F;&#x2F; OutputStream outSize4 &#x3D; new FileOutputStream(path_store+&quot;&#x2F;size4&#x2F;&quot;+fileName); &#x2F;&#x2F; OutputStream outSize3 &#x3D; new FileOutputStream(path_store+&quot;&#x2F;size3&#x2F;&quot;+fileName); &#x2F;&#x2F;保存文件 IOUtils.copy(in, outSize2); in.close(); outSize2.close();","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://imlike.cc/tags/java/"}]},{"title":"使用hexo搭建个人静态博客","slug":"使用hexo搭建个人静态博客","date":"2019-03-24T00:53:16.000Z","updated":"2024-03-14T14:55:29.483Z","comments":true,"path":"1573622544.html","link":"","permalink":"https://imlike.cc/1573622544.html","excerpt":"","text":"参考的博客地址包括使用github搭建个人博客 史上最新版GitHub+Hexo配置系列教程-Hexo配置1-史上最新版GitHub+Hexo配置系列教程-Hexo配置2-史上最新版GitHub+Hexo配置系列教程-Hexo配置3类似于jekyll、Octopress、Wordpress，我们可以用hexo创建自己的博客，托管到github或Heroku上，绑定自己的域名，用markdown写文章。本博客即使用hexo创建并托管在github上。先上效果图 首先是按照文章开头提供的链接安装node.js 通过NPM安装需要的各种js文件还有hexo 在你新建的文件夹hexo init初始化一下就会有文件夹和配置文件 然后下载hexo需要的js文件通过NPM hexo generate生成静态网站 然后运行hexo server启动hexo服务,如果报错就修改一下配置文件是 _config.yml,这个配置文件可以修改很多东西,但是要和你安装的主题匹配不然就可能不兼容无法启动 有个source/_posts是放你写的博客你可以将写好的md文件放在这里 在网上也可以比较方便的找到hexo支持的主题 我初始的主题是这样的 将_config.yml上面的 1# Extensions## Plugins: https:&#x2F;&#x2F;hexo.io&#x2F;plugins&#x2F;## Themes: https:&#x2F;&#x2F;hexo.io&#x2F;themes&#x2F;theme: anatole 修改为themes中对于的文件夹,我用的主题链接为hexo-theme-Anatole本人hexo的文件在这里","categories":[],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://imlike.cc/tags/hexo/"}]},{"title":"使用vue-cli脚手架","slug":"使用vue-cli脚手架","date":"2019-03-24T00:53:16.000Z","updated":"2024-03-14T14:55:29.483Z","comments":true,"path":"46994988.html","link":"","permalink":"https://imlike.cc/46994988.html","excerpt":"","text":"使用vue-cli脚手架创建vue初始化项目感觉vue init webpack-simple这样的方式适合小项目，vue init webpack这样的方式适合中大型项目，两种方式初始化Vue-Cli的项目的目录差别比较大，你可以发现使用vue init webpack这样的方式来初始化项目，默认提供了很多的webpack的配置，也更加的方便你对代理(跨域)、最终打包资源放到服务器什么目录、以及js、css、img和项目在打包过程等优化的配置等等 当 Vue Loader 编译单文件组件中的 块时，它也会将所有遇到的资源 URL 转换为 webpack 模块请求。webpack 和 Vue Loader 的结合为你提供了一个现代、灵活且极其强大的前端工作流，来帮助撰写 Vue.js 应用。vue-loader 转换规则资源 URL 转换会遵循如下规则： 有序列表如果路径是绝对路径 (例如 /images/foo.png)，会原样保留。 有序列表如果路径以 . 开头，将会被看作相对的模块依赖，并按照你的本地文件系统上的目录结构进行解析。 有序列表如果路径以 ~ 开头，其后的部分将会被看作模块依赖。这意味着你可以用该特性来引用一个 Node 依赖中的资源： 有序列表如果路径以 @ 开头，也会被看作模块依赖。如果你的 webpack 配置中给 @ 配置了 alias，这就很有用了。所有 vue-cli 创建的项目都默认配置了将 @ 指向 /src。在导入模块和文件的简写要注意还包括./这种是有特殊含义的 浅谈Vue.use写一个组件命名part.vue 12345&lt;template&gt; &lt;div class&#x3D;&quot;loading-box&quot;&gt; 我是组件 &lt;&#x2F;div&gt;&lt;&#x2F;template&gt; 在js中引入上面这个组件 1234567891011&#x2F;&#x2F; 引入组件import LoadingComponent from &#39;.&#x2F;part.vue&#39;&#x2F;&#x2F; 定义 Loading 对象const Loading&#x3D;&#123; &#x2F;&#x2F; install 是默认的方法。当外界在 use 这个组件的时候，就会调用本身的 install 方法，同时传一个 Vue 这个类的参数。 install:function(Vue)&#123; Vue.component(&#39;Loading&#39;,LoadingComponent) &#125;&#125;&#x2F;&#x2F; 导出export default Loading 注意如果 在其他地方用上面的index.js就必须要申明Vue.use(Loading)当外界在 use 这个组件的时候，就会调用本身的 install 方法，同时传一个 Vue 这个类的参数。参考地址","categories":[],"tags":[{"name":"vue-cli","slug":"vue-cli","permalink":"https://imlike.cc/tags/vue-cli/"}]},{"title":"网上收集的关于spring特点的总结和spring介绍","slug":"网上收集的关于spring特点的总结和spring介绍","date":"2019-03-24T00:53:16.000Z","updated":"2024-03-14T14:55:29.486Z","comments":true,"path":"fd8471fb.html","link":"","permalink":"https://imlike.cc/fd8471fb.html","excerpt":"","text":"朱小厮CNDS Spring框架的作用 轻量：Spring是轻量级的，基本的版本大小为2MB 控制反转：Spring通过控制反转实现了松散耦合，对象们给出它们的依赖，而不是创建或查找依赖的对象们。 面向切面的编程AOP:Spring支持面向切面的编程，并且把应用业务逻辑和系统服务分开。 容器：Spring包含并管理应用中对象的生命周期和配置 MVC框架： Spring-MVC 事务管理：Spring提供一个持续的事务管理接口，可以扩展到上至本地事务下至全局事务JTA 异常处理：Spring提供方便的API把具体技术相关的异常 Spring的组成 这里写图片描述 Spring由7个模块组成： Spring Core: 核心容器提供 Spring 框架的基本功能。核心容器的主要组件是BeanFactory，它是工厂模式的实现。BeanFactory 使用控制反转 （IOC） 模式将应用程序的配置和依赖性规范与实际的应用程序代码分开。 Spring 上下文：Spring 上下文是一个配置文件，向 Spring 框架提供上下文信息。Spring 上下文包括企业服务，例如 JNDI、EJB、电子邮件、国际化、校验和调度功能。 Spring AOP：通过配置管理特性，Spring AOP 模块直接将面向方面的编程功能集成到了 Spring 框架中。所以，可以很容易地使 Spring 框架管理的任何对象支持 AOP。Spring AOP 模块为基于 Spring 的应用程序中的对象提供了事务管理服务。通过使用 Spring AOP，不用依赖 EJB 组件，就可以将声明性事务管理集成到应用程序中。 Spring DAO：JDBC DAO 抽象层提供了有意义的异常层次结构，可用该结构来管理异常处理和不同数据库供应商抛出的错误消息。异常层次结构简化了错误处理，并且极大地降低了需要编写的异常代码数量（例如打开和关闭连接）。Spring DAO 的面向 JDBC 的异常遵从通用的 DAO 异常层次结构。 Spring ORM：Spring 框架插入了若干个 ORM 框架，从而提供了 ORM 的对象关系工具，其中包括 JDO、Hibernate 和 iBatis SQL Map。所有这些都遵从 Spring 的通用事务和 DAO 异常层次结构。 Spring Web 模块：Web 上下文模块建立在应用程序上下文模块之上，为基于 Web 的应用程序提供了上下文。所以，Spring 框架支持与 Jakarta Struts 的集成。Web 模块还简化了处理多部分请求以及将请求参数绑定到域对象的工作。 Spring MVC 框架：MVC 框架是一个全功能的构建 Web 应用程序的 MVC 实现。通过策略接口，MVC 框架变成为高度可配置的，MVC 容纳了大量视图技术，其中包括 JSP、Velocity、Tiles、iText 和 POI。 Spring容器 Sping的容器可以分为两种类型 BeanFactory：（org.springframework.beans.factory.BeanFactory接口定义）是最简答的容器，提供了基本的DI支持。最常用的BeanFactory实现就是XmlBeanFactory类，它根据XML文件中的定义加载beans，该容器从XML文件读取配置元数据并用它去创建一个完全配置的系统或应用。 ApplicationContext应用上下文：（org.springframework.context.ApplicationContext）基于BeanFactory之上构建，并提供面向应用的服务。 ApplicationContext通常的实现 ClassPathXmlApplicationContext：从类路径下的XML配置文件中加载上下文定义，把应用上下文定义文件当做类资源。 FileSystemXmlApplicationContext：读取文件系统下的XML配置文件并加载上下文定义。 XmlWebApplicationContext：读取Web应用下的XML配置文件并装载上下文定义。ApplicationContext context = new ClassPathXmlApplicationContext(“applicationContext.xml”); IOC &amp; DI Inversion of Control， 一般分为两种类型：依赖注入DI(Dependency Injection)和依赖查找（Dependency Lookup）.依赖注入应用比较广泛。 Spring IOC负责创建对象，管理对象（DI），装配对象，配置对象，并且管理这些对象的整个生命周期。 优点：把应用的代码量降到最低。容器测试，最小的代价和最小的侵入性使松散耦合得以实现。IOC容器支持加载服务时的饿汉式初始化和懒加载。 DI依赖注入是IOC的一个方面，是个通常的概念，它有多种解释。这概念是说你不用床架对象，而只需要描述它如何被创建。你不在代码里直接组装你的组件和服务，但是要在配置文件里描述组件需要哪些服务，之后一个IOC容器辅助把他们组装起来。 IOC的注入方式：1. 构造器依赖注入；2. Setter方法注入。 如何给spring容器提供配置元数据 XML配置文件 基于注解的配置 基于Java的配置@Configuration, @Bean bean标签中的属性： id name class init-method：Bean实例化后会立刻调用的方法 destory-method:Bean从容器移除和销毁前，会调用的方法 factory-method:运行我们调用一个指定的静态方法，从而代替构造方法来创建一个类的实例。 scope：Bean的作用域，包括singleton(默认)，prototype(每次调用都创建一个实例), request,session, global-session（注意spring中的单例bean不是线程安全的） autowired:自动装配 byName, byType, constructor, autodetect(首先阐释使用constructor自动装配，如果没有发现与构造器相匹配的Bean时，Spring将尝试使用byType自动装配) beans标签中相关属性 default-init-method default-destory-method default-autowire：默认为none，应用于Spring配置文件中的所有Bean，注意这里不是指Spring应用上下文，因为你可以定义多个配置文件 Bean的生命周期 创建Bean的实例(factory-method, autowireConstrutor) 属性注入(autowireByName, autowireByType) 初始化Bean 3.1 激活Aware方法：（invokeAwaresMethods）Spring中提供了一些Aware相关接口，比如BeanNameAware, BeanFactoryAware, ApplicationContextAware等，实现这些Aware接口的bean在被初始化之后，可以取得一些相对应的资源。 private void invokeAwareMethods(final String beanName, final Object bean){ if(bean instanceof Aware) { if(bean instanceof BeanNameAware){ ((BeanNameAware) bean).setBeanName(beanName); } if(bean instanceof BeanClassLoaderAware){ ((BeanClassLoaderAware) bean).setBeanClassLoader(getBeanClassLoader()); } if(bean instanceof BeanFactoryAware){ ((BeanFactoryAware) bean).setBeanFactory(AbstactAutowire CapableBeanFactory.this); } } } 3.2 处理器的应用(BeanPostProcessor接口)：调用客户自定义初始化方法前以及调用自定义初始化方法后分别会调用BeanPostProcessor的postProcessBeforeInitialization和postProcessAfterInitialization方法，使用户可以根据自己的业务需求进行响应的处理。 3.3 激活自定义的init方法（init-method &amp; 自定义实现InitializingBean接口） protected Object initializeBean(final String beanName, final Object bean, RootBeanDefinetion mbd){ if(System.getSecurityManager() != null){ AccessController.doPrivileged(new PrivilegedAction(){ @Override public Object run() { invokeAwareMethods(beanName,bean); return null; } }); } else{ //对特殊的bean处理：Aware, BeanClassLoaderAware, BeanFactoryAware invokeAwareMethods(beanName,bean); } Object wrappedBean = bean; if(mbd == null !! !mbd.isSynthetic()){ wrappedBean = applyBeanPostProcessorsBeforeInitialization(wappedBean,beanName); } try{ invokeInitMethods(beanName, wappedBean, mbd); } catch(Throwable ex){ throw new BeanCreationException((mbd != null ? mbd.getResourceDescription():null),beanName,”Invocation of init method failed”,ex); } if(mbd == null || ！mbd.isSynthetic()){ wrappedBean = applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName); } return wappedBean; } 使用Bean。 驻留在应用的上下文中，直到该应用上下文被销毁。 销毁(destory-mthod &amp; 实现DisposableBean接口) Or represent like this: Bean的构造 调用setXXX()方法设置Bean的属性 调用BeanNameAware的setBeanName() 调用BeanFactoryAware的setBeanFactory()方法 调用BeanPostProcessor的postProcessBeforeInitialization()方法 调用InitializingBean的afterPropertiesSet()方法 调用自定义的初始化方法 调用BeanPostProcessor类的postProcessAfterInitialization()方法 调用DisposableBean的destroy()方法 调用自定义的销毁方法 Spring中注入集合 允许值相同 不允许值相同 键和值都可以为任意类型，key, key-ref, value-ref, value可以任意搭配 XXX键和值都只能是String类型 装配空值 自动装配(autowiring) 有助于减少甚至消除配置和元素，让Spring自动识别如何装配Bean的依赖关系。context:annotation-config/ 与之对应的是：自动检测(autodiscovery)，比自动装配更近了一步，让Spring能够自动识别哪些类需要被配置成SpringBean，从而减少对元素的使用。context:component-scan 注解 Spring容器默认禁用注解装配。最简单的开启方式context:annotation-config/。 Spring支持的几种不同的用于自动装配的注解： Spring自带的@Autowired注解 JSR-330的@Inject注解 JSR-250的@Resource注解 @Autowired @Autowired具有强契约特征，其所标注的属性或参数必须是可装配的。如果没有Bean可以装配到@Autowired所标注的属性或参数中，自动装配就会失败，抛出NoSuchBeanDefinitionException. 属性不一定非要装配，null值也是可以接受的。在这种场景下可以通过设置@Autowired的required属性为false来配置自动装配是可选的，如： @Autowired(required=false) private Object obj; 注意required属性可以用于@Autowired注解所使用的任意地方。但是当使用构造器装配时，只有一个构造器可以将@Autowired的required属性设置为true。其他使用@Autowired注解所标注的构造器只能将required属性设置为false。此外，当使用@Autowired标注多个构造器时，Spring就会从所有满足装配条件的构造器中选择入参最多的那个构造器。 可以使用@Qualifier明确指定要装配的Bean.如下： @Autowired @Qualifier(“objName”) private Object obj; 自定义的限定器 @Target({ElementType.FIELF, ElementType.PARAMETER, ElementType.TYPE}) @Retention(RetentionPolicy.RUNTIME) @Qualifier public @Interface SpecialQualifier{} 此时，可以通过自定义的@SpecialQualifier注解来代替@Qualifier来标注，也可以和@Autowired一起使用： @Autowired @SpecialQualifier private Object obj; 此时,Spring会把自动装配的范围缩小到被@SpecialQualifier标注的Bean中。如果被@SpecialQualifier标注的Bean有多个，我们还可以通过自定义的另一个限定器@SpecialQualifier2来进一步缩小范围。 @Autowired优缺点 Spring的@Autowired注解是减少Spring XML配置的一种方式。但是它的类会映入对Spring的特定依赖（即使依赖只是一个注解）。 @Inject 和@Autowired注解一样，@Inject可以用来自动装配属性、方法和构造器；与@Autowired不同的是，@Inject没有required属性。因此@Inject注解所标注的依赖关系必须存在，如果不存在，则会抛出异常。 @Named 相对于@Autowired对应的Qualifier，@Inject所对应的是@Named注解。 @Inject @Named(“objName”) private Object obj; SpEL表达式 语法形式在#{}中使用表达式,如： @Value @Value是一个新的装配注解，可以让我们使用注解装配String类型的值和基本类型的值，如int, boolean。我们可以通过@Value直接标注某个属性，方法或者方法参数，并传入一个String类型的表达式来装配属性，如： @Value(“Eruption”) private String song; @Value可以配合SpEL表达式一起使用，譬如有些情况下需要读取properties文件中的内容，可以使用： @Value(“#{configProperties[‘ora_driver’]}”) 详细可以参考Spring+Mybatis多数据源配置（三）——Spring如何获取Properties文件的信息 自动检测Bean context:component-scan元素除了完成与context:annotation-config一样的工作，还允许Spring自动检测Bean和定义Bean.context:component-scan元素会扫描指定的包和其所有子包，如下： &lt;context:component-scan base-package=”com.zzh.dao” /&gt; 为自动检测标注Bean 默认情况下，查找使用构造型（stereotype）注解所标注的类，这些特殊的注解如下： @Component：通用的构造型注解，标志此类为Spring组件 @Controller：标识将该类定义为SpringMVC controller @Repository：标识将该类定义为数据仓库 @Service：标识将该类定义为服务 以@Component为例： @Component public class Guitar implements Intrument{} 这里@Component会自动注册Guitar 为Spring Bean，并设置默认的Bean的Id为guitar，首字母大写变小写。注意如果第一个和第二个字母都是大写，默认的Bean的id会有特殊处理。 也可以指定Bean的Id如： @Component(“guitarOne”) public class Guitar implements Intrument{} AOP 面向切面的编程AOP，是一种编程技术，允许程序模块化横向切割关注点，或横切典型的责任划分，如日志和事务管理。 AOP的核心是切面，它将多个类的通用行为封装成可重用的模块，该模块含有一组API提供横切功能。比如，一个日志模块可以被称作日志的AOP切面。根据需求的不同，一个应用程序可以有若干切面。在SpringAOP中，切面通过带有@Aspect注解的类实现。 关注点是应用中的一个模块的行为，一个关注点可能会被定义成一个我们想实现的一个功能。 横切关注点一个关注点，此关注点是整个应用都会使用的功能，并影响整个应用，比如日志，安全和数据传输，几乎应用的每个模块都需要的功能。因此这些都属于横切关注点。 连接点代表一个应用程序的某个位置，在这个位置我们可以插入一个AOP切面，它实际上是个应用程序执行Spring AOP的位置。 切点是一个或一组连接点，通知将在这些位置执行。可以通过表达式或匹配的方式指明切入点。 引入运行我们在已存在的类中添加新的方法和属性。 AOP通知 通知是个在方法执行前后要做的动作，实际上是程序执行时要通过SpringAOP框架触发的代码 Spring切面可以应用五种类型的通知： before：前置通知，在一个方法执行前被调用。@Before after: 在方法执行之后调用的通知，无论方法执行是否成功。@After after-returning: 仅当方法成功完成后执行的通知。@AfterReturning after-throwing: 在方法抛出异常退出时执行的通知。@AfterThrowing around: 在方法执行之前和之后调用的通知。@Around Spring的事务类型 编程式事务管理：这意味你通过编程的方式管理事务，给你带来极大的灵活性，但是难维护。 声明式事务管理：这意味着你可以将业务代码和事务管理分离，你只需用注解和XML配置来管理事务。 ACID Atomic原子性：事务是由一个或多个活动所组成的一个工作单元。原子性确保事务中的所有操作全部发生或者全部不发生。 Consistent一致性：一旦事务完成，系统必须确保它所建模的业务处于一致的状态 Isolated隔离线：事务允许多个用户对象头的数据进行操作，每个用户的操作不会与其他用户纠缠在一起。 Durable持久性：一旦事务完成，事务的结果应该持久化，这样就能从任何的系统崩溃中恢复过来。 JDBC事务 如果在应用程序中直接使用JDBC来进行持久化，譬如博主采用的是Mybatis，DataSourceTransactionManager会为你处理事务边界。譬如： 1&lt;bean id&#x3D;&quot;dataSource&quot; class&#x3D;&quot;org.apache.commons.dbcp.BasicDataSource&quot; destroy-method&#x3D;&quot;close&quot;&gt; &lt;property name&#x3D;&quot;driverClassName&quot; value&#x3D;&quot;$&#123;driver&#125;&quot; &#x2F;&gt; &lt;property name&#x3D;&quot;url&quot; value&#x3D;&quot;$&#123;url&#125;&quot; &#x2F;&gt; &lt;property name&#x3D;&quot;username&quot; value&#x3D;&quot;zzh&quot; &#x2F;&gt; &lt;property name&#x3D;&quot;password&quot; value&#x3D;&quot;zzh&quot; &#x2F;&gt; &lt;property name&#x3D;&quot;validationQuery&quot; value&#x3D;&quot;SELECT 1&quot;&#x2F;&gt; &lt;&#x2F;bean&gt; &lt;bean id&#x3D;&quot;transactionManager&quot; class&#x3D;&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;property name&#x3D;&quot;dataSource&quot; ref&#x3D;&quot;dataSource&quot;&#x2F;&gt; &lt;&#x2F;bean&gt; JTA事务 如果你的事务需要跨多个事务资源（例如：两个或多个数据库；或者如Sping+ActiveMQ整合需要将ActiveMQ和数据库的事务整合起来），就需要使用JtaTransactionManager: &lt;bean id=”jtaTransactionManager”class=”org.springframework.transaction.jta.JtaTransactionManager”/&gt;JtaTransactionManager将事务管理的职责委托给了一个JTA的实现。JTA规定了应用程序与一个或多个数据源之间协调事务的标准API。transactionManagerName属性指明了要在JNDI上查找的JTA事务管理器。 \\29. 声明式事务 \\1. 隔离级别(isolation) ISOLATION_DEFAULT: 使用底层数据库预设的隔离层级 ISOLATION_READ_COMMITTED: 允许事务读取其他并行的事务已经送出（Commit）的数据字段，可以防止Dirty read问题 ISOLATION_READ_UNCOMMITTED: 允许事务读取其他并行的事务还没送出的数据，会发生Dirty、Nonrepeatable、Phantom read等问题 ISOLATION_REPEATABLE_READ: 要求多次读取的数据必须相同，除非事务本身更新数据，可防止Dirty、Nonrepeatable read问题 ISOLATION_SERIALIZABLE: 完整的隔离层级，可防止Dirty、Nonrepeatable、Phantom read等问题，会锁定对应的数据表格，因而有效率问题2. 传播行为(propagation) PROPAGATION_REQUIRED–支持当前事务，如果当前没有事务，就新建一个事务。这是最常见的选择。 PROPAGATION_SUPPORTS–支持当前事务，如果当前没有事务，就以非事务方式执行。 PROPAGATION_MANDATORY–支持当前事务，如果当前没有事务，就抛出异常。 PROPAGATION_REQUIRES_NEW–新建事务，如果当前存在事务，把当前事务挂起。 PROPAGATION_NOT_SUPPORTED–以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。 PROPAGATION_NEVER–以非事务方式执行，如果当前存在事务，则抛出异常。 PROPAGATION_NESTED–如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则进行与PROPAGATION_REQUIRED类似的操作。3. 只读(read-only) \\4. 事务超时(timeout) \\5. 回滚规则(rollback-for, no-rollback-for)：rollback-for指事务对于那些检查型异常应当回滚而不提交；no-rollback-for指事务对于那些异常应当继续运行而不回滚。默认情况下，Spring声明事务对所有的运行时异常都进行回滚。 \\6. SpringMVC 核心架构的具体流程：9. 首先用户发送请求——&gt;DispatcherServlet，前端控制器收到请求后自己不进行处理，而是委托给其他的解析器进行处理，作为统一访问点，进行全局的流程控制；10. DispatcherServlet——&gt;HandlerMapping， HandlerMapping将会把请求映射为HandlerExecutionChain对象（包含一个Handler处理器（页面控制器）对象、多个HandlerInterceptor拦截器）对象，通过这种策略模式，很容易添加新的映射策略；11. DispatcherServlet——&gt;HandlerAdapter，HandlerAdapter将会把处理器包装为适配器，从而支持多种类型的处理器，即适配器设计模式的应用，从而很容易支持很多类型的处理器；12. HandlerAdapter——&gt;处理器功能处理方法的调用，HandlerAdapter将会根据适配的结果调用真正的处理器的功能处理方法，完成功能处理；并返回一个ModelAndView对象（包含模型数据、逻辑视图名）；13. ModelAndView的逻辑视图名——&gt; ViewResolver， ViewResolver将把逻辑视图名解析为具体的View，通过这种策略模式，很容易更换其他视图技术；14. View——&gt;渲染，View会根据传进来的Model模型数据进行渲染，此处的Model实际是一个Map数据结构，因此很容易支持其他视图技术；15. 返回控制权给DispatcherServlet，由DispatcherServlet返回响应给用户，到此一个流程结束。16. DispatcherServlet viewspace org.springframework.web.servlet.DispatcherServlet 2 接下来，必须申明DispatcherServlet处理那些URL： 通过将DispatcherServlet映射到/，声明了它会作为默认的servlet并且会处理所有的请求，包括对静态资源的请求。 &lt;mvc:resources mapping=”/images/“ location=”/js/“ cache-period=”31556926” /&gt; &lt;mvc:resources mapping=”/css/**” location=”/css/“ cache-period=”31556926” /&gt; \\32. 配置HandlerMapping &lt;mvc:annotation-driven /&gt; 在构建控制器的时候，我们还需要使用注解将请求参数绑定到控制器的方法参数上进行校验以及信息转换。提供注解驱动的特性。 配置HandlerAdapter 配置视图 在SpringMVC中大量使用了约定优于配置的开发模式。InternalResourceViewResolver就是一个面向约定的元素。它将逻辑视图名称解析为View对象，而该对象将渲染的任务委托给Web应用程序上下文中的一个模板。 1&lt;!-- 配置视图解析器，将ModelAndView及字符串解析为具体的页面 --&gt; &lt;bean class&#x3D;&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt; &lt;property name&#x3D;&quot;viewClass&quot; value&#x3D;&quot;org.springframework.web.servlet.view.JstlView&quot; &#x2F;&gt; &lt;property name&#x3D;&quot;prefix&quot; value&#x3D;&quot;&#x2F;WEB-INF&#x2F;jsp&#x2F;&quot; &#x2F;&gt; &lt;property name&#x3D;&quot;suffix&quot; value&#x3D;&quot;.jsp&quot; &#x2F;&gt; &lt;&#x2F;bean&gt; 当DispatcherServlet要求InternalResourceViewResolver解析视图的时候，它将获取一个逻辑视图名称，添加”/WEB-INF/jsp/”前缀和”.jsp”后缀。等待的结果就是渲染输出的JSP路径。在内部，InternalResourceViewResolver接下来会将这个路径传递给View对象，View对象将请求传递给JSP. 持续更新~ 请保留转载连接~参考文献 《Sping In Action》 Craig Walls. 69道Spring面试题和答案 Sping+ActiveMQ整合欢迎支持笔者新书：《RabbitMQ实战指南》以及关注微信公众号：Kafka技术专栏。 这里写图片描述","categories":[],"tags":[{"name":"spring","slug":"spring","permalink":"https://imlike.cc/tags/spring/"}]},{"title":"最近折腾Ubuntu16的过程记录","slug":"最近折腾Ubuntu16的过程记录","date":"2019-03-24T00:10:04.000Z","updated":"2024-03-14T14:55:29.484Z","comments":true,"path":"3642578636.html","link":"","permalink":"https://imlike.cc/3642578636.html","excerpt":"","text":"上周到了一台戴尔的xps13，考虑win10喜欢频繁升级和对高分辨的显示不算好的支持，所以觉得折腾一下Ubuntu，其实很久之前就折腾过也尝试过将它设为主力系统，其实当时陆陆续续已经出现时网易云音乐和WPS已经类似于IDM的uget 当时还是不算好用，同时当时没怎么涉及到开发，现在算是比较合适与我开发。先说一下安装的情况，刚开始由于没有课BIOS硬盘的acid的设置导致，启动盘不能识别硬盘不能继续安装，后面是一安装重启就出现,在grub引导后都没有出现登录见面，当时就很慌 ，以为是安装过程中BIOS到处出现安装出的问题吧BIOS全部折腾几遍，又安装又TMD的是黑屏–晚上看了看都是登录时后面出现黑屏，是NVIDIA和Intel的集成显卡的问题，我照着网上先是尝试在grub前面编辑一下 123456789101112131415161718192021启动黑屏在启动Ubuntu的时候出现黑屏的情况，是因为升级了内核导致显卡不兼容，启动的时候应该告诉内核不要加载显卡：在引导界面，按e进入grub编辑界面，在quiet splash后面添加nomodeset按F10接着引导启动进入登录界面，输入用户名和密码发现闪一下又跳回登录界面 先不用着急解决登录界面循环的问题，前面的grub配置文件的修改只是暂时的，现在需要永久性的修改 Ctrl+Alt+（F1-F6）进入命令行tty界面，登录之后，sudo vi &#x2F;etc&#x2F;default&#x2F;grub1找到quiet splash并在后面添加nomodeset。然后输入sudo update-grub1更新了grub配置文件后，重启即可。 对于在tty命令行界面中文乱码的问题可以参考博客：http:&#x2F;&#x2F;blog.csdn.net&#x2F;VictoriaW&#x2F;article&#x2F;details&#x2F;56484552--------------------- 作者：CodeTutor 来源：CSDN 原文：https:&#x2F;&#x2F;blog.csdn.net&#x2F;VictoriaW&#x2F;article&#x2F;details&#x2F;56484119 版权声明：本文为博主原创文章，转载请附上博文链接！ 没想到不行当时就检查一遍又一遍看，又在安装一遍然后设置不安装显卡的驱动，避开还是不行搞到两点没看到头绪第二天我先可能是我这边是我安装引导文件的问题，然后选择try ubuntu 进入系统安装引导修复文件Boot-Repair. 试了一遍也不行真的是心灰意冷了，最后将启动盘工具universal usb installer这个工具换成软碟通，搞完后就成功了，就是这个工具的问题","categories":[],"tags":[{"name":"ubuntu","slug":"ubuntu","permalink":"https://imlike.cc/tags/ubuntu/"}]},{"title":"ES相关的知识总结一","slug":"ES相关的知识总结一","date":"2019-03-23T18:59:20.000Z","updated":"2024-03-14T14:55:29.473Z","comments":true,"path":"3632233998.html","link":"","permalink":"https://imlike.cc/3632233998.html","excerpt":"","text":"1）type Elasticsearch中支持的数据类型非常丰富： String类型，又分两种： text：可分词，不可参与聚合 keyword：不可分词，数据会作为完整字段进行匹配，可以参与聚合 Numerical：数值类型，分两类 基本数据类型：long、interger、short、byte、double、float、half_float 浮点数的高精度类型：scaled_float 需要指定一个精度因子，比如10或100。elasticsearch会把真实值乘以这个因子后存储，取出时再还原。 Date：日期类型elasticsearch可以对日期格式化为字符串存储，但是建议我们存储为毫秒值，存储为long，节省空间。 2）index index影响字段的索引情况。 true：字段会被索引，则可以用来进行搜索。默认值就是true false：字段不会被索引，不能用来搜索 index的默认值就是true，也就是说你不进行任何配置，所有字段都会被索引。 但是有些字段是我们不希望被索引的，比如商品的图片信息，就需要手动设置index为false。 3）store 是否将数据进行额外存储。_source 在学习lucene和solr时，我们知道如果一个字段的store设置为false，那么在文档列表中就不会有这个字段的值，用户的搜索结果中不会显示出来。 但是在Elasticsearch中，即便store设置为false，也可以搜索到结果。 原因是Elasticsearch在创建文档索引时，会将文档中的原始数据备份，保存到一个叫做_source的属性中。最终我们查询数据是从_source中来取值，所以可以通过过滤_source来选择哪些要显示，哪些不显示。 而如果设置store为true，就会在_source以外额外存储一份数据，多余，因此一般我们都会将store设置为false，事实上，store的默认值就是false。 4）boost 激励因子，这个与lucene中一样 其它的不再一一讲解，用的不多，大家参考官方文档： 还有查询的更多东西(mathch all) 12345678910111213141516171819202122232425262728293031323334353637383940414243GET &#x2F;heima&#x2F;_search&#123;​ “query”:&#123;​ “match_all”: &#123;&#125;​ &#125;&#125;匹配查询(match)GET &#x2F;heima&#x2F;_search&#123;​ “query”:&#123;​ “match”:&#123;​ “title”:”小米电视”​ &#125;​ &#125;&#125;多字段查询(multi_match)GET &#x2F;heima&#x2F;_search&#123;​ “query”:&#123;​ “multi_match”: &#123;​ “query”: “小米”,​ “fields”: [ “title”, “subTitle” ]​ &#125;​ &#125;&#125;词条匹配(term)GET &#x2F;heima&#x2F;_search&#123;​ “query”:&#123;​ “term”:&#123;​ “price”:2699.00​ &#125;​ &#125;&#125;多词条精确匹配(terms)GET &#x2F;heima&#x2F;_search&#123;​ “query”:&#123;​ “terms”:&#123;​ “price”:[2699.00,2899.00,3899.00]​ &#125;​ &#125;&#125; 结果过滤 默认情况下，elasticsearch在搜索的结果中，会把文档中保存在_source的所有字段都返回。 如果我们只想获取其中的部分字段，我们可以添加_source的过滤 123456789GET &#x2F;heima&#x2F;_search&#123;“_source”: [“title”,”price”],&#x3D;&#x3D;&#x3D;&#x3D;&gt;includi exclud“query”: &#123;​ “term”: &#123;​ “price”: 2699​ &#125;&#125;&#125;","categories":[],"tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"https://imlike.cc/tags/elasticsearch/"}]},{"title":"ES相关的知识总结二","slug":"ES相关的知识总结二","date":"2019-03-23T18:59:20.000Z","updated":"2024-03-14T14:55:29.473Z","comments":true,"path":"3632233997.html","link":"","permalink":"https://imlike.cc/3632233997.html","excerpt":"","text":"聚合aggregations 聚合可以让我们极其方便的实现对数据的统计、分析。例如：Elasticsearch中的聚合，包含多种类型，最常用的两种，一个叫桶，一个叫度量： 桶（bucket）,阿里文档称之为存储单元 桶的作用，是按照某种方式对数据进行分组，每一组数据在ES中称为一个桶， 例如我们根据国籍对人划分，可以得到中国桶、英国桶，日本桶……或者我们按照年龄段对人进行划分：010,1020,2030,3040等。 Elasticsearch中提供的划分桶的方式有很多： Date Histogram Aggregation：根据日期阶梯分组，例如给定阶梯为周，会自动每周分为一个存储单元 Histogram Aggregation：根据数值阶梯分组，与日期类似 Terms Aggregation：根据词条内容分组，词条内容完全匹配的为一个存储单元 Range Aggregation：数值和日期的范围分组，指定开始和结束，然后按段分 …… 综上所述，我们发现bucket aggregations 只负责对数据进行分组，并不进行计算，因此往往bucket中往往会嵌套另一种聚合：metrics aggregations即度量 度量（metrics） 分组完成以后，我们一般会对组中的数据进行聚合运算，例如求平均值、最大、最小、求和等，这些在ES中称为度量 比较常用的一些聚合度量方式： Avg Aggregation：求平均值 Max Aggregation：求最大值 Min Aggregation：求最小值 Percentiles Aggregation：求百分比 Stats Aggregation：同时返回avg、max、min、sum、count等 Sum Aggregation：求和 Top hits Aggregation：求前几 Value Count Aggregation：求总数 聚合为桶GET /cars/_search{​ “size” : 10,​ “aggs” : {​ “popular_colors” : {​ “terms” : {​ “field” : “color”​ }​ }​ }} hits：查询结果为空，因为我们设置了size为0 aggregations：聚合的结果 popular_colors：我们定义的聚合名称 buckets：查找到的桶，每个不同的color字段值都会形成一个桶 key：这个桶对应的color字段的值 doc_count：这个桶中的文档数量","categories":[],"tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"https://imlike.cc/tags/elasticsearch/"}]},{"title":"Spring定时任务","slug":"Spring定时任务","date":"2019-03-21T05:18:34.000Z","updated":"2024-03-14T14:55:29.474Z","comments":true,"path":"1075226630.html","link":"","permalink":"https://imlike.cc/1075226630.html","excerpt":"","text":"123456789101112131415161718 &lt;bean id&#x3D;&quot;schedulerJobDetail&quot; class&#x3D;&quot;org.springframework.scheduling.quartz.MethodInvokingJobDetailFactoryBean&quot;&gt; &lt;property name&#x3D;&quot;targetObject&quot; ref&#x3D;&quot;scheduleInfoAction&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;targetMethod&quot; value&#x3D;&quot;simpleJobTest&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;concurrent&quot; value&#x3D;&quot;false&quot;&#x2F;&gt; &lt;&#x2F;bean&gt; &lt;bean id&#x3D;&quot;cronTrigger&quot; class&#x3D;&quot;org.springframework.scheduling.quartz.CronTriggerBean&quot; &gt; &lt;property name&#x3D;&quot;jobDetail&quot; ref&#x3D;&quot;schedulerJobDetail&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;cronExpression&quot;&gt; &lt;value&gt;0&#x2F;10 * * * * ?&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;&#x2F;bean&gt; &lt;bean id&#x3D;&quot;schedulerFactory&quot; class&#x3D;&quot;org.springframework.scheduling.quartz.SchedulerFactoryBean&quot;&gt; &lt;property name&#x3D;&quot;triggers&quot;&gt; &lt;list&gt; &lt;ref local&#x3D;&quot;cronTrigger&quot;&#x2F;&gt; &lt;&#x2F;list&gt; &lt;&#x2F;property&gt;&lt;&#x2F;bean&gt; 链接 1234567891011定时任务每隔5秒执行一次：*&#x2F;5 * * * * ? 每隔1分钟执行一次：0 *&#x2F;1 * * * ? 每天23点执行一次：0 0 23 * * ? 每天凌晨1点执行一次：0 0 1 * * ? 每月1号凌晨1点执行一次：0 0 1 1 * ? 每月最后一天23点执行一次：0 0 23 L * ? 每周星期天凌晨1点实行一次：0 0 1 ? * L 在26分、29分、33分执行一次：0 26,29,33 * * * ? 每天的0点、13点、18点、21点都执行一次：0 0 0,13,18,21 * * ? 参考链接 特殊字符 意义 * 表示所有值； ? 表示未说明的值，即不关心它为何值； - 表示一个指定的范围； , 表示附加一个可能值； / 符号前表示开始时间，符号后表示每次递增的值； L(“last”) (“last”) “L” 用在day-of-month字段意思是 “这个月最后一天”；用在 day-of-week字段, 它简单意思是 “7” or “SAT”。如果在day-of-week字段里和数字联合使用，它的意思就是 “这个月的最后一个星期几” – 例如： “6L” means “这个月的最后一个星期五”. 当我们用“L”时，不指明一个列表值或者范围是很重要的，不然的话，我们会得到一些意想不到的结果。 W(“weekday”) 只能用在day-of-month字段。用来描叙最接近指定天的工作日（周一到周五）。例如：在day-of-month字段用“15W”指“最接近这个月第15天的工作日”，即如果这个月第15天是周六，那么触发器将会在这个月第14天即周五触发；如果这个月第15天是周日，那么触发器将会在这个月第16 天即周一触发；如果这个月第15天是周二，那么就在触发器这天触发。注意一点：这个用法只会在当前月计算值，不会越过当前月。“W”字符仅能在day- of-month指明一天，不能是一个范围或列表。也可以用“LW”来指定这个月的最后一个工作日。 # 只能用在day-of-week字段。用来指定这个月的第几个周几。例：在day-of-week字段用”6#3”指这个月第3个周五（6指周五，3指第3个）。如果指定的日期不存在，触发器就不会触发。 C 指和calendar联系后计算过的值。例：在day-of-month 字段用“5C”指在这个月第5天或之后包括calendar的第一天；在day-of-week字段用“1C”指在这周日或之后包括calendar的第一天。","categories":[],"tags":[{"name":"spring","slug":"spring","permalink":"https://imlike.cc/tags/spring/"}]},{"title":"关于泛型和Java的伪泛型","slug":"关于泛型和Java的伪泛型","date":"2018-03-12T04:39:19.000Z","updated":"2024-03-14T14:55:29.483Z","comments":true,"path":"4d077bfe.html","link":"","permalink":"https://imlike.cc/4d077bfe.html","excerpt":"","text":"范型 就是在集合中为了将本来是object的数据进行限定但其实在编译以后还是object还有范型通配符多态 为什么有多态和实现而多态却只能多重继承二实现却可以多实现并且多线程还有正则表达式还有设计对象还有包括 预习数据库异常 数组 多线程 （多线程安全包括同步锁） 高内聚底耦合高内聚底耦合 单继承多实现高内聚第耦合 运行时的时间 但继承多实现面试中问到的单例设计模式（单例模式，工厂模式，装饰着模式）SeriablizableComparableCharsequence final修改关于设计模式和 1)instanceof:实现了某接口的类的对象也是该接口的实例；某类的对象也是该类超类的实例。 2)native()修饰的方法没有方法体，用一个分号”；”代替大括号。 3)main不是关键字。 4)System.exit(int value),也就是说，只要是int的，就能够使JVM退出System.exit(‘a’);同样的是合法的。 5)&amp;,|能用在int和boolean上，而^（异或）只能用在int上。 6）如果一个类和一个接口被定义在同一文件中，而且类被声明为public，则接口前什么都不能写，因为接口只能用public来修饰，而一个文件中又只能有一个public访问控制关键字。 7）unreachable statement 下面的几种情况会出现unreachable statement （1）在reutrn语句后写语句。 （2）在throw语句后写语句。 （3）break、continue语句之后定义语句。8）“\\u10100”//合法，相当于‘\\u1010’和字符串“0” 9)移位运算符可以用于long int char short byte 10)类的访问控制符可以是public或什么都不加. 11)goto是保留字但不是关键字.then什么都不是. 12)把超类的类型强制转换后赋给子类的对象时,编译无异常,但运行时会出现异常Value值 field为属性","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://imlike.cc/tags/java/"}]}],"categories":[],"tags":[{"name":"sublime","slug":"sublime","permalink":"https://imlike.cc/tags/sublime/"},{"name":"flyio","slug":"flyio","permalink":"https://imlike.cc/tags/flyio/"},{"name":"alist","slug":"alist","permalink":"https://imlike.cc/tags/alist/"},{"name":"git","slug":"git","permalink":"https://imlike.cc/tags/git/"},{"name":"gitlab","slug":"gitlab","permalink":"https://imlike.cc/tags/gitlab/"},{"name":"github","slug":"github","permalink":"https://imlike.cc/tags/github/"},{"name":"加速","slug":"加速","permalink":"https://imlike.cc/tags/%E5%8A%A0%E9%80%9F/"},{"name":"转载","slug":"转载","permalink":"https://imlike.cc/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"java","slug":"java","permalink":"https://imlike.cc/tags/java/"},{"name":"linux","slug":"linux","permalink":"https://imlike.cc/tags/linux/"},{"name":"gdal","slug":"gdal","permalink":"https://imlike.cc/tags/gdal/"},{"name":"windows","slug":"windows","permalink":"https://imlike.cc/tags/windows/"},{"name":"repost","slug":"repost","permalink":"https://imlike.cc/tags/repost/"},{"name":"maven","slug":"maven","permalink":"https://imlike.cc/tags/maven/"},{"name":"Java","slug":"Java","permalink":"https://imlike.cc/tags/Java/"},{"name":"nginx","slug":"nginx","permalink":"https://imlike.cc/tags/nginx/"},{"name":"spotify","slug":"spotify","permalink":"https://imlike.cc/tags/spotify/"},{"name":"no_ad","slug":"no-ad","permalink":"https://imlike.cc/tags/no-ad/"},{"name":"docker","slug":"docker","permalink":"https://imlike.cc/tags/docker/"},{"name":"视频流","slug":"视频流","permalink":"https://imlike.cc/tags/%E8%A7%86%E9%A2%91%E6%B5%81/"},{"name":"kubernetes","slug":"kubernetes","permalink":"https://imlike.cc/tags/kubernetes/"},{"name":"ceph","slug":"ceph","permalink":"https://imlike.cc/tags/ceph/"},{"name":"kuboard","slug":"kuboard","permalink":"https://imlike.cc/tags/kuboard/"},{"name":"container","slug":"container","permalink":"https://imlike.cc/tags/container/"},{"name":"k8s","slug":"k8s","permalink":"https://imlike.cc/tags/k8s/"},{"name":"ubuntu","slug":"ubuntu","permalink":"https://imlike.cc/tags/ubuntu/"},{"name":"jdk","slug":"jdk","permalink":"https://imlike.cc/tags/jdk/"},{"name":"binary","slug":"binary","permalink":"https://imlike.cc/tags/binary/"},{"name":"树莓派","slug":"树莓派","permalink":"https://imlike.cc/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/"},{"name":"cloud","slug":"cloud","permalink":"https://imlike.cc/tags/cloud/"},{"name":"nas","slug":"nas","permalink":"https://imlike.cc/tags/nas/"},{"name":"clickhouse","slug":"clickhouse","permalink":"https://imlike.cc/tags/clickhouse/"},{"name":"jenkins","slug":"jenkins","permalink":"https://imlike.cc/tags/jenkins/"},{"name":"知识管理","slug":"知识管理","permalink":"https://imlike.cc/tags/%E7%9F%A5%E8%AF%86%E7%AE%A1%E7%90%86/"},{"name":"thunderbolt","slug":"thunderbolt","permalink":"https://imlike.cc/tags/thunderbolt/"},{"name":"搬家","slug":"搬家","permalink":"https://imlike.cc/tags/%E6%90%AC%E5%AE%B6/"},{"name":"activiti","slug":"activiti","permalink":"https://imlike.cc/tags/activiti/"},{"name":"软件配置","slug":"软件配置","permalink":"https://imlike.cc/tags/%E8%BD%AF%E4%BB%B6%E9%85%8D%E7%BD%AE/"},{"name":"网站","slug":"网站","permalink":"https://imlike.cc/tags/%E7%BD%91%E7%AB%99/"},{"name":"推荐","slug":"推荐","permalink":"https://imlike.cc/tags/%E6%8E%A8%E8%8D%90/"},{"name":"开发","slug":"开发","permalink":"https://imlike.cc/tags/%E5%BC%80%E5%8F%91/"},{"name":"天翼云","slug":"天翼云","permalink":"https://imlike.cc/tags/%E5%A4%A9%E7%BF%BC%E4%BA%91/"},{"name":"vue","slug":"vue","permalink":"https://imlike.cc/tags/vue/"},{"name":"axios","slug":"axios","permalink":"https://imlike.cc/tags/axios/"},{"name":"hexo","slug":"hexo","permalink":"https://imlike.cc/tags/hexo/"},{"name":"Yelee","slug":"Yelee","permalink":"https://imlike.cc/tags/Yelee/"},{"name":"Yilia","slug":"Yilia","permalink":"https://imlike.cc/tags/Yilia/"},{"name":"hexo主题","slug":"hexo主题","permalink":"https://imlike.cc/tags/hexo%E4%B8%BB%E9%A2%98/"},{"name":"mail","slug":"mail","permalink":"https://imlike.cc/tags/mail/"},{"name":"yandex","slug":"yandex","permalink":"https://imlike.cc/tags/yandex/"},{"name":"域名","slug":"域名","permalink":"https://imlike.cc/tags/%E5%9F%9F%E5%90%8D/"},{"name":"邮件服务","slug":"邮件服务","permalink":"https://imlike.cc/tags/%E9%82%AE%E4%BB%B6%E6%9C%8D%E5%8A%A1/"},{"name":"内网穿透","slug":"内网穿透","permalink":"https://imlike.cc/tags/%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/"},{"name":"Linux","slug":"Linux","permalink":"https://imlike.cc/tags/Linux/"},{"name":"vscode","slug":"vscode","permalink":"https://imlike.cc/tags/vscode/"},{"name":"npm","slug":"npm","permalink":"https://imlike.cc/tags/npm/"},{"name":"spring","slug":"spring","permalink":"https://imlike.cc/tags/spring/"},{"name":"vmware","slug":"vmware","permalink":"https://imlike.cc/tags/vmware/"},{"name":"vpn","slug":"vpn","permalink":"https://imlike.cc/tags/vpn/"},{"name":"onedrive","slug":"onedrive","permalink":"https://imlike.cc/tags/onedrive/"},{"name":"腾讯云","slug":"腾讯云","permalink":"https://imlike.cc/tags/%E8%85%BE%E8%AE%AF%E4%BA%91/"},{"name":"gnome","slug":"gnome","permalink":"https://imlike.cc/tags/gnome/"},{"name":"postgres","slug":"postgres","permalink":"https://imlike.cc/tags/postgres/"},{"name":"webdav","slug":"webdav","permalink":"https://imlike.cc/tags/webdav/"},{"name":"坚果云","slug":"坚果云","permalink":"https://imlike.cc/tags/%E5%9D%9A%E6%9E%9C%E4%BA%91/"},{"name":"post","slug":"post","permalink":"https://imlike.cc/tags/post/"},{"name":"put","slug":"put","permalink":"https://imlike.cc/tags/put/"},{"name":"资源","slug":"资源","permalink":"https://imlike.cc/tags/%E8%B5%84%E6%BA%90/"},{"name":"云播","slug":"云播","permalink":"https://imlike.cc/tags/%E4%BA%91%E6%92%AD/"},{"name":"orientDb","slug":"orientDb","permalink":"https://imlike.cc/tags/orientDb/"},{"name":"搜狗输入法","slug":"搜狗输入法","permalink":"https://imlike.cc/tags/%E6%90%9C%E7%8B%97%E8%BE%93%E5%85%A5%E6%B3%95/"},{"name":"deepin","slug":"deepin","permalink":"https://imlike.cc/tags/deepin/"},{"name":"php","slug":"php","permalink":"https://imlike.cc/tags/php/"},{"name":"zsh","slug":"zsh","permalink":"https://imlike.cc/tags/zsh/"},{"name":"intellj","slug":"intellj","permalink":"https://imlike.cc/tags/intellj/"},{"name":"wine","slug":"wine","permalink":"https://imlike.cc/tags/wine/"},{"name":"es6","slug":"es6","permalink":"https://imlike.cc/tags/es6/"},{"name":"springcloud","slug":"springcloud","permalink":"https://imlike.cc/tags/springcloud/"},{"name":"软件","slug":"软件","permalink":"https://imlike.cc/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"kibana","slug":"kibana","permalink":"https://imlike.cc/tags/kibana/"},{"name":"elasticsraech","slug":"elasticsraech","permalink":"https://imlike.cc/tags/elasticsraech/"},{"name":"springboot","slug":"springboot","permalink":"https://imlike.cc/tags/springboot/"},{"name":"vue-cli","slug":"vue-cli","permalink":"https://imlike.cc/tags/vue-cli/"},{"name":"elasticsearch","slug":"elasticsearch","permalink":"https://imlike.cc/tags/elasticsearch/"}]}